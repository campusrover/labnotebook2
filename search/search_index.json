{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Lab Notebook","text":""},{"location":"#brandeis-univerity-robotics-lab","title":"Brandeis Univerity Robotics Lab","text":"<p>This repository represents the collective learnings of the Brandeis robotics lab. As a general rule we try to collect any information or learning about ROS, robotics hardware, software, etc. </p> <p>We have information about a cluster, about a robots, about many of the projects that have been conducted by students, and also lots and lots of knowledge base articles about various topics.</p>"},{"location":"SUMMARY/","title":"SUMMARY","text":"<ul> <li>Home</li> <li>FAQs<ul> <li>faq/*</li> </ul> </li> <li>Cosi119 Projects<ul> <li>reports/*</li> </ul> </li> <li>Cheat Sheets<ul> <li>cheatsheets/*.md</li> </ul> </li> <li>BRU<ul> <li>bru/*.md</li> </ul> </li> <li>Networking<ul> <li>networking/*</li> </ul> </li> <li>Lab Robots</li> <li>Cluster<ul> <li>cluster/*</li> </ul> </li> <li>Campus Rover Package<ul> <li>crpackage/*</li> </ul> </li> <li>Our Software</li> </ul>"},{"location":"home/","title":"Lab Notebook!","text":"<p>Welcome! This is our Lab Notebook. Over the 6 years that we have been doing Robotics at Brandeis, we all have been maintaining this material. It is more than a Lab Notebook. It contains an infinite (almost) amount of information, with contributions especially from students but also from the instructors, teaching assistants, research assistants and others. As such it has grown organically and there is no doubt old informationn here.</p>"},{"location":"home/#contributing","title":"Contributing","text":"<ol> <li>Git clone this repository to your computer: campusrover/labnotebook2</li> <li>Find an appropriate folder to put your contribution. There are many places, and if you can't decide you can also create a new folder.</li> <li>Write your contribution using markdown. You can include images as well as links. There are many tutorials on Markdown but you most likely already know it.</li> <li>Add the a header like this one before the start of the markdown:</li> <li>Git add, commit and push your changes.</li> </ol> <pre><code>---\ntitle: Gooder Title\nauthor: Pito Salas\ndescription: nicer subtitle\nstatus: obsolete|new|tophit|dated|current\ncreated: 12-mar-2023\nmodified: 15-mar-2024\nversion: 1\n---\n</code></pre>"},{"location":"bru/commands/","title":"BRU commands","text":"<p>bru is a simple command line tool (currently implemented as a bash script.) All it really does is help you properly set up the required ROS environment variables.</p> <p>Here are the commands:</p> <pre><code>env     Display all relevant Environment variables\nexport  Display bash commands to export state\nmode    Set running modes\nname    Set name of Robot\nrobot   Control the attached robot remotely\nstatus  Display current Bru sttings\n</code></pre>"},{"location":"bru/commands/#bashrc-set-up","title":".bashrc set up","text":"<p>The most important part of BRU is the set up in the .bashrc script on computers, robots and vms. They all get it.</p> <pre><code># Support for bru mode setter\nexport BRU_MY_IP=$(myip)\nexport BRU_VPN_IP=$(myvpnip)\n\n# Setting for simulation mode\n# $(bru mode sim)\n# $(bru name roba -m $(myvpnip))\n\n# Settings for a physical robot \n# $(bru mode real)\n# $(bru name platform1 -m 100.107.118.103)\n</code></pre>"},{"location":"bru/concepts/","title":"BRU Concepts","text":""},{"location":"bru/concepts/#types-of-robots","title":"Types of robots","text":"<p>BRU knows about types of robots. Currently:</p> <ol> <li>platform</li> <li>bullet</li> <li>minirover</li> <li>cat</li> </ol> <p>Bru knows about all the instances of those types, for example, that platform2 is a specific robot of category platform. This knowledge is encoded in the bru.py script. This script is modified when creating a new type or instance of robot.</p>"},{"location":"bru/concepts/#our-software-stack-for-all-robots-and-vvms","title":"Our software stack for all robots and vVMs","text":"<p>Currently gpg_bran4 is the repo containing all the custom ROS bits and pieces to make our robots go. There you will find, among other things, a directory for each BRU robot type. Each of those directories is structures as a standard ROS package, including in particular, a launch subdirectory.</p>"},{"location":"bru/concepts/#how-each-robot-type-is-set-up-in-bru","title":"How each robot type is set up in BRU","text":"<p>As mentioned, under <code>~/catkin_ws/src/gpg_bran4</code> there are standard ROS package directories named after each of the robot types. For example, <code>~/catkin_ws/src/gpg_bran4/platform</code>. The most important subdirectory below that would be the launch directory, so for example <code>~/catkin_ws/src/gpg_bran4/platform/launch</code>. BRU defines a set of standard launch file names that should exist, and do the same thing, for each type of robot</p> <p>NB: This is a work in progress. Treat this description more as a spec than as a description of reality. We are working to this goal</p> <ul> <li><code>bringup_mini.launch</code> - Will launch the minimum nodes to allow the robot to move and have odometry.</li> <li><code>bringup_full.launch</code> - Will launch the full set of nodes, for motion, odometry, lidar, and camera`</li> </ul>"},{"location":"bru/concepts/#robot-and-remote-computer","title":"Robot and \"remote computer\"","text":"<p>A key idea in ROS is that it is a distributed operating system. Separate computers, who can access each other over the network, together form a ROS system. ROS <code>nodes</code> are run on each of the computers, and they cooperate with each other. There is a key node called <code>roscore</code> which acts essentially as the traffic cop between them.</p> <p>ROS nodes are always running at least on \"the robot\". But there can be other computers involved. In the Brandeis scenario, typically, the remote computer is a VM running on our cluster. And so you will have ROS nodes on the robot and additional ros nodes on the \"remote computer\" - the VM.</p> <p>The coordination relies on the network and in particular that each side knows about the otjers' IP addresses. That is where BRU comes in.</p>"},{"location":"bru/connect-to-robot/","title":"Connecting to Turtlebot3 or Plaform Robots from VNC","text":"<p>Before you get started, make sure you have the latest version of <code>rosutils</code> on your VNC To update rosutils, use the command <code>cd ~/rosutils &amp;&amp; git pull</code></p> <p>You will then need to run the command <code>source ~/.bashrc</code> after updating rosutils</p>"},{"location":"bru/connect-to-robot/#turning-on-the-robot-turtlebot3","title":"Turning on the robot (Turtlebot3)","text":"<ol> <li> <p>Make note of the name of your robot written on the top.  </p> </li> <li> <p>Connect the battery to the robot. </p> </li> <li> <p>Turn on the robot using the switch just under the camera. </p> </li> <li> <p>You should see the lights on the raspberry pi in the robot turn green. The robot may take up to a minute to finish its boot sequence.</p> </li> </ol>"},{"location":"bru/connect-to-robot/#steps-to-connect","title":"Steps to connect","text":"<ol> <li>Make note of the name of your robot. It should be written on the top of the robot. For the purposes of this tutorial, I will be connecting to roba, but you should replace roba with the name of your robot.</li> <li>Run <code>tailscale status | grep roba</code> in a terminal. You should get an output that looks something like this: <code>100.86.78.102   roba               pitosalas@   linux   -</code>. Make note of the IP starting with 100, this is the IP address of your robot.</li> <li>Edit the file <code>~/.bashrc</code> in your vscode editor on the VNC. Add the following two lines to the bottom of the file, but in the second line you will need to replace where it says <code>roba</code> and the IP address with the name of your robot and your robot's IP address respectively. <pre><code>$(bru mode real)\n$(bru name roba -m 100.86.78.102)\n</code></pre></li> <li>Run the command <code>sb</code> in the terminal. You should see your command prompt change to something that looks like this: <code>[real:roba]</code></li> <li>If the robot is already on, run the command <code>sshrobot</code> in your terminal in your VNC. You might be asked to add a new ssh fingerprint, if so simply type <code>yes</code> in the prompt and continue. You will be asked the password of the robot, which is going to be <code>ROSlab134</code> for all of the lab robots.</li> <li>Once you see that your terminal prompt looks like <code>[onboard:roba]</code> you are connected to the robot. The final step is to type <code>bringup</code> in the robot's terminal. This will run the bringup script so it is crucial that you do not close the terminal where this is running.</li> </ol>"},{"location":"bru/home/","title":"BRU - Brandeis Robotics Utilities","text":""},{"location":"bru/home/#what-it-is","title":"What it is","text":"<p>First of all, BRU is not a finished product, it is constantly under development. You should expect inconsistencies and incompleteness. You are invited to fix and improve it. BRU embodies a series of python scripts, bash shells, and conventions, to make it more convenient to operate robots in the Brandeis Robotics Lab. We have many different robots that are all similiar but a little different. BRU attempts to unify and organize things in a sane manner.</p>"},{"location":"bru/home/#what-is-the-problem-bru-solves","title":"What is the problem BRU solves?","text":"<p>While all our robots run ROS (so far, just ROS1) they are all a little different. They use different SBC (single board computers) and different controllers. They have different peripherals. Also students use the robots either from their own laptop but more commonly from a virtual machine in our cluster environment. ROS has very partoicular requirements about IP addresses and environment variables. All together this can become very confusing.</p>"},{"location":"bru/rosutils/","title":"ROSUTILS - Standard directory","text":""},{"location":"bru/rosutils/#intro","title":"Intro","text":"<p>All the key scripts for bru are in a standard github repo, rosutils. You will find the following scripts:</p> <ul> <li><code>bru.py</code> - the main script as described</li> <li><code>common_alias.bash</code> - the standard collection of aliases for our robots andVMs.</li> <li><code>pi_connect.sh</code> - the script for setting up a robot or VM with tailscale</li> <li><code>bashrc_template.bash</code> - A template for the bashrc script for all Robots or VMs that are running BRU</li> </ul>"},{"location":"bru/rosutils/#process-for-setting-up-a-new-robot-or-vm","title":"Process for setting up a new robot or VM","text":"<p>Note that in general we set up a new robot by copying the MicroSD from a similar robot. This is here for completeness. Here are the steps:</p> <ol> <li>Follow the Robot vendor instructions to set it up and set up ROS. All libraries, environment variables and so on should be set up in a standard way</li> <li>Clone rosutils to ~ on the robot</li> <li>Clone gpg_bran4 to ~/catkin_ws/src</li> <li>Create a symbolic link from ~/rosutils/bru.py to ~/bin and chmod +x that file (details may be different.)</li> </ol>"},{"location":"bru/script/","title":"bru.py - the script","text":""},{"location":"bru/script/#installation","title":"Installation","text":"<p>The bru.py script can be found in the rosutils repository. All our robots and VMs have this directory. However on a new robot, we create a symbolic link accessible to the path that is called just <code>bru</code> so the commands are <code>bru help</code> etc.</p>"},{"location":"bru/script/#configuration","title":"Configuration","text":"<p>Whenever a new robot or robot type is created, certain key lines of bru.py need to be updated. Also of course, as we develop or fix bugs in the BRU package, that code will need to ne revised from time to time.</p>"},{"location":"bru/script/#installation_1","title":"Installation","text":"<p>In order to make it convenient to use BRU as a cli:</p> <ol> <li>Create a symbolic link to the bru.py package.</li> </ol> <p><code>ln -s /my_ros_data/rosutils/bru.py /usr/local/bin/bru</code></p> <ol> <li>Make it executable</li> </ol> <p><code>chmod +x /my_ros_data/rosutils/bru.py</code></p>"},{"location":"cheatsheets/alias/","title":"Aliases","text":"<ul> <li>cm - catkin make - compile and build</li> <li>cmsingle - catkin make a specifici package</li> <li>myip - display local ip for me</li> <li>myvpnip - display ip on vpn</li> <li>cw - change to catkin_ws and set up environment</li> <li>stopnow - stop the connected robot immediately</li> <li>teleop - run a generaic teleop client</li> </ul>"},{"location":"cheatsheets/home/","title":"Cheat Sheets","text":"<p>Here you will find a series of cheat sheets. There are not too many yet. They are very concise, lists of commands, options, apis, and other small details. They will not be super helpful to you until you have been introduced to them elsewhere.</p>"},{"location":"cheatsheets/roscli/","title":"ROS Cli Commands","text":"<ul> <li>rostopic list</li> <li>rostopic pub</li> <li>rostopic show</li> <li>rostopic echo</li> <li>rosrun   - run a single ros program <li>roslaunch   - launch a ROS application with a .launch file <li>roscd  - change directory to a certain package <li>catkin_make - build a ros package</li>"},{"location":"cheatsheets/units/","title":"Units used in ROS","text":"<ul> <li>meters - distance</li> <li>meters per second - velocity</li> <li>radians - angle</li> <li>quaternions - 3d angula positiom (in radians)</li> <li>euler angles - 3d angular position (in rediance)</li> </ul>"},{"location":"cluster/home/","title":"Introduction","text":""},{"location":"cluster/home/#overview","title":"Overview","text":"<p>Cloud desktop is a cloud-based virtual desktop environment which you can access with just your browser. It allows you to run intensive robot simulations without your computer breaking down.</p>"},{"location":"cluster/home/#features","title":"Features","text":"<ul> <li>Ubuntu 18.04</li> <li>ROS Melodic</li> <li>Web-based GUI</li> <li>Web-based code editor</li> <li>Optional GPU support</li> </ul>"},{"location":"cluster/operation-guide/","title":"Operator's Guide","text":"<p>For new operators, please please please read all the documentations here before proceeding!</p>"},{"location":"cluster/operation-guide/#questions-and-notes-from-pito","title":"Questions and notes from Pito","text":"<ul> <li>exactly how and where is \"supervisor\" used?</li> <li>Is the copy of tb3-ros in pitosalas still used for anything?</li> <li>Using <code>lens.app</code> to look at and work with Rover cluster</li> </ul>"},{"location":"cluster/operation-guide/architecture/","title":"Architecture","text":""},{"location":"cluster/operation-guide/architecture/#overview","title":"Overview","text":"<p>The cloud desktop architecture is simple. On a high level, it looks like this:</p> <p></p>"},{"location":"cluster/operation-guide/architecture/#cloud-desktop-cluster","title":"Cloud Desktop Cluster","text":"<p>Cloud desktop cluster is a cluster of cloud desktops. It is implemented as a K8s cluster for easy scheduling and orchestration of cloud desktop containers.</p>"},{"location":"cluster/operation-guide/architecture/#cloud-desktop-container","title":"Cloud Desktop Container","text":"<p>The cloud desktop container provides a virtualized desktop environment that is isolated and portable. It consists of 3 components.</p> <ul> <li>VNC server paired with a NoVNC server</li> <li>VSCode server</li> <li>Tailscale client</li> </ul> <p>For details, see Container Image.</p>"},{"location":"cluster/operation-guide/architecture/#networking","title":"Networking","text":"<p>K8s network: - Used for communication with the load balancer to allow each container to be accessible from a URL - Implemented with Flannel</p> <p>Tailscale network: - Used for communication between cloud desktops and robots globally - Managed with Tailscale Dashboard</p> <p>AWS Route53: - Provides DNS records for redirecting traffic to the cluster</p>"},{"location":"cluster/operation-guide/cluster/","title":"Kubernetes Cluster","text":"<p>Cloud Desktop cluster is managed by Kubernetes. Specifically, it is a distro of Kubernetes named <code>K3s</code>.</p>"},{"location":"cluster/operation-guide/cluster/#management","title":"Management","text":"<p>For cluster management, see Cluster Management.</p>"},{"location":"cluster/operation-guide/cluster/#configurations","title":"Configurations","text":"<p>We use <code>k3s</code> to run our k8s cluster. While leaving most configurations as default, we made the following customization,</p> <ul> <li>container runtime: <code>docker</code></li> </ul> <p></p>"},{"location":"cluster/operation-guide/cluster/#namespaces","title":"Namespaces","text":"<ul> <li><code>clouddesktop-prod</code></li> <li>Main namespace</li> <li>Used for all active cloud desktops</li> <li><code>clouddesktop-dev</code></li> <li>For testing cloud desktop images</li> </ul>"},{"location":"cluster/operation-guide/cluster/#networking","title":"Networking","text":"<p>The default networking backend is <code>flannel</code> with VXLAN as backend.</p>"},{"location":"cluster/operation-guide/cluster/#ingress","title":"Ingress","text":"<p>The default ingress controller is <code>traefik</code>.</p>"},{"location":"cluster/operation-guide/cluster/#storage","title":"Storage","text":"<p>The default storage provider is <code>local-path</code>. In other words, we store all cloud desktop files locally on the node.</p>"},{"location":"cluster/operation-guide/cluster/#node-roles","title":"Node roles","text":"<p>As of May 2021</p> Name Role robotics-rover1 Master robotics-rover2 Agent"},{"location":"cluster/operation-guide/faq/","title":"FAQ","text":""},{"location":"cluster/operation-guide/faq/#user","title":"User","text":""},{"location":"cluster/operation-guide/faq/#how-do-i-add-a-new-user","title":"How do I add a new user?","text":"<p>Read User Management.</p>"},{"location":"cluster/operation-guide/faq/#how-do-i-delete-a-user","title":"How do I delete a user?","text":"<p>Read User Management.</p>"},{"location":"cluster/operation-guide/faq/#how-do-i-upgradedowngrade-a-desktop","title":"How do I upgrade/downgrade a desktop?","text":"<p>Read User Management.</p>"},{"location":"cluster/operation-guide/faq/#how-do-i-see-all-the-active-desktops-in-the-cluster","title":"How do I see all the active desktops in the cluster?","text":"<pre><code>kubectl -n clouddesktop-prod get pods\n</code></pre>"},{"location":"cluster/operation-guide/faq/#image","title":"Image","text":""},{"location":"cluster/operation-guide/faq/#how-do-i-add-a-package-to-the-image","title":"How do I add a package to the image?","text":"<p>Read Adding a new package.</p> <p>To make sure that new desktops are receiving this update, modify the deployment template under <code>clouddesktop-k8s/clouddesktop-template/deployment.yaml</code>.</p> <pre><code>spec:\n  initContainers:\n    - name: init-clouddesktop\n-     image: cosi119/tb3-ros:v2.1.1\n+     image: cosi119/tb3-ros:v2.1.2\n</code></pre>"},{"location":"cluster/operation-guide/faq/#node","title":"Node","text":""},{"location":"cluster/operation-guide/faq/#is-it-safe-to-restart-a-node","title":"Is it safe to restart a node?","text":"<p>Yes, but make sure to drain the node first. To drain the node,</p> <pre><code>kubectl drain robotics-rover2\n</code></pre> <p>After the reboot, <code>k3s</code> will be started automatically. If it is not started,</p> <pre><code>sudo systemctl restart k3s.service\n</code></pre>"},{"location":"cluster/operation-guide/faq/#troubleshooting","title":"Troubleshooting","text":""},{"location":"cluster/operation-guide/faq/#desktop-connectivity-issue","title":"Desktop connectivity issue","text":"<p>If a group or all desktops are not connecting,</p> <p></p> <p>If only 1 desktop is not connecting,</p> <p></p>"},{"location":"cluster/operation-guide/graphs/","title":"graphs","text":""},{"location":"cluster/operation-guide/image/","title":"Container Image","text":"<p>Cloud Desktop Container uses a custom docker image. The <code>Dockerfile</code> is located here.</p>"},{"location":"cluster/operation-guide/image/#internals","title":"Internals","text":""},{"location":"cluster/operation-guide/image/#components","title":"Components","text":"<p>There are 3 main components in the container image,</p> <ul> <li>VNC server paired with a NoVNC server</li> <li>VSCode server</li> <li>Tailscale client</li> </ul>"},{"location":"cluster/operation-guide/image/#defaults","title":"Defaults","text":"<p>Catkin Workspace: <code>/my_ros_data/catkin_ws</code></p> <p>Ports:</p> <ul> <li><code>novnc</code> 80</li> <li><code>vnc</code> 5900</li> <li><code>vscode</code> 8080</li> </ul>"},{"location":"cluster/operation-guide/image/#layers","title":"Layers","text":"<p>The current container image is structured this way:</p> <p></p> <p><code>cosi119/tb3-ros</code>   - Installs ROS melodic and ROS packages   - Installs custom packages used in class, like <code>prrexamples</code></p> <p><code>cosi119/ubuntu-desktop-lxde-vnc</code>   - Provides a Ubuntu image with novnc and lxde preconfigured.   - Provides a CUDA enabled variant (image with <code>-cuda</code> tag suffix)</p>"},{"location":"cluster/operation-guide/image/#process-management","title":"Process Management","text":""},{"location":"cluster/operation-guide/image/#supervisord","title":"Supervisord","text":"<p>Each of the components are managed by a process control system called <code>supervisord</code>. Supervisor is responsible for spawning and restarting these components. For detailed configs, see supervisord.conf.</p>"},{"location":"cluster/operation-guide/image/#modifing-startup-processes","title":"Modifing startup processes","text":"<p>Modify the <code>supervisord.conf</code> under <code>tb3-ros/tb3-ros/files/supervisor/supervisord.conf</code>.</p>"},{"location":"cluster/operation-guide/image/#packages","title":"Packages","text":""},{"location":"cluster/operation-guide/image/#default-packages","title":"Default packages","text":"<p>As of version <code>2.1.1</code>,</p> <ul> <li><code>turtlebot3_msgs</code></li> <li><code>turtlebot3</code></li> <li><code>turtlebot3_simulations</code></li> <li><code>https://github.com/campusrover/prrexamples</code></li> <li><code>https://github.com/campusrover/gpg_bran4</code></li> </ul>"},{"location":"cluster/operation-guide/image/#adding-a-new-package","title":"Adding a new package","text":"<p>To add a package to the default catkin workspace, modify the <code>Dockerfile</code> under <code>tb3-ros/tb3-ros/Dockerfile</code>:</p> <pre><code># Add the following lines\nWORKDIR /my_ros_data/catkin_ws/src\nRUN git clone --recursive --depth=1 https://github.com/ROBOTIS-GIT/turtlebot3_msgs.git\n</code></pre>"},{"location":"cluster/operation-guide/image/#github-repo","title":"Github repo","text":"<p>pitosalas/tb3-ros</p>"},{"location":"cluster/operation-guide/lifecycle/","title":"Lifecycle of a Cloud Desktop","text":""},{"location":"cluster/operation-guide/lifecycle/#desktop-creation","title":"Desktop Creation","text":""},{"location":"cluster/operation-guide/lifecycle/#desktop-destruction","title":"Desktop Destruction","text":""},{"location":"cluster/operation-guide/nodes/","title":"Nodes","text":""},{"location":"cluster/operation-guide/nodes/#overview","title":"Overview","text":"<p>Each nodes in the cluster is setup in a similar way. The software stack on a node looks like this:</p> <p></p>"},{"location":"cluster/operation-guide/nodes/#container-runtime","title":"Container runtime","text":"<p>The default runtime is <code>nvidia</code> for all Nvidia-enabled nodes. For details, see <code>/etc/docker/daemon.json</code>.</p>"},{"location":"cluster/operation-guide/nodes/#software-versions","title":"Software versions","text":"<p>As of May 2021</p> Name OS K3s Version Docker Version robotics-rover1 Ubuntu 20.04.1 v1.19.5+k3s2 <code>(746cf403)</code> Docker version 20.10.1, <code>build 831ebea</code> robotics-rover2 Ubuntu 18.04.5 v1.19.5+k3s2 <code>(746cf403)</code> Docker version 20.10.1, <code>build 831ebea</code>"},{"location":"cluster/operation-guide/nodes/#node-hardwares","title":"Node hardwares","text":"Name IP Hardware Configuration robotics-rover1 rover1.cs.brandeis.edu 12C/24T, 32GB, 1TB, RTX2060S robotics-rover2 rover2.cs.brandeis.edu 12C/24T, 32GB, 1TB, RTX2060S"},{"location":"cluster/operation-guide/sources/","title":"Source Code","text":"<p>All the source codes of Cloud Desktop is distributed in the following GitHub repos,</p> <ul> <li>Cloud Desktop Image https://github.com/pitosalas/tb3-ros</li> <li>Cloud Desktop K8s files https://github.com/campusrover/clouddesktop-k8s</li> <li>Standalone Cloud Desktop https://github.com/campusrover/clouddesktop-docker</li> </ul>"},{"location":"cluster/operation-guide/operating/","title":"Operating the Cloud Desktop Cluster","text":""},{"location":"cluster/operation-guide/operating/cluster/","title":"Cluster Management","text":"<p>We administrate all our operations through the use of <code>kubectl</code>.</p>"},{"location":"cluster/operation-guide/operating/cluster/#prerequisites","title":"Prerequisites","text":"<p>Read up on the following materials,</p> <ul> <li><code>K8s</code> Concepts</li> <li><code>kubectl</code> cheatsheet</li> </ul>"},{"location":"cluster/operation-guide/operating/cluster/#requirements","title":"Requirements","text":"<ul> <li><code>kubectl</code> Installation Guide</li> </ul>"},{"location":"cluster/operation-guide/operating/cluster/#setup","title":"Setup","text":"<p>To access the K8s cluster, you will need to have a <code>k3s.yaml</code> credential file. It can be obtained by ssh into the <code>master</code> node of the cluster, under the directory <code>/etc/rancher/k3s/k3s.yaml</code>.</p> <p>Once you have obtained the <code>k3s.yaml</code> file, make the following modification,</p> <pre><code># Update with the IP of the master node\n- server: https://localhost:6443\n+ server: https://123.123.123.123:6443\n</code></pre> <p>After the modification, this file is ready for use. Update your shell to always use this file,</p> <pre><code>export KUBECONFIG=/.../k3s.yaml\n</code></pre> <p>To confirm it working,</p> <pre><code>kubectl get pods --all-namespaces\n</code></pre>"},{"location":"cluster/operation-guide/operating/cluster/#common-operations","title":"Common Operations","text":""},{"location":"cluster/operation-guide/operating/cluster/#see-all-nodes-in-cluster","title":"See all nodes in cluster","text":"<pre><code>kubectl get nodes\n</code></pre>"},{"location":"cluster/operation-guide/operating/cluster/#see-a-specific-node","title":"See a specific node","text":"<pre><code>kubectl describe node robotics-rover1\n</code></pre>"},{"location":"cluster/operation-guide/operating/cluster/#see-all-deployed-pods","title":"See all deployed pods","text":"<ul> <li>Notice that <code>-n clouddesktop-prod</code> refers to the clouddesktop-prod k8s <code>namespace</code></li> </ul> <pre><code>kubectl -n clouddesktop-prod get pods\n</code></pre>"},{"location":"cluster/operation-guide/operating/cluster/#see-a-specific-pod","title":"See a specific pod","text":"<pre><code>kubectl -n clouddesktop-prod describe pod julianho-clouddesktop-deployment-abc123efg-123abc\n</code></pre>"},{"location":"cluster/operation-guide/operating/cluster/#draining-a-node","title":"Draining a node","text":"<pre><code>kubectl drain robotics-rover2\n</code></pre>"},{"location":"cluster/operation-guide/operating/users/","title":"User Management","text":""},{"location":"cluster/operation-guide/operating/users/#requirements","title":"Requirements","text":"<ul> <li><code>kubectl</code> Installation Guide</li> <li>Setup Instructions</li> <li><code>terraform</code> Installation Guide</li> <li>AWS Credentials to AWS Route53<ul> <li>Access Key</li> <li>Secret Key</li> </ul> </li> </ul>"},{"location":"cluster/operation-guide/operating/users/#setup","title":"Setup","text":"<p>First obtain the users repo from here.</p> <p>Setup <code>.env</code> file by filling in all required fields:</p> <pre><code>mv .env.sample .env\n</code></pre> <p>It's better to use an IAM <code>user group</code> to create a new <code>user</code> associated with the <code>clouddesktop</code> user group. It will generate a access and secret key for you to put in the above file. The ingress IP is the ip address of the main node. Once everything is properly setup, do:</p> <pre><code>export $(make env)\n</code></pre> <p>Setup terraform:</p> <pre><code>terraform init\n</code></pre>"},{"location":"cluster/operation-guide/operating/users/#common-operations","title":"Common Operations","text":"<p>To under what each of these commands do under the hood, see here.</p>"},{"location":"cluster/operation-guide/operating/users/#add-a-new-user","title":"Add a new user","text":"<p><code>id</code> is the ID of the new user.</p> <pre><code>make add-user id=example\n</code></pre>"},{"location":"cluster/operation-guide/operating/users/#delete-a-user","title":"Delete a user","text":"<p>Warning: This will remove any persisted data!!</p> <pre><code>make delete-user id=example\n</code></pre>"},{"location":"cluster/operation-guide/operating/users/#change-resources-allocation-for-user","title":"Change resources allocation for user","text":"<p>For user <code>example</code>, modify the file <code>example-clouddesktop/deployment.yaml</code>.</p>"},{"location":"cluster/operation-guide/operating/users/#to-increase-minimum-resource","title":"To increase minimum resource","text":"<p>For detailed explanation of what units you can change it to, see here.</p> <pre><code>resources:\n  requests:  # increase minimum to at least 4 cores\n-   cpu: 2\n+   cpu: 4\n    memory: 2Gi\n</code></pre>"},{"location":"cluster/operation-guide/operating/users/#to-increase-maximum-resource-limit","title":"To increase maximum resource limit","text":"<p>For detailed explanation of what units you can change it to, see here.</p> <pre><code>resources:\n  limits: # increase to maximum 16GB of ram\n    cpu: 8\n-   memory: 8Gi\n+   memory: 16Gi\n</code></pre>"},{"location":"cluster/operation-guide/operating/users/#add-gpu-support","title":"Add GPU support","text":"<p>Note: Beware if we have enough free GPUs in the cluster</p> <p>Note: Make sure the docker image is a CUDA enabled variant (ie. <code>tb3-ros:v2.1.1-cuda</code>)</p> <pre><code>resources:\n  limits: # increase to maximum 16GB of ram\n    cpu: 8\n    memory: 8Gi\n+   nvidia.com/gpu: 1\n</code></pre>"},{"location":"cluster/operation-guide/operating/users/#apply-changes","title":"Apply changes","text":"<p>Warning: This will restart the cloud desktop container!!</p> <p>To apply the previously changed values,</p> <pre><code>kubectl apply -k example-clouddesktop\n</code></pre>"},{"location":"cluster/operation-guide/operating/users/#restarting-a-desktop","title":"Restarting a Desktop","text":"<p>To restart a desktop, you need to delete and redeploy the desktop.</p> <p>This will NOT lead to loss of data.</p> <pre><code>kubectl delete -k example-clouddesktop\n\nkubectl apply -k example-clouddesktop\n</code></pre>"},{"location":"cluster/user-guide/","title":"Using Cloud Desktop","text":""},{"location":"cluster/user-guide/cloud-desktop/","title":"How to use the Rover Cluster","text":""},{"location":"cluster/user-guide/cloud-desktop/#introduction","title":"Introduction","text":"<p>Cloud desktop is a virtual desktop environment which you can access with just your browser. This allows you to run intensive robot simulations without your computer breaking down.</p> <p>You can access your cloud desktop with one of three ways:</p> <ul> <li>Virtual desktop environment via <code>novnc</code></li> <li>Browser-based code editor via <code>vscode</code></li> <li>SSH</li> </ul>"},{"location":"cluster/user-guide/cloud-desktop/#accessing-virtual-desktop","title":"Accessing virtual desktop","text":""},{"location":"cluster/user-guide/cloud-desktop/#requirements","title":"Requirements","text":"<ul> <li>Browser with javascript blocking turned off</li> </ul> <p>For access link and password, please refer to the credentials given to you by the TA.</p>"},{"location":"cluster/user-guide/cloud-desktop/#accessing-code-editor","title":"Accessing code editor","text":""},{"location":"cluster/user-guide/cloud-desktop/#requirements_1","title":"Requirements","text":"<ul> <li>Browser with javascript blocking turned off</li> </ul> <p>For access link and password, please refer to the credentials given to you by the TA.</p>"},{"location":"cluster/user-guide/cloud-desktop/#accessing-via-ssh","title":"Accessing via SSH","text":""},{"location":"cluster/user-guide/cloud-desktop/#enable-ssh-for-remote-access","title":"Enable SSH for remote access","text":"<p>Cloud desktop comes with SSH access disabled.</p> <ul> <li>To enable ssh, first login using cloud desktop/VScode</li> <li>Open up the terminal</li> <li>Run <code>sudo passwd root</code> to setup a password</li> </ul>"},{"location":"cluster/user-guide/cloud-desktop/#access","title":"Access","text":"<p>You can now <code>ssh</code> in using the cloud desktop link given to you.</p> <p>Using the terminal</p> <pre><code>ssh root@desktop1.ros.campusrover.org -p 2222\n</code></pre>"},{"location":"cluster/user-guide/cloud-desktop/#private-networking","title":"Private Networking","text":"<p>All cloud desktops are, by default, connected to a shared private Tailscale network. Using the cloud desktop, you can connect to any robots on the network anywhere in the world.</p> <pre><code>  100.89.2.122 desktop-1 \\       / robot-1 100.89.2.122\n                          \\     /\n  100.99.32.12 desktop-2 - - - - - robot-2 100.99.31.234\n                           /    \\\n  100.88.77.234 desktop-3 /      \\ robot-3 100.86.232.111\n</code></pre> <p>Each robot on the network is assigned with a Tailscale IP which looks like this: <code>100.xx.xxx.xxx</code></p> <pre><code>ssh root@100.89.2.122 # IP of robot-1\n</code></pre> <p>For more details and troubleshooting about Private Networking, see this.</p>"},{"location":"cluster/user-guide/cloud-desktop/#troubleshooting","title":"Troubleshooting","text":""},{"location":"cluster/user-guide/cloud-desktop/#what-is-my-ip-address","title":"What is my IP address?","text":"<p>Run the following command</p> <pre><code>ip addr show dev tailscale0 | grep -Eo '([0-9]{1,3}[\\.]){3}[0-9]{1,3}'\n</code></pre>"},{"location":"cluster/user-guide/cloud-desktop/#what-is-my-hostname","title":"What is my hostname?","text":"<p>Run the following command</p> <pre><code>cat /etc/hostname\n</code></pre>"},{"location":"cluster/user-guide/cloud-desktop/#virtual-desktop-refused-to-login","title":"Virtual desktop refused to login","text":"<p>If repeated attempts to login through the browser failed, it is likely that your desktop has crashed. Please notify your TA, he/she will restart your desktop for you.</p>"},{"location":"cluster/user-guide/cloud-desktop/#white-blank-screen-when-visiting-the-code-editor","title":"White blank screen when visiting the code editor","text":"<p>This is usually an issue with slow internet connection, give it a while to load.</p> <p>If nothing shows up within 5 minutes, try <code>right click &gt; inspect &gt; console</code> to look for any error messages. Please then notify your TA with the error message.</p>"},{"location":"cluster/user-guide/code-editor/","title":"Code Editor","text":""},{"location":"cluster/user-guide/code-editor/#access","title":"Access","text":"<p>Login to the provided link <code>http://code.USER.ros.campusrover.org</code> and enter your password (should be provided to you by TA).</p>"},{"location":"cluster/user-guide/code-editor/#open-a-folder","title":"Open a Folder","text":"<p>To open a folder or file, go to <code>File</code> -&gt; <code>Open</code>.</p> <p></p>"},{"location":"cluster/user-guide/code-editor/#autosave","title":"Autosave","text":"<p>VSCode allows auto-saving. This is super helpful in case of a crash. To enable it, go to <code>File</code> -&gt; Choose <code>Auto Save</code>.</p> <p></p>"},{"location":"cluster/user-guide/code-editor/#terminal","title":"Terminal","text":"<p>VSCode comes with a terminal. This is the same terminal as the one in the Desktop UI. To open a new terminal, go to <code>Terminal</code> -&gt; Choose <code>New Terminal</code>.</p> <p></p>"},{"location":"cluster/user-guide/desktop-ui/","title":"Desktop","text":""},{"location":"cluster/user-guide/desktop-ui/#access","title":"Access","text":"<p>Login to the provided link <code>http://vnc.USER.ros.campusrover.org</code> and enter your password (should be provided to you by TA).</p> <p></p>"},{"location":"cluster/user-guide/desktop-ui/#control","title":"Control","text":"<p>On the left you can find a panel for desktop control such as window scaling.</p> <p>Left panel:</p> <ul> <li>Extra keys (ctrl-alt, esc key)</li> <li>Clipboard (use this for copy-and-paste)</li> <li>Fullscreen</li> <li>Settings</li> <li>Scaling Mode -&gt; \"Local Scaling\" (always)</li> </ul> <p>On the bottom is the taskbar (similar to the one in Windows).</p>"},{"location":"cluster/user-guide/desktop-ui/#window-scaling","title":"Window Scaling","text":"<p>To make sure the desktop fits your browser window, go to <code>Settings</code> -&gt; <code>Scaling Mode</code> -&gt; Choose <code>Local Scaling</code>.</p> <p></p>"},{"location":"cluster/user-guide/desktop-ui/#open-terminal","title":"Open Terminal","text":"<p>To open the terminal, click the start button at bottom-left corner, go to <code>System Tools</code> -&gt; Choose <code>LXTerminal</code>.</p> <p></p>"},{"location":"cluster/user-guide/desktop-ui/#tailscale-ip","title":"Tailscale IP","text":"<p>Each clouddesktop has a special IP for connecting to our global private network. The tailscale IP can be found on the terminal.</p> <p></p>"},{"location":"cluster/user-guide/desktop-ui/#restart-desktop","title":"Restart Desktop","text":"<p>To restart the clouddesktop in case of a freeze, open a terminal in Code Editor, type <code>restart</code>.</p> <p>If that did not work, please reach out to your TA.</p>"},{"location":"cluster/user-guide/getting-started/","title":"Getting Started","text":""},{"location":"cluster/user-guide/getting-started/#logging-in-for-the-first-time","title":"Logging in for the first time","text":"<p>You should receive the following credentials for accessing your clouddesktop environment:</p> <ul> <li>Desktop link</li> <li>e.g. <code>http://vnc.julianho.ros.campusrover.org</code></li> <li>Code editor link</li> <li>e.g. <code>http://code.julianho.ros.campusrover.org</code></li> <li>Password</li> </ul> <p>If you do not have those credentials, please reach out to your TA.</p>"},{"location":"cluster/user-guide/getting-started/#accessing-your-cloud-desktop","title":"Accessing your cloud desktop","text":"<p>There are two ways to access your cloud desktop,</p> <ul> <li>Desktop UI</li> <li>Code Editor</li> </ul>"},{"location":"cluster/user-guide/getting-started/#data-storage","title":"Data storage","text":"<p>Your data is securely saved on our servers and will persist on reboots.</p>"},{"location":"crpackage/architecture/","title":"Overview","text":"<p>Architecture diagram here (Outdated. Sorry (No worry, GEN3 will update a new one\ud83d\ude09))</p> <p>On the first step, we need to bring the laptop inside the Mark-I out and turn it on. Afterwards we need to SSH into the Mark-I laptop with our own device by <code>ssh turtlebot@129.64.243.64</code>. Then we attach the Mark-I laptop to her mothership and do a bringup for her by entering <code>roslaunch cr_ros campus_rover.launch</code> in the ssh terminal (Note this bringup will automatically star roscore so we don't need to do roscore seperately). By our past experience, we need to wait for a 'bep bep bep' sound effect made by the Kabuki machine and check for \"Odom received\" message on the ssh terminal.</p> <p>Then on the side of our own device, we want to open another terminal and enter <code>roslaunch cr_ros offboard_launch.launch</code>. This will do the off_board bringup and boot the Rviz for Mark-I to navigating herself. Once Mark-I has correctly localized herself, then we can minimize the Rviz window or even close it without any harm.</p> <p>For the final step of booting Mark-I, here comes to the web server. First I assume our device has already had Flask installed and cr_web repo cloned. Then we want to open another terminal window and cd into the cr_web directory. Then entering command <code>flask run --no-reload --host=0.0.0.0 --with-threads</code>. This will start the localhost web server and we can manually open a browser and go to <code>localhost:5000/</code> and we shall see that Mark-I on the web server is ready for you to do some rock! \ud83e\udd84</p> <p>The navigation algorithm we used is the most complex software controlling the rover. It must efficiently process sensor data and plan routes over long range maps and avoid the short distance obstacles. A full-fledged campus rover must also handle complex obstacles like doors, elevators, and road crossings which would each require special navigation decision making. A fully functioning rover would also incorporate a unique localization algorithm to combine sensor data from fiducials, GPS, wifi signals, and camera/lidar inputs, etc.</p> <p>Currently, reliability is a more pressing concern in building a rover to meet the mark 1 requirements. While a solution which provides more control in navigation would allow us to better expand our solution beyond Volen 1, it is not feasible at this stage. The turtlebot navigation package has been well tested and is proven to carefully plan routes and avoid obstacles. It also incorporates a powerful adaptive Monte Carlo localization algorithm which draws on data points from a single-beam lidar or depth camera.</p> <p>A better solution for the mark 1 rover is to make use of the robust turtlebot navigation code and supplement the AMCL localization with a separate fiducial localization system which will supply poses to the existing algorithm by publishing pose estimates. A navigation_controller will also wrap the standard navigation and localization nodes to provide additional control and feedback where possible.</p> <p>It is also beneficial to wrap the cmd_vel outputs of the navigation will allow for distinct wandering to refine an initial pose and for teleop commands. We use the <code>/cmd_vel_mux/inputs</code> topics. This allows teleop commands to block navigation directions which in turn will block localization commands.</p> <p>At the top level, a central control node will manage core tasks like interacting with the tablet controls, publishing a map stored on disk, and deciding when the robot must return to recharge. Keeping these simple high level tasks combined will allow for rapid development but could be spun off into independent ROS nodes if the functionality required grows in complexity.</p> <p><code>process_fiducial_transforms</code> also publishes acts primarily as a transform from the camera-relative pose from the built-in aruco_detect node, to a map relative pose based on it\u2019s knowledge of the locations of fiducials in the map. The node is always running, but only when it sees a fiducial will it publish a cur_pose message to assist the AMCL localization.</p>"},{"location":"crpackage/architecture/#teleop","title":"Teleop","text":"<p>Takes button presses from the UI and sends <code>cmd_vels</code>. (However, there is a bug in this application. After the user teleop the rover, it can\u2019t respond to new commands, such as back to charging station or go to xxx office. In gen3, we will fix this bug.)</p>"},{"location":"crpackage/architecture/#rover-controller","title":"Rover controller","text":"<p>Uses a <code>move_base</code> action to navigate. Subscribes to <code>/web/destination</code> which parses JSON input and <code>/destination</code> which takes a PoseStamped.</p>"},{"location":"crpackage/architecture/#alexander-feldman-feldmanaygmailcom-10262018_","title":"@Alexander Feldman, feldmanay@gmail.com 10/26/2018_","text":""},{"location":"crpackage/architecture/#sibo-zhu-sibozbrandeisedu-3112019_","title":"@Sibo Zhu, siboz@brandeis.edu 3/11/2019_","text":""},{"location":"crpackage/architecture/#yuchen-zhang-yzhang71brandeisedu-3112019_","title":"@Yuchen Zhang, yzhang71@brandeis.edu 3/11/2019_","text":""},{"location":"crpackage/cr-3-project/","title":"Campus Rover Code Base 3 Project Summary","text":""},{"location":"crpackage/cr-3-project/#mission","title":"Mission","text":"<p>To improve upon version 2 of the code base, focusing on changes to:</p> <ul> <li>Ease of expansion - building infrastructure that makes adding functional modules to the code base easy</li> <li>Reclaiming functionality of some nodes from version 1, that were unused in version 2</li> <li>Improving upon existing nodes</li> <li>Cleaning up the code base by throwing away unused files</li> </ul> <p>Continue reading for details on each aspect of this project.</p>"},{"location":"crpackage/cr-3-project/#ease-of-expansion","title":"Ease of Expansion","text":""},{"location":"crpackage/cr-3-project/#launch-file-modularity","title":"Launch File Modularity","text":"<p>Rather than lumping all nodes into either \"onboard\" or \"offboard\" launch files, we have created a series of launch files that can be added to either main launch file. Furthermore, these launch modules can be disabled in the command line. Here is how this was achieved:</p> <p>Here is the launch module for alexa voice integration:</p> <pre><code>&lt;launch&gt;\n  &lt;!-- nodes that work with alexa to take orders and deliver items--&gt;\n  &lt;node pkg=\"cr_ros_3\" type=\"ngrok_launch.sh\" name=\"ngrok_launch\" output=\"screen\"/&gt;\n  &lt;node pkg=\"cr_ros_3\" type=\"voice_webhook.py\" name=\"voice_webhook\" output=\"screen\"&gt;&lt;/node&gt;\n  &lt;node pkg=\"cr_ros_3\" type=\"voice_destination_pub.py\" name=\"voice_destination_pub\" output=\"screen\"&gt;&lt;/node&gt;\n&lt;/launch&gt;\n</code></pre> <p>It is added to the offboard launch like so:</p> <pre><code>&lt;arg name=\"voice\" default=\"true\"/&gt;\n&lt;group if=\"$(arg voice)\"&gt;\n  &lt;include file=\"$(find cr_ros_3)/launch/voice.launch\"/&gt;\n&lt;/group&gt;\n</code></pre> <p>SO if you wish to launch offboard without voice, use this command:</p> <pre><code>roslaunch cr_ros_3 mutant_offboard_rpicam.launch voice:=false\n</code></pre> <p>Going forward, all new features should be added to their own launch module, where the module contains all nodes that are required for the feature to work properly. The only excepition is if a new feature is added to the core of the package, and is required for the package to function properly.</p>"},{"location":"crpackage/cr-3-project/#state-tools-for-state-manager-interfacing","title":"State Tools for State Manager Interfacing","text":"<p>A handful of changes have been made to the state manager interface. They include</p> <ul> <li>get_state now communicates via the state query service, rather than directly accessing the current_state field.</li> <li>get_state and change_state now have error handling to deal for the event that the state serivces are unavailable.</li> <li>New functions have been added to the state_tools file which should make interfacing with the state manager easier.</li> </ul>"},{"location":"crpackage/cr-3-project/#install-scripts","title":"Install Scripts","text":"<p>To make it easier for new users to begin working with the existing campus rover platform, we have created a set of install scripts that will make the setup time significantly faster, and remove all the guesswork. Refer to the cr_ros_3 package readme for installation instructions.</p>"},{"location":"crpackage/cr-3-project/#alien-compatibilty","title":"Alien Compatibilty","text":"<p>Prior to cr_ros_3, the campus roverr platform only worked on robots running ROS Kinetic. Now, the platform will run on robot running ROS Melodic.</p>"},{"location":"crpackage/cr-3-project/#reclaiming-functionality","title":"Reclaiming Functionality","text":""},{"location":"crpackage/cr-3-project/#talk-queue","title":"Talk Queue","text":"<p>Formerly known as \"message_switch\", the talking queue was an overlooked node that did not recieve much attention until now, as it was overlooked for directly interfacing with the talk service. As the code base grows and more features may wish to use the on-board text to speech feature, a queue for all talking requests is a nessecary feature.</p> <p>Using the talk queue from another node is easy. First, import the following:</p> <pre><code>from cr_ros_3.msg import ThingsToSay\nfrom state_tools import talker\n</code></pre> <p>Set up a publisher to the /things_to_say topic</p> <pre><code>talker_pub = rospy.Publisher('/things_to_say', ThingsToSay, queue_size=1)\n</code></pre> <p>Whenever your node wishes to produce speech, utilize a line of code similar to this:</p> <pre><code>talker(\"I can talk!\", talker_pub)\n</code></pre>"},{"location":"crpackage/cr-3-project/#narrate-location","title":"Narrate Location","text":"<p>The location narrator is a novelty feature that has been reclaimed by:</p> <ul> <li>indtroducing a new set of waypoints in files/waypoints.json that correspond to the basement demo area</li> <li>modifying the existing script to narrate whenever the nearest waypoint or the state changes.</li> </ul>"},{"location":"crpackage/cr-3-project/#improving-existing-nodes","title":"Improving Existing Nodes","text":""},{"location":"crpackage/cr-3-project/#clearing-costmaps","title":"Clearing Costmaps","text":"<p>Innaccuracies in costmaps can lead to difficulty in navigation. As such, we have implemented costmap clearing under the following circumstances:</p> <ul> <li>Navigation returns the 'aborted' code</li> <li>The robot exits the flying state and enters the lost state</li> </ul> <p>Here's why:</p> <ul> <li>When navigation fails it is often because the costmap is muddled in some way that prevents a new route from being plotted. If the costmap is cleared and the goal is re-sent, then navigation may become successful.</li> <li>A human lifting the robot create a significant amount of lidar noise that is innacurate because the robot is not on the ground. Therefore, when the robot is place back on the ground it deserves a clean slate.</li> </ul> <p>Here's how:</p> <p>First, the Empty service topic must be impported</p> <pre><code>from std_srvs.srv import Empty\n</code></pre> <p>Then establish a serivce proxy</p> <pre><code>costmap_clearer = rospy.ServiceProxy('/move_base/clear_costmaps', Empty)\n</code></pre> <p>Finally, any time a node requires the costmap cleared, ultilize this line:</p> <pre><code>costmap_clearer()\n</code></pre>"},{"location":"crpackage/cr-3-project/#automatic-dynamic-camera-reconfigure","title":"Automatic Dynamic Camera Reconfigure","text":"<p>The Rasbperry Pi camera that is mounted to Mutant is mounted such that by default, it is upside down. Fortunately, Ubiqutiy's rapicam_node that we ultize supports dynamic reconfiguration. This can be done manually using the following shell command:</p> <pre><code>rosrun rqt_reconfigure rqt_reconfigure\n</code></pre> <p>This will bring up a GUI which will allow various aspects of the camera configuration to be manipulated. It also allows for a given configuration to be saved in a yaml file. Unfortunately, changes made through dynamic reconfiguration are temporary, and using a GUI to reconfigure every time is tedium that we strive to eliminate. The solution lies in camera_reconfigure.sh. We saved our desired configuration to camera.yaml from rqt_reconfigure. camera_reconfigure.sh uses that yaml file in this crucial line:</p> <pre><code>roscd cr_ros_3 &amp;&amp; rosrun dynamic_reconfigure dynparam load raspicam_node camera.yaml\n</code></pre> <p>Thus, automatic reconfiguration is acheived.</p>"},{"location":"crpackage/cr-3-project/#fiducials","title":"Fiducials","text":"<p>Transforms are tricky. Version 2 never quite had accurate fiducial localizations due to bad static fiducial transforms. Through trial and error, static fiducial transforms have been edited and the pose esitmates have become more accurate as a result. We found it useful to refer to this static transform documentation to help remember the order of the arguments supplied to the static transform publisher:</p> <pre><code>static_transform_publisher x y z yaw pitch roll frame_id child_frame_id period_in_ms\n</code></pre>"},{"location":"crpackage/cr-3-project/#rviz-settings","title":"Rviz Settings","text":"<p>If Rviz is not supplied the right settings file, it can produce a lot of frustrating errors. This was the case with our old settings file. We have generated a new settings file. How? Here are the steps:</p> <ol> <li>launch default turtlebot navigation</li> <li>save the settings from Rviz as a new file</li> <li>set Rviz in mutant_navigation.launch to use the new settings file</li> <li>make the necessary changes to the left bar to make Rviz visualize navigation properly (ex. using scan_filter as the scan topic rather than scan)</li> <li>save your changes to your settings file</li> </ol>"},{"location":"crpackage/cr-3-project/#ngrok","title":"Ngrok","text":"<p>We have added ngrok_launcher.sh which will bringup an ngrok tunnel to the alexa webhook to allow the alexa voice nodes to function.</p>"},{"location":"crpackage/cr-3-project/#updates-to-cr_web","title":"Updates to cr_web","text":"<p>Minor updates have been made to cr_web to accomodate a few changes to the code base. They are:</p> <ul> <li>localhost port changed from 5000 (default) to 3000, to accomodate cr web app and ngrok webhook running on the same machine</li> <li>Updated the map file to match current demo area layout.</li> </ul>"},{"location":"crpackage/cr-3-project/#throw-away-unused-files","title":"Throw Away Unused Files","text":"<p>Files related to the following were removed:</p> <ul> <li>gen2 facial recognition</li> <li>gen2 package delivery</li> <li>gen3 facial recognition</li> <li>gen3 hand detection</li> </ul> <p>Fortunately, all these files still exist in version 1 and/or version 2 of the code base, so if they wish to be salvaged, they could be.</p>"},{"location":"crpackage/cr-3-project/#conclusion","title":"Conclusion","text":"<p>The campus rover code base is arguably in it's best state that it has ever been in. We eagerly look forward to how it will continue to grow in the future.</p>"},{"location":"crpackage/home/","title":"Campus Rover Code Base","text":"<p>This section of the Lab Notebook contains pages related to the ongoing campus rover project, specifically the cr_ros, cr_web, cr_ros_2, cr_ros_3 and rover_4_core repositories.</p>"},{"location":"crpackage/home/#the-difference-between-the-repositories","title":"The difference between the repositories","text":"<ul> <li>cr_ros and cr_web were the original pair of repos that gens 1 and 2 created to use with a TurtleBot2 (Spring 2018, Fall 2018)</li> <li>cr_ros_2 is a port of cr_ros to work with TurtleBot3. It features a number of unstable features from external repos that were created that semester. It works with the mutant branch of cr_web. (Spring 2019)</li> <li>cr_ros_3 is a more stable version of cr_ros_2 which also works on TB3 with a number of tweeks under the hood which help the package to run smoothly and be easy to pick up and develop on. (Fall 2019)</li> <li>rover_4_core is low-level code that is intended to run on a new custom model of robot hardware, but it could be interfaced and produce the same information as a TB3. A companion package to cr_ros_3. Development was cut short. (Spring 2020)</li> </ul>"},{"location":"crpackage/nodes/","title":"Nodes","text":"<p>Updated May 2019 with progress following gen3 and mutant mark 1.</p>"},{"location":"crpackage/nodes/#adjust_position","title":"adjust_position","text":"<p><code>Dormant</code> Converts the Pose messages it receives from its subscription to PoseWithCovarianceStamped messages and passes them on via its publication</p> <p>Publications</p> <ul> <li>/initialpose</li> </ul> <p>Subscriptions</p> <ul> <li>/fid_pose</li> </ul>"},{"location":"crpackage/nodes/#check_docked","title":"check_docked","text":"<p><code>Defunct</code> Updates the robot's state to reflect whether it is currently being charged at its dock based on charging data from its subscription</p> <p>Now defunct - mutant does not dock, because it is not based on the kobuki base.</p> <p>Subscriptions</p> <ul> <li>/mobile_base/sensors/core_throttle</li> </ul>"},{"location":"crpackage/nodes/#cpu_checker","title":"cpu_checker","text":"<p><code>Current</code> Publishes CPU usage data and prints it to the warning log if it is high or otherwise to the debug log based on data from process and system utilities</p> <p>Publications</p> <ul> <li>/laptop_cpu_usage</li> </ul>"},{"location":"crpackage/nodes/#greeter","title":"greeter","text":"<p><code>Dormant</code> Uses facial recognition to detect and recognize known faces in the camera feed based on provided data and greets them appropriately by name via a vocal service</p> <p>Subscriptions</p> <ul> <li>/camera/rgb/image_raw/compressed_throttle</li> </ul>"},{"location":"crpackage/nodes/#lost_and_found","title":"lost_and_found","text":"<p><code>Current</code> Uses pickup detector data to determine whether the robot is flying or not. Handles localization recovery upon returning to the ground.</p> <p>Publications</p> <ul> <li>/initialpose</li> <li>/cmd_vel</li> <li>/destination</li> </ul> <p>Subscriptions</p> <ul> <li>/airborne</li> <li>/destination</li> </ul>"},{"location":"crpackage/nodes/#message_switch","title":"message_switch","text":"<p><code>Dormant</code> Organizes speech messages chronologically and feeds them to the speech service at appropriate times</p> <p>Subscriptions</p> <ul> <li>/things_to_say</li> </ul>"},{"location":"crpackage/nodes/#location_narration","title":"location_narration","text":"<p><code>Dormant</code> Publishes speech messages narrating the robot's behavior current and proximate location based on its state and on data from its subscription</p> <p>Publications</p> <ul> <li>/things_to_say</li> </ul> <p>Subscriptions</p> <ul> <li>/nearest_waypoint</li> </ul>"},{"location":"crpackage/nodes/#navigation_controller","title":"navigation_controller","text":"<p><code>Defunct</code> All functionality was moved to rover_controller</p> <p>Publications</p> <ul> <li>/cmd_vel_mux/input/navi</li> </ul> <p>Subscriptions</p> <ul> <li>/amcl_pose</li> </ul>"},{"location":"crpackage/nodes/#package_handler","title":"package_handler","text":"<p><code>Dormant</code> Detects the presence of a physical package via its publications and converses with a user to determine goals and to communicate successes and errors while updating its goals to respond to expected and unexpected changes.</p> <p>Currently not in use due to the lack of a sensor to detect packages on gen3's mutant.</p> <p>Publications</p> <ul> <li>/release_package</li> <li>/record_start</li> <li>/record_stop</li> <li>/physical_package</li> <li>/destination</li> </ul> <p>Subscriptions</p> <ul> <li>/release_package</li> <li>/receive_package</li> <li>/mobile_base/events/button</li> <li>/mobile_base/events/digital_input</li> <li>/destination</li> </ul>"},{"location":"crpackage/nodes/#package_sender","title":"package_sender","text":"<p><code>Dormant</code> Publishes filename of appropriate prerecorded message for the robot to play based on data from its subscription</p> <p>Dormant for same reason as package_handler</p> <p>Publications</p> <ul> <li>/receive_package</li> </ul> <p>Subscriptions</p> <ul> <li>/physical_package</li> </ul>"},{"location":"crpackage/nodes/#pose_converter","title":"pose_converter","text":"<p><code>Current</code> Provides scripts for automatically converting from different pose types</p>"},{"location":"crpackage/nodes/#process_fid_tfs","title":"process_fid_tfs","text":"<p><code>Current</code> Uses fiducial data from its subscription to to determine and publish the robot's position relative to the map</p> <p>Publications</p> <ul> <li>initialpose</li> </ul> <p>Subscriptions</p> <ul> <li>fiducial_transforms</li> </ul>"},{"location":"crpackage/nodes/#recording_sender","title":"recording_sender","text":"<p><code>Current</code> Records short audio clips featuring user instructions to a file and publishes its name</p> <p>Publications</p> <ul> <li>/receive_package</li> </ul> <p>Subscriptions</p> <ul> <li>/record_start</li> <li>/record_stop</li> </ul>"},{"location":"crpackage/nodes/#rover_controller","title":"rover_controller","text":"<p><code>Current</code> Controls the robot and its state with respect to a wide range of input sources and publishes a wide range of data for other nodes to use</p> <p>Publications</p> <ul> <li>temp_pose</li> <li>/teleop_keypress</li> <li>/destination</li> <li>/web/camera</li> <li>/web/state</li> <li>/web/map</li> <li>/cmd_vel</li> </ul> <p>Subscriptions</p> <ul> <li>/raspicam_node/image/compressed</li> <li>/web/teleop</li> <li>/web/destination</li> <li>/destination</li> </ul>"},{"location":"crpackage/nodes/#scan_filter","title":"scan_filter","text":"<p><code>Current</code> applies a filter to scan data to ignore the structural posts of the mutant</p> <p>Publications</p> <ul> <li>/scan_filter</li> </ul> <p>Subscriptions</p> <ul> <li>scan</li> </ul>"},{"location":"crpackage/nodes/#state","title":"state","text":"<p><code>Current</code> Handles and validates requested state changes for legality and publishes relevant information accordingly</p> <p>Publications</p> <ul> <li>/move_base_simple/goal</li> <li>/initialpose</li> <li>/goal_pose_for_fids</li> <li>/state</li> </ul>"},{"location":"crpackage/nodes/#talk","title":"talk","text":"<p><code>Current</code> Uses text to speech to turn strings into audio output</p>"},{"location":"crpackage/nodes/#turtlebot_teleop","title":"turtlebot_teleop","text":"<p><code>Current</code> Cancels existing robot goals and allows for manual control of the robot via teleoperation</p> <p>Publications</p> <ul> <li>/cmd_vel_mux/input/teleop</li> </ul> <p>Subscriptions</p> <ul> <li>/web/teleop</li> <li>initialpose</li> </ul>"},{"location":"crpackage/nodes/#whereabouts","title":"whereabouts","text":"<p><code>Dormant</code> Publishes the name of the nearest waypoint when it changes based on data from its subscription</p> <p>Publications</p> <ul> <li>/nearest_waypoint</li> </ul> <p>Subscriptions</p> <ul> <li>/amcl_pose</li> </ul>"},{"location":"crpackage/nodes/#detect_pickup","title":"detect_pickup","text":"<p><code>Current</code> Uses IMU accelerometer data to decide whether the robot has been lifted, and when it has been placed on the ground.</p> <p>Publications</p> <ul> <li>/airborne</li> </ul> <p>Subscriptions</p> <ul> <li>/imu</li> </ul>"},{"location":"crpackage/nodes/#voice_destination_pub","title":"voice_destination_pub","text":"<p><code>Current</code> takes information from the alexa webhook, and if it involves going to a destination, publishes the goal pose of the specified destination.</p> <p>Publications</p> <ul> <li>/destination</li> </ul> <p>Subscriptions</p> <ul> <li>/voice_intents</li> </ul>"},{"location":"crpackage/nodes/#hand_gesture","title":"hand_gesture","text":"<p><code>Current</code> only slightly usable in demo pauses navigation for ten seconds if it receives signal that a hand is in view of the camera.</p> <p>Publications</p> <ul> <li>/destination</li> </ul> <p>Subscriptions</p> <ul> <li>/destination</li> <li>/hand_command</li> </ul>"},{"location":"crpackage/nodes/#go_to_person","title":"go_to_person","text":"<p><code>Current</code> only slightly usable in demo spins, searching for recognized person, then stops.</p> <p>Publications</p> <ul> <li>/destination</li> <li>/cmd_vel</li> </ul> <p>Subscriptions</p> <ul> <li>/odom</li> <li>/face_detection</li> <li>/has_package</li> </ul>"},{"location":"crpackage/state-management-services/","title":"State Management","text":""},{"location":"crpackage/state-management-services/#overview","title":"Overview","text":"<p>This week, we built an enum, <code>all_states.py</code> to represent the varying states of the robot, and two ROS services: StateChange manages state changes, as well as any associated publishes, and StateQuery reports the current state of the robot. Both services are hosted in the <code>state.py</code> node.</p> <p>While the robot's states aren't too complicated yet, our goal was to create a modular architecture to make it easy to add new states as the robot's functionality expands.</p>"},{"location":"crpackage/state-management-services/#statequery","title":"StateQuery","text":"<p><code>StateQuery</code> is a service that takes no arguments, and returns the state of the robot as the string value of the appropriate enum.</p>"},{"location":"crpackage/state-management-services/#all_statespy","title":"all_states.py","text":"<p><code>all_states.py</code> serves two purposes: It defines a series of enums for the various states of the robot, and it defines methods to act as the service clients. In any node that needs to request a state change or get the current state of the robot, add <code>from all_states import *</code> to gain access to the enums (type <code>State</code>) and the two methods.</p> <ul> <li><code>get_state</code> acts as a client method for the <code>StateQuery</code> service, taking no parameters and returning the string value of the robot's state.</li> <li><code>change_state</code> acts as a client method for the <code>StateChange</code> service, taking up to three parameters and returning a boolean value of whether or not the state change was legal:</li> <li><code>new_state</code>: The string value of the desired new state</li> <li><code>to_say</code>: A message to be said by <code>talk.py</code> upon the state change</li> <li><code>pose_to_pub</code>: A <code>Pose</code> to be published, if the new state is either <code>NAVIGATING</code> or <code>LOCALIZING</code></li> </ul>"},{"location":"crpackage/state-management-services/#statepy","title":"state.py","text":"<p><code>state.py</code> contains the code needed to keep track of the robot's current state and facilitate state changes. It includes the <code>is_legal</code> method, which contains a dict mapping every state to an array of the states that could legally follow it.</p> <p>If an illegal state change is requested, the current state of the robot is set to <code>States.ILLEGAL_STATE_CHANGE</code>.</p>"},{"location":"crpackage/state-management-services/#ari-carr-and-ben-albert-11142018","title":"Ari Carr and Ben Albert 11/14/2018","text":""},{"location":"crpackage/campus-rover-4/progress-report/","title":"Where the Campus Rover 4 Project concluded","text":"<p>Due to novel Coronavirus COVID-19, the efforts of the CR4 team in spring 2020 was cut short. In about two months, we were able to complete quite a bit but unable to get the new rover to a stable and usable state for future students. This document lists what was finished, and elaborates on what remains to be done.</p>"},{"location":"crpackage/campus-rover-4/progress-report/#what-was-done","title":"What Was Done","text":"<ol> <li>The biggest endeavor of this project was to use the <code>Diff_Drive_Controller</code> to produce stable and reliable motor movement (in tandem with a PID loop on the Tivac board) and odometry.</li> <li>the embedded ROS node on Tivac publishes IMU and Sonar topics, but their data is unreliable.</li> <li>a basic urdf was constructed</li> <li>basic launch files created for robot bringup</li> <li>depth camera and lidar fusion as a point cloud</li> </ol>"},{"location":"crpackage/campus-rover-4/progress-report/#what-needs-to-be-completed","title":"WHat needs to be completed","text":"<ul> <li>PID gains need to be more finely tuned. Refer to motor.h for the defines of the gains</li> <li>IMU should be calibrated more finely. Perhaps DMP should be used?</li> <li><code>Diff_Drive</code> published NAN in odoms by default. perhaps some of the arrays in hw_interface.cpp need to be initialized with 0's?</li> <li>attach a more reliable lidar and prove that SLAM works with diff_drive</li> <li>finish tuning navigation params</li> <li>create a more deatiled robot model and urdf (perhaps usinf xacro) in a CAD or other modeling softawre (blender?)</li> <li>build a sinple system that can detect when the battery is at low levels and indicate to the user in some way (a beeping noise, a topic, a light, turning off the motors, etc)</li> </ul> <p>We hope that this is a substantial base with a clear direction to move forward with, once the time is right. When resuming work, please go to the <code>hardware_interface</code> branch of <code>rover_4_core</code>, which is the latest branch.</p>"},{"location":"crpackage/campus-rover-4/tivac-setup/","title":"Tivac setup","text":"<p>title: Working with TIVAC description: Project based on Chefbot status: dated modified: jun-14-2024 version: 2 --</p>"},{"location":"crpackage/campus-rover-4/tivac-setup/#working-with-the-low-level-of-chefbot","title":"Working with the low-level of \"chefbot\"","text":"<p>The campus rover 4 prototype is based off of the \"chefbot\" from the textbook \"Learning Robotics Using Python\" by Lentin Joseph. Rather than using an OpenCR board like a turtlebot3, it uses a Tiva C Launchpad. To write code for the Tiva C, we need two things:</p> <ol> <li>The Energia IDE to write our code, compile it and upload it to the board.</li> <li>rosserial_tivac to allow the Tiva C to act like a ROS node.</li> </ol>"},{"location":"crpackage/campus-rover-4/tivac-setup/#step-1-download-the-energia-ide","title":"Step 1. Download the Energia IDE","text":"<p>go to The Energia Download Page to get the IDE. Then, choose where to extract the files. You can put them wherever you like, I chose to keep them in my Downlaods folder, so for me the path to the IDE and it's related files/ folders is <code>~/Downloads/energia-1.8.10E23-linux64/energia-1.8.10E23</code>, because my version of energia is 1.8.10E23. In the future, your version may be different.</p> <p>To make it easy to open the Energia IDE, I added this alias to my <code>.bashrc</code>:</p> <pre><code>alias energiaIDE='cd ~/Downloads/energia-1.8.10E23-linux64/energia-1.8.10E23 &amp;&amp; ./energia'\n</code></pre>"},{"location":"crpackage/campus-rover-4/tivac-setup/#step-2-download-and-install-rosserial_tivac","title":"Step 2: Download and install rosserial_tivac","text":"<p>We need to download, build and install the rosserial_tivac package.</p> <p>Run these five commands in your terminal:</p> <pre><code>cd ~/catkin_ws/src\ngit clone https://github.com/vmatos/rosserial_tivac.git\ncd ~/catkin_ws\ncatkin_make\ncatkin_make install\n</code></pre> <p>Add this line to your <code>.bashrc</code> (preferably near similar lines that were automatically added when ROS was installed):</p> <pre><code>source ~/catkin_ws/install/setup.bash\n</code></pre>"},{"location":"crpackage/campus-rover-4/tivac-setup/#step-3-enable-energia-to-complie-with-ros","title":"Step 3: Enable energia to complie with ROS","text":"<p>In the terminal, cd to where energia is. So, because my Energia is in the Downloads directory, I input:</p> <pre><code>cd ~/Downloads/energia-1.8.10E23-linux64/energia-1.8.10E23\n</code></pre> <p><code>ls</code> to see the files and subdirectories in the folder. There should be a <code>libraries</code> directory. Run this command:</p> <pre><code>rosrun rosserial_tivac make_libraries_energia libraries/\n</code></pre> <p>If that command completes without error, you should find the directory <code>ros_lib</code> inside <code>libraries</code>. Congratulations! You can now turn your Tiva C launchpad into a ROS node.</p> <p>This guide up to this point was adapted from here</p>"},{"location":"crpackage/campus-rover-4/tivac-setup/#step-4-configure-energia-for-tiva-c","title":"Step 4: Configure Energia for Tiva C","text":"<p>Open Energia. From the Menu at the top, Select <code>Tools</code>, then <code>Boards</code>, then <code>Boards Manager</code>.</p> <p>In the Boards Manager, scroll down and select <code>Energia TivaC boards</code>, and install it.</p> <p>Under <code>Tools/Boards</code> again, select <code>LaunchPad (Tiva C) tm4c123 (80MHz)</code></p> <p>Now, plug the Tiva C into your PC via USB. Now, under <code>Tools</code> you should now be able to select <code>Ports</code>, and the only option should be <code>/dev/ttyACM0</code>. Select it.</p> <p>You are now configured to compile and upload your code to the Tiva C board by using the <code>Upload</code> button (it looks like an arrow pointing to the right)</p>"},{"location":"crpackage/campus-rover-4/tivac-setup/#step-5-enabling-your-computer-to-send-data-to-the-tivac","title":"Step 5: enabling your computer to send data to the TivaC","text":"<ol> <li>Download the Ti udev rules</li> <li>in a terminal, cd to where you downloaded the udev rules. then, move them using this command: <code>sudo mv 71-ti-permissions.rules /etc/udev/rules.d/</code></li> <li>restart the udev service using this command: <code>sudo service udev restart</code></li> </ol> <p>PRO TIP: Follow the same three steps above on the robot's raspberry pi to give rosserial access to communicate with the tivac. (a restart may be required)</p>"},{"location":"crpackage/campus-rover-4/tivac-setup/#step-6-running-the-node-and-communicating-with-ros","title":"Step 6: running the node and communicating with ROS","text":"<p>For the node on your Tiva C to communicate with ROS, the computer it is attached to must be running the <code>serial_node</code> from the <code>rosserial_python</code> package. This node is installed by default along with ROS. Run it independantly with <code>rosrun</code>, or add it to your project's launch file.</p>"},{"location":"crpackage/demos/demo-script-fall-2018/","title":"Demo Script","text":"<ul> <li>Pito Salas, November 2018, pitosalas@brandeis.edu</li> </ul>"},{"location":"crpackage/demos/demo-script-fall-2018/#setup","title":"Setup","text":"<ul> <li>Basement space is set up like an office</li> <li>We have to update the map to include fake hallways and offices</li> <li>Lines will be taped to the floor to indicate the walls</li> <li>There will be four \"offices\", two \"hallways\", one \"lab\", and one \"reception\"</li> <li>We will move furniture into the \"rooms\" but navigation will be based on the </li> <li>Map is updated to match the tapes on the floor</li> <li>Locations are added to the location table</li> </ul>"},{"location":"crpackage/demos/demo-script-fall-2018/#demo","title":"Demo","text":"<ol> <li>A package has been delivered to reception</li> <li>The robot is sitting in its charging station</li> <li>Using a computer sitting on his desk, the receptionist summons the robot over </li> <li>When the robot arrives at the receptionist it says: \"hello Jo, did you call?\"</li> <li>The receptionish gives the robot the package and uses the laptop to send the robot to Jane's office</li> <li>THere are some people chatting and in the way</li> <li>As the robot navigates around them it says, \"Excuse me I am delivering a package to Jane's office\"</li> <li>Jane accepts the package and the robot starts driving back to its charging station</li> <li>An evil genius picks the robot up and starts walking it to the Evil Genius office.</li> <li>When placed back on the floor the robot realizes that it was kidnapped</li> <li>It recovers by going back into the hall and finding the nearby fiducial</li> <li>It then continues back to the charging station, awaiting it's next mission</li> <li>Along the way the robot sees Alex and says \"Hello Alex, how's it going?\"</li> <li>The robot comes into the charging station and relaxes until the next mission</li> <li></li> </ol>"},{"location":"crpackage/demos/demo-script-fall-2018/#pito-salas","title":"Pito Salas","text":""},{"location":"crpackage/demos/gen2-demo-instructions/","title":"Demo Setup","text":""},{"location":"crpackage/demos/gen2-demo-instructions/#intro","title":"Intro","text":"<p>Basically all the steps we think are needed to get the Fall 208 Demo Script to work. This was gathered by the team and then experimented and revised further by Pito during the post-semester break.</p>"},{"location":"crpackage/demos/gen2-demo-instructions/#onboard-laptop-steps","title":"Onboard Laptop Steps:","text":"<ul> <li>SSH into the robot\u2019s onboard laptop (turtlebot@129.64.243.64)</li> <li>Start a roscore</li> <li>Do a bringup for the onboard laptop: \u201croslaunch cr_ros campus_rover.launch\u201d</li> <li>Wait until the bringup finishes. Look for \u201cOdom received\u201d message</li> </ul>"},{"location":"crpackage/demos/gen2-demo-instructions/#on-another-machine-needs-to-be-more-powerful-for-this-than-for-the-web-app","title":"On another machine (needs to be more powerful for this than for the web app)","text":"<ul> <li>Run <code>roslaunch cr_ros offboard_launch.launch</code></li> <li>Wait until the bringup finishes. Look for \u201cOdom received\u201d message</li> </ul>"},{"location":"crpackage/demos/gen2-demo-instructions/#on-a-third-machine","title":"On a third machine","text":"<ul> <li>Install Flask</li> <li>Clone cr_web repo</li> <li>cd into cr_web</li> <li>Make sure to bring up TB2 (with cr_ros), or run a local roscore</li> <li>export ROS_MASTERURI=_http://______</li> <li>export FLASK_APP=rover_app</li> <li>flask run --no-reload --host=0.0.0.0 --with-threads</li> </ul> <p>Note: --host=0.0.0.0 and --with-threads are only needed if other client machines need to access the hosted app.</p> <ul> <li>Go to localhost:5000/, or replace localhost with the other machine\u2019s IP if you\u2019re accessing it from a client machine. (edited)</li> </ul>"},{"location":"crpackage/demos/gen2-demo-instructions/#known-dependencies","title":"Known dependencies:","text":"<ul> <li>aruco_detect (for fiducial_msgs import error)</li> <li>pip install face_recognition</li> <li>Kobuki docking might be part of the general TB2 install or may need to be separate</li> </ul>"},{"location":"crpackage/demos/gen2-demo-instructions/#on-offboard-computer","title":"On offboard computer","text":"<ul> <li>sudo apt-get install ros-kinetic-moveit-ros-visualization</li> <li>make sure that ROS_MASTER_URI=http://129.64.243.64:11311 (or whereever roscore is runnung)</li> </ul>"},{"location":"crpackage/mutant/","title":"mutant","text":""},{"location":"crpackage/mutant/mutant-description/","title":"The Mutant","text":"<p>There are currently two versions of the mutant in existence. They both have the same footprint, that of a Turtlebot 3 Waffle.</p> <p>One of those robots, has large green wheels but operates as a standard Turtlebot3 with ROS etc. The only difference is in the OpenCR firmware, where the wheel diameter, robot radius, etc. fields are modified to account for the different chassis and wheels. It is fully operational.</p> <p>The other robot, has blue wheels of similar size to the standard TB3 wheels. This model is where new motors/motor controllers are tested. The current combination are Uxcell 600 RPM encoder gear motors. When installing/wiring new motors ALWAYS GO OFF OF THE PCB (printed circuit board). Meaning follow whatever is actually written on the back of the motor. They are plugged in and controlled by a RoboClaw motor controller. For our purposes, it is communicating over packet serial with an Arduino, which gives it serial commands. The RoboClaw is well documented and has a functional Arduino library that is used for most of its operation. The RoboClaw comes with a Windows software interface that allows for simple initial programming and Autotuning of the PID control. The robot is equipped with a latching Estop button. The robot does not currently drive particularly straight but that does not seem to be the result of the speed control (the encoder counts stay similar). Rather it is likely a result of the alignment of the wheels. To attach the larger wheels to these motors, an adapter needs to be made or found from the smaller hex of the driver pin to a larger size. The RoboClaw controller is top of the line across the board. To interface with ROS, the best approaches would be to do so through an Arduino using ROSSerial or directly over MicroUSB, using the existing ROS-RoboClaw libraries.</p> <p>Uxcell Motors</p> <p>RoboClaw Manual</p> <p>Wiring diagrams and pictures:</p> <p></p> <p></p> <p></p>"},{"location":"crpackage/mutant/mutant-description/#aaron-marks","title":"Aaron Marks","text":""},{"location":"crpackage/mutant/mutant-usage/","title":"Mutant","text":"<p>To launch Mutant, follow these steps:</p> <ol> <li>Ensure that mutant has the most recent version of <code>cr_ros_2</code>. This can be accomplished by running <code>roscd cr_ros_2</code> and then <code>gp</code>.</li> <li>SSH into the mutant and run <code>bu-mutant</code>. This will launch the mutant onboard bringup.</li> <li>On your local machine (again after making sure that you have the most recent version of <code>cr_ros_2</code>, run <code>roslaunch cr_ros_2 mtnt_onb_rpicam.launch</code>. This command will start the web app, and you can proceed from there.</li> </ol>"},{"location":"crpackage/mutant/mutant-usage/#longterm-troubleshooting","title":"Longterm Troubleshooting","text":"<p>This section should be updated as problems arise and their solutions are discovered</p>"},{"location":"crpackage/mutant/mutant-usage/#ssh-is-slowunresponsivewont-connect-etc","title":"SSH is slow/unresponsive/won't connect, etc","text":"<p>We've found that forcing powercycles with the switch on the OpenCR board can be detrimental to the robot's ability to SSH. We recommend running <code>sudo poweroff</code> every time you want to powercycle, if possible. Give the robot ample time to fully shut down before turning it back on. Usually, when turning the robot back on, waiting for the Echo Dot to fully turn on is a good indicator of when the robot will be ready to SSH - if the Echo is powered through the Raspberry pi board. Another indicator is that if the echo is setup to communicate with the robot via bluetooth, then the Echo will say \"now connected to mutant\". This means the robot is ready. (please note - even though the Echo says it is connected via bluetooth, the raspberry pi does not default to use the Echo as an audio sink, so it will not play audio from the robot.)</p> <p>Another solution we have found is to disable DNS in SSH settings on the robot. Go to <code>etc/ssh</code> and then open the config file with <code>sudo nano sshd_config</code>. If there is a line that says <code>UseDNS yes</code> then change the <code>yes</code> to <code>no</code>. If <code>UseDNS</code> is not present in the file, then add the line <code>UseDNS no</code> to the bottom of the file.</p>"},{"location":"crpackage/mutant/mutant-usage/#the-amazon-echo-attached-to-the-robot-is-in-an-infinite-boot-loop","title":"The Amazon Echo attached to the robot is in an infinite boot loop","text":"<p>This is probably because the volume was turned up too high, and the raspberry pi cannot supply enough power. Plug the Echo into a wall outlet, turn the volume down, and then plug it back into the robot. Rule of thumb: keep the echo's volume no greater than 70%.</p>"},{"location":"crpackage/mutant/mutant-usage/#one-of-mutants-wheels-isnt-spinning","title":"One of mutant's wheels isn't spinning","text":"<p>Turn the robot all the way off (this means powering off the reaspberry pi, then switching the openCR board off as well), then turn it back on. If the wheel continues to not spin after this, then consult the lab's resident roboticist, Charlie.</p>"},{"location":"crpackage/mutant/mutant-usage/#the-robot-isnt-working-and-i-have-literally-no-idea-why","title":"The robot isn't working and I have literally no idea why","text":"<ol> <li>Shut down and restart roscore.</li> <li>Make sure everything is namespaced correctly (on your local machine) - see above for namespacing troubleshooting.</li> <li>Check that the battery is fully charged.</li> <li>Your node may have crashed right off the bat - check <code>rqt_graph</code> and check that all nodes that are supposed to be communicating with each other are actually communicating.</li> </ol>"},{"location":"crpackage/mutant/mutant-usage/#in-rviz-the-robot-appears-to-be-jumping-around-and-wont-get-any-sort-of-pose","title":"In RVIZ, the robot appears to be jumping around and won't get any sort of pose","text":"<p>This is usually caused by interference from another robot. Even with namespacing, another robot running on the roscore can cause interference specifically on rviz. We have determined that the likely cause of this is because the odom tf (transform) is not namespaced.</p>"},{"location":"crpackage/mutant/mutant-usage/#the-camera-feed-from-the-raspicam-is-flipped","title":"The camera feed from the raspicam is flipped","text":"<ol> <li>Type <code>rosrun rqt_reconfigure rqt_reconfigure</code> into the command line.</li> <li>Click the boxes to flip the image hortizontally/vertically.</li> <li>To check whether the image is flipped correctly, just run <code>rqt_imageview</code>.</li> </ol> <p>For further details on the raspicam, look at Hardware - Raspberry Pi Camera page in the lab notebook.</p>"},{"location":"crpackage/mutant/mutant-usage/#nodes-arent-subscribing-to-the-right-topics","title":"Nodes aren't subscribing to the right topics","text":"<p>The best way to debug topic communication is through rqt_graph in the terminal. This will create a visual representation of all nodes and topics currently in use. There are two setting to toggle:</p> <ul> <li>First, in the upper left, select Nodes/Topics (all) (the default setting is Nodes only)</li> <li>Next, in the check box bar, make sure it is grouped by namespace and that dead sinks, leaf topics and unreachable topics are un-hidden.</li> </ul> <p>Now it's important to note that nodes appear on the graph as ovals, and topics appear in the graph as boxes. hovering over a node or topic will highlight it and all topics/ nodes connected to it. This will help to show whether a node is subscribing to and publishing all the nodes that it is expected to.</p> <p>Based on the rqt graph information, update your topic names in your nodes and launch files to match what you expect.</p>"},{"location":"crpackage/mutant/mutant-usage/#namespacing-a-topic-from-a-node-that-you-cant-edit","title":"Namespacing a topic from a node that you can't edit","text":"<p>Some nodes automatically namespace their published topics, but some don't. This can be an annoyance when you desire a mutli-robot setup. Fear not, it is possible to namespace topics that aren't auto-namespaced. There are two ways to do this, and both require some launch file trickery. One way is to declare a topic name, then to remap the default topic name to your new one. The below example is used in the file move_base_mutant.launch</p> <pre><code>&lt;arg name=\"cmd_vel_topic\" default=\"/$(env ROS_NAMESPACE)/cmd_vel\" /&gt;\n...\n&lt;remap from=\"cmd_vel\" to=\"$(arg cmd_vel_topic)\"/&gt;\n</code></pre> <p>The new topic name argument can be declared anywhere in the launch file. The remapping must occur nested within the node tag of the node which publishes the topic you want renamed. This method has been used and approved by the students of gen3.</p> <p>The other way is to use a group tag, and declare a namespace for the entire group. gen3 has not tested this method, but there does exist some documentation about it on the ROS wiki.</p> <p>Gen3's preferred method of namespacing nodes that we have written ourselves is as follows: Before initializing any publishers or subscribers, make sure this line of code is present: <code>ns = rospy.get_namespace()</code> Then, use python string formatting in all your publishers and subscribers to namespace your topics: <code>cmd_pub = rospy.Publisher('{}cmd_vel'.format(ns), . . .)</code> <code>ns</code> will contain both backslashes needed for the topic to conform to ROS's topic formatting guidelines, e.g. the publisher above will publish to the topic <code>/namespace/cmd_vel</code>.</p>"},{"location":"crpackage/mutant/mutant-usage/#list-of-known-nodes-that-dont-get-automatically-namespaced","title":"list of known nodes that don't get automatically namespaced","text":"<ul> <li>/fiducial_transforms (from the node aruco_detect)</li> <li>/diagnostics (from turtlebot3_core)</li> <li>/cmd_vel (from move_base)</li> <li>Any and all transforms</li> </ul>"},{"location":"crpackage/mutant/mutant-usage/#configuring-the-size-of-the-wheels-within-the-opencr-firmware","title":"Configuring the size of the wheels within the OpenCR firmware","text":"<p>Many components of Turtlebot3 software depend on knowing the size of wheels of the robot. Some examples include Odometry, cmd_vel (the turtlebot core), and move_base. By default, turtlebot3 wheels are 6.6cm in diameter. Mutant has 10cm diameter wheels. If you use larger wheels, but the software believes it has smaller wheels, then movement behavior will not be as expected.</p> <p>On your remote pc, follow steps 4.1.1 through 4.1.6 of this Robotis e-manual guide to install the arduino IDE and configure it to work with the OpenCR board. Please note that as of May 2019, the latest version of the Arduino IDE is 1.8.9, but the guide displays version 1.6.4.</p> <p>In the IDE, go to <code>File</code> --&gt; <code>Examples</code> --&gt; <code>TurtleBot3</code> --&gt; <code>turtlebot3_waffle</code> (or <code>turtlebot3_burger</code> if that better fits the form factor of your mutant TB3) --&gt; <code>turtlebot3_core</code>. This will open three files: <code>turtlebot3_core</code>, <code>turtlebot3_core_config.h</code> and <code>turtlebot3_waffle.h</code>. Go to <code>turtlebot3_waffle.h</code>. You will see that this file defines a number of characteristics of a robot, including wheel radius! Edit your wheel radius variable, then save your work - two windows will pop up. In the first one, click \"ok\" and in the second, click \"save\" - don't edit anything!</p> <p>Now it's time to upload the edited firmware to the OpenCR board. This is actually not too difficult - first, unplug the usb cable that connects the OpenCR board to the Raspberry Pi (or whatever SBC your mutant is using), and plug it into your remote PC. You may have noticed that you weren't able to select a port in step 4.1.5.3 of the emanual instructions - now that the OpenCR board is connected, you should be able to select the port. Once you've done that, click the upload button at the top of the IDE to upload the firmware to your robot. Once the upload is complete, your firmware should be updated and your robot should behave as expected in movement-based tasks. To test, make the robot move forward at 0.1 m/s for 10 seconds - it should travel about a meter. If not, your firmware may not have been updated properly or your wheel measurements may be incorrect.</p>"},{"location":"crpackage/mutant/mutant-usage/#what-if-bashrc-accidentally-gets-deleted","title":"What if .bashrc (accidentally) gets deleted","text":"<p>Restoring <code>.bashrc</code> is not very difficult.</p> <p>First, type <code>/bin/cp /etc/skel/.bashrc ~/</code> into your terminal. This will replace a corrupt .bashrc file with the default .bashrc. Then, try <code>ls -a</code> in your user directory to view all files, including hidden ones, such as .bashrc. If a file such as <code>.bashrc.swp</code> appears, delete it with <code>rm .bashrc.swp</code>. Continue removing files that look related to .bashrc until <code>nano ~/.bashrc</code> properly opens the file without giving an error. Now you'll have to restore all the lines at the bottom of .bashrc that were added when ROS was installed. Fortunately, there are many computers in the lab and they all have almost identical .bashrc files! ssh from one of those computers to the one that you are restoring, copy the lines that need to be re-added, and then edit them to fit the computer that the .bashrc file is located on (things to consider: namespace, ip address, aliasas that are useful on the given machine) Don't forget to <code>source ~/.bashrc</code> after you are done editing the file, so the changes you made can take effect in the terminal!</p> <p>Now don't make the same mistake twice.</p>"},{"location":"crpackage/mutant/mutantsetup/","title":"Mutant Setup","text":"<p>In the event that a new mutant ever has to be set up, or software problems require mutant to be reset, here are the necessary tips to installing everything that is needed to operate using the most recent campus rover ros package.</p>"},{"location":"crpackage/mutant/mutantsetup/#installing-ros","title":"Installing ros","text":"<p>The Robotis Emanual serves as an adequate guide to getting 95% of the way to setting up ROS on a mutant turtlebot. There are a few divergences from their instructions, though:</p> <ul> <li>In part 3 of section 6.2.1.1, you could easily miss the note about installing what is needed to use a raspi camera. Either do not miss it (it is right below the first block of terminal commands) or check out our labnotebook page on configuring the raspberry pi camera.</li> <li>just below the camera hint, the emanual instructs to use these commands to remove unneeded packages:</li> </ul> <pre><code>cd ~/catkin_ws/src/turtlebot3\nsudo rm -r turtlebot3_description/ turtlebot3_teleop/ turtlebot3_navigation/ turtlebot3_slam/ turtlebot3_example/\n</code></pre> <p>However, slam and navigation are actually useful to us, so use these commands instead:</p> <pre><code>cd ~/catkin_ws/src/turtlebot3\nsudo rm -r turtlebot3_description/ turtlebot3_teleop/ turtlebot3_example/\n</code></pre> <ul> <li>Once you have finished the emanual SBC setup, you still need to install a few dependencies that Robotis assumes you will only use on a remote pc. For your convenience, run this command:</li> </ul> <pre><code>sudo apt-get install ros-kinetic-laser-proc ros-kinetic-rgbd-launch ros-kinetic-depthimage-to-laserscan ros-kinetic-rosserial-arduino ros-kinetic-rosserial-python ros-kinetic-rosserial-server ros-kinetic-rosserial-client ros-kinetic-rosserial-msgs ros-kinetic-amcl ros-kinetic-map-server ros-kinetic-move-base ros-kinetic-urdf ros-kinetic-xacro ros-kinetic-compressed-image-transport ros-kinetic-gmapping ros-kinetic-navigation ros-kinetic-interactive-markers\n</code></pre> <p>If you are curious as to where this came from, it is an edited version of the first command of emanual section 6.1.3, under PC setup.</p>"},{"location":"crpackage/mutant/mutantsetup/#on-the-remote-pc","title":"On the remote PC","text":"<p>you will need:</p> <ul> <li>the fiducial package <code>sudo apt-get install ros-kinetic-fiducials</code></li> <li>cr_ros_2 github repo on branch mutant_transfer</li> <li>cr_web github repo on branch mutant</li> <li>the turtlebot3_navigation package, which should come with turtlebot3.</li> <li>google chrome (for the web app - you can use another browser, but that would require editing a shell script in the cr_web package)</li> <li>flask module for python</li> </ul> <p>Google how to get Chrome and flask. cr_ros_2 and cr_web are repos in the campus rover github community.</p>"},{"location":"crpackage/mutant/mutantsetup/#using-git-and-github","title":"Using Git and Github","text":"<p>If this class is your first time using github - don't worry! Though it may seem mildly confusing and daunting at first, it will eventually become your best friend. There's a pretty good guide called git - the simple guide - no deep shit! which can walk you through the basics of using git in the terminal. Here's a bit of a short guide to the commands we have used most in gen3:</p> <ul> <li><code>git clone</code> is how you will initially pull a repository off of Github</li> <li>in a repository's main directory in your terminal, <code>gs</code> (or <code>git status</code>) is a super useful command that will show you all the files you have edited.</li> <li><code>git add</code> --&gt; <code>git commit -m</code> --&gt; <code>git push</code> is how you will be updating your repos on github. Pro Tip: if you've edited multiple files in a subdirectory, for example in src, then you can type <code>git add src</code> to add all modified files in src, rather than typing each file individually.</li> <li>always do a pull before a push if you think someone else has made edits to your repo.</li> <li>if you've made changes locally that you don't want to keep, <code>git reset --hard</code> will revert back to your last pull or clone.</li> </ul>"},{"location":"crpackage/mutant/raspicam/","title":"Raspberry Pi Camera","text":""},{"location":"crpackage/mutant/raspicam/#setup","title":"Setup","text":"<p>This Robotis emanual page describes how to setup the Raspberry Pi camera to be used with Turtlebot3.</p> <p>Here is a streamlined guide to quickly get a raspi camera working with a TB3. This entire process may take a few minutes - worst case, if you have to fix apt-get errors, upwards of 30 minutes.</p> <ol> <li><code>sudo raspi-config</code> in the TB3's terminal. Navigate to option 3: Interfacing options. The first option in the sub-menu is camera - select it, then select yes when prompet to enable camera interfacing. Then, navigate to finish and reboot the robot for the change to take effect.</li> <li>do a <code>sudo apt-get update</code> and <code>sudo apt-get upgrade</code> to make sure there are no errors. If update throws a missing pubkey error, then record the pubkey and use this command: <code>sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys &lt;PUBKEY&gt;</code> where  is the pubkey that you recorded. once the pubkey is added, update &amp; upgrade. If there are no errors, continue.</li> <li>run the following two commands to add Ubiquity Robotic's repos to apt</li> </ol> <pre><code>sudo sh -c 'echo \"deb https://packages.ubiquityrobotics.com/ubuntu/ubiquity xenial main\" &gt; /etc/apt/sources.list.d/ubiquity-latest.list'\nsudo apt-key adv --keyserver hkp://ha.pool.sks-keyservers.net:80 --recv-key C3032ED8\n</code></pre> <ol> <li>update &amp; upgrade again.</li> <li><code>sudo apt-get install ros-kinetic-raspicam-node</code></li> <li>catkin make</li> <li>if catkin_make fails due to missing diagnostics, install this: <code>sudo apt-get install ros-kinetic-diagnostic-updater</code></li> </ol>"},{"location":"crpackage/mutant/raspicam/#how-to-launch","title":"How to launch","text":"<p><code>roslaunch turtlebot3_bringup turtlebot3_rpicamera.launch</code> will launch the camera alone at a resolution of 640x480. Alternatively, you can also use <code>roslaunch raspicam_node camerav1_1280x720.launch</code> to launch at a higher resolution. To include in a custom launch file, consider using a command like this:</p> <pre><code>&lt;node pkg=\"raspicam_node\" type=\"raspicam_node\" name=\"raspicam_node\" output=\"screen\"&gt;\n  &lt;param name=\"camera_info_url\" value=\"package://turtlebot3_bringup/camera_info/turtlebot3_rpicamera.yaml\"/&gt;\n  &lt;param name=\"width\" value=\"640\"/&gt;\n  &lt;param name=\"height\" value=\"480\"/&gt;\n  &lt;param name=\"framerate\" value=\"10\"/&gt;\n  &lt;param name=\"enable_raw\" value=\"true\"/&gt;\n  &lt;param name=\"camera_frame_id\" value=\"camera\"/&gt;\n&lt;/node&gt;\n</code></pre>"},{"location":"crpackage/mutant/raspicam/#handy-parameters","title":"handy parameters","text":"<p>The following parameters can be edited in a launch file that launches the Raspi cam to alter its performance:</p> <ul> <li><code>enable_raw</code> : allows the camera to publish a topic <code>~/image</code>, of topic type <code>sensor_msgs/Image</code> if set to <code>true</code>. If not true, only <code>~/image/compressed</code> will publish (which publishes a topic type <code>sensor_msgs/CompressedImage</code>).</li> <li><code>height</code> and <code>width</code> : change the resolution of the image.</li> <li><code>framerate</code> : changes the rate at which the camera publishes images (maximum 90 fps). Max FPS is also affected by the resolution (higher resolution -&gt; lower max fps)</li> </ul>"},{"location":"crpackage/mutant/raspicam/#useful-commands","title":"useful commands","text":"<ul> <li><code>rqt_image_view</code> : opens a gui where you can select an image topic currently being published and view it from your remote PC.</li> <li><code>rosrun rqt_reconfigure rqt_reconfigure</code> : opens a gui which can edit various raspi settings, such as vertical/ horizontal flipping, image stabilization, and other sliders for various settings</li> </ul>"},{"location":"crpackage/navigation/","title":"Navigation","text":""},{"location":"crpackage/navigation/cpu-usage-and-errors-in-navigation/","title":"CPU Hungry Nodes","text":"<p>During navigation I've run into a lot different errors and warnings. I copied some of the frequent ones here:</p> <pre><code>[ WARN] [1542144731.816164220]: Costmap2DROS transform timeout. Current time: 1542144731.8160, global_pose stamp: 1542144730.2532, tolerance: 1.5000\n[ WARN] [1542144731.816266907]: Could not get robot pose, cancelling reconfiguration\n[ WARN] [1542144732.472418469]: Unable to get starting pose of robot, unable to create global plan\n\n\n[ WARN] [1542144726.841978275]: Map update loop missed its desired rate of 1.0000Hz... the loop actually took 1.2625 seconds\n[ WARN] [1542144737.244639028]: Map update loop missed its desired rate of 5.0000Hz... the loop actually took 0.2093 seconds\n[ WARN] [1542144737.244865990]: Control loop missed its desired rate of 3.0000Hz... the loop actually took 0.8109 seconds\n\n[ERROR] [1542146023.217567014]: Extrapolation Error: Lookup would require extrapolation into the future.  Requested time 1542146022.709356378 but the latest data is at time 1542146021.995298140, when looking up transform from frame [odom] to frame [map]\n[ERROR] [1542146023.217646860]: Global Frame: odom Plan Frame size 1889: map\n[ WARN] [1542146023.217688426]: Could not transform the global plan to the frame of the controller\n[ERROR] [1542146023.217720997]: Could not get local plan\n</code></pre> <p>What I found in common from these problems is that they all have something to do with information loss in the update cycle of different parts of navigation. And this could be caused by computer not having enough processing power for the desired update frequency, which is actually not high at all, like the 1.0000Hz and 5.0000Hz in the warning messages above.</p> <p>Then I found that the laptop's both CPUs' usage is nearly at 100% during navigation. I checked each node one by one. Rviz is very CPU-hungry, when running navigation and rviz together, the CPUs will have nearly 100% usage. But we can avoid using rviz once we have fiducial working. Besides Rviz, several custom nodes made by us are also very CPU-hungry.</p> <p>For now, we are using Dell 11 inch laptop as the onboard computer for Turtlebot. The situation might not be the same if more powerful devices are used. However, generally speaking, in our custom nodes we should avoid pulishing/updating with a frequency that's too high.</p> <p>Also please remember to check CPU usage if you find these errors and warnings again during navigation.</p>"},{"location":"crpackage/navigation/cpu-usage-and-errors-in-navigation/#huaigu-lin-11132018","title":"Huaigu Lin 11/13/2018","text":""},{"location":"crpackage/navigation/fiducials/","title":"Fiducial Integration","text":""},{"location":"crpackage/navigation/fiducials/#overview","title":"Overview","text":""},{"location":"crpackage/navigation/fiducials/#goals","title":"Goals","text":"<p>Fiducials are an attempt to localize a robot without any prior knowledge about location. They use unique images which are easily recognizable by a camera. The precise size and orientation of a fiducial tag in an image can uniquely localize a camera with respect to the tag. By measuring the location of the tag ahead of time, the location of the camera with respect to the global frame can be found.</p>"},{"location":"crpackage/navigation/fiducials/#aruco-tags","title":"Aruco tags","text":"<p>Aruco is a standard for fiducial tags. There are many variants but generally look like this:</p> <p></p>"},{"location":"crpackage/navigation/fiducials/#tag-detection","title":"Tag detection","text":"<p>Ubiquity Robotic's <code>aruco_detect</code> module accurately finds aruco tags in camera images. It relies on a compressed image stream from the robot camera and the camera's specs published as well. <code>aruco_detect</code> publishes a number of outputs, but crucially it sends a <code>geometry_msgs</code> transform relating the fiducial to the camera on the topic <code>/fiducial_transforms</code>. This needs to be inverted to get the transform from camera to tag.</p>"},{"location":"crpackage/navigation/fiducials/#tf-publishing","title":"<code>TF</code> publishing","text":"<p>The transform from camera to fiducial can be combined with a transform between the map and fiducial to find the pose of the camera. This is best done using <code>ROS</code>'s extensive <code>tf</code> tooling. Each fiducial location can be published as a static transform and a <code>TransformListener</code> can find the total transform from map to camera.</p>"},{"location":"crpackage/navigation/fiducials/#usage","title":"Usage","text":"<p><code>aruco_detect</code> is brought up as part of the <code>cr_ros</code> <code>bringup.launch</code> file, but can be independently launched with <code>roslaunch aruco_detect aruco_detect.launch</code> if the proper topics are supplied in the launch file. <code>process_fid_transforms.py</code> takes the output of <code>aruco_detect</code> and publishes pose messages. It is in <code>cr_ros</code> and also in the bringup file. Static <code>tf</code> publishers are also needed for tag position. See \"adding a new tag\" below. One static publisher is needed to correct a 90 offset. This should relate the frame <code>fiducial_camera</code> to <code>rotated_fiducial_camera</code> with a transform of \u03c0/2 in the yaw (<code>rosrun tf static_transform_publisher 0 0 0 1.57079632679 0 0 /fiducial_camera /rotated_fiducial_camera 100</code>).</p>"},{"location":"crpackage/navigation/fiducials/#locating-tag-on-map","title":"Locating tag on map","text":"<p>New tags can be placed on the map and published as static transforms from within the <code>bringup.launch</code> file. To find the <code>x</code>, <code>y</code>, and <code>z</code> position, use a tape measure.</p> <p>The three euler angles describing the angle of the tag are more difficult to determine. To find the first rotation parameter, x, consider the orientation of the fiducial relative to the map. If the fiducial faces north x = 0, if west x = \u03c0/2, if south x = \u03c0, if east x = 3\u03c0/2. The second (y) component accounts for leaning left or right of fiducial on the verical wall. If positioned straight up, it should be set to \u03c0 which is approximately 3. The third (z) component describes how far forward or back the fiducial is oriented. If the wall is vertical, roll = 0. If leaning forward, 0 &lt; z &lt; \u03c0 /2. If leaning backwards, 2\u03c0 &gt; z &gt; 3\u03c0/2.</p> <p></p> <p>x is red, y is green, z is blue</p>"},{"location":"crpackage/navigation/fiducials/#adding-a-new-tag","title":"Adding a new tag","text":"<p>Add a new tag to the bringup. The static publisher accepts x, y, z, for position and either yaw, pitch and roll or a quaternion for rotation, but for your own sake please chose the former.</p>"},{"location":"crpackage/navigation/fiducials/#challenges-in-implementation","title":"Challenges in implementation","text":"<p>Fiducials were very challenging to implement and it's worth spending some time discussing the reasons why and how we can learn from the process.</p> <p>To start, fiducial localization involves complex coordinate transforms from image space to tag-relative space and then to map-relative space. By itself, this would have been fine. It is easy for a computer to calculate the matrix multiplication at the heart of these transforms, but it is hard for a developer to debug. More clarity and better documentation about the output of the tooling would have helped. In addition, the transforms are implemented in quaternions and other complex formats which are difficult to understand in any form and took some time to get used to.</p> <p>A few tools exist which solve \"fiducial slam\" and originally we tried to implement one of those to compute the transforms from image space to map-relative space. These tools are, in fact, not built to solve that problem, but to assist in mapping the fiducials in the first place - a problem less challenging in our use case.</p> <p>The biggest breakthrough came when I began using the built in <code>tf</code> tooling. This allowed me to work with a robust set of tools including <code>rviz</code> for easy debugging. Through this process I was able to see that the <code>y</code> and <code>z</code> axes needed to be swapped and that an inversion of the transform was needed. These were not clear when using other tools, but were at the heart of the strange results were were seeing early on.</p> <p>More familiarity with <code>ROS</code> would have brought me to <code>tf</code>s sooner, but now I have that familiarity for next time. All together, I'm not sure what lesson there is to take away from this. Sometimes hardcore debugging is required.</p>"},{"location":"crpackage/navigation/fiducials/#alexander-feldman-feldmanaygmailcom-thanks-ben-and-ari-too","title":"@Alexander Feldman, feldmanay@gmail.com (thanks @Ben and @Ari too!)","text":""},{"location":"crpackage/navigation/floormapping/","title":"Mapping a floor","text":"<ul> <li>Siyuan Chen, December 2018, sychen1996@brandeis.edu</li> </ul>"},{"location":"crpackage/navigation/floormapping/#steps","title":"Steps","text":"<ol> <li>Measure the static obstacles such as walls, doors, and corners that will not be changed ever. The measurements should be in centimeters.</li> <li>Use keynote to draw the space.</li> </ol>"},{"location":"crpackage/navigation/floormapping/#little-tips","title":"Little Tips","text":"<ol> <li>better to have two people measure the walls because it could be longer than 10 meters(10000 centimeters) that is very hard for one person to do the job.</li> <li>Use a mouse to draw the map is going to save you a lot of time because you can easily drag the lines on the keynote.</li> <li>Be patient. Each map is going to take you about four hours.</li> </ol>"},{"location":"crpackage/navigation/lost-and-found/","title":"Handling Robot Relocation While Navigating","text":""},{"location":"crpackage/navigation/lost-and-found/#overview","title":"Overview","text":"<p>This week, we built a node to handle the problem of fiducial detection - namely, the question of what the robot does when it doesn't know where it is. This would happen in two cases:</p> <ul> <li>Bringup: When the robot is initially started up, it won't know where it is unless it happens to have a fiducial in view already.</li> <li>The \"kidnapped robot problem\": When someone picks up the robot and moves it, its localization won't recognize the new position, so the robot needs to identify that it's been moved.</li> </ul> <p>In both of these cases, the robot must act on its state of being lost by semi-intelligently wandering around the area and avoiding obstacles until it sees a fiducial and can re-localize.</p>"},{"location":"crpackage/navigation/lost-and-found/#new-states","title":"New States","text":"<p>To solve this problem, we first added two new states to <code>state.py</code>: FLYING and LOST. These states will be used to identify when the robot is being picked up and when the robot doesn't know where it is, respectively.</p>"},{"location":"crpackage/navigation/lost-and-found/#the-lost_and_found-node","title":"The <code>lost_and_found</code> node","text":""},{"location":"crpackage/navigation/lost-and-found/#state-changes","title":"State Changes","text":"<p>Becoming lost:</p> <p>The node includes a subscriber to the <code>/mobile_base/events/wheel_drop</code> topic, which publishes a message every time the robot's wheels move up or down. As the wheels' natural state is to be pushed up as the body of the TurtleBot rests on top of them, a message that the wheels have moved down indicates that the robot has been picked up, triggering a change into the FLYING state. Similarly, once the robot is flying, a message that the wheels are up again indicates that the robot has been set down, such that the robot is now LOST and able to start looking for fiducials.</p> <p>Becoming found:</p> <p>The while loop of our wandering is controlled by an if statement designed to catch <code>state != States.LOST</code>. Ideally, the fiducial detection will trigger the localization of the TurtleBot, which will change the state of the TurtleBot to LOCALIZING. Once the state changes, the while loop will break and the node will stop making the TurtleBot wander until LOST is detected again.</p>"},{"location":"crpackage/navigation/lost-and-found/#the-wander-algorithm","title":"The wander algorithm","text":"<p>We ensure maximum camera coverage, for the best odds of finding a fiducial, by having the robot drive in a rectangular outward spiral away from where it had been:</p> <ul> <li>The robot starts by spinning 360\u00ba, then driving a set distance to its right</li> <li>The robot then spins 360\u00ba, turns 90\u00ba to the left, and drives that same distance</li> <li>Until a fiducial is found:</li> <li>The robot spins 360\u00ba</li> <li>The robot turns left and drives for a slightly further distance than last time</li> <li>The robot spins 360\u00ba</li> <li>The robot turns left and drives the same distance as the previous step</li> <li>Repeat, increasing the distance to be driven for the next two turns.</li> </ul> <p>Implementation:</p> <p>In order to ensure the best possible obstacle avoidance in this algorithm, rather than implement the driving ourselves, we send the movements described above to the robot as a series of AMCL goals using the following algorithm:</p> <pre><code>initial_pose = a point in the map's whitespace area\npublish initial_pose\ngoal = initial_pose\noffset = 1\npolarity = -1\nwhile not shutdown:\n    if not lost:\n        reset offset and polarity\n        continue\n\n    spin 360 degrees at a rate of 72 degrees/second\n\n    if goal.x == goal.y:\n        goal.x += offset\n    else:\n        goal.y += offset\n        offset = polarity * (|offset| + 1)\n        polarity *= -1\n    publish goal\n    wait for amcl to complete or a 5-second timeout\n</code></pre>"},{"location":"crpackage/navigation/lost-and-found/#goal-saving","title":"Goal saving","text":"<p>One potential bug arising from AMCL-based wandering is that the robot would forget any AMCL goal it had been working towards when it was kidnapped. To fix this, we have included a <code>/move_base_simple/goal</code> subscriber. Whenever it receives a message, indicating a new AMCL goal, it saves that goal in this node as <code>current_goal</code>.</p> <p>In our <code>flying_or_lost</code> method, which recognizes wheel drops as described above, we have included a check for if the robot's state at the moment of kidnapping was <code>FLYING</code>. If the state was <code>NAVIGATING</code>, such that the robot was in the middle of AMCL, we set <code>lock_current_goal</code> to True, which acts as a flag to indicate that our node should stop saving new incoming goals because our AMCL-based wandering is about to start.</p> <p>Finally, our <code>if get_state() != States.LOST</code> block, which is responsible for resetting the node once wandering is complete, includes a check for <code>lock_current_goal</code>. If <code>lock_current_goal</code> is True, then the robot must have been working towards an AMCL goal prior to the kidnapping, so our node re-publishes that goal with an updated timestamp and the robot can continue its journey.</p>"},{"location":"crpackage/navigation/lost-and-found/#ari-carr-and-jacky-chen-11282018-updated-1212018-with-a-new-wander-algorithm","title":"Ari Carr and Jacky Chen 11/28/2018, updated 12/1/2018 with a new wander algorithm","text":""},{"location":"crpackage/navigation/costmap-clearing/","title":"Costmap Clearing","text":""},{"location":"crpackage/navigation/costmap-clearing/costmap-clearing-part-1/","title":"Costmap Clearing 1","text":"<p>Our objective of this iteration is to find a way to clean Turtlebot's costmap, so any long gone obstacle will not stay on the map, but the Turtlebot should also not run into a transient obstacle like a person or a chair.</p>"},{"location":"crpackage/navigation/costmap-clearing/costmap-clearing-part-1/#packages-enviroments","title":"Packages &amp; Enviroments","text":"<p>We are using <code>roslaunch cr_ros campus_rover.launch</code> command to bring up Turtlebot. This launch file launches amcl with a configuration similar to <code>amcl_demo.launch</code> file in <code>turtlebot_navigation</code> package. Then we run rviz to visualize the static floor plan map and costmap.</p>"},{"location":"crpackage/navigation/costmap-clearing/costmap-clearing-part-1/#known-issues-before-this-iteration","title":"Known Issues Before This Iteration","text":"<p>In previous demos, we found that Turtlebot would successfully mark transient obstacles, such as a person passing by, on its costmap and avoid them. But it failed to unmark them even after they are gone. These marks of long gone obstacles would cause the path planner to avoid them. Eventually Turtlebot would stuck because it cannot find a valid path to its goal.</p>"},{"location":"crpackage/navigation/costmap-clearing/costmap-clearing-part-1/#research-and-findings","title":"Research and Findings","text":"<p>We found a possible pattern and cause for this problem. In this post thread, someone mentions that:</p> <p>\"Costmap2D seems not to \"clear\" the space around the robot if the laser scan range is max.\"</p> <p>We tested this claim. Indeed, when an obstacle in front of Turtlebot is out of max range of its camera sensor, someone who pass through the empty space between the obstacle and Turtlebot's camera would be marked permanently on the costmap. However, if an obstacle is within max range of camera sensor, someone pass through will be marked and then unmarked immediately once this person is gone.</p> <p>The above post thread also mentions an explanation for this behavior:</p> <p>\"Its actually not the costmap that is ignoring max_scan_range values from the laser, its the laser projector that takes a laser scan and turns it into a point cloud. The reason for this is that there is no guarantee that max_scan_range actually corresponds to the laser not seeing anything. It could be due to min_range, a dark object, or another error condition... all the laser knows is that it didn't get a return for one reason or another. \"</p> <p>Based on our experiment and this explanation, a possible solution for the max_range problem could be setting up a scan filter chain. Theoretically when a scan value is \"max_range\", we could replace it with a big number such as 100 (Turtlebot's scan range is 10 meters). However we could not make it work this week, so we will do more experiments in the coming week.</p>"},{"location":"crpackage/navigation/costmap-clearing/costmap-clearing-part-1/#our-implementation-recovery-behavior","title":"Our Implementation -- Recovery Behavior","text":"<p>The <code>campus_rover.launch</code> file includes another launch file <code>move_base.launch.xml</code> from <code>turtlebot_navigation</code> package. In <code>move_base.launch.xml</code>, a <code>move_base</code> node is launched with a bunch of parameters stored in yaml files. This node basically runs navigation stack for Turtlebot and also includes all the costmap drawing/clearing behaviors.</p> <p>What we found out was that in <code>turtlebot_navigation/param/move_base_params.yaml</code>, all the parameters for recovery behaviors were commented out. According to ros documentation on expected robot behaviors, recovery behaviors are an essential part of robot's navigation. When the robot perceive itself as stuck (unable to find a valid path to its goal), it should perform recovery behaviors to clear its costmap and then replan a path.</p> <p>Therefore, we brought back an recovery behavior with the code:</p> <pre><code>recovery_behaviors:\n- name: 'aggressive_reset2'\n  type: 'clear_costmap_recovery/ClearCostmapRecovery'\n\naggressive_reset2:\nreset_distance: 0.0\n</code></pre> <p><code>reset_distance: 0.0</code> means Turtlebot clears its costmap outside of 0.0 radius, so it will clear all the costmaps when it perceive itself as stuck. Based on our experiments and previous experience, Turtlebot was pretty good at dodging obstacles that were not on its costmap, so this \"aggressive\" reset is safe for most occasions, unless Turtlebot is physically surrounded by obstacles that are very close to it, but in this extreme circumstance, \"conservative\" costmap clearing would also be useless because clearing costmaps several meters away would not be enough to unstuck it.</p> <p>We also specified the costmap layer name since <code>obstacle layer</code> is the one contains all the marks of dynamic obstacles:</p> <pre><code>layer_names: [\"obstacle_layer\"]\n</code></pre> <p>These changes would ensure the clearing of costmap when Turtlebot perceive itself as stuck, and it will no longer get stuck by marks of long-gone obstacles.</p>"},{"location":"crpackage/navigation/costmap-clearing/costmap-clearing-part-1/#next-step","title":"Next Step","text":"<p>We will look more into implementing the scan filter, so Turtlebot would immediately unmark a gone obstacle even if the scan is out of max range.</p>"},{"location":"crpackage/navigation/costmap-clearing/costmap-clearing-part-1/#huaigu-lin-jacky-chen-11112018","title":"Huaigu Lin &amp; Jacky Chen 11/11/2018","text":""},{"location":"crpackage/navigation/costmap-clearing/costmap-clearing-part-2/","title":"Costmap Clearing 2","text":"<p>I investigated further with the max_range of camera and found that it was indeed 10 meters. When the camera is more than 10 meters away from an obstacle, the range readings in the <code>/scan</code> topic corresponding to the angle of the obstacle will be <code>nan</code>. Also, when an obstacle is within the minimum range of camera or the surface of the obstacle does not reflect any laser, the laser readings will be <code>nan</code>. These <code>nan</code> readings make the move base think there\u2019s something wrong with laser and will not unmark an obstacle once it\u2019s gone... I wrote a filter node called <code>scan_filter.py</code> which will replace the <code>nan</code> readings with 9.9 (a number slightly smaller than max_range), and publish to a new topic called <code>/scan_filtered</code>. Then I passes an argument to the move base in our launch file to make the cost map in move base subscribe to <code>/scan_filtered</code>. However, amcl should still subscribe to the original <code>/scan</code> topic because localization relies on unfiltered readings.</p> <p>At first I changed all the <code>nan</code> readings to 9.90, but later Alex help me notice that the <code>nan</code> readings at the beginning and end of the array should not be changed, because they correspond to the laser being blocked by robot's body. Therefore I chose not to change these <code>nan</code> readings.</p> <p>Now the robot will immediately unmark an obstacle on cost map once it is gone even the camera is out of range.</p>"},{"location":"crpackage/navigation/costmap-clearing/costmap-clearing-part-2/#huaigu-lin-11212018","title":"Huaigu Lin 11/21/2018","text":""},{"location":"crpackage/package-delivery/","title":"Package Delivery","text":""},{"location":"crpackage/package-delivery/#related-nodes","title":"Related Nodes","text":"<ul> <li><code>package_handler.py</code></li> <li><code>package_sender.py</code></li> <li><code>recording_sender.py</code></li> </ul>"},{"location":"crpackage/package-delivery/#overview","title":"Overview","text":"<p>The package delivery stack is comprised of one main node, <code>package_handler.py</code>, and two secondary nodes <code>package_sender.py</code> and <code>recording_sender.py</code>. Each secondary node corresponds to a type of package that can be handled by the handler (the current implementation supports physical packages on top of the robot, and voice recordings recorded on the on-board laptop. Each secondary node (when pinged on its respective topic) generates a file with a specific suffix (<code>.package</code> and <code>.wav</code> respectively) that will be processed accordingly by the package handler. Each file type has an associated package release protocol, which removes the package filename from the <code>packages</code> queue (maintained by the handler node).</p>"},{"location":"crpackage/package-delivery/#demo","title":"Demo","text":"<ul> <li>Run <code>recording_sender.py</code> on-board (so computer microphone is accessed)</li> <li>Run <code>package_handler.py</code> and <code>package_sender.py</code></li> <li>After navigating to the package sender (wait for WAITING state) place package on the robot</li> <li>After being prompted, hold down B0, B1, or B2 on the robot's base to record a message</li> <li>Send the robot a delivery navigation goal</li> <li>Once the robot has reached the goal (wait for WAITING state) pick up the package.  The robot will also play back the audio recording</li> <li>After the </li> </ul>"},{"location":"crpackage/package-delivery/#ben-albert-121618","title":"Ben Albert 12/16/18","text":""},{"location":"crpackage/package-delivery/talker-node/","title":"Voice Implementation","text":""},{"location":"crpackage/package-delivery/talker-node/#overview","title":"Overview","text":"<p>This week, we built a ROS node that subscribes to a new topic, <code>/things_to_say</code>, and uses a text-to-speech service to make the computer speak the messages of type <code>std_msgs/String</code>.</p>"},{"location":"crpackage/package-delivery/talker-node/#prerequisite","title":"Prerequisite","text":"<p>If you receive an error when you try to run this node, the computer likely does not have <code>pyttsx</code> installed. To install it, simply run <code>pip install pyttsx --user</code>.</p> <p>If that command fails, the computer likely does not have pip installed. To install it, run <code>sudo apt-get install python-pip</code> and then attempt the <code>pyttsx</code> install again.</p>"},{"location":"crpackage/package-delivery/talker-node/#running-the-node","title":"Running the node","text":"<p>To run the node, run <code>rosrun cr_ros talk.py</code></p>"},{"location":"crpackage/package-delivery/talker-node/#making-it-talk","title":"Making it talk","text":"<p>Once the node is launched, publish a String to the <code>/things_to_say</code> topic from any device connected to the same ROS core as the node.</p> <p>Sample standalone command: <code>rostopic pub /things_to_say std_msgs/String 'Kill all humans'</code></p> <p>Sample Python script:</p> <pre><code>import rospy\nfrom std_msgs import String\n\npub = rospy.Publisher('/things_to_say', String, queue_size=1)\nrospy.init_node('say_something')\npub.publish('Kill all humans')\n</code></pre>"},{"location":"crpackage/package-delivery/talker-node/#ari-carr-and-ben-albert-10312018","title":"Ari Carr and Ben Albert 10/31/2018","text":""},{"location":"crpackage/voice/","title":"Voice with Amazon Alexa","text":""},{"location":"crpackage/voice/voice%20integration/","title":"Voice Integration","text":""},{"location":"crpackage/voice/voice%20integration/#overview","title":"Overview","text":""},{"location":"crpackage/voice/voice%20integration/#goals","title":"Goals","text":"<p>Control robot through robust voice commands. This includes simple motion as well as global and local planning.</p>"},{"location":"crpackage/voice/voice%20integration/#ngrok-and-webhook-setup","title":"Ngrok and webhook setup","text":"<p>Ngrok is installed on the roscore1 computer is ran with './ngrok http 5000' which opens up an ngrok window which should have a link that looks like 'https://96ba716c.ngrok.io'. From there open up the Alexa Skills developer console (get the login from Pito) and click on the endpoints section of the console in Build. From there paste the ngrok link in the endpoint followed by '/alexa_webhook' there should be a SSL certificate set up if not you must aquire one by going to the ngrok url and downloading a certificate. After the endpoint is set up is done run the webhook node on roscore1 with 'rosrun webhooks voice_webhook.py'. This is now set up to publish intents based on Alexa input.</p>"},{"location":"crpackage/voice/voice%20integration/#arjun-albert-arjunalbertbrandeisedu","title":"@Arjun Albert arjunalbert@brandeis.edu","text":""},{"location":"crpackage/voice/voice-integration.md-.-cr-package-voice-voice-integration.md/","title":"[voice integration.md](./cr-package/voice/voice integration.md)","text":""},{"location":"crpackage/web-application/","title":"Web Application","text":""},{"location":"crpackage/web-application/flask-and-ros/","title":"Integrating with ROS","text":"<p>Our objective is to initialize ROS nodes within a Flask app, enabling publishing and subscribing to ROS topics via API calls or UI interactions.</p> <p>For example, when run/served, the Flask app would:</p> <ol> <li>Initialize a node and a publisher to a teleop topic.</li> <li>Define a Flask route as an endpoint for receiving POST requests containing data representing teleop commands (e.g. 'forward', 'back', 'left', 'right', 'stop').</li> <li>Handle POST requests to the endpoint by executing a Python function in which the ROS publisher publishes the appropriate cmd_vel message to a teleop topic.</li> </ol> <p>It's important to note that this differs from a more common approach to Web-ROS interfacing, which involves the following:</p> <ol> <li>Establish a websocket connection between the web client and the machine running ROS, often via a 3rd party Python package called \"rosbridge.\"</li> <li>Within the web client's JavaScript, import a 3rd party library called \"roslibjs,\" which provides ROS-like classes and actions for subscribing and publishing to ROS topics.</li> <li>Unlike publishers and subscribers implemented in ROS, roslibjs sends JSON to rosbridge which, in turn, publishes and subscribes to actual ROS messages. In short, rosbridge is required as an intermediary between a web client and a machine running ROS.</li> </ol> <p>This has the advantage of providing a standard way for any web client to interface with ROS via JSON. However, this not only makes running rosbridge a necessity, but it also requires ROS developers to implement ROS-like programming in JavaScript. Flask, on the other hand, seems to offer a way to implement ROS patterns purely in Python on both client and server, without rosbridge and roslibjs as dependencies.</p> <p>There is an apparent obstacle to implementing ROS within Flask, though. It seems to involve the way Flask serves an app and the way ROS nodes need to be initialized. More specifically, the issue might arise from initializing a ROS node in a thread other than the main thread, which seems to be the case for some of the ways Flask apps can be run/served. Others in the ROS community seem to have encountered this issue:</p> <ol> <li>Example 1</li> <li>Example 2</li> <li>Example 3</li> </ol> <p>Note that the 3rd example proposes a solution; their Flask app's main thread initializes and starts a new thread in which a ROS node is initialized:</p> <pre><code>    # ROS node, publisher, and parameter.\n    # The node is started in a separate thread to avoid conflicts with Flask.\n    # The parameter *disable_signals* must be set if node is not initialized in the main thread.\n\n    threading.Thread(target=lambda: rospy.init_node('test_node', disable_signals=True)).start()\n    pub = rospy.Publisher('test_pub', String, queue_size=1)\n</code></pre> <p>However, to actually serve the app, they call <code>Flask.run()</code>:</p> <pre><code>if __name__ == '__main__':\n    if NGROK:\n        print 'NGROK mode'\n        app.run(host=os.environ['ROS_IP'], port=5000)\n    else:\n        print 'Manual tunneling mode'\n        dirpath = os.path.dirname(__file__)\n        cert_file = os.path.join(dirpath, '../config/ssl_keys/certificate.pem')\n        pkey_file = os.path.join(dirpath, '../config/ssl_keys/private-key.pem')\n        app.run(host=os.environ['ROS_IP'], port=5000,\n                ssl_context=(cert_file, pkey_file))\n</code></pre> <p>Flask's documentation on Flask.run() advises against using it in a production environment:</p> <p>\"Do not use run() in a production setting. It is not intended to meet security and performance requirements for a production server. Instead, see Deployment Options for WSGI server recommendations.\"</p> <p>\"It is not recommended to use this function for development with automatic reloading as this is badly supported. Instead you should be using the flask command line script\u2019s run support.\"</p> <p>\"The alternative way to start the application is through the Flask.run() method. This will immediately launch a local server exactly the same way the flask script does. This works well for the common case but it does not work well for development which is why from Flask 0.11 onwards the flask method is recommended. The reason for this is that due to how the reload mechanism works there are some bizarre side-effects (like executing certain code twice, sometimes crashing without message or dying when a syntax or import error happens). It is however still a perfectly valid method for invoking a non automatic reloading application.\"</p>"},{"location":"crpackage/web-application/flask-and-ros/#solution","title":"Solution","text":"<p>Instead of using <code>Flask.run()</code> within a Flask app's main method/script, we've had success with using the following via Flask's command line interface:</p> <pre><code>    flask run --no-reload\n</code></pre> <p>Without the <code>--no-reload</code> argument, the lines in which your ROS node is initialized will be executed twice, resulting in a ROS error stating that the node was shut down because another with the same name was initialized.</p>"},{"location":"crpackage/web-application/flask-and-ros/#brad-nesbitt-huaigu-lin-10312018","title":"Brad Nesbitt &amp; Huaigu Lin 10/31/2018","text":""},{"location":"crpackage/web-application/flask/","title":"Creating the app","text":""},{"location":"crpackage/web-application/flask/#creating-a-new-flask-app","title":"Creating a New Flask App","text":""},{"location":"crpackage/web-application/flask/#create-the-projects-directory","title":"Create the project's directory","text":"<p><code>mkdir MyNewFlaskApp</code> <code>cd MyNewFlaskApp</code></p>"},{"location":"crpackage/web-application/flask/#install-flask","title":"Install Flask","text":"<p><code>pip install Flask</code></p>"},{"location":"crpackage/web-application/flask/#create-activate-the-projects-virtual-environment","title":"Create &amp; activate the project's virtual environment","text":"<p><code>python3 -m venv venv</code> <code>. venv/bin/activate</code></p>"},{"location":"crpackage/web-application/flask/#create-the-directory-file-that-will-hold-the-apps-main-python-code","title":"Create the directory &amp; file that will hold the app's main Python code","text":"<p><code>mkdir flaskr</code> <code>touch __init__.py</code></p> <ul> <li>Adding an <code>__init__.py</code> file to a directory tells Python to treat it as a package rather than a module. Though it can be left empty, it usually contains code used to initialize a package. We can use it to house our main application code.</li> </ul>"},{"location":"crpackage/web-application/flask/#basic-layout-in-python-file-eg-__init__py","title":"Basic layout in Python file (e.g. <code>__init__.py</code>)","text":"<pre><code>import flask from Flask\n\ndef create_app( test_config=None ):\n    app = Flask( __name__, instance_relative_config=True )\n    ...\n    return app\n</code></pre> <ul> <li>See full tutorial here.</li> </ul> <p>### Set environment variables &amp; run the application\\</p> <pre><code>export FLASK_APP=flaskr\nexport FLASK_ENV=development\nflask run\n</code></pre> <ul> <li>Make these are run from the project's directory, not within the flaskr directory.</li> </ul>"},{"location":"crpackage/web-application/flask/#about-flask","title":"About Flask","text":"<ul> <li>A Flask app is an instance of the Flask class. However, instead of using just one, global Flask object, a Flask app is best implemented by defining a function (e.g. <code>create_app()</code>) that creates and returns an instance of the Flask class whenever called. This function is often referred to as the \"Application Factory.\" See the Flask tutorial for further details.</li> <li>Note that the <code>create_app()</code> function takes the name of a configuration file, which contains the names and values of environmental variables to be used by the Flask application.</li> </ul>"},{"location":"crpackage/web-application/flask/#about-virtual-environments","title":"About Virtual Environments","text":"<ul> <li>Using a python virtual environment in a project is a way to ensure all of project's dependencies (e.g. python version, python packages, etc.) \"accompany\" it and are met wherever it happens to be run.</li> <li>Here's the Flask documentation on virtual environments.</li> <li>To add a virtual environment to a project, cd into the project's directory and run <code>python3 -m venv venv</code></li> <li>Whenever you work on your project, activate its virtual environment first, by running <code>. venv/bin/activate</code></li> </ul>"},{"location":"crpackage/web-application/flask/#about-sqlite","title":"About SQLite","text":"<ul> <li>SQLite is a serverless relational database. Simply put, it allows you to implement a database in your project without having to run/connect to a separate database server.</li> <li>It's intended for light use, ideal for a development environment (or in production with light activity).</li> <li>Python also has built-in support for SQLite3, so there's no need to install it. Adding it to a project is a simple as <code>import sqlite3</code>. Further Flask-specific documentation is available here.</li> <li>A tutorial on using Flask with SQLAlchemy to interface with a SQLite database in a more object-oriented way can be found here.</li> </ul>"},{"location":"crpackage/web-application/flask/#adding-a-ui","title":"Adding a UI","text":""},{"location":"crpackage/web-application/flask/#bootstrap","title":"Bootstrap","text":"<ul> <li>Bootstrap provides a large selection of pre-made UI components.</li> <li>To enable using Bootstrap via CDN: 1. Paste this into your HTML document's header, before any other links to css: <code>&lt;link rel=\"stylesheet\" href=\"https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css\" integrity=\"sha384-MCw98/SFnGE8fJT3GXwEOngsV7Zt27NXFoaoApmYm81iuXoPkFOJwJ8ERdknLPMO\" crossorigin=\"anonymous\"&gt;</code></li> <li>Paste these in order near the very of your HTML document, right bofore the <code>&lt;/body&gt;</code> closing tag:</li> </ul> <pre><code>    &lt;script src=\"https://code.jquery.com/jquery-3.3.1.slim.min.js\" integrity=\"sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo\" crossorigin=\"anonymous\"&gt;&lt;/script&gt;\n    &lt;script src=\"https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js\" integrity=\"sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49\" crossorigin=\"anonymous\"&gt;&lt;/script&gt;\n    &lt;script src=\"https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/js/bootstrap.min.js\" integrity=\"sha384-ChfqqxuZUCnJSK3+MXmPNIyE6ZbWh2IMqE241rYiqJxyMiZ6OW/JmZQ5stwEULTy\" crossorigin=\"anonymous\"&gt;&lt;/script&gt;\n</code></pre> <ul> <li>The full set-up instructions can also be found here.</li> <li>Most Bootstrap elements are added by creating a <code>&lt;div&gt;</code> with the <code>class</code> attribute set to one of Bootstrap's predefined classes. For example, adding a Boostrap alert element consists of the following: <code>&lt;div class=\"alert alert-primary\" role=\"alert\"&gt;A simple primary alert\u2014check it out!&lt;/div&gt;</code></li> <li>Bootstrap uses the <code>role</code> attribute to ensure accessability.</li> </ul>"},{"location":"crpackage/web-application/flask/#icons","title":"Icons","text":"<ul> <li>To enable use of free FontAwesome icons via CDN, add the following link tag to the header of your HTML document: <code>&lt;link rel=\"stylesheet\" href=\"https://use.fontawesome.com/releases/v5.4.1/css/all.css\" integrity=\"sha384-5sAR7xN1Nv6T6+dT2mhtzEpVJvfS3NScPQTrOxhwjIuvcA67KV2R5Jz6kr4abQsz\" crossorigin=\"anonymous\"&gt;</code></li> <li>To add a specific icon, pick the one you want from the FontAwesome gallery, then simply copy its html tag (e.g. <code>&lt;i class=\"fas fa-arrow-alt-circle-up\"&gt;&lt;/i&gt;</code>) and paste it into the desired section of your HTML document.</li> </ul>"},{"location":"crpackage/web-application/flask/#brad-nesbitt-10262018","title":"Brad Nesbitt 10/26/2018","text":""},{"location":"crpackage/web-application/livemap/","title":"livemap.md","text":""},{"location":"crpackage/web-application/livemap/#campus-rover-live-map","title":"Campus Rover Live Map","text":"<p>The objective is to implement a 2D map in the CR_Web application that depicts:</p> <ul> <li>The floorplan Campus Rover is currently using to navigate</li> <li>Campus Rover's \"real-time\" location as it navigates</li> <li>The goal destination, toward which Campus Rover is navigating</li> </ul>"},{"location":"crpackage/web-application/livemap/#first-iteration","title":"First Iteration","text":"<p>Our first implementation was based on a tutorial that relied on a websocket connection between the robot and web client, and had the following dependencies on 3rd party libraries:</p> <ul> <li>RosBridge</li> <li>2Djs</li> <li>RosLibJs</li> <li>EaselJs</li> <li>EventEmitter2</li> </ul> <p>This initial implementation (repo here) was successful, but presented several issues:</p> <ul> <li>Building upon 3rd party dependencies risked future breaks and maintenance.</li> <li>As discussed here, it entailed \"ROS-like\" programming in JavaScript instead of Python.</li> <li>The implementation described in the tutorial generates a 2D map image from an amcl occupancy grid. This is unecessary for our purposes, because Campus Rover uses a pre-generated floorplan image; re-generating it is redundant and thus computationally wasteful.</li> <li>Generating the map and loading the 4 JavaScript libraries mentioned above on every page load created noticeable performance issues, limiting any additional page content.</li> </ul>"},{"location":"crpackage/web-application/livemap/#current-iteration","title":"Current Iteration","text":"<p>The current iteration resolves the issues identified through the first iteration and enables additional map features:</p> <ul> <li>Instead of generating a map image from an occupancy grid, an existing floorplan image file is rendered.</li> <li>Instead of using 3rd-party JavaScript libraries, the map is rendered using HTML5's Canvas element.</li> <li>Instead of writing \"ROS-like\" JavaScript in the front end as before, all ROS code is implemented with regular ROS Python programming in the Flask layer of the application.</li> <li>Unlike the initial iteration, the current map includes the option to \"track\" the robot as it traverses the map, automatically scrolling to keep up with the robot as it moves.</li> <li>The current iteration now displays the robot's goal location, too.</li> </ul>"},{"location":"crpackage/web-application/livemap/#next-steps","title":"Next Steps","text":"<p>Support for:</p> <ul> <li>Multiple floorplans/maps</li> <li>Switching between different floorplans</li> <li>Adjusting the size and scale of a map (for zooming in/out, resizing, etc.)</li> </ul>"},{"location":"crpackage/web-application/livemap/#follow-up-iteration","title":"Follow-up Iteration","text":"<p>Brad Nesbitt 11/18/2018</p>"},{"location":"crpackage/web-application/livemap/#overview-of-the-livemap-class","title":"Overview of the LiveMap class","text":"<p>After several preceding iterations of \"live\" 2D maps, it became clear that a single abstraction for such mapping would be appropriate. An instance of the <code>LiveMap</code> class maps waypoints, the robot's current pose, and its goal poses onto 2D floorplan for display within a web application.</p> <p>The <code>static</code> directory in <code>rover_app</code> now contains <code>map_files</code>, which contains the local files needed to generate a given map, including a JSON file with parameters specific to each map. For example:</p> <p>--</p>"},{"location":"crpackage/web-application/livemap/#all_mapsjson","title":"<code>all_maps.json</code>","text":"<pre><code>\"Gerstenzang Basement\": {\n    \"files\": {\n        \"path\": \"rover_app/static/map_files/basement/\",\n        \"png_file\": {\n            \"file_name\": \"basement.png\",\n            \"cm_per_pixel\": 1\n        },\n        \"waypoint_file\": \"basement_waypoints.json\"\n    },\n    \"yaml_parameters\": {\n        \"resolution\":  0.01,\n        \"origin\": [0.0, 0.0, 0.0]\n    }\n</code></pre> <p>The JSON object for a map includes references to local files comprising the map's floorplan <code>.png</code> file, a JSON file of the map's waypoint data, and a copy of the yaml parameters used for amcl navigation of the <code>.png</code>-based map.</p> <p>--</p>"},{"location":"crpackage/web-application/livemap/#live_mappy","title":"<code>live_map.py</code>","text":"<p>Initializing a LiveMap object requires 2 parameters:</p> <ol> <li>The name/String corresponding to a map in <code>all_maps.json</code>, such as \"Gerstenzang Basement\"</li> <li>The desired centimeters per pixel ratio to be used when displaying the map.</li> <li>An optional parameter is the centimeter diameter of the robot, which is the Turtlebot2's spec of 35.4 by default.</li> </ol> <p>For example, <code>live_map = LiveMap(\"Gerstenzang Basement\", 2)</code> initializes a LiveMap object of the Gerstenzang Basement floorplan with a 2cm/pixel scale. The object maintains the following abstraction representing the state of the map, including the robot's current place within it and it's goal destination:</p> <pre><code>    self.map_state = {\n        \"map_parameters\": {\n            \"map_name\": map_name_string,\n            \"files\": {\n                \"path\": path,\n                \"png\": {\n                    \"file_name\": map_json[\"files\"][\"png_file\"][\"file_name\"],\n                    \"cm_per_pixel\": map_json[\"files\"][\"png_file\"][\"cm_per_pixel\"],\n                    \"pixel_width\": png_height,\n                    \"pixel_height\": png_width,\n                },\n                \"yaml\": map_json[\"yaml_parameters\"]\n            },\n            \"bot_radius\": bot_cm_diameter/2,\n            \"cm_per_pixel\": scale_cm_per_pixel, # Desired scale\n            \"waypoints\": waypoints,\n            \"current_pose\": {},\n            \"goal_pose\": {}\n        },\n        \"scaled_pixel_values\": {\n            \"bot_radius\": (bot_cm_diameter / 2) * png_cm_per_pixel / scale_cm_per_pixel,\n            \"cm_per_pixel\": scale_cm_per_pixel,\n            \"png_pixel_width\": png_width * png_cm_per_pixel / scale_cm_per_pixel,\n            \"png_pixel_height\": png_height * png_cm_per_pixel / scale_cm_per_pixel,\n            \"current_pose\": {},\n            \"goal_pose\": {}\n        },\n        \"subscribers\": {\n            \"current_pose_sub\": rospy.Subscriber('/amcl_pose', PoseWithCovarianceStamped, self.update_current_pose),\n            \"goal_pose_sub\": rospy.Subscriber('/move_base/current_goal', PoseStamped, self.update_goal_pose),\n            \"rviz_goal_pose_sub\": rospy.Subscriber('/move_base_simple/goal', PoseStamped, self.update_goal_pose)\n        }\n}\n</code></pre> <p>Note that a nested dictionary of ROS subscribers continually updates the scaled pixel value equivalents of the current and goal poses.</p> <p>Implementing 2D mapping in this way aims to achieve two main advantages:</p> <ol> <li>The LiveMap class allows the initialization of multiple, differing maps, with custom scales in the web application. For instance, a small, \"thumbnail\" map could be implemented on one page, while large map could be displayed somewhere else. This also makes switching between maps is also possible.</li> <li>Representing a <code>map_state</code> as a Python dictionary (shown above) makes it easy to send the data needed to work with a live 2D map as JSON. For instance, a map route or endpoint could be implemented to return a <code>map_state</code> JSON object which could, in turn, be used to render or update a map in the UI.</li> </ol>"},{"location":"crpackage/web-application/livemap/#brad-nesbitt-huaigu-lin-11102018","title":"Brad Nesbitt &amp; Huaigu Lin 11/10/2018","text":""},{"location":"faq/home/","title":"Knowledge Base","text":"<p>This is an extensive collection of short notes on all things Robotics and ROS, of course from the perspective of the Brandeis Robotics Lab. The best way to discover what's there is by using the search feature in the box at the top of the page.</p>"},{"location":"faq/home/#adding-an-faq","title":"Adding an FAQ","text":"<ol> <li>Git clone this repository to your computer: </li> <li>In the directory structure, find the folder called FAQ. </li> <li>Notice that there are many folders that categorize entries. </li> <li>Locate the one that is closest to what you are preparing to contribute.</li> <li>Use the template below as a starting point</li> <li>Git add, commit and push your changes</li> <li>If you did it right then they will appear in the [Labnotebook]https://campusrover.github.io/labnotebook2/faq/)</li> </ol>"},{"location":"faq/home/#template-for-your-faq","title":"Template for your FAQ","text":"<pre><code>---\ntitle: How I write an FAQ\nauthor: Pito Salas\ndate: Dec 5 2024\n---\n## Author\n* Pito Salas\n* Dec 5 2024\n* ROS version\n\n## Summary\n\nBrief instructions for writing a new FAQ in the Labnotebook\n\n## Details\n....\n</code></pre>"},{"location":"faq/Lidar/SLAM_mapping/","title":"Mapping surrounding with SLAM","text":""},{"location":"faq/Lidar/SLAM_mapping/#author","title":"Author","text":"<ul> <li>TsunOn Kwok</li> <li>Dec 10 2024</li> <li>Mapping surrounding with SLAM</li> </ul>"},{"location":"faq/Lidar/SLAM_mapping/#summary","title":"Summary","text":"<p>Brief instructions for how ot mapping a surrounding with SLAM in both real robot and gazebo</p>"},{"location":"faq/Lidar/SLAM_mapping/#details","title":"Details","text":"<p>To map the surrouding and generate a <code>.yaml</code> world file and a <code>.pgm</code> map, follow these steps:</p> <ol> <li> <p>Bring up the robot in real life or launch you robot and world in gazebo to get started.</p> </li> <li> <p>Launch SLAM using <code>gmapping</code> by running the following command:</p> </li> </ol> <p><pre><code>roslaunch turtlebot3_slam turtlebot3_slam.launch slam_methods:=gmapping\n</code></pre>     Rviz should be automatically opened after the command is typed.</p> <ol> <li> <p>Open a new terminal and run the command: <code>teleop</code> to control the robot with the keyboard. Drive the robot around the target area to scan its surroundings.</p> </li> <li> <p>Once the entire target area has been scanned, open another terminal and execute the following command to save the map:</p> </li> </ol> <pre><code>rosrun map_server map_saver -f `rospack find &lt;package_name&gt;`/maps/&lt;map_name&gt;\n</code></pre> <ol> <li>This will generate two files in your Ros package <code>&lt;package_name&gt;/maps</code>: <code>&lt;map_name&gt;.yaml</code> and <code>&lt;map_name&gt;.pgm</code>.</li> </ol>"},{"location":"faq/Lidar/SLAM_mapping/#things-to-notice","title":"Things to notice","text":"<ul> <li> <p>In gazebo, due to lagging and its poor accuary, the SLAM mapping is very unaccuary. For any world over 10x10 meters, the SLAM mapping is unusable. It is highly remmcomended that you keep the world as small as poposible in gazebo. Our gazebo world is 5x5 meters.</p> </li> <li> <p>Adjust SLAM Parameters: Modify parameters like linearUpdate, angularUpdate, and particles in the gmapping configuration file to suit your robot and environment.</p> </li> </ul>"},{"location":"faq/Lidar/Simplifying_Lidar/","title":"Simplifying Lidar","text":""},{"location":"faq/Lidar/Simplifying_Lidar/#author-aiden-dumas","title":"Author: Aiden Dumas","text":"<p>Using Lidar is fundamental for a robot\u2019s understanding of its environment. It is the basis of many critical mapping and navigation tools such as SLAM and AMCL, but when not using a premade algorithm or just using Lidar for more simple tasks, some preprocessing can make Lidar readings much more understandable for us as programmers. Here I share a preprocessing simplification I use to make Lidar intuitive to work with for my own algorithms.</p> <p>It essentially boils down to bundling subranges of the Lidar readings into regions. The idea for this can be found from:  Article     The simple preprocessing takes the 360 values of Lidar\u2019s range method (360 distance measurements for 360 degrees around the robot) and gives you a much smaller number of values representing more intuitive concepts such as \u201cforward\u201d, \u201cleft\u201d, \u201cbehind\u201d. The processing is done by creating your subscriber for the Lidar message: scan_sub = rospy.Subscriber(\u2018scan\u2019, LaserScan, scan_cb)</p> <p>Our callback function (named scan_cb as specified above), will receive the message as such: def scan_cb(msg):</p> <p>And depending on how often we want to preprocess a message, we can pass the 360 degree values to our preprocessing function: ranges = preprocessor(msg.ranges)</p> <p>In my practice, calling this preprocessing step as often as the message was received by the subscriber didn\u2019t have any slowdown effect on my programs. The processor function itself is defined as follows: <pre><code>def preprocessor(all_ranges):\n    ranges = [float[(\u2018inf\u2019)] * 20\n    ranges _index = 0\n    index = -9\n    sum = 0\n    batch = 0\n    actual = 0\n    for i in range(360):\n        curr = all_ranges[index]\n        if curr != float(\u2018inf\u201d and not isnan(curr):\n            sum += curr\n            actual += 1\n        batch += 1\n        index += 1\n        if batch == 18:\n            if actual != 0:\n                ranges[ranges_index] = sum/actual\n            ranges_index += 1\n            sum = 0\n            batch = 0\n            actual = 0\n    return ranges\n</code></pre> Essentially all we do here is take the average of valid values in a region of the 360 degree scan and put it into our corresponding spot in our new abbreviated/averaged ranges array that will serve as our substitute for the more lofty 360 degree value msg.ranges (passed to all_ranges). Something implicit to note here is that in the first line of this function we choose how many regions to divide our Lidar data into. Here 20 is chosen. I found that the size of these regions work well for balancing the focus of their region while not compromising being too blind to what lies in a region\u2019s peripheral. This 20 then computes 360/20 = 18 which is our batch size condition (number of data points per region) and where we start our index 0 - 18/2 = -9 which is the offset we use to start to get a range for the \u201cfront\u201d region (makes sense looking at the figure in the pdf linked).  With this base code, any modifications can be made to suit more specific purposes. The averaging of the regions could be replaced by finding a max or min value for example. In one of my projects, I was focused on navigation in a maze where intersections were always at right angles, so I only needed 4 of these regions (forward, backward, left, right). For this I used the following modification of the code with a helper function listed first again using 18 as the number of values sampled per region: (Note here for reference that in the original msg.ranges Front is at index 0, Back at 180, Left at 90, and Right at 270 (implying the Lidar data is read counterclockwise)) <pre><code>def averager(direction_ranges):\n    real = 0\n    sum = 0\n    for i in ranges:\n        if i != float(\u2018inf\u2019) and not isnan(i):\n            real += 1\n            sum += 1\n    return float(\u2018inf\u2019) if real == 0 else sum/real\n</code></pre> <pre><code>def cardinal_directions(ranges):\n    directions = {\u201cFront\u201d: float(\u2018inf\u2019), \u201cBack\u201d: float(\u2018inf\u2019), \u201cLeft\u201d: float(\u2018inf\u2019), \u201cRight\u201d: float(\u2018inf\u2019)}\n    plus_minus = 9\n    Front = ranges[-plus_minus:] + ranges[:plus_minus]\n    Backward = ranges[180 - plus_minus:180+plus_minus]\n    Left = ranges[90 - plus_minus:90+plus_minus]\n    Right = ranges[270-plus_minus:270 + plus_minus]\n    directions_before_processed = {\u201c\u201dFront\u201d: Front, \u201cBack\u201d: Back, \u201cLeft\u201d: Left, \u201cRight\u201d: Right}\n    for direction, data in directions_before_processed:\n        directions[direction] = averager(data)\n    return directions\n</code></pre> Hopefully this code or at least this idea of regional division helps simplify your coding with Lidar.</p>"},{"location":"faq/Lidar/calculating_coordinates_amcl_lidar/","title":"Calculating coordinates using AMCL and LiDAR","text":""},{"location":"faq/Lidar/calculating_coordinates_amcl_lidar/#author","title":"Author","text":"<ul> <li> <p>Chloe Wahl-Dassule</p> </li> <li> <p>Dec 9 2024</p> </li> <li> <p>ROS1</p> </li> </ul>"},{"location":"faq/Lidar/calculating_coordinates_amcl_lidar/#summary","title":"Summary","text":"<p>This is a tutorial for calculating the coordinates of an object, point_a, detected on LiDAR, assuming AMCL is running.</p>"},{"location":"faq/Lidar/calculating_coordinates_amcl_lidar/#details","title":"Details","text":""},{"location":"faq/Lidar/calculating_coordinates_amcl_lidar/#first-get-the-current-position-of-the-robot-using-data-from-amcl","title":"First get the current position of the robot using data from amcl.","text":"<p>Example:</p> <pre><code>def get_robot_position(self):\n    try:\n        self.tf_listener.waitForTransform('/map', '/base_link', rospy.Time(0), rospy.Duration(4.0))\n        (trans, rot) = self.tf_listener.lookupTransform('/map', '/base_link', rospy.Time(0))\n\n        x, y, z = trans  #Position in 3D space\n        roll, pitch, yaw = tf.transformations.euler_from_quaternion(rot)\n\n        return x, y, yaw  #Return position and yaw angle\n        except (tf.Exception, tf.ConnectivityException, tf.LookupException) as e:\n            rospy.logerr(f\"TF Error: {e}\")\n            return None, None, None\n</code></pre>"},{"location":"faq/Lidar/calculating_coordinates_amcl_lidar/#take-into-account-that-the-robot-may-not-be-facing-a-yaw-of-0","title":"Take into account that the robot may not be facing a yaw of 0","text":"<p>Take the yaw calculation from step 1 and combine it with the LiDAR angle to figure out what angle the object is at from the origin of the map.</p> <p>Example:</p> <pre><code>def get_angle(self, yaw, lidar):\n    lidar = lidar\n    if lidar &gt; 180:\n        remainder = lidar - 180\n        lidar = (180 - remainder) * -1\n        result = math.radians(lidar) + yaw\n        return result\n</code></pre>"},{"location":"faq/Lidar/calculating_coordinates_amcl_lidar/#simple-trigonometry-to-get-the-coordinates-of-point_a-based-on-the-distance-reading-and-the-angle-from-step-2","title":"Simple trigonometry to get the coordinates of point_a, based on the distance reading and the angle from step 2","text":"<p>Make sure to convert degrees to radians</p> <p>Example:</p> <pre><code>def get_coord(self, distance, radians, coordinate):\n    x = coordinate[0]\n    y = coordinate[1]\n    new_x = x + distance * math.cos(radians)\n    new_y = y + distance * math.sin(radians)\n    return new_x, new_y\n</code></pre>"},{"location":"faq/Lidar/laserscan-definition-modify/","title":"Modifying LaserScan Message Definition(Or How to Access Turtlebot Files)","text":""},{"location":"faq/Lidar/laserscan-definition-modify/#by-harry-zhu-courtesy-of-ta-adam-ring","title":"by Harry Zhu (courtesy of TA Adam Ring)","text":"<p>This guide showcases how you can modify the definitions (e.g. range_min) of the LaserScan message published to the /scan topic. It is also a brief \u201cguide\u201d on how to access and modify files on a Turtlebot. </p> <p>Turtlebot\u2019s LaserScan message is specified by the ld08_driver.cpp file from this repo: https://github.com/ROBOTIS-GIT/ld08_driver/tree/develop/src. You can modify this file on your Turtlebot through a terminal on vnc. </p>"},{"location":"faq/Lidar/laserscan-definition-modify/#step-1-ssh-into-your-robot-do-not-do-bringup","title":"Step 1: ssh into your robot, do not do bringup","text":""},{"location":"faq/Lidar/laserscan-definition-modify/#step-2-cd-to-the-right-directory","title":"Step 2: cd to the right directory","text":"<p><code>cd catkin_ws/src/ld08_driver/src</code></p>"},{"location":"faq/Lidar/laserscan-definition-modify/#step-3-enter-nano-editor","title":"Step 3: enter nano editor","text":"<p><code>nano ld08_driver.cpp</code></p>"},{"location":"faq/Lidar/laserscan-definition-modify/#step-4-go-to-line-63-and-fine","title":"Step 4: go to line 63 and fine","text":"<p><code>scan.range_min = 0.0;</code></p>"},{"location":"faq/Lidar/laserscan-definition-modify/#step-5-modify-the-value-as-you-see-fit","title":"Step 5: modify the value as you see fit","text":""},{"location":"faq/Lidar/laserscan-definition-modify/#step-6-save-and-exit","title":"Step 6: save and exit","text":"<p>ctrl + S to save, ctrl + X to exit the editor</p>"},{"location":"faq/Lidar/laserscan-definition-modify/#step-7-remember-to-cm","title":"Step 7: remember to cm","text":"<p><code>cm</code></p>"},{"location":"faq/Lidar/lidar_placement_and_drift/","title":"Issues with Lidar placement and callibration","text":""},{"location":"faq/Lidar/lidar_placement_and_drift/#source","title":"Source","text":"<p>A long thread on a Robotics list which yielded a lot of sophisticated and important insights.</p>"},{"location":"faq/Lidar/lidar_placement_and_drift/#question","title":"Question","text":"<p>When it comes to mounting a lidar on a robot, what are some of the considerations? My robot runs ROS and I've mounted the Lidar and set up the various transforms (\"tf\"s) correctly  --  I believe. </p> <p>When I display the Lidar data while moving the robot forward and backwards, it is fairly stable. In other words, I think that the odometry data reporting the motion of the robot, correctly \"compensates\" for the movement so that the lidar data as displayed stays more or less in the same place.</p> <p>However when I turn the robot in place, the Lidar data drifts a little and then compensates somewhat. I also was able to create a decent Slam map with this setup. Although not as reliable as I would like.</p> <p>It turns out that the place where the lidar is mounted is near the casters, and as a result, in an in place turn, the lidar doesn't simply rotate in place, but instead moves a lot. Because, it is not over the center of rotation, in fact its as far away from it as it could be.</p> <p>My question is: does the math which is used to compute the lidar data during an in place turn compensate for the placement. Does it use the various transforms (which reflect the relative placement of the lidar) or does it just use the odometry of the robot as a whole?</p> <p>(Hard to explain, but I hope you follow)</p> <p>Yes, I could rebuild my robot to do the more intuitive thing and place the lidar over the center of turn. But I would want to avoid all that work if it it's not really needed.</p>"},{"location":"faq/Lidar/lidar_placement_and_drift/#answer-1","title":"Answer 1","text":"<p>TF should negate any physical offsets - but it really depends on the SLAM package using TF correctly. The widely used ones (karto, cartographer, slam_toolbox) all should do this.</p> <p>That said, you might also have timing issues - which TF won't handle (since the timing reported to it will be wrong!). If your odometry or laser are lagging/etc relative to one another, that could cause some issues.</p>"},{"location":"faq/Lidar/lidar_placement_and_drift/#answer-2","title":"Answer 2","text":"<p>Note that the effect is visible without slam. In rviz as the robot moves forward, the \"image\" of the wall or obstacle stays more or less in place relative to the odom. However if I rotate or turn, then the image moves around and when the rotation stops it settles back down.</p> <p>Is the timing problem exacerbated by having the lidar offset from the center of rotation? I could imagine that if the lidar is over the center of rotation then the timing is less critical... but I'm just going by gut feel. </p> <p>When you watch the physical robot rotate in place, the lidar follows a pretty wide circular arc around the center of rotation. You can easily imagine that this causes havoc with the calculations that produce the /scan topic.</p> <p>My big question is, is it worth rearranging things to bring the lidar back to the center of rotation. I think my answer is yes.</p>"},{"location":"faq/Lidar/lidar_placement_and_drift/#answer-3","title":"Answer 3","text":"<p>Almost every real robot out there has the laser offset from the center of the robot, so it's pretty much a solved problem (with TF and proper drivers giving correct timestamps).</p> <p>If the timing is wrong, the change in offset really doesn't help much, since small angular errors result in LARGE x/y offsets when a scan point is several meters away from a robot (and this will be the majority of your error, far larger than the small offset from your non-centered laser).</p> <p>Your comment that \"when the rotation stops it settles back down\", really makes me think it is a timing related issue. One way to get a better idea is to go into RVIZ, and set the \"decay time\" of the laser scanner display to something like 60 seconds. Then do your driving around - this will layer all the scans on top of each other and give you a better idea of how accurate the odometry/laser relationship is. In particular - if you just do a bit of rotation, does the final scan when you stop rotating line up with the first scan before you started rotating? If so, it's almost certainly timing related.</p>"},{"location":"faq/Lidar/lidar_placement_and_drift/#answer-4","title":"Answer 4","text":"<p>There are a few parts to the answer. And I\u2019ll mostly deal with what happens in a two wheel, differential drive robot.</p> <p>It\u2019s not clear what you mean by LIDAR data drifting. If, as I suspect, you mean visualizing the LIDAR with rviz, then what you may be seeing is the averaging effect. In rviz, the display component for LIDAR has an option to include the last \u201cN\u201d points, as I recall, showing up as the Decay Time parameter for the plugin. So, the LIDAR data is likely just fine, and you\u2019re seeing the effect of history disappearing over time. Crank down the Decay Time parameter to, say, 1 and see if things look better.</p> <p>Otherwise, the problem with LIDAR usually only shows up in something like a SLAM algorithm, and then it\u2019s because the odometry and LIDAR disagree with each other. And this usually has nothing to do with LIDAR, which is typically pretty truthful.</p> <p>In SLAM, the algorithm gets two competing truths (typically), odometry and LIDAR. And, besides the normal problems with wheel odometry, which have been bitched about repeatedly in this group, there is also the geometry problem that typically shows up in the motor driver.</p> <p>Whether using gazebo (simulation) or real motor drivers, both typically rely on knowing the distance between the two wheels, and the wheel circumference. With those two constants and the wheel encoder counts (and some constants such has how many encoder ticks there are per revolution), simple math computes how far the left and right wheels moved during some small change in time, which allows computing the angle of revolution for that same time period. </p> <p>You can really see when the circumference is wrong by moving the robot back and forth with SLAM happening and the resulting map showing. If you find that the map shows a wall while the robot is moving and the LIDAR shows that what it thinks is the wall changes as the robot moves, then you have the wheel circumference constant wrong. </p> <p>That is, with a wall ahead of the robot, move the robot in a straight line forward. The map wall will stay in place in the rviz map display, but if the LIDAR dots corresponding to the wall move closer or farther than the map wall while movement is happeningl, the wheel circumference constant is wrong. Just adjust your wheel circumference until moving forwards and backwards shows the LIDAR points corresponding to the wall ahead of the robot staying atop the map\u2019s wall.</p> <p>An error in the distance between the two wheels shows up as an error when the robot rotates. For example, if you have a wall ahead of the robot again and you rotate in place, if you see the wall stays stable in the rviz map display but the LIDAR points corresponding to the wall change the angle of the wall as the robot rotates in place, then you have the wheel circumference wrong. Change the circumference value, usually in the URDF and often repeated in other YAML files as well, and try the experiment again.</p> <p>My saying that the wall in the map stays stable also implies that in rviz you are setting your global frame to the map frame.</p> <p>When you get both the circumference and inter wheel distance correct, the SLAM map will closely match the LIDAR points even while the robot moves. It won\u2019t be perfect while the robot moves, partly which I\u2019ll explain next, but it does mean that when the robot slows down or stops, SLAM will fix the error very quickly.</p> <p>I\u2019m sure people will talk about the time stamp of data as well. In ROS, sensor data is given a time stamp. This should faithfully record when the sensor reading took place. If you have multiple computers in your robot, the sense of time on all computers must match within, at most, say a couple of milliseconds. Less than a millisecond discrepancy between all the computers is better. </p> <p>Note that if you are getting sensor data from an Arduino over, say, ROS serial, you will have to deal with the getting the time stamp adjusted before the sensor data gets posted as a ROS message. Since the Arduino didn\u2019t tag the time, and the Arduino is unlikely to know the time to high accuracy, you have to figure out the latency in reading the data over a serial port, and the serialization, deserialization and message conversion delays.</p> <p>When various components work with sensor data, they often predict what the sensor values will be in some future after the readings actually took place. That\u2019s because the sensor values are coming in with some delay (lag), and the algorithms, especially SLAM, want to predict the current state of the robot. SLAM is monitoring the commanded movement of the robot (the cmd_vel sent to the motors) and when odometry and LIDAR data comes in, it needs to predict where the odometry and LIDAR points would be NOW, not when they were last observed. If the timestamp of the sensor data is wrong, the guess is wrong and SLAM gets very sloppy.</p> <p>To cope with bad time and unexpectant latencies, especially the notorious big scheduling delays in a Linux system (think of what preemptive time sharing really does to your threads when they think they know what the current time is), one simple filter ROS usually provides is to ignore any data that is too \u201cold\u201d. There are configuration parameters, for instance, for SLAM that say to just ignore data that is older than some relatively small time from now (on order of, say 10 milliseconds might by typical).</p> <p>This is especially a problem with multiple computers in a robot, but even with a single computer. Remember that Linux is trying to run on order of 100 threads at the same time. On a Raspberry Pi, it\u2019s tying to give each of those 100 threads the illusion that they are running in real time by giving them a tiny slice of time to run before going on to the next thread. A thread can ask for \u201cwhat time is it now\u201d and as soon as it gets the answer, before the very next instruction in that thread executes, tens or hundreds of milliseconds may have gone by.</p> <p>Finally, as for positioning of the LIDAR, ROS provides a marvelous mathematical modeling package in the TF system. If you have pretty good modeling of fixed components, like LIDARs mounted solidly to the robot and you get the offsets correct to within a millimeter or two from 0,0,0 in the base_link frame of reference, you needn\u2019t worry about where you put the LIDAR. Make sure you correctly account for all the rotations as well, though. For instance, with the casters on my robot, the plate holding the LIDAR isn\u2019t exactly level with the floor, and my URDF needs to include the rotation of the LIDAR.</p> <p>My LIDAR is NOT mounted at the center of rotation and rviz shows things just fine. And I\u2019m about to replace my single LIDAR with 4 LIDARS mounted at the 4 corners of my robot\u2019s frame at different heights. It will be no problem for rviz and SLAM to deal with this.</p> <p>Finally there is an issue with computation speed and robot speed. The faster you robot moves, the faster it needs to get sensor data. SLAM works best when it gets, say, 20 to 100 sensor readings a second for a robot moving on order of a couple of meters per second. SLAM wants to robot to not have moved very far before it does its thing between sensor frame readings. If your odometry and LIDAR readings are coming in at, say, 5 frames per second, and your computer is slow and loaded (such as trying to do everything on a single Raspberry Pi), and you robot is moving at the equivalent of a couple of miles per hour, all bets are off.</p>"},{"location":"faq/Lidar/lidar_placement_and_drift/#answer-5","title":"Answer 5","text":"<p>Actually, This is a very good question because </p> <p>Some robots use multiple LIDAR units and obviously only one of them can be the center point of a turn. and</p> <p>If the robot used four wheel \"Ackerman steering\" (as almost every car) the center of rotation is not within the footprint of the robot. but on the ground some distance to the side of the robot. So mounting the Lidar in the center of rotation is physically impossible in the above cases.   I hope the software \"works\".</p> <p>It could be that you have wheel slip.  In fact I'd guess this is the problem and you should not be using raw odometry but rather the ROS Robot_Localization package.    This package will \"fuse\" odometry, IMUs, GPS, Visual Odometry and other sources to get the robot's position and orientation and account for wheel slip and other sensor errors.  http://docs.ros.org/en/noetic/api/robot_localization/html/index.html</p>"},{"location":"faq/Lidar/lidar_placement_and_drift/#answer-6","title":"Answer 6","text":"<p>The \"drift\" you describe is caused by the fact that, as the robot rotates, the lidar is not sampled at exactly the same spot.    This causes flat surfaces to appear to bend, but only while the robot is rotating.</p> <p>Chris Netter has documented this extensively and written code that corrects for this effect for his ROS robot,  which I can't seem to locate at the moment.    </p> <p>But it won't be fixed by moving the lidar to the center of rotation.  It is a result of the fact that both the robot and the lidar are rotating while sampling.  It's not seen when the robot is going straight, hence the \"setteling down\" which you mention.</p>"},{"location":"faq/Lidar/lidar_placement_and_drift/#answer-7","title":"Answer 7","text":"<p>But Pito says he is using software to compensate for the movement of the LIDAR during a turn.   In theory, what he did should work.  The ROS tf2 package should be calculating the LIDAR real-world location as it swings around the center of rotation and interpolating the position at the time the LIDAR scan was processed.</p> <p>He said he was using this already. For more on tf2 see http://wiki.ros.org/tf2</p> <p>My guess is that the odometry data that drives the transforms that tf2 uses not accurate.   Odometry assumes a perford differential drive system and none are perfect.   </p> <p>One way to test my guess would be to run the robot in a simulation where odometry is in fact \"perfect\" and see if the problem goes away</p>"},{"location":"faq/Lidar/obstacle_avoidance_using_lidar/","title":"Obstacle Avoidance using LIDAR","text":""},{"location":"faq/Lidar/obstacle_avoidance_using_lidar/#author-parthiv-ganguly","title":"Author: Parthiv Ganguly","text":"<p>There are many scenarios where a robot has to navigate an environment that is not previously mapped. While SLAM can technically map the environment as the robot moves, in reality, this process is unreliable if the environment is completely unknown to the robot. In a case like this, straightforward obstacle avoidance algorithms can be helpful. This page details one that was helpful in our situation. This algorithm divides the 360 degrees around it into regions, and when faced with an obstacle in front, it will turn (by rotating backwards) to point to the most \"obstacle-free\" region which is closest (i.e. lowest angular difference) to the front region.</p>"},{"location":"faq/Lidar/obstacle_avoidance_using_lidar/#initialization","title":"Initialization","text":"<p>First, we define the variables referenced throughout the algorithm, in the <code>__init__</code> method of the class. <pre><code># If an object is detected in front of robot, \"obstacle_detected\" is set to True,\n# and an avoid_angular_vel is calculated to avoid the obstacle\nself.robot_state = {\"obstacle_detected\": False, \"avoid_angular_vel\": 0}\n# div_distance keeps track of the LIDAR distances in each region,\n# 0 is the front region, 1 is front-left, 2 is left, etc.\nself.div_distance = {\"0\": [], \"1\": [], \"2\": [], \"3\": [], \"4\": [], \"5\": [], \"6\": [], \"7\": []}\n# div_cost calculates the cost of region based on how far it is from 0, and the sign gives the direction\nself.div_cost = {\"0\": 0, \"1\": 1, \"2\": 2, \"3\": 3, \"4\": 4, \"5\": -3, \"6\": -2, \"7\": -1}\n</code></pre></p>"},{"location":"faq/Lidar/obstacle_avoidance_using_lidar/#using-scan_cb-to-populate-div_distance","title":"Using scan_cb to populate div_distance","text":"<p>scan_cb is the callback function to the /scan Subscriber, and is used to populate div_distance. Obstacles or LIDAR readings closer than <code>OBJ_THRESHOLD</code> are stored in div_distance and used to update the robot state and decide whether the robot is facing an obstacle. Adjust <code>OBJ_THRESHOLD</code> based on how much distance you want your robot to maintain from the obstacles. <code>ANGLE_THRESHOLD</code> is the angular width of each region. For my case, it was set to 45 degrees.</p> <pre><code>def scan_cb(self, msg):\n    for key in self.div_distance.keys():\n        values = []\n        if key == \"0\":\n            # The front region is wider compared to the other regions (60 vs 45),\n            # because we need to avoid obstacles in the front\n            for x in msg.ranges[int((330/360) * len(msg.ranges)):] + msg.ranges[:int((30/360) * len(msg.ranges))]:\n                if x &lt;= OBJ_THRESHOLD and not(math.isinf(x)) and not(math.isnan(x)) and x &gt; msg.range_min:\n                    values.append(x)\n        else:\n            for x in msg.ranges[int((23/360) * len(msg.ranges)) + int((ANGLE_THRESHOLD/360) * len(msg.ranges)) * (int(key)-1) : int((23/360) * len(msg.ranges)) + int((ANGLE_THRESHOLD/360) * len(msg.ranges)) * int(key)]:\n                if x &lt;= OBJ_THRESHOLD and not(math.isinf(x)) and not(math.isnan(x)) and x &gt; msg.range_min:\n                    values.append(x)\n        self.div_distance[key] = values\n</code></pre>"},{"location":"faq/Lidar/obstacle_avoidance_using_lidar/#calculating-robot_state","title":"Calculating robot_state","text":"<p>This function calculates whether or not an obstacle is in front of the robot (<code>self.robot_state[\"obstacle_detected\"]</code>), and it calculates the angular velocity needed to avoid the obstacle and point in the most \"obstacle-free\" zone (i.e. zone that is without obstacles or has the farthest LIDAR reading) with the lowest cost (the closer the zone is to the front, the lower the cost).</p> <pre><code>def calc_robot_state(self):\n    nearest = math.inf\n    region_diff = 0\n    # Regional differences are calculated relative to the front region\n    goal = \"0\"\n    # The 4th region gives the highest regional diff so we start with that\n    max_destination = \"4\"\n    max_distance = 0\n\n    for key, value in self.div_distance.items():\n        region_diff = abs(self.div_cost[key] - self.div_cost[goal])\n\n        # If there're no obstacles in that region\n        if not len(value):\n            # Find the obstacle-free region closest to the front\n            if (region_diff &lt; nearest):\n                nearest = region_diff\n                max_distance = OBJ_THRESHOLD\n                max_destination = key\n        # Check if the region is the most \"obstacle-free\", i.e. the LIDAR distance is the highest\n        elif max(value) &gt; max_distance:\n            max_distance = max(value)\n            max_destination = key\n\n    # Difference between the most obstacle-free region and the front\n    region_diff = self.div_cost[max_destination] - self.div_cost[goal]\n\n    # If the obstacle free path closest to the front is not the front (i.e. nearest != 0),\n    # this means that there is an obstacle in the front\n    self.robot_state[\"obstacle_detected\"] = (nearest != 0)\n    # The avoid_angular_vel is 0.7, and it's sign is the same as the sign of the regional difference\n    # We do the max(1, ) thing to avoid division by 0 when the regional difference is 0\n    self.robot_state[\"avoid_angular_vel\"] = ((region_diff/max(1, abs(region_diff))) * 0.7)\n</code></pre>"},{"location":"faq/Lidar/obstacle_avoidance_using_lidar/#bringing-it-all-together","title":"Bringing it all together","text":"<p>Have the following snippet of code inside your main ros loop, so that the robot state is constantly up to date. Replace the variables <code>linear_velocity</code> and <code>angular_velocity</code> with whatever linear and angular velocity the robot should have when the course is clear and it is heading directly towards its goal.</p> <pre><code>cmd_vel = Twist()\nself.calc_robot_state()\nif self.robot_state[\"obstacle_detected\"]:\n    # Avoid obstacle by turning back\n    cmd_vel.linear.x = -0.1\n    cmd_vel.angular.z = self.robot_state[\"avoid_angular_vel\"]\nelse:\n    cmd_vel.linear.x = linear_velocity\n    cmd_vel.angular.z = angular_velocity\nself.vel_pub.publish(cmd_vel)\n</code></pre>"},{"location":"faq/OS/UDP-socket/","title":"UDP- Sockets","text":""},{"location":"faq/OS/UDP-socket/#author-lucian-fairbrother","title":"Author: Lucian Fairbrother","text":"<p>Do you need to give information to your roscore that you can't transport with rosnodes? </p> <p>You may have trouble running certain libraries or code in your vnc environment, a UDP connection could allow you to run it somewhere else and broadcast it into your vnc. There are many reasons this could happen and UDP sockets are the solution. In our project we used multiple roscores to broadcast the locations of our robots. We send the robot coordinates over a UDP socket that the other roscore can then pickup and use.</p> <p></p>"},{"location":"faq/OS/UDP-socket/#simple-sender","title":"Simple Sender","text":"<p>Here is an example of the most basic sender that you could use for your project. In this example the sender sends out a string to be picked up:</p> <pre><code>import socket\nhost = &lt;enter host IP here&gt;\nport = 5000\ns = socket.socket(socket.AF_INET,socket.SOCK_STREAM)\ns.bind(('', port))\ns.listen(1)\nc, addr = s.accept()\nprint(\"CONNECTION FROM:\", str(addr))\nc.send(b\"HELLO, How are you ? Welcome to Akash hacking World\")\nmsg = \"Bye..............\"\nc.send(msg.encode())\nc.close()\n</code></pre>"},{"location":"faq/OS/UDP-socket/#simple-receiver","title":"Simple Receiver","text":"<p>You need to run a receiver to pickup the information that your sender put out</p> <pre><code>import socket\nhost = &lt;Same IP&gt;\nport = 5000\ns = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\ns.connect(('127.0.0.1', port))\nmsg = s.recv(1024)\nwhile msg:\n    print('Received:' + msg.decode())\n    msg = s.recv(1024)\ns.close()\n</code></pre>"},{"location":"faq/OS/UDP-socket/#my-sender","title":"My Sender","text":"<p>Often times you may want to create a sender node that will take information from your ROS environment and publish it to the outside world. Here is an example of how I went about doing this.</p> <pre><code>#!/usr/bin/env python\nimport os\nfrom socket import *\nimport rospy\nfrom std_msgs.msg import String\nfrom nav_msgs.msg import Odometry\n# Because of transformations\nimport tf_conversions \nimport tf2_ros\nimport geometry_msgs.msg\nimport math \nfrom geometry_msgs.msg import PoseWithCovarianceStamped \nfrom tf.transformations import euler_from_quaternion, quaternion_from_euler\n\nrospy.init_node(\"sender\")\n#100.71.173.127\n#100.74.41.103\nhost = \"100.71.173.127\" # set to IP address of target computer\nport = 13000\naddr = (host, port)\nUDPSock = socket(AF_INET, SOCK_DGRAM)\n\n\n\nroll = 0.0\npitch = 0.0\nyaw = 0.0\n\ndef get_rotation (msg):\n    if msg is not None:\n        global roll, pitch, yaw \n        orientation_q = msg.pose.pose.orientation\n        orientation_list = [orientation_q.x, orientation_q.y, orientation_q.z, orientation_q.w]\n        (roll, pitch, yaw) = euler_from_quaternion (orientation_list)\n        print('X =',msg.pose.pose.position.x, 'Y =',msg.pose.pose.position.y, 'Yaw =',math.degrees(yaw))\n        mess=str(msg.pose.pose.position.x)+\" \"+str(msg.pose.pose.position.y)\n        data = bytes(str(mess), 'utf-8')\n        UDPSock.sendto(data, addr)\n\n\nsub = rospy.Subscriber ('/amcl_pose', PoseWithCovarianceStamped, get_rotation) # geometry_msgs/PoseWithCovariance pose\n\nwhile not rospy.is_shutdown():\n    hi = \"r\"\n</code></pre>"},{"location":"faq/OS/UDP-socket/#my-receiver","title":"My Receiver","text":"<p>And here is the receiver we created to handle our sender</p> <pre><code>#!/usr/bin/env python\nimport os\nfrom socket import *\nimport rospy\nfrom std_msgs.msg import String\nfrom nav_msgs.msg import Odometry\nimport json\nfrom std_msgs.msg import Float64MultiArray\n\n\n\n\nrospy.init_node(\"receiver\")\nmypub = rospy.Publisher('/other_odom', Float64MultiArray,queue_size = 10)\n\n\nhost = \"100.74.41.103\"\nport = 13000\nbuf = 1024\naddr = (host, port)\nUDPSock = socket(AF_INET, SOCK_DGRAM)\nUDPSock.bind(addr)\nwhile not rospy.is_shutdown():\n    (data, addr) = UDPSock.recvfrom(buf)\n    data=data.decode('utf-8')\n    data=data.split()\n    x=data[0]\n    y=data[1]\n    x=float(x)\n    y=float(y)\n    my_msg = Float64MultiArray()\n    d=[x, y, 67.654236]\n    my_msg.data = d\n    mypub.publish(my_msg)\n    if data == \"exit\":\n        break\nUDPSock.close()\nos._exit(0)\n</code></pre> <p>Overall UDP-sockets aren't very difficult to make. Ours simplified the complexity of our project and helped build modularity. Overall this receiver and sender acts as another Ros publisher and subscriber. It has the same function it instead builds a metaphorical over-arching roscore for both roscores the sender and receiver live in. </p>"},{"location":"faq/OS/connect_to_robot/","title":"Connecting to the robot","text":"<ol> <li>plug in battery and turn on robot with the power switch, give it a moment and wait for the lidar to start spinning. </li> <li>run <code>tailscale status | grep &lt;name&gt;</code> to find the robot\u2019s IP address. Replace  with the name of the robot you are trying to connect to.  <ol> <li>go to .bashrc in my_ros_data folder and get it to look like this with your robot\u2019s name and IP address instead: </li> </ol> <ol> <li>Open a new terminal and you should see: </li> </ol> <ol> <li>You can then ssh into the robot, <code>ssh ubuntu@100.117.252.97</code> (enter your robot\u2019s IP) and enter the password that is in the lab. </li> <li>Once onboard the robot, enter the command <code>bringup</code> which starts roscore and the Turtlebot\u2019s basic functionalities.  For the Platform robots, run this: <code>roslaunch platform full_bringup.launch</code></li> <li>After that, open a new terminal (you\u2019ll be in real mode again) and run your program!</li> <li>To go back to simulation mode, go back to .bashrc and uncomment the settings for simulation mode and comment out the settings for a physical robot. Or type the command <code>sim</code> in the terminal. You will need to do this in every terminal that you open then. To switch to real mode, type command <code>real</code>.</li> </ol> <p>When you're done, run <code>sudo shutdown now</code> onboard the robot (where is ran bringup) and then turn off the robot with the power switch. </p>"},{"location":"faq/OS/copy_microsd/","title":"Copy MicroSD","text":"<ul> <li>Current best resource on web: The fastest way to clone an SD card on macOS</li> </ul>"},{"location":"faq/OS/copy_microsd/#identify-your-sd-card","title":"Identify your sd card","text":"<p>You\u2019ll need to find out which disk your SD card represents. You can run <code>diskutil list</code> and should see an output like below: <pre><code>$ diskutil list\n\n/dev/disk1 (synthesized):\n   #:                       TYPE NAME                    SIZE       IDENTIFIER\n   0:      APFS Container Scheme -                      +500.0 GB   disk1\n                                 Physical Store disk0s2\n   1:                APFS Volume Macintosh HD \u2014 Data     396.0 GB   disk1s1\n   2:                APFS Volume Preboot                 81.9 MB    disk1s2\n   3:                APFS Volume Recovery                528.5 MB   disk1s3\n   4:                APFS Volume VM                      4.3 GB     disk1s4\n   5:                APFS Volume Macintosh HD            11.0 GB    disk1s5\n\n/dev/disk4 (external, physical):\n   #:                       TYPE NAME                    SIZE       IDENTIFIER\n   0:     FDisk_partition_scheme                        *31.9 GB    disk4\n   1:             Windows_FAT_32 boot                    268.4 MB   disk4s1\n   2:                      Linux                         31.6 GB    disk4s2\n</code></pre> From that output we can see that our SD card must be <code>/dev/disk4</code> as our card is 32GB in size and has a fat32 and linux partition (standard for most raspberry pi images). You should add an r in front of disk4 so it looks like this <code>/dev/rdisk4</code>. The r means when we\u2019re copying, it will use the \u201craw\u201d disk. For an operation like this, it is much more efficient.</p>"},{"location":"faq/OS/copy_microsd/#copy-the-sd-card-as-a-disk-image-dmg","title":"Copy the SD card as a disk image (dmg)","text":"<p>Now you should run the following command, replacing 4 with whatever number you identified as your sd card:</p> <p><code>sudo gdd if=/dev/rdisk4 of=sd_backup.dmg status=progress bs=16M</code></p> <p>Tip</p> <p>You can experiment with different numbers for the block size by replacing bs=16M with larger or smaller numbers to see if it makes a difference to the speed. I\u2019ve found 16M the best for my hardware.</p> <p>You should see some progress feedback telling you the transfer speed. If you\u2019d like to experiment with different block sizes, just type ctrl + c to cancel the command, then you can run it again.</p> <p>Once the command has finished running, you\u2019ll end up with a file in your home directory called sd_backup.dmg. If you\u2019d like to backup multiple SD cards (or keep multiple backups!) simply replace sd_backup.dmg with a different file name. This will contain a complete disk image of your SD card. If you\u2019d like to restore it, or clone it to another SD card, read on.</p>"},{"location":"faq/OS/copy_microsd/#copy-the-disk-image-dmg-to-your-sd-card","title":"Copy the disk image (dmg) to your SD card","text":"<p>Resource busy or locked</p> <p>You\u2019ll first need to unmount your SD card. Do not click the eject button in finder, but run this command, replacing 4 with whatever number you identified as your sd card. Without this you will get an error.</p> <p><code>sudo diskutil unmountDisk /dev/disk4</code></p> <p>Then to copy the image, run the following command:</p> <p><code>sudo gdd of=/dev/rdisk4 if=sd_backup.dmg status=progress bs=16M</code></p> <p>Tip</p> <p>You can experiment with different numbers for the block size by replacing bs=16M with larger or smaller numbers to see if it makes a difference to the speed. I\u2019ve found 16M the best for my hardware.</p> <p>You should see some progress feedback telling you the transfer speed. If you\u2019d like to experiment with different block sizes, just type ctrl + c to cancel the command, then you can run it again.</p> <p>Once the command has finished running, your SD card should be an exact copy of the disk image you specified.</p>"},{"location":"faq/OS/download_file_from_code/","title":"How to Download Files from your online VSCode to your computer","text":""},{"location":"faq/OS/download_file_from_code/#author-michael-jiang","title":"Author: Michael Jiang","text":"<p>How do I download files from my online VSCode to my local computer?</p> <p>While the online VSCode does not offer a way to download entire folders and their contents with one click, there is a non-obvious functionality that allows you to download individual files directly from VSCode.</p> <p>Right click the desired file and select 'Download' from the menu. The file will download to your default download location on your computer.</p> <p></p> <p>You will be able to now access the files from your local system and submit them to Gradescope. To submit packages like this, you can set up a Git repository and push the files there, or you can individually download each file in the package using this method, arrange them properly, and submit it on Gradescope.</p>"},{"location":"faq/OS/handy-commands/","title":"Handy Commands","text":""},{"location":"faq/OS/handy-commands/#turtlebot3-control","title":"TurtleBot3 Control","text":"<ul> <li><code>roslaunch turtlebot3_fake turtlebot3_fake.launch</code> - Launch the TB3 Fake Turtlebot Simulator plus RViz</li> <li><code>roslaunch turtlebot3_teleop turtlebot3_teleop_key.launch</code> - Launch TB3 teleop</li> <li><code>roslaunch turtlebot3_gazebo turtlebot3_simulation.launch</code> - Drives a turtlebot around on autopilot</li> </ul>"},{"location":"faq/OS/handy-commands/#gazebo-worlds","title":"Gazebo worlds","text":"<ul> <li><code>roslaunch turtlebot3_gazebo turtlebot3_empty_world.launch</code> - Empty world - no walls</li> <li><code>roslaunch turtlebot3_gazebo turtlebot3_stage_1.launch</code> - Simple square walled area</li> <li><code>roslaunch turtlebot3_gazebo turtlebot3_stage_2.launch</code> - Simple square walled area</li> <li><code>roslaunch turtlebot3_gazebo turtlebot3_stage_3.launch</code> - Simple square walled area</li> <li><code>roslaunch turtlebot3_gazebo turtlebot3_world.launch</code> - Weird 6 sided wall world</li> <li><code>roslaunch turtlebot3_gazebo turtlebot3_house.launch</code> - Elaborate house world</li> </ul>"},{"location":"faq/OS/handy-commands/#running-rviz-with-gazebo","title":"Running RViz with Gazebo","text":"<ul> <li><code>roslaunch turtlebot3_gazebo turtlebot3_gazebo_rviz.launch</code> - Not sure why its needed.</li> </ul>"},{"location":"faq/OS/launch-files/","title":"Launch Files","text":"<p>This page will serve as a one stop shop for understanding the onboard and offboard launch files used for campus rover mark 3: 'Mutant' and 'Alien'. Along the way, it will also serve as a launch file tutorial in general.</p> <p>This is a simple walkthrough of how to create a launch file:  labnotebook/faq/launch_file_create.md</p>"},{"location":"faq/OS/launch-files/#on-board-launch-file","title":"On-board launch file","text":"<p>On-board is a short and rather simple launch file. Generally, a node should be run on-board if it meets one of two criteria:</p> <ol> <li>The node interfaces directly with the hardware of the robot, and therefore must be onboard</li> <li>The node is lightweight enough that it can run on the raspberry pi without causing too much strain to the cpu.</li> </ol>"},{"location":"faq/OS/launch-files/#nodes-that-must-be-on-board-for-hardware-purposes","title":"Nodes that must be on-board for hardware purposes","text":"<pre><code>&lt;include file=\"$(find turtlebot3_bringup)/launch/turtlebot3_robot.launch\"/&gt;\n</code></pre> <p>The line above launches the normal turtlebot bringup script. This makes the <code>include</code> tag very useful, because it is the equivalent of a <code>roslaunch</code> terminal command. In short - by using the include tag, a launch file can launch other launch files. The <code>file</code> argument is the path to the launch file you want to include. <code>$(find &lt;package name&gt;)</code> does what it says - finds the path to the specified package in your catkin workspace.</p> <pre><code>&lt;node pkg=\"raspicam_node\" type=\"raspicam_node\" name=\"raspicam_node\" output=\"screen\"&gt;\n  &lt;param name=\"camera_info_url\" value=\"package://turtlebot3_bringup/camera_info/turtlebot3_rpicamera.yaml\"/&gt;\n  &lt;param name=\"width\" value=\"640\"/&gt;\n  &lt;param name=\"height\" value=\"480\"/&gt;\n  &lt;param name=\"framerate\" value=\"50\"/&gt;\n  &lt;param name=\"enable_raw\" value=\"true\"/&gt;\n  &lt;param name=\"camera_frame_id\" value=\"camera\"/&gt;\n&lt;/node&gt;\n</code></pre> <p>Here we see a node with parameters. This snippet launches the raspberry pi camera, which must be on-board in order to publish images captured by the camera. Often, the documentation on nodes like this will inform you of all the parameters and what their values mean.</p> <p>The cpu checker node is also on-board, because it uses a python module to monitor the current cpu usage at any given time, then publish it as a ROS topic.</p> <p>the talk service must be on-board so that it can produce audio through the robot's audio port.</p>"},{"location":"faq/OS/launch-files/#nodes-that-are-onboard-because-they-are-lightweight","title":"Nodes that are onboard because they are lightweight","text":"<pre><code>&lt;node pkg=\"cr_ros_2\" type=\"scan_filter.py\" name=\"scan_filter\" output=\"screen\"&gt;&lt;/node&gt;\n&lt;node pkg=\"cr_ros_2\" type=\"detect_pickup.py\" name=\"pickup_checker\" output=\"screen\"&gt;&lt;/node&gt;\n&lt;node pkg=\"cr_ros_2\" type=\"rover_controller.py\" name=\"rover_controller\" output=\"screen\"&gt;&lt;/node&gt;\n&lt;node pkg=\"cr_ros_2\" type=\"state.py\" name=\"state\" output=\"screen\"&gt;&lt;/node&gt;\n</code></pre> <p>pickup_checker and scan_filter are both lightweight nodes that are ideal for being included on-board. The state manager is also a rather small node, if you believe it or not - all it has to do is store the current state and make state changes.</p> <p>rover_controller is arguably the anomaly - it does quite a bit to communicate with the web app and deal with navigation goals and completion. It could easily be moved off-board.</p>"},{"location":"faq/OS/launch-files/#off-board-launch-file","title":"Off-board launch file","text":"<pre><code>&lt;arg name=\"map_file\" default=\"$(find cr_ros_2)/files/basement_map.yaml\"/&gt;\n</code></pre> <p>Here we see an arg defined on it's own, outside of a node launch. This behaves the same way as assigning a variable. The value can be accessed at any point in the launch file, as demonstrated below:</p> <pre><code>&lt;include file=\"$(find cr_ros_2)/launch/mutant_navigation.launch\"&gt;\n  &lt;arg name=\"map_file\" value=\"$(arg map_file)\"/&gt;\n  &lt;arg name=\"scan_topic\" value=\"scan_filter\"/&gt;\n  &lt;arg name=\"open_rviz\" value=\"true\"/&gt;\n  &lt;arg name=\"move_forward_only\" value=\"false\"/&gt;\n&lt;/include&gt;\n</code></pre> <p>This snippet launches our custom navigation launch file, and you can see on line 2 how <code>$(arg &lt;arg name&gt;)</code> passes the value. Since this value is only passed once, you could say in this case is is redundant, but you can image how if you had to use the value multiple times how it could be very useful.</p> <pre><code>&lt;node pkg=\"topic_tools\" type=\"throttle\" name=\"cam_throttle\" args=\"messages /$(env ROS_NAMESPACE)/raspicam_node/image/compressed 2\" /&gt;\n</code></pre> <p>This line uses a provided topic tool to throttle the publish rate of the given topic down to 2 hz. More importantly, <code>$(env &lt;var&gt;)</code> is used to get the value of the given environment variable, which must be defined in .bashrc.</p> <pre><code>&lt;node pkg=\"aruco_detect\" name=\"aruco_detect\"\n  type=\"aruco_detect\" respawn=\"false\"&gt;\n  &lt;param name=\"image_transport\" value=\"$(arg transport)\"/&gt;\n  &lt;param name=\"publish_images\" value=\"true\" /&gt;\n  &lt;param name=\"fiducial_len\" value=\"$(arg fiducial_len)\"/&gt;\n  &lt;param name=\"dictionary\" value=\"$(arg dictionary)\"/&gt;\n  &lt;param name=\"do_pose_estimation\" value=\"true\"/&gt;\n  &lt;remap from=\"/camera/compressed\"\n      to=\"$(arg camera)/$(arg image)/$(arg transport)_throttle\"/&gt; &lt;!-- removed throttle --&gt;\n  &lt;remap from=\"/camera_info\" to=\"$(arg camera)/camera_info\"/&gt;\n  &lt;remap from=\"/fiducial_transforms\" to=\"/$(env ROS_NAMESPACE)/fiducial_transforms\" /&gt;\n&lt;/node&gt;\n</code></pre> <p>Now we see the use of remapping topics. This is very useful particularly in namespacing, as well as ensuring that a node is subscribed to the right topic for a certain kind of data - such as camera feed, in this case.</p> <pre><code>&lt;node pkg=\"tf\" type=\"static_transform_publisher\" name=\"fid_153\" args=\"19.6 21.8 0.825 0 3.14159 0 /map /fid_153 100\" /&gt; &lt;!-- charging dock --&gt;\n</code></pre> <p>The final part of the file are all of the static transforms - these are entities that do not move with respect to their reference frame. The six numerical args are, in order, from left to right, x, y, z, yaw, pitch, roll. NOTE: the orientation of fiducials needs to be reviewed and corrected - though the positions are very accurate</p>"},{"location":"faq/OS/launch-files/#additional-tricks","title":"Additional Tricks","text":""},{"location":"faq/OS/launch-files/#args-vs-params","title":"Args vs Params","text":"<p>Here is the key difference:</p> <ol> <li>args are self-contined within launch files. They are defined and accessed only within launch files.</li> <li>Params are how to pass values (such as args) into an executable node. They send values to the ROS parameter server. For a param to be accepted within a node, it must get the paramter from the server. Here is an example from rover_controller.py:</li> </ol> <pre><code># This is how the python file gets the parameter. notice the paramter is namespaced to the name of the node\ncam_topic = rospy.get_param(\"/rover_controller/cam_topic\")\n</code></pre> <pre><code>&lt;!-- This is how the paramter is passed into the node--&gt;\n&lt;node pkg=\"cr_ros_3\" type=\"rover_controller.py\" name=\"rover_controller\" output=\"screen\"&gt;\n    &lt;param name=\"cam_topic\" value=\"/usb_cam_node/image_raw/compressed\"/&gt;\n  &lt;/node&gt;\n</code></pre>"},{"location":"faq/OS/launch-files/#grouping-and-logic","title":"Grouping and Logic","text":"<p>The <code>&lt;group&gt;</code> tag allows for clusters of node to only be launched if the value passed to the group tag is true. A basic example is seen with modules like voice:</p> <pre><code>&lt;arg name=\"voice\" default=\"true\"/&gt;\n&lt;group if=\"$(arg voice)\"&gt;\n  &lt;include file=\"$(find cr_ros_3)/launch/voice.launch\"/&gt;\n&lt;/group&gt;\n</code></pre> <p>You can see that voice.launch is included only if the voice arg is true. the voice arg can be manually set to false to diable voice, as documented in the cr_ros_3 readme.</p> <p>But what if your condition is based on something else? There is a way to evaluate operations in launch to a boolean. Observe:</p> <pre><code>&lt;group if=\"$(eval arg('robot') == 'ALIEN')\"&gt;\n</code></pre> <p><code>eval</code> will take a python-evaluable boolean expression and evaluate it to true or false. This group above will only launch when the campus rover model is set to \"ALIEN\".</p>"},{"location":"faq/OS/launch-files/#file-path-shortcut","title":"File path shortcut","text":"<p>As you know, <code>$(find &lt;package name&gt;)</code> will return the absolute path to the specified package, which allows for launch files from eternal packages to be included in your launch. A handy shortcut exists for launch files in the same package. Suppose A.launch and B.launch are in the same launch folder. A.launch can include B.launch like so using <code>$(dirname)</code>:</p> <pre><code>&lt;include file=\"$(dirname)/B.launch\"&gt;\n</code></pre> <p>This trick only works in ROS versions that are newer than Lunar.</p>"},{"location":"faq/OS/launch-files/#shell-scripts","title":"Shell scripts","text":"<p>Shell scripts can be run in roslaunch the same way nodes are launched. Here's an example:</p> <pre><code>&lt;node pkg=\"cr_ros_3\" type=\"camera_reconfigure.sh\" name=\"camera_reconfigure\" output=\"screen\"/&gt;\n</code></pre> <p>The above example runs a script that will dynamically flip upside down raspberry pi camera's video output.</p>"},{"location":"faq/OS/launch_file_create/","title":"Creating and Executing Launch Files","text":"<p>by Helen Lin edited by Jeremy Huey</p>"},{"location":"faq/OS/launch_file_create/#introduction","title":"Introduction","text":"<p>This guide shows you how to create launch files for your code so that you can launch multiple nodes at once rather than running each node individually.</p>"},{"location":"faq/OS/launch_file_create/#step-1","title":"Step 1","text":"<p>Open a new terminal window and navigate to a package you want to create a launch file in. Create a folder called 'launch'.</p> <p><code>mkdir launch</code></p>"},{"location":"faq/OS/launch_file_create/#step-2","title":"Step 2","text":"<p>Navigate to the launch directory and create a new launch file ending in .launch. Replace 'name' with the name of your launch file.</p> <p><code>touch name.launch</code></p>"},{"location":"faq/OS/launch_file_create/#step-3","title":"Step 3","text":"<p>Add the following code and fill in the parameters within the double quotes.</p> <pre><code>&lt;launch&gt;\n  &lt;node name=\" \" pkg=\" \" type=\" \" output=\" \"/&gt;\n  &lt;node name=\" \" pkg=\" \" type=\" \" output=\" \"/&gt;\n&lt;/launch&gt;\n</code></pre> <p>Here is an example of what the node will look like filled in, using code from the Mini Scouter project:</p> <p><pre><code>&lt;launch&gt;\n  &lt;node name=\"MiniScoutMain\" pkg=\"mini_scouter\" type=\"MiniScoutMain.py\" output=\"screen\"&gt;&lt;/node&gt;\n  &lt;node name=\"laser_scan\" pkg=\"mini_scouter\" type=\"scan_arouund.py\" output=\"screen\"&gt;&lt;/node&gt;\n&lt;/launch&gt;\n</code></pre> The pkg name can be found in the package.xml. Eg.  <pre><code>&lt;?xml version=\"1.0\"?&gt;\n&lt;package format=\"2\"&gt;\n  &lt;name&gt;object_sorter&lt;/name&gt;\n  &lt;version&gt;0.0.0&lt;/version&gt;\n</code></pre></p>"},{"location":"faq/OS/launch_file_create/#step-4","title":"Step 4","text":"<p>Make sure you have run the following command on all of the files used in the launch file so they can all be found by ROS launch. Replace 'name' with the name of the python file.</p> <p><code>chmod +x name.py</code></p> <p>Change the permissions of the launch file as well by going to the launch directory and running the following command. Replace 'name' with the name of the launch file.</p> <p><code>chmmod +x name.launch</code></p>"},{"location":"faq/OS/launch_file_create/#step-5","title":"Step 5","text":"<p>Open a new terminal window and run the following command. Replace 'package_name' with the name of the package and 'name' with the name of the launch file.</p> <p><code>roslaunch package_name name.launch</code></p> <p>For example, to run the Mini Scouter launch file:</p> <p><code>roslaunch mini_scouter teleop.launch</code></p> <p>All of the nodes you specified in the launch file should now be running.</p>"},{"location":"faq/OS/launch_file_create/#optional-launch-process-in-new-window","title":"Optional, launch process in new window","text":"<p>To have a node launch and open in a new window, such as to run things like key_publisher.py, you can modify the line to include this:  <code>&lt;node pkg=\"object_sorter\" type=\"key_publisher.py\" name=\"key\" output=\"screen\" launch-prefix=\"xterm -e\"/&gt;</code> You must then run in terminal:  <pre><code>sudo apt-get install xterm\n</code></pre></p>"},{"location":"faq/OS/launch_file_create/#more-info-on-launch-files","title":"More info on launch files","text":"<p>To continue to get more information on launch files, go to here:  labnotebook/faq/launch-files.md</p>"},{"location":"faq/OS/moving-files/","title":"How do I Download a file from code.[yourname].ros.campusrover.org to your actual computer","text":"<p>If you're having trouble getting a file from your code virtual machine onto your actual computer to submit it onto Gradescope, never fear, an easy solution is here:</p> <ol> <li>Right click on the desired file from the file explorer (normally on the left panel) on your code viewer.</li> <li>Select 'Download' and the file will download to your browser's default download location (typically your 'Downloads' folder).</li> </ol> <p></p> <p>Voila! You have successfully moved a file from your online code to your machine.</p>"},{"location":"faq/OS/publish_commands_to_commandline/","title":"Publish commands to commandline","text":""},{"location":"faq/OS/publish_commands_to_commandline/#adam-rogers","title":"Adam Rogers","text":""},{"location":"faq/OS/publish_commands_to_commandline/#how-to-publish-commands-to-a-commandline-in-python","title":"How to publish commands to a commandline in Python","text":"<p>First, import the <code>subprocess</code> module. This module has functions that allow Python to interact with a commandline.</p> <p><code>import subprocess</code></p> <p>Use the Popen class from subprocess to publish commands. Keep in mind, these commands will be published to the terminal that you are currently running the python script in. The variable <code>args</code> should be a list that represents the command being published with each index being a word in the command separated by spaces. For example, if the command I wanted to run was <code>ls -l</code>, args would be <code>['ls', '-l']</code>.</p> <p><code>p = subprocess.Popen(args, stdout=subprocess.PIPE)</code></p> <p>In some cases, you will want to terminate certain commands that you have previously published. If so, the best way is to add each Popen object to a dictionary, and when you want to terminate a command, find it in the dictionary and call the <code>terminate()</code> function on that object. For the dictionary, I suggest using the command as a String separated by spaces for the key and the Popen object as the value. For example:</p> <pre><code>p = command_dictionary['ls -l']\np.terminate()\n</code></pre>"},{"location":"faq/ROS/Self-Defined-Message/","title":"How to define and Use your own message types","text":""},{"location":"faq/ROS/Self-Defined-Message/#how-to-create-a-new-message-type","title":"How to create a new message type?","text":"<p>After created a ROS package, our package is constructed by a src folder, a CMakeLists.txt file, and a package.xml file. We need to create a msg folder to hold all of our new msg file. Then in your msg folder, create a new file .msg, which contains fields you needed for this message type.  For each fields in your msg file, define the name of the field and the type of the field (usually use std_msgs/ or geometry_msgs/). &lt;br &gt;   For example, if you want your message to have a list of string and an integer, you msg file should look like:  <pre><code>std_msgs/String[] list\nstd_msgs/Int16 int</code></pre>"},{"location":"faq/ROS/Self-Defined-Message/#how-to-let-the-new-message-type-recognized-by-ros","title":"How to let the new message type recognized by ROS?","text":"<p>There are some modifications you need to make to CMakeLists.txt and package.xml in order to let the new message type recognized by ROS. &lt;br &gt; For CMakeLists.txt: 1. Make sure message_generation is in find_package(). 2. Uncomment add_message_files() and add your .msg file name to add_message_files(). 3. Uncomment generate_messages() 4. Modify catkin_package() to</p> <pre><code>catkin_package(\n  CATKIN_DEPENDS message_runtime\n)</code></pre> <ol> <li>Uncomment include in include_directories()</li> </ol> <p>For package.xml:   1. Uncomment message_generation on line 40   2. Uncomment message_runtime on line 46</p>"},{"location":"faq/ROS/Self-Defined-Message/#how-to-use-the-newly-created-message-type","title":"How to use the newly created message type?","text":"<ol> <li>How to import the message?  <pre><code>from package_name.msg import message_name as message_name</code></pre></li> </ol> <p>If your message type contains a list of buildin message type, also make sure to import that buildin message type:  <pre><code>from std_msgs.msg import String</code></pre></p> <ol> <li>How to use the message? &lt;br &gt;   The publisher and subscriber's syntax are the same. However, we want to create a new topic name and make sure the new topic message type is specified. For example in my project I have a message type called see_intruder:  <pre><code>self.detect_intruder_pub = rospy.Publisher('/see_intruder', see_intruder, queue_size=1)\nself.detect_intruder_sub=rospy.Subscriber('/see_intruder', see_intruder,self.see_intruder_callback)\n  </code></pre></li> </ol> <p>Since our new message depends on some build in message type, when we try to access the feild of our msg, we need to do msg..data given that the build in message type has a field named data. So in the first example I had, if we want to access the string stored in index int of list, I need to do  <pre><code>msg.list[msg.int.data].data</code></pre>"},{"location":"faq/ROS/Self-Defined-Message/#how-to-check-if-the-new-message-type-is-recognized-by-ros","title":"How to check if the new message type is recognized by ROS?","text":"<p>Do a cm in your vnc. if the message is successfully recognized by ROS, you will see the msg file being successfully generated. </p>"},{"location":"faq/ROS/Self-Defined-Message/#having-an-error-no-module-named-msg","title":"Having an error \"No module named &lt;&gt;.msg\"?","text":"<p>In your vnc terminal, type the command</p> <pre><code>source ~/catkin_ws/devel/setup.bash</code></pre> <p>This should solve the error.</p>"},{"location":"faq/ROS/custom_msg_FAQ/","title":"FAQ: How to Add Custom Messages in ROS","text":"<ul> <li>Yutian (Tim) Fan</li> <li>Dec 13 2024</li> <li>ROS version: Noetic</li> </ul>"},{"location":"faq/ROS/custom_msg_FAQ/#advantages-of-custom-messages","title":"Advantages of Custom Messages","text":"<ul> <li>Enable custom communication tailored to specific use cases.</li> <li>Improve code clarity by organizing data into structured messages.</li> <li>Reduce the need for parsing generic messages in your nodes.</li> <li>Facilitate efficient collaboration by defining clear data contracts between nodes.</li> </ul>"},{"location":"faq/ROS/custom_msg_FAQ/#steps-to-create-a-custom-message","title":"Steps to Create a Custom Message","text":""},{"location":"faq/ROS/custom_msg_FAQ/#1-create-the-msg-files","title":"1. Create the .msg Files","text":"<ul> <li>Inside your package directory, create a <code>msg</code> folder to store the custom .msg files.</li> <li>Define your message file using the format:   <pre><code>dependency/data_type name\n</code></pre></li> <li>Example: For a message representing a frontier:   <pre><code>geometry_msgs/Point centroid\nint32 size\n</code></pre></li> </ul>"},{"location":"faq/ROS/custom_msg_FAQ/#2-edit-cmakeliststxt","title":"2. Edit CMakeLists.txt","text":"<ul> <li>In the <code>find_package()</code> section, add <code>message_generation</code>:</li> </ul> <p><pre><code>find_package(catkin REQUIRED COMPONENTS\n  ...\n  message_generation\n  ...\n)\n</code></pre> - Uncomment <code>add_message_files()</code> and specify your .msg files:</p> <p><pre><code>add_message_files(\n  FILES\n  Frontier.msg\n  FrontierList.msg\n  PathList.msg\n)\n</code></pre> - Uncomment <code>generate_messages()</code> and include <code>std_msgs</code> under dependencies:</p> <pre><code>generate_messages(\n  DEPENDENCIES\n  std_msgs\n  geometry_msgs\n)\n</code></pre>"},{"location":"faq/ROS/custom_msg_FAQ/#3-edit-packagexml","title":"3. Edit package.xml","text":"<p>Add the necessary dependencies for generating and using custom messages:</p> <pre><code>&lt;build_depend&gt;message_generation&lt;/build_depend&gt;\n&lt;exec_depend&gt;message_runtime&lt;/exec_depend&gt;\n</code></pre>"},{"location":"faq/ROS/custom_msg_FAQ/#4-build-your-workspace","title":"4. Build Your Workspace","text":"<ul> <li>Navigate to your workspace directory:</li> </ul> <p><pre><code>cd ~/catkin_ws\n</code></pre> - Run <code>catkin_make</code> to compile your workspace and generate the custom message files.</p>"},{"location":"faq/ROS/custom_msg_files/","title":"Installing and Building Custom Message Files in ROS","text":""},{"location":"faq/ROS/custom_msg_files/#author","title":"Author","text":"<ul> <li>Harry Yu</li> <li>Dec 10 2024</li> </ul>"},{"location":"faq/ROS/custom_msg_files/#overview","title":"Overview","text":"<p>In ROS, custom message files allow you to define your own message types for communication between nodes. This guide will walk you through the process of creating, installing, and building custom message files in a ROS package.</p>"},{"location":"faq/ROS/custom_msg_files/#step-1-define-your-custom-message-files","title":"Step 1: Define Your Custom Message Files","text":"<ol> <li>Create a <code>msg</code> Directory:</li> </ol> <p>Inside your package, create a directory named <code>msg</code>:</p> <pre><code>cd ~/catkin_ws/src/my_custom_msgs\nmkdir msg\n</code></pre> <ol> <li>Create Message Files:</li> </ol> <p>Inside the <code>msg</code> directory, create your custom message files. For example, create a file named <code>MyMessage.msg</code>:</p> <pre><code> int32 round_time_remaining\n string game_phase\n bool bomb_planted\n geometry_msgs/Point bomb_location\n string[] dead_players\n</code></pre>"},{"location":"faq/ROS/custom_msg_files/#step-3-modify-cmakeliststxt","title":"Step 3: Modify <code>CMakeLists.txt</code>","text":"<p>Edit the <code>CMakeLists.txt</code> file in your package to include the message generation dependencies:</p> <p>Find and uncomment/add the following lines:</p> <pre><code>find_package(catkin REQUIRED COMPONENTS\n  std_msgs\n  message_generation\n)\n</code></pre> <p>Add your message files:</p> <pre><code>add_message_files(\n  FILES\n  GameStateMsg.msg\n)\n</code></pre> <p>Generate messages:</p> <pre><code>generate_messages(\n  DEPENDENCIES\n  std_msgs\n)\n</code></pre> <p>Include <code>message_runtime</code> in <code>catkin_package</code>:</p> <pre><code>catkin_package(\n  CATKIN_DEPENDS message_runtime\n)\n</code></pre>"},{"location":"faq/ROS/custom_msg_files/#step-4-modify-packagexml","title":"Step 4: Modify <code>package.xml</code>","text":"<p>Edit the <code>package.xml</code> file to include the message generation dependencies:</p> <ol> <li>Add the following dependencies:</li> </ol> <pre><code>&lt;build_depend&gt;message_generation&lt;/build_depend&gt;\n&lt;exec_depend&gt;message_runtime&lt;/exec_depend&gt;\n</code></pre>"},{"location":"faq/ROS/custom_msg_files/#step-5-build-your-package","title":"Step 5: Build Your Package","text":"<pre><code>```\nws\ncatkin_make\n```\n</code></pre> <p>And you've successfully created your own message type!</p>"},{"location":"faq/ROS/logging/","title":"Logging","text":""},{"location":"faq/ROS/logging/#what-are-ros-logs","title":"What are <code>ROS</code> logs?","text":"<ul> <li>Two different things called logs, data logs and normal (<code>rosout</code>) logs</li> <li>Data logs are collected with <code>rosbag</code> and are used to replay or simulate all messages which were sent.</li> <li>This is about the other kind of logs, which are primarily used for sending warnings, errors and debugging.</li> <li>Solves the problem of distributed information. Many nodes have information to share, which requires a robust logging infrastructure</li> </ul>"},{"location":"faq/ROS/logging/#what-do-logs-do","title":"What do logs do?","text":"<ol> <li>Get printed to <code>stdout</code> or <code>stderr</code> (screen of terminal window running node)</li> <li>Get sent to <code>rosout</code> which acts as unified log record</li> <li>Get written a file on the computer running <code>roscore</code>. The logs are stored at <code>~/.ros/logs/</code>. The subdirectory <code>latest</code> can be used, or the command <code>$ roscd log</code> will point to the current log directory.</li> </ol>"},{"location":"faq/ROS/logging/#log-implementation","title":"Log implementation","text":"<p><code>std_msgs/Log</code> type: </p> <p>No publisher needs to be made to log in <code>rospy</code>. Instead, use the following functions: </p> <p>what goes where? </p> <p>A well formatted GUI exists to show all log messages and can be accessed with <code>$ rqt_console</code>: </p>"},{"location":"faq/ROS/logging/#more-resources","title":"More resources","text":"<ul> <li>My slides</li> <li><code>rospy</code> logging overview</li> <li>Programming Robots with ROS ch 21, Quigley, Gerkey, Smart</li> </ul>"},{"location":"faq/ROS/logging/#message-node","title":"Message Node","text":"<ul> <li>It subscribes to the /rosout topic. Every node brought up on the roscore can be seen in the /rosout. Therefore,</li> </ul> <p>the Message Node, as a traffic cop, could communicate with any node through /rosout.</p> <ul> <li>To pass the message from a node to the Message Node, the code just needs one line of code, i.e. rospy.log(). The API can be   found in the Log Implementation section above.</li> <li>User who wants to use Message Node only needs to put its name and what operations he needs to do inside the</li> </ul> <p>callback function. In the example, if user wants to subscribe to the /talker_demo_node, he can just find it by</p> <p><code>msg.name == \"/talker_demo_node\"</code>.  Then, he can do some operation in it.</p> <p></p>"},{"location":"faq/ROS/logging/#alexander-feldman-feldmanaygmailcom","title":"@Alexander Feldman, feldmanay@gmail.com","text":""},{"location":"faq/ROS/message_types_cheatsheet/","title":"Package common_msgs","text":""},{"location":"faq/ROS/message_types_cheatsheet/#geometry_msgstwistmsg","title":"geometry_msgs/Twist.msg","text":"<ul> <li>Vector3 linear</li> <li>Vector3 angular</li> </ul>"},{"location":"faq/ROS/message_types_cheatsheet/#geometry_msgspose2dmsg","title":"geometry_msgs/Pose2D.msg","text":"<ul> <li>float64 x</li> <li>float64 y</li> <li>float64 theta</li> </ul>"},{"location":"faq/ROS/message_types_cheatsheet/#geometry_msgsposemsg","title":"geometry_msgs/Pose.msg","text":"<ul> <li>Point position</li> <li>Quaternion orientation</li> </ul>"},{"location":"faq/ROS/message_types_cheatsheet/#geometry_msgsquaternionmsg","title":"geometry_msgs/Quaternion.msg","text":"<ul> <li>float64 x</li> <li>float64 y</li> <li>float64 z</li> <li>float64 w</li> </ul>"},{"location":"faq/ROS/message_types_cheatsheet/#geometry_msgsquaternionstampedmsg","title":"geometry_msgs/QuaternionStamped.msg","text":"<ul> <li>Header header</li> <li>Quaternion quaternion</li> </ul>"},{"location":"faq/ROS/message_types_cheatsheet/#std_msgsheadermsg","title":"std_msgs/Header.msg","text":"<ul> <li>uint32 seq</li> <li>time stamp</li> <li>string frame_id</li> </ul>"},{"location":"faq/ROS/message_types_cheatsheet/#geometry_msgsvector3msg","title":"geometry_msgs/Vector3.msg","text":"<ul> <li>float64 x</li> <li>float64 y</li> <li>float64 z</li> </ul>"},{"location":"faq/ROS/message_types_cheatsheet/#geometry_msgspointmsg","title":"geometry_msgs/Point.msg","text":"<ul> <li>float64 x</li> <li>float64 y</li> <li>float64 z</li> </ul>"},{"location":"faq/ROS/model_teleportation/","title":"Teleport Model within Gazebo Simulation","text":"<p>by Frank Hu</p> <p>If you want to move existing model within Gazebo simulation, you can do so via <code>rosservice call</code>.</p> <p>Open a new terminal tab, and enter</p> <pre><code>rosservice call /gazebo/set_model_state '{model_state: { model_name: MODEL+NAME, pose: { position: { x: 0, y: 0 ,z: 0 }, orientation: {x: 0, y: 0, z: 0, w: 0 } }, twist: { linear: {x: 0.0 , y: 0 ,z: 0 } , angular: { x: 0.0 , y: 0 , z: 0.0 } } , reference_frame: world } }'\n</code></pre> <p>Special Note:</p> <p>Some spaces in the above command CANNOT be deleted, if there is an error when using the above command, check you command syntax first.</p> <p>E.g: Right: <code>{ x: 0, y: 0 ,z: 0 }</code> Wrong:  <code>{ x:0, y:0 ,z:0 }</code> (missing space after <code>:</code>)</p> <p>You can also experiment with this command to set robot arm's state, to make it into a certain orientation like this rrbot</p> <p></p> <p><code>rosservice call /gazebo/set_model_state '{model_state: { model_name: rrbot, pose: { position: { x: 1, y: 1 ,z: 10 }, orientation: {x: 0, y: 0.491983115673, z: 0, w: 0.870604813099 } }, twist: { linear: {x: 0.0 , y: 0 ,z: 0 } , angular: { x: 0.0 , y: 0 , z: 0.0 } } , reference_frame: world } }'</code></p> <p></p>"},{"location":"faq/ROS/modular_teleop/","title":"Use teleop in your code","text":"<p>Chris Minkwon Choi</p>"},{"location":"faq/ROS/modular_teleop/#introduction","title":"Introduction","text":"<p>Among multiple ways to move a robot, teleop is one of the most intuitive methods. In your project, you can add teleop feature easily for various purposes. For example, this code is used in the Robotag Project. Robotag Project is a game where robots play game of tag. In Robotag Project, this code is used to let the user take over the control and play as either cop or rubber. </p>"},{"location":"faq/ROS/modular_teleop/#how-to-use","title":"How to use","text":"<p>The control is very intuitive, and there is short instruction built in to the system. w,a,d,x is for each directions, and s stops the robot. You can also play around with the original teleop and familiarize with the control using the original teleop.</p>"},{"location":"faq/ROS/modular_teleop/#what-to-add","title":"What to add","text":"<p>Add the following code to the main class (not in a loop or if statement). These declares constants and mothods for teleop. </p> <pre><code>BURGER_MAX_LIN_VEL = 0.22\nBURGER_MAX_ANG_VEL = 2.84\nWAFFLE_MAX_LIN_VEL = 0.26\nWAFFLE_MAX_ANG_VEL = 1.82\nLIN_VEL_STEP_SIZE = 0.01\nANG_VEL_STEP_SIZE = 0.1\n\nmsg = \"\"\"\nControl Your TurtleBot3!\n---------------------------\nMoving around:\n        w\n   a    s    d\n        x\n\nw/x : increase/decrease linear velocity (Burger : ~ 0.22, Waffle and Waffle Pi : ~ 0.26)\na/d : increase/decrease angular velocity (Burger : ~ 2.84, Waffle and Waffle Pi : ~ 1.82)\n\nspace key, s : force stop\n\nCTRL-C to quit\n\"\"\"\n\ne = \"\"\"\nCommunications Failed\n\"\"\"\n\ndef getKey():  \n    if os.name == 'nt':\n      if sys.version_info[0] &gt;= 3:\n        return msvcrt.getch().decode()\n      else:\n        return msvcrt.getch()\n\n    tty.setraw(sys.stdin.fileno())\n    rlist, _, _ = select.select([sys.stdin], [], [], 0.1)\n    if rlist:\n        key = sys.stdin.read(1)\n    else:\n        key = ''\n\n    termios.tcsetattr(sys.stdin, termios.TCSADRAIN, settings)\n    return key\n\ndef vels(target_linear_vel, target_angular_vel):\n    return \"currently:\\tlinear vel %s\\t angular vel %s \" % (target_linear_vel,target_angular_vel)\n\ndef makeSimpleProfile(output, input, slop):\n    if input &gt; output:\n        output = min( input, output + slop )\n    elif input &lt; output:\n        output = max( input, output - slop )\n    else:\n        output = input\n    return output\n\ndef constrain(input, low, high):\n    if input &lt; low:\n      input = low\n    elif input &gt; high:\n      input = high\n    else:\n      input = input\n    return input\n\ndef checkLinearLimitVelocity(vel):\n    vel = constrain(vel, -BURGER_MAX_LIN_VEL, BURGER_MAX_LIN_VEL)\n    return vel\n\ndef checkAngularLimitVelocity(vel):\n    vel = constrain(vel, -BURGER_MAX_ANG_VEL, BURGER_MAX_ANG_VEL)\n    return vel\n</code></pre> <p>Then when you want to use teleop, add following code. These are the actual moving parts. Change 'use-teleop' to any other state name you need. If you want teleop to work in all times, take this code outside the if statement. </p> <pre><code>if state=='use-teleop':\n    if rospy.Time.now().to_sec()-time_switch.to_sec()&gt;10:\n        inc_x = posex2 -posex1\n        inc_y = posey2 -posey1\n        angle_to_goal = atan2(inc_y, inc_x)\n        z=math.sqrt((inc_x*inc_x)+(inc_y*inc_y))\n        if z &gt; .05:\n            if z &lt; .3:\n                twist.linear.x=0\n                twist.angular.z=0\n                state=\"cop\"\n                time_switch=rospy.Time.now()\n\n\n\n    if os.name != 'nt':\n        settings = termios.tcgetattr(sys.stdin)\n\n    #rospy.init_node('turtlebot3_teleop')\n    cmd_vel_msg = '/cmd_vel'\n    cmd_vel_pub = rospy.Publisher(cmd_vel_msg, Twist, queue_size=10)\n    turtlebot3_model = rospy.get_param(\"model\", \"burger\")\n    cmd_vel_pub.publish(twist)\n    inc_x = posex2 -posex1\n    inc_y = posey2 -posey1\n\n    status = 0\n    target_linear_vel   = 0.0\n    target_angular_vel  = 0.0\n    control_linear_vel  = 0.0\n    control_angular_vel = 0.0\n\n    try:\n        print(msg)\n        while(1):\n            key = getKey()\n            if key == 'w' :\n                target_linear_vel = checkLinearLimitVelocity(target_linear_vel + LIN_VEL_STEP_SIZE)\n                status = status + 1\n                print(vels(target_linear_vel,target_angular_vel))\n            elif key == 'x' :\n                target_linear_vel = checkLinearLimitVelocity(target_linear_vel - LIN_VEL_STEP_SIZE)\n                status = status + 1\n                print(vels(target_linear_vel,target_angular_vel))\n            elif key == 'a' :\n                target_angular_vel = checkAngularLimitVelocity(target_angular_vel + ANG_VEL_STEP_SIZE)\n                status = status + 1\n                print(vels(target_linear_vel,target_angular_vel))\n            elif key == 'd' :\n                target_angular_vel = checkAngularLimitVelocity(target_angular_vel - ANG_VEL_STEP_SIZE)\n                status = status + 1\n                print(vels(target_linear_vel,target_angular_vel))\n            elif key == ' ' or key == 's' :\n                target_linear_vel   = 0.0\n                control_linear_vel  = 0.0\n                target_angular_vel  = 0.0\n                control_angular_vel = 0.0\n                print(vels(target_linear_vel, target_angular_vel))\n            elif key == 'r':\n                state = 'robber'\n                break\n            else:\n                if (key == '\\x03'):\n                    break\n\n            if status == 20 :\n                print(msg)\n                status = 0\n\n            twist = Twist()\n\n            control_linear_vel = makeSimpleProfile(control_linear_vel, target_linear_vel, (LIN_VEL_STEP_SIZE/2.0))\n            twist.linear.x = control_linear_vel; twist.linear.y = 0.0; twist.linear.z = 0.0\n\n            control_angular_vel = makeSimpleProfile(control_angular_vel, target_angular_vel, (ANG_VEL_STEP_SIZE/2.0))\n            twist.angular.x = 0.0; twist.angular.y = 0.0; twist.angular.z = control_angular_vel\n\n            cmd_vel_pub.publish(twist)\n\n    except:\n        print(e)\n\n    finally:\n        twist = Twist()\n        twist.linear.x = 0.0; twist.linear.y = 0.0; twist.linear.z = 0.0\n        twist.angular.x = 0.0; twist.angular.y = 0.0; twist.angular.z = 0.0\n        cmd_vel_pub.publish(twist)\n\n    if os.name != 'nt':\n        termios.tcsetattr(sys.stdin, termios.TCSADRAIN, settings)\n</code></pre> <p>If you want to enter this 'use-teleop' using 'z' key,</p> <pre><code>if os.name != 'nt':\n    settings = termios.tcgetattr(sys.stdin)\nkey = getKey()\nif key == 'z': #h for human\n    state = 'use-teleop'\n</code></pre>"},{"location":"faq/ROS/modular_teleop/#customization","title":"Customization","text":"<p>This code can be edited for customization. Edit the 'msg' String at the top of the code for GUI, and edit constants for maximum speed. </p>"},{"location":"faq/ROS/robot-multitasking/","title":"robot multitasking","text":""},{"location":"faq/ROS/robot-multitasking/#question","title":"Question","text":"<p>How can I make my robot do more than one thing at once, while being in the same state?</p>"},{"location":"faq/ROS/robot-multitasking/#background","title":"Background","text":"<p>It's common for the driving logic of a ROS program to be structured like this:</p> <p><pre><code>rate = rospy.Rate(20)\n\nwhile not rospy.is_shutdown():\n  # your code\n  rate.sleep()\n</code></pre> Here, <code>your code</code> runs at most 20 times per second (it can run less frequently if <code>your code</code> takes longer than 1/20<sup>th</sup> of a second to execute). This is a useful framework, especially if you want to have your robot continually check for its current state and execute code accordingly.</p> <p>For example, in the code below, the robot executes different functions depending on whether its state is that of <code>follower</code> or <code>leader</code>.</p> <pre><code>rate = rospy.Rate(20)\nwhile not rospy.is_shutdown():\n  if robot_state == 'follower':\n    follow()\n  elif robot_state == 'leader':\n    lead()\n  rate.sleep()\n</code></pre> <p>But suppose your robot must do more than one thing at once, that conceptually falls under its responsibilities as a <code>leader</code>. For example, it might have to execute some <code>complex_function</code>, while at the same time publishing messages over a topic.</p> <p>In my case, my robot had to publish messages to other robots, letting it know that it was the leader, while at the same time telling the robots where to go.</p> <p>One solution to this would be to write a whole new ROS node that publishes the required messages (in its own <code>while not rospy.is_shutdown</code> loop), and have <code>complex_function</code> run on the current node. But this separates into two processes what belongs as a logical unit, and also carries with it the overhead of having to launch another ROS node.</p>"},{"location":"faq/ROS/robot-multitasking/#answer","title":"Answer","text":"<p>A simpler solution is to use multithreading. For example:</p> <p><pre><code>def lead():\n  if send_messages_thread is None:\n    send_messages_thread = Thread(target = send_messages, daemon = True)\n    send_messages_thread.start()\n  else:\n    complex_function()\n\ndef send_messages():\n  while True:\n    your_publisher.publish('message')\n</code></pre> ===WARNING: This is pseudocode, since, among other reasons, we didn't define the variable <code>send_messages_thread</code>. In real code, <code>send_messages_thread</code> should probably be an attribute of your robot, which you should define as a python <code>Class</code> (along the same lines, <code>robot_state</code> above should also be an attribute).===</p> <p>This code first checks if the variable <code>send_messages_thread</code> has been initialized. If it hasn't, it defines a thread that, when started in <code>send_messages_thread.start()</code>, executes the function <code>send_messages</code>. Note that the thread is defined as a <code>daemon</code> thread, which shuts down when your current thread (the one in which you're defining the <code>send_messages_thread</code> shuts down).</p>"},{"location":"faq/ROS/robot-multitasking/#upshot","title":"Upshot","text":"<p>This is the basis of a design framework you can apply to make your robot multitask appropriately. It's only the basis, because by leveraging a couple more items in python's concurrency library you can make your robot multitask in more sophisticated ways.</p> <p>For example, suppose you have a mechanism which allows for communication between threads. By this device, thread A can tell thread B that something has happened that should cause B to act differently, or to simply terminate.</p> <p>This would open up a lot of doors. In our example above, for example, you would be able to shut down the <code>send_messages_thread</code> at will by having the while loop of <code>send_messages</code> check for the communication from thread A that an event has happened:</p> <pre><code>def send_messages():\n  while not threadA_told_me_to_stop:\n    your_publisher.publish('message')\n</code></pre> <p>But this mechanism that allows for threads to communicate with each other is just what is provided by python's <code>Event</code> object. Together with the <code>join</code> function that allows a thread to wait for another thread to finish, you can do a suprising variety of multitasking.</p>"},{"location":"faq/ROS/ros-bridge-and-roslibjs/","title":"Overview of ROSBridge and ROSLIBJS","text":"<p>This FAQ is deigned to provide an overview of ROSBridge and ROSLIBJS, two important tools for integrating web applications, such as the command control dashboard, with the Robot Operating System (ROS).</p>"},{"location":"faq/ROS/ros-bridge-and-roslibjs/#table-of-contents","title":"Table of contents","text":"<ul> <li>What are ROSBridge and ROSLIBJS?</li> <li>How do ROSBridge and ROSLIBJS work together?</li> <li>How do I install and use ROSBridge and ROSLIBJS?</li> <li>Are there any limitations or challenges when using ROSBridge and ROSLIBJS?</li> </ul>"},{"location":"faq/ROS/ros-bridge-and-roslibjs/#_1","title":"ROSBridge and ROSLIBJS","text":""},{"location":"faq/ROS/ros-bridge-and-roslibjs/#what-are-rosbridge-and-roslibjs","title":"What are ROSBridge and ROSLIBJS?","text":"<p>ROSBridge is a package for the Robot Operating System (ROS) that provides a JSON-based interface for interacting with ROS through WebSocket protocol (usually through TCP). It allows external applications to communicate with ROS over the web without using the native ROS communication protocol, making it easier to create web-based interfaces for ROS-based robots.</p> <p>ROSLIBJS is a JavaScript library that enables web applications to communicate with ROSBridge, providing a simple API for interacting with ROS. It allows developers to write web applications that can send and receive messages, subscribe to topics, and call ROS services over websockets.</p> <p></p>"},{"location":"faq/ROS/ros-bridge-and-roslibjs/#how-do-rosbridge-and-roslibjs-work-together","title":"How do ROSBridge and ROSLIBJS work together?","text":"<p>ROSBridge acts as a bridge between the web application and the ROS system. It listens for incoming WebSocket connections and translates JSON messages to ROS messages, and the other way around. ROSLIBJS, on the other hand, provides an API for web applications to interact with ROSBridge, making it easy to send and receive ROS messages, subscribe to topics, and call services.</p> <p>In a typical application utilizing ROSBridge and ROSLIBJS, it would have the following flow:</p> <ol> <li>Web client uses ROSLIBJS to establish a WebSocket connection to the ROSBridge server, through specifying a specific IP address and port number.</li> <li>The web client sends JSON messages to ROSBridge, which converts them to ROS messages and forwards them to the appropriate ROS nodes.</li> <li>If the node has a reply, the ROS nodes send messages back to ROSBridge, which converts them to JSON and sends them over the WebSocket connection to the web client.</li> <li>ROSLIBJS API processes the incoming JSON messages so it can be displayed/utilized to the web client</li> </ol> <p></p>"},{"location":"faq/ROS/ros-bridge-and-roslibjs/#how-do-i-install-and-use-rosbridge-and-roslibjs","title":"How do I install and use ROSBridge and ROSLIBJS?","text":""},{"location":"faq/ROS/ros-bridge-and-roslibjs/#rosbridge-installation","title":"ROSBridge Installation","text":"<p>The following assumes that you have ROS Noetic installed on your system.</p> <p>To install ROSBridge you need to install the <code>rosbridge-server</code> package</p> <pre><code>sudo apt install ros-noetic-rosbridge-server\n</code></pre>"},{"location":"faq/ROS/ros-bridge-and-roslibjs/#roslibjs-installation","title":"ROSLIBJS Installation","text":"<p>You can either use the hosted version of ROSLIBJS or download it for use in your project. To use the hosted version, include the following script tag in your HTML file: <pre><code>&lt;script src=\"https://static.robotwebtools.org/roslibjs/current/roslib.min.js\"&gt;&lt;/script&gt;\n</code></pre></p> <p>To download ROSLIBJS, visit the Github repository builds and download and save the files. To use the local version, include the following script tag in your HTML file: <pre><code>&lt;script src=\"PATH-TO-DOWNLOADED-SCRIPT\"&gt;&lt;/script&gt;\n</code></pre></p>"},{"location":"faq/ROS/ros-bridge-and-roslibjs/#simple-example","title":"Simple Example","text":"<p>The following example is for React Application </p> <p>First, download the <code>roslib.min.js</code> file from the ROSLIBJS GitHub repository and place it in your project's public directory.</p> <p>Next, create a new React component called <code>RosConnect</code>:</p> <p>RosConnect.jsx <pre><code>import React, { Component } from 'react';\nimport { Alert } from 'react-bootstrap';\n\nclass ROSConnect extends Component {\n\n    constructor() {\n        super()\n        this.state = { connected: false, ros: null } \n\n    }\n    // run the function as soon as the page renders\n    componentDidMount() {\n        this.init_connection()\n    }\n\n    // a function to connect to the robot using ROSLIBJS\n    init_connection() {\n        this.state.ros = new window.ROSLIB.Ros()\n        this.state.ros.on(\"connection\", () =&gt; {\n            this.setState({connected: true})\n        })\n\n        this.state.ros.on(\"close\", () =&gt; {\n            this.setState({connected: false})\n            // try to reconnect to rosbridge every 3 seconds\n            setTimeout(() =&gt; {\n                try{\n                    // ip address of the rosbridge server and port\n                    this.state.ros.connect('ws://127.0.0.1:9090')\n                }catch (error) {\n                    console.log(\"connection error:\", error);\n                }\n            // if the robot disconnects try to reconnect every 3 seconds (1000 ms = 1 second)\n            }, 3000); \n        })\n\n        try{\n            // connect to rosbridge using websocket \n            this.state.ros.connect('ws://127.0.0.1:9090')\n        }catch (error) {\n            console.log(\"connection error:\", error);\n        }\n\n    }\n\n    render() { \n        return (\n            // a Alert component from react-bootstrap showing if the robot is connected or not\n            &lt;div&gt;\n                &lt;Alert className='text-center m-3' variant={this.state.connected ? \"success\" : \"danger\"}&gt;\n                    {this.state.connected ? \"Robot Connected\" : \"Robot Disconnected\"}\n                &lt;/Alert&gt;\n            &lt;/div&gt;\n        );\n    }\n}\n\nexport default ROSConnect;\n</code></pre></p> <p>This component can be rendered to any page, for example it can be used in the App.js component which already comes with <code>creat-react-app</code></p> <p></p>"},{"location":"faq/ROS/ros-bridge-and-roslibjs/#challenges-and-limitations-of-rosbridge-and-roslibjs","title":"Challenges and Limitations of ROSBridge and ROSLIBJS","text":"<p>Even though ROSBridge and ROSLIBJS is has a lot of use cases from being able to view camera feed from a robot to getting its GPS data display on a dashboard, it does have some prominent limitations. </p> <p>While working on the campus command control project, one of the issues that was encountered was lag. ROSLIBJS uses web socket, which is built on top of <code>Transmission Control Protocol (TCP)</code>. While TCP is more reliable, it transfers data more slowly, which led to lag in robot controls and video feed. It is worth mentioning that ROSBridge does support <code>User Datagram Protocol (UDP)</code>, which comes at a cost of reliability for speed, but ROSLIBJS current implementation does not support UDP. </p>"},{"location":"faq/ROS/ros2_docker-tutorial/","title":"Question","text":"<p>I want to try out a distribution of ROS2, but it does not support my computer's operating system. But I also want to avoid installing a compatible version of Ubuntu or Windows on my computer just for running ROS2.</p> <p>I heard that Docker might allow me to do this. How would I go about it?</p>"},{"location":"faq/ROS/ros2_docker-tutorial/#answer","title":"Answer","text":"<p>You would run ROS2 on your existing OS as a Docker image in a Docker container. To do this, you first need to install Docker.</p>"},{"location":"faq/ROS/ros2_docker-tutorial/#installing-docker","title":"Installing Docker","text":""},{"location":"faq/ROS/ros2_docker-tutorial/#on-linux-distributions","title":"On Linux Distributions","text":"<p>Docker's official manuals and guides push you to install Docker Desktop, regardless of your operating system. But Docker Desktop is just a GUI that adds layers of abstraction on top of Docker Engine, which is the technology that drives Docker. And we can communicate with Docker Engine directly after installing it via the Docker CLI.</p> <p>So if you use a Linux distribution (like Ubuntu, Debian, Fedora, etc.), just install the Docker Engine directly; we won't need the features Docker Desktop offers. Go to this page to install Docker Engine for Ubuntu. </p> <p>Even if you don't use Ubuntu, just go to the page mentioned, and find your distribution on the Table of Contents you find at the left bar of the website. Don't expect to find help on the Docker Engine Overview page: they'll just start pushing you towards Docker Desktop again. </p>"},{"location":"faq/ROS/ros2_docker-tutorial/#on-macos","title":"On MacOS","text":"<p>You'll have to install Docker Desktop. Go to this page and follow the instructions.</p>"},{"location":"faq/ROS/ros2_docker-tutorial/#after-installation","title":"After Installation","text":"<p>After installing Docker Engine or Docker Desktop, open a terminal and execute:</p> <pre><code>sudo docker --version\n</code></pre> <p>You should see an output like:</p> <pre><code>Docker version 25.0.3, build 4debf41\n</code></pre> <p>if you installed Docker correctly.</p>"},{"location":"faq/ROS/ros2_docker-tutorial/#running-ros2-on-docker","title":"Running ROS2 on Docker","text":"<p>It's possible to develop your ROS2 package entirely via the minimalistic Docker image the ROS2 community recommends. In fact, this seems to be a, if not the, common practice in the ROS2 community.</p> <p>But the learning curve for this is probably, for most software engineers, very, very steep. From what I learned, you'll need an excellent grasp of Docker, networking, and the Linux kernel to pull it off successfully.</p> <p>If you don't have an excellent grasp of these three technologies, it's probably better to use a more robust Docker image that has enough configured for you to go through ROS2's official tutorial. Afterwards, if you want to use ROS2 for actual development, I recommend installing the necessary OS on your computer and running ROS2 directly on it.</p> <p>Such a repository has been provided by tiryoh on github. To use it, just run:</p> <pre><code>docker run -p 6080:80 --security-opt seccomp=unconfined --shm-size=512m tiryoh/ros2-desktop-vnc:iron\n</code></pre> <p>It will take a while for the command to run. After it does, keep the terminal window open, and visit the address <code>http://127.0.0.1:6080/</code> on your browser, and click \"Connect\" on the VNC window. This should take you to an Ubuntu Desktop, where you can use the Terminator application to go through ROS2's official tutorial.</p> <p>If afterwards you want to remove the docker image (since it's quite large) enter</p> <pre><code>docker ps -a\n</code></pre> <p>Locate the <code>NAME</code> of your image in the output, and execute:</p> <pre><code>docker rm NAME\n</code></pre> <p>E.g. <code>docker rm practical_mclaren</code>.</p> <p>Tiryoh's repository of the image can be found here. But you don't need to clone it to use his image.</p>"},{"location":"faq/ROS/ros2_px100_arm_setup/","title":"Question","text":"<p>How do I set up the PX-100 arm to work with ROS2?</p>"},{"location":"faq/ROS/ros2_px100_arm_setup/#answer","title":"Answer","text":""},{"location":"faq/ROS/ros2_px100_arm_setup/#introduction","title":"Introduction","text":"<p>The PX-100 arm currently supports ROS2 Galactic on Ubuntu Linux 20.04, or ROS2 Humble on Ubuntu Linux 22.04. This is unlikely to change in the future, as Trossen Robotics seems to have stopped working further on the PX-100.</p> <p>This guide assumes you will be using ROS2 Humble on Ubuntu Linux 22.04. If you need to, it wouldn't be too hard to adapt the instructions here for a setup on ROS2 Galactic.</p>"},{"location":"faq/ROS/ros2_px100_arm_setup/#install-ubuntu","title":"Install Ubuntu","text":"<p>Follow these official instructions, or others you might find on the web, to install Ubuntu Linux 22.04 on your computer. You might meet with a few hiccups along the way, e.g., about Ubuntu being incompatible with RST, etc. When you do, don't panic, and search for the simplest solution to the problem.</p>"},{"location":"faq/ROS/ros2_px100_arm_setup/#check-for-hardware-compatibility","title":"Check for Hardware Compatibility","text":"<p>Plug in the arm to a usb port of your computer. After waiting for a bit, run the <code>lsusb</code> command to see the USB devices connected to your computer. If you see something like:</p> <pre><code>Bus 001 Device 003: ID 0403:6014 Future Technology Devices International, Ltd FT232H Single HS USB-UART/FIFO IC\n</code></pre> <p>as a line in your output, your computer is probably compatible with the arm. </p>"},{"location":"faq/ROS/ros2_px100_arm_setup/#install-ros2-humble-colcon-and-rosdep","title":"Install ROS2 Humble, <code>colcon</code>, and <code>rosdep</code>","text":"<p>First, install ROS2 Humble. The official guide is good. The only recommendation I'd make is to add <code>source /opt/ros/humble/setup.bash</code> to your <code>.bashrc</code> file (if you're using a BASH shell. To see whether you are, run <code>echo $SHELL</code>. If the output is <code>/bin/bash</code>, you're using BASH).</p> <p>Second, install colcon, ROS2's build tool.</p> <pre><code>sudo apt install python3-colcon-common-extensions\n</code></pre> <p>Third, install rosdep. Run:</p> <pre><code>sudo apt install python3-rosdep\n</code></pre> <p>Then execute</p> <pre><code>sudo rosdep init\nrosdep update\n</code></pre>"},{"location":"faq/ROS/ros2_px100_arm_setup/#install-interbotixs-software-for-ros2","title":"Install Interbotix's Software for ROS2","text":"<p>Finally, it's time to install software for the PX-100 arm. Execute:</p> <pre><code>sudo apt install curl\ncurl 'https://raw.githubusercontent.com/Interbotix/interbotix_ros_manipulators/main/interbotix_ros_xsarms/install/amd64/xsarm_amd64_install.sh' &gt; xsarm_amd64_install.sh\nchmod +x xsarm_amd64_install.sh\n./xsarm_amd64_install.sh -d humble\n</code></pre> <p>If you're using galactic, replace 'humble' with 'galactic' in the last line.</p> <p>After installation, follow these Installation Checks from Trossen Robotics, to confirm that your installation was successful.</p>"},{"location":"faq/ROS/ros2_px100_arm_setup/#checking-that-things-work-as-expected","title":"Checking that Things Work as Expected","text":"<p>Execute the following line:</p> <pre><code>ros2 launch interbotix_xsarm_control xsarm_control.launch.py robot_model:=px100\n</code></pre> <p>This should launch an RVIZ window displaying a graphical representation of your arm. Don't panic if you don't get the arm on the first try, and there are white blocks where components of the robot should be. Try closing rviz, and running the command again. </p> <p>If this still doesn't work, check to see if the arm is in the sleeping position. If it isn't, unplug the arm, and gently move it into the sleeping position.</p> <p>Even if the arm is in a sleeping position, unplug, and then replug the arm.</p> <p>Run the command above again. It should have worked this time. If it still doesn't, there was probably something wrong with the installation process. Or the hardware of the arm is incompatible with your computer for mysterious reasons.</p> <p>If rviz does work, execute the following command:</p> <pre><code>ros2 service call /px100/torque_enable interbotix_xs_msgs/srv/TorqueEnable \"{cmd_type: 'group', name: 'all', enable: false}\"\n</code></pre> <p>This command disables the mechanisms on the robot that keep its joints fixed in their positions. This means that you should be able to move the robot's joints manually. Try this, GENTLY, and see the graphical representation of the robot on RVIZ move in tandem.</p> <p>After you're satisfied with your experiment, place the robot back into its sleeping position, and quit the two ros2 processes you started.</p>"},{"location":"faq/ROS/ros2_px100_arm_setup/#next-steps","title":"Next Steps","text":"<p>The documentation for Interbotix's ROS2 API is very lacking. If you're curious about something, try searching our lab notes first to see if you can find what you're looking for. </p> <p>Otherwise, dig into the source code, installed under the <code>interbotix_ws</code> directory. The directory's path is likely <code>~/interbotix_ws</code>. Don't be afraid to do this! It's what you'll have to do anyway if you join a company or work on a substantial open source project. Don't be afraid to modify the source code either, if you feel it's not working as it should! The developers are only human, and may have made mistakes.</p>"},{"location":"faq/ROS/rosbridge/","title":"send dynamic JSON messages to ROS","text":""},{"location":"faq/ROS/rosbridge/#rosbridge_server","title":"rosbridge_server","text":"<p>\"Rosbridge server creates a WebSocket connection and passes any JSON messages from the WebSocket to rosbridge_library, so rosbridge library can convert the JSON strings into ROS calls...\" (to read more, see the rosbridge_server documentation)</p>"},{"location":"faq/ROS/rosbridge/#roslibpy","title":"roslibpy","text":"<p>\"Python ROS Bridge library allows to use Python and IronPython to interact with ROS, the open-source robotic middleware. It uses WebSockets to connect to rosbridge 2.0 and provides publishing, subscribing, service calls, actionlib, TF, and other essential ROS functionality...\" (to read more, see the roslibpy documentation)</p>"},{"location":"faq/ROS/rosbridge/#installation","title":"installation","text":"<p>to install rosbridge_server and roslibpy: </p> <p><code>sudo apt install ros-noetic-rosbridge-server</code></p> <p><code>pip install roslibpy</code></p>"},{"location":"faq/ROS/rosbridge/#the-code","title":"the code","text":"<pre><code>import roslibpy\n\nclient = roslibpy.Ros(host='localhost', port=9090)\nclient.run()\n\npub = roslibpy.Topic(client, topic_name, message_type)\n\npub.publish(roslibpy.Message(message_data))\n\npub.unadvertise()\nclient.terminate()\n</code></pre>"},{"location":"faq/ROS/rosbridge/#json-format","title":"json format","text":"<p>a json file with the format shown below where </p> <pre><code>\"command\": { \n    \"receiver\": \"/cmd_vel\",\n    \"type\": \"geometry_msgs/Twist\",\n    \"msg\" : {\n        \"linear\": {\n            \"x\": 0.0,\n            \"y\": 0.0,\n            \"z\": 0.0\n        },\n        \"angular\": {\n            \"x\": 0.0,\n            \"y\": 0.0,\n            \"z\": 0.0\n        }\n    }\n}\n</code></pre>"},{"location":"faq/ROS/rosbridge/#to-run","title":"to run","text":"<pre><code>roslaunch rosbridge_server rosbridge_websocket.launch\n\nrosrun package_name node_name.py\n</code></pre>"},{"location":"faq/ROS/why-does-roscd-go-to-the-wrong-place/","title":"Why does roscd go wrong?","text":"<p>The issue is with your ROS_PACKAGE_PATH. roscd brings you to the first workspace on the path. So they should be the other way around on you package path.</p> <p>I highly discourage from manually fiddling with the ROS_PACKAGE_PATH variable (if you've done that or plan on doing that). Also note that you don't need to have multiple source statements for workspaces in your .bashrc. With catkin, the last setting wins.</p> <ul> <li>clean your workspace (i.e. remove build, devel and logs folder, if they exist; if you've built with catkin-tools, you can use the catkin clean command)</li> <li>clean the bashrc (i.e. remove all source commands that source a ROS workspace)</li> <li>start a new terminal (without any ROS environment sourced)</li> <li>manually source /opt/ros/melodic/setup.bash</li> <li>build your workspace again and source the workspaces setup.bash</li> </ul>"},{"location":"faq/ROS/whyismyrobotnotmoving/","title":"Basic Start Guide","text":"<p>Created by Jeremy Huey 05/04/2023</p>"},{"location":"faq/ROS/whyismyrobotnotmoving/#basic-trouble-shooting","title":"Basic Trouble Shooting:","text":"<p>Are you connected to roscore?  Ensure that roscore is running either in simulation (create a terminal window running '$ roscore') or have run '$ bringup' on an real robot. </p> <p>If you are onboard a real robot, the window you ran <code>bringup</code> in should say [onboard].  To check that you have the connection, type <code>$ rostopic list</code>.  This should output a list of topics the system can see. </p> <p>Is the battery dead or the hardware not working?  Check to see if the battery has died/beeping or if any cables have come loose.</p> <p>To check to see if the robot is getting commands, you can type in a single cmd_vel message.  <pre><code>$ rostopic pub -r 10 /cmd_vel geometry_msgs/Twist  '{linear:  {x: 0.1, y: 0.0, z: 0.0}, angular: {x: 0.0,y: 0.0,z: 0.0}}'\n</code></pre> You must ensure there is a space after things like <code>x:</code>. Info here: https://answers.ros.org/question/218818/how-to-publish-a-ros-msg-on-linux-terminal/</p> <p>Let's say you make a new project and now you cannot find that package, or you try to run your launch file and it gives you the error that it cannot find that package. To check or find out if you have the package you want you can do this:  <code>$ roscd package_name</code> This should auto complete. If it does not, you may not have the right package_name as designated in the package.xml.  Other packages you may find will send you to opt noetic something. You can likely download these packages to your local somehow. </p>"},{"location":"faq/ROS/whyismyrobotnotmoving/#getting-onboard-a-robot","title":"Getting onboard a robot","text":"<p>Follow the link here to get onboard a robot: https://campus-rover.gitbook.io/lab-notebook/faq/02_connect_to_robot</p>"},{"location":"faq/ROS/whyismyrobotnotmoving/#how-to-understand-and-make-a-simple-launch-file","title":"How to understand and make a simple launch file:","text":"<p>https://campus-rover.gitbook.io/labnotebook/faq/launch_file_create.md</p>"},{"location":"faq/ROS/whyismyrobotnotmoving/#how-to-turn-in-a-zip-submission-for-class","title":"how to turn in a zip submission for class","text":"<p>TAR AND UNTAR ==== How to turn it in:  On the VM/VNC, go to System Tools &gt; File Manager.  In a terminal window,  use to TAR the file: <code>$ tar -czvf foldername.tar.gz foldername</code></p> <p>Then inside of the CODE window, you'll see the tar file appear in the vscode file manager. If you right click on a tar file, only this kind of file will seem to give the option to \"Download\". Do this to download to your computer. </p> <p>Then, go to your own PC's powershell/terminal. The UNTAR command is = -x. <pre><code>$ cd .\\Downloads\\\n$ tar -xvkf .\\pa2.tar.gz \n</code></pre> Optionally also instead of downloading, you can also On the VM, go to the bottom menu, Internet, use Firefox or Chrome to log into your email or github or whatever to send that way.</p>"},{"location":"faq/ROS/whyismyrobotnotmoving/#how-to-simple-git-and-create-package","title":"How to simple Git (and create package)","text":"<p>go to github.com and get a username and personal key. This key is important, save it somewhere safe where you can get access to it. When asked for a password, this is what you'll place in (as of 2023). </p> <p>On github.com, you should be able to create a repository do this.  Then on your local computer or your CODE window go to the folder where you want to place your files. (In this class, as of 2023, you're recommended to place them in <code>$ cd ~/catkin_ws/src</code>). Then copy the link to the github repo you just made, and run: <code>git clone your_link.git</code>.  Make sure you add .git to the end of it. You should now see a copy of it appear on your local computer. In the CODE window, you will see a circular arrow \"Refresh Explorer\" at the top of the File Explorer side tab. </p> <p>(Create Package) To create a package run this: <code>$ catkin_create_pkg name_of_my_package std_msgs rospy roscpp</code> replace name_of_my_package with the name you want for your package, eg. dance_pa.</p> <p>The easiest thing to do when working on these group projects is to split the work up into separate areas/files/commented_segments so that you do not incur merge conflicts. Eg. Abby works on fileA.py, while you work on fileB.py. Or you both work on fileA.py but you comment out a segment line 1-50 for Abby to work, while you work on line51-100. </p> <p>Once you've made some changes, do this:  (Be in the folder you want to make changes to, eg. your package folder.) <pre><code>git add . //Alternatively, you can specify a folder or file here.\ngit commit -m \"some commit message\"\ngit pull // updates your local to take any new changes. \ngit push //pushes your changes to origin/github. \n</code></pre></p>"},{"location":"faq/ROS/whyismyrobotnotmoving/#using-the-python-objectclass-structure","title":"Using the Python Object/Class structure","text":"<p>If you use a Python class and get an undefined error, you most likely forgot to make that variable a self variable. eg:  linear_speed is undefined. change to: self.linear_speed.  The same applies to functions and callback cb functions, reminder to place these inside the Class classname: section and to write them as:  <pre><code>import rospy\nfrom geometry_msgs.msg import TransformStamped, Twist\nfrom std_msgs.msg import Bool, String # if you get errors for creating your own publisher, see if you have the right type here.\n\nClass Classname:\n    def __init__(self):\n        rospy.init_node('main')\n        self.linear_speed = 0.0\n        self.rate = rospy.Rate(30) # reminder: this must be self. too. \n        self.twist = Twist()\n        # CORE pub subs ====\n        self.cmd_pub = rospy.Publisher(\"/cmd_vel\", Twist, queue_size=1) \n        self.state_sub = rospy.Subscriber(\"/state\", String, self.state_cb) #self.state_cb\n\n    def state_cb(self, msg):\n        self.state = msg.data \n        # when accessing the information, you most likely will need to access .data\n\n    def funcname(self):\n        self.linear_speed = 0.3 # This is where you might see the error if you forgot self.\n        self.twist.linear.x = self.linear_speed\n        self.twist.angular.z = 0.0\n        self.cmd_pub.publish(self.twist)\n\n    def run(self):\n        while not rospy.is_shutdown():\n            self.funcname()\n            self.rate.sleep()\n\nif __name__ == '__main__':\nc = Classname()\nc.run()\n# EOF\n</code></pre></p>"},{"location":"faq/ROS/whyismyrobotnotmoving/#more-commands-for-running-gazebo-simulations","title":"More commands for running gazebo simulations:","text":"<p>https://campus-rover.gitbook.io/labnotebook/faq/handy-commands.md</p>"},{"location":"faq/advanced/advanced-fixing-build-problems/","title":"Advanced troubleshooting of build problems","text":""},{"location":"faq/advanced/advanced-fixing-build-problems/#notes","title":"Notes","text":"<p>You really need to know what you are doing when you are dealing at this level. </p>"},{"location":"faq/advanced/advanced-fixing-build-problems/#radical-cleanup","title":"Radical Cleanup","text":"<ul> <li>It turns out that it is safe to delete the build and devel subdirectories in ROS. </li> <li>Sometimes this helps:</li> </ul> <pre><code>cd ~/catkin_ws\nrm -rf build/ devel/\ncatkin_make\nsource devel/setup.bash\n</code></pre>"},{"location":"faq/advanced/advanced-fixing-build-problems/#another-way","title":"Another way","text":"<ul> <li>First: clean your build by running \"catkin_make clean\" in the root of your workspace.</li> <li>Second: remake your project with \"catkin_make\"</li> <li>Third: re-source the devel/setup.bash in your workspace.</li> </ul>"},{"location":"faq/advanced/advanced-visualization/","title":"Webviz is an advanced online visualization tool","text":"<p>I have spent many hours tracking down a tricky tf tool and in doing that came across some new techniques for troubleshooting. In this FAQ I introduce using rosbag with Webviz</p>"},{"location":"faq/advanced/advanced-visualization/#rosbag","title":"Rosbag","text":"<p>This CLI from ROS monitors all topic publications and records their data in a timestamped file called a bag or rosbag. This bag can be used for many purposes. For example the recording can be \"played back\" which allows you to test the effects that a certain run of the robot would have had. You can select which topics to collect.</p> <p>One other use is to analyze the rosbag moment to moment.</p>"},{"location":"faq/advanced/advanced-visualization/#webiz","title":"Webiz","text":"<p>Is an online tool (https://webviz.io). You need to understand topics and messages to use it and the UI is a little bit obscure. But it's easy enough. You supply your bag file (drag and drop) and then arrange a series of panes to visualize all of them in very fancy ways.</p>"},{"location":"faq/advanced/frontier-exploration/","title":"Frontier Exploration Guide","text":""},{"location":"faq/advanced/frontier-exploration/#overview","title":"Overview","text":"<p>Frontier exploration is a key capability for mobile robots navigating unknown environments. This guide explains the concept, scenarios, implementation strategies, and practical usage in ROS Noetic.</p>"},{"location":"faq/advanced/frontier-exploration/#author","title":"Author","text":"<ul> <li>Name: ZHENXU Chen</li> <li>Date: Dec 10, 2024</li> <li>ROS Version: Noetic</li> </ul>"},{"location":"faq/advanced/frontier-exploration/#concept","title":"Concept","text":""},{"location":"faq/advanced/frontier-exploration/#key-strategies","title":"Key Strategies","text":"<p>Frontier exploration uses map updates to identify unexplored areas and direct robots safely. Integration with the ROS navigation stack ensures obstacle avoidance and adaptability to environmental changes.</p>"},{"location":"faq/advanced/frontier-exploration/#frontier-detection-algorithm","title":"Frontier Detection Algorithm","text":"<p>The detection process involves:</p> <ol> <li>Robot Pose Retrieval: Using <code>getRobotPose</code> to determine the current position.</li> <li>Frontier Search: <code>searchFrom</code> performs Breadth-First Search (BFS) to locate boundaries between known and unknown areas, sorted by cost.</li> <li>Empty Frontier Handling: If no frontiers are found, exploration stops.</li> <li>Visualization Markers: Publish visual markers for debugging or monitoring.</li> <li>Target Selection: Selects a valid frontier centroid as the next goal.</li> </ol>"},{"location":"faq/advanced/frontier-exploration/#explore-lite","title":"Explore Lite","text":"<p>Explore Lite is a ROS package for lightweight autonomous exploration. It provides tools for efficient navigation and mapping in unknown environments.</p>"},{"location":"faq/advanced/frontier-exploration/#usage","title":"Usage","text":""},{"location":"faq/advanced/frontier-exploration/#path-planning-example","title":"Path Planning Example","text":"<pre><code>&lt;launch&gt;\n  &lt;arg name=\"model\" default=\"$(env TURTLEBOT3_MODEL)\" doc=\"Model type [burger, waffle, waffle_pi]\"/&gt;\n  &lt;arg name=\"cmd_vel_topic\" default=\"/cmd_vel\" /&gt;\n  &lt;arg name=\"odom_topic\" default=\"odom\" /&gt;\n  &lt;arg name=\"move_forward_only\" default=\"false\"/&gt;\n\n  &lt;node pkg=\"move_base\" type=\"move_base\" name=\"move_base\" output=\"screen\"&gt;\n    &lt;param name=\"base_local_planner\" value=\"dwa_local_planner/DWAPlannerROS\" /&gt;\n    &lt;param name=\"base_global_planner\" value=\"global_planner/GlobalPlanner\"/&gt;\n    &lt;rosparam file=\"$(find turtlebot3_navigation)/param/costmap_common_params_$(arg model).yaml\" command=\"load\" ns=\"global_costmap\" /&gt;\n    &lt;rosparam file=\"$(find turtlebot3_navigation)/param/costmap_common_params_$(arg model).yaml\" command=\"load\" ns=\"local_costmap\" /&gt;\n    &lt;rosparam file=\"$(find turtlebot3_navigation)/param/local_costmap_params.yaml\" command=\"load\" /&gt;\n    &lt;rosparam file=\"$(find turtlebot3_navigation)/param/global_costmap_params.yaml\" command=\"load\" /&gt;\n    &lt;rosparam file=\"$(find turtlebot3_navigation)/param/move_base_params.yaml\" command=\"load\" /&gt;\n    &lt;rosparam file=\"$(find turtlebot3_navigation)/param/dwa_local_planner_params_$(arg model).yaml\" command=\"load\" /&gt;\n    &lt;remap from=\"cmd_vel\" to=\"$(arg cmd_vel_topic)\"/&gt;\n    &lt;remap from=\"odom\" to=\"$(arg odom_topic)\"/&gt;\n    &lt;param name=\"DWAPlannerROS/min_vel_x\" value=\"0.0\" if=\"$(arg move_forward_only)\" /&gt;\n  &lt;/node&gt;\n&lt;/launch&gt;\n</code></pre>"},{"location":"faq/advanced/frontier-exploration/#slam-example","title":"SLAM Example","text":"<pre><code>&lt;launch&gt;\n  &lt;arg name=\"model\" default=\"$(env TURTLEBOT3_MODEL)\" doc=\"Model type [burger, waffle, waffle_pi]\"/&gt;\n  &lt;arg name=\"slam_methods\" default=\"gmapping\" doc=\"SLAM type [gmapping, cartographer, hector, karto, frontier_exploration]\"/&gt;\n  &lt;arg name=\"configuration_basename\" default=\"turtlebot3_lds_2d.lua\"/&gt;\n  &lt;arg name=\"open_rviz\" default=\"true\"/&gt;\n\n  &lt;include file=\"$(find turtlebot3_bringup)/launch/turtlebot3_remote.launch\"&gt;\n    &lt;arg name=\"model\" value=\"$(arg model)\" /&gt;\n  &lt;/include&gt;\n\n  &lt;include file=\"$(find turtlebot3_slam)/launch/turtlebot3_$(arg slam_methods).launch\"&gt;\n    &lt;arg name=\"model\" value=\"$(arg model)\"/&gt;\n    &lt;arg name=\"configuration_basename\" value=\"$(arg configuration_basename)\"/&gt;\n  &lt;/include&gt;\n\n  &lt;group if=\"$(arg open_rviz)\"&gt;\n    &lt;node pkg=\"rviz\" type=\"rviz\" name=\"rviz\" required=\"true\"\n          args=\"-d $(find turtlebot3_slam)/rviz/turtlebot3_$(arg slam_methods).rviz\"/&gt;\n  &lt;/group&gt;\n&lt;/launch&gt;\n</code></pre>"},{"location":"faq/advanced/frontier-exploration/#exploration-configuration-example","title":"Exploration Configuration Example","text":"<pre><code>&lt;launch&gt;\n  &lt;node pkg=\"explore_lite\" type=\"explore\" respawn=\"false\" name=\"explore\" output=\"screen\"&gt;\n    &lt;param name=\"robot_base_frame\" value=\"base_footprint\" /&gt;\n\n    &lt;param name=\"costmap_topic\" value=\"move_base/global_costmap/costmap\" /&gt;\n    &lt;param name=\"costmap_updates_topic\" value=\"move_base/global_costmap/costmap_updates\" /&gt;\n    &lt;param name=\"visualize\" value=\"true\" /&gt;\n    &lt;param name=\"planner_frequency\" value=\"0.20\" /&gt;\n\n    &lt;param name=\"progress_timeout\" value=\"30.0\" /&gt;\n    &lt;param name=\"potential_scale\" value=\"3.0\" /&gt;\n    &lt;param name=\"orientation_scale\" value=\"0.0\" /&gt;\n    &lt;param name=\"gain_scale\" value=\"1.0\" /&gt;\n\n    &lt;param name=\"transform_tolerance\" value=\"0.3\" /&gt;\n    &lt;param name=\"min_frontier_size\" value=\"0.1\" /&gt;\n  &lt;/node&gt;\n&lt;/launch&gt;\n</code></pre>"},{"location":"faq/advanced/frontier-exploration/#communication-between-nodes","title":"Communication Between Nodes","text":""},{"location":"faq/advanced/frontier-exploration/#troubleshooting","title":"Troubleshooting","text":""},{"location":"faq/advanced/frontier-exploration/#path-planning-failures","title":"Path Planning Failures","text":"<p>Use recovery behaviors and parameters to handle path planning failures during robot navigation. The recovery behaviors include clearing costmaps at varying levels of conservativeness and performing rotation-based costmap clearing. Key parameters such as planner_patience, controller_patience, conservative_reset_dist, planner_frequency, and oscillation_timeout ensure robust and reliable robot navigation by preventing getting stuck.</p> <pre><code>recovery_behaviors:\n  - name: 'super_conservative_reset1'\n    type: 'clear_costmap_recovery/ClearCostmapRecovery'\n  - name: 'conservative_reset1'\n    type: 'clear_costmap_recovery/ClearCostmapRecovery'\n  - name: 'aggressive_reset1'\n    type: 'clear_costmap_recovery/ClearCostmapRecovery'\n  - name: 'clearing_rotation1'\n    type: 'rotate_recovery/RotateRecovery'\n\nplanner_patience: 2.0 \ncontroller_patience: 5.0\nconservative_reset_dist: 3.0\nplanner_frequency: 5.0 \noscillation_timeout: 5.0\n</code></pre>"},{"location":"faq/advanced/frontier-exploration/#parameter-tuning","title":"Parameter Tuning","text":"<p>This configuration focuses on parameter tuning for trajectory scoring and simulation settings to optimize robot navigation. Trajectory scoring parameters, such as path_distance_bias, goal_distance_bias, and occdist_scale, influence the path planning preferences. Simulation settings include sim_time, ensuring accurate local navigation planning.</p>"},{"location":"faq/advanced/frontier-exploration/#trajectory-scoring","title":"Trajectory Scoring","text":"<pre><code>path_distance_bias: 6000.0 \ngoal_distance_bias: 0.1 \noccdist_scale: 0.0\nforward_point_distance: 0.325\nstop_time_buffer: 0.3\nscaling_speed: 0.25\nmax_scaling_factor: 0.2\n</code></pre>"},{"location":"faq/advanced/frontier-exploration/#simulation-settings","title":"Simulation Settings","text":"<pre><code>sim_time: 4\nvx_samples: 20\nvy_samples: 0\nvth_samples: 40\ncontroller_frequency: 10.0\n</code></pre>"},{"location":"faq/ai/AI_Detector/","title":"FAQ: How to Build a Plant Detector or any Detector with OpenAI GPT-4o-mini","text":""},{"location":"faq/ai/AI_Detector/#introduction","title":"Introduction","text":"<p>In this FAQ, we\u2019ll walk through how to build a plant detector using OpenAI GPT-4o-mini for image analysis. This method leverages external API calls for plant type identification and general plant presence detection, ensuring computational efficiency for resource-constrained robotics platforms like Turtlebot. This guide is designed to help future students understand how to implement a similar solution for their projects.</p>"},{"location":"faq/ai/AI_Detector/#problem-overview","title":"Problem Overview","text":"<p>Building a robust plant detection system is challenging due to:</p> <ul> <li>Variability in lighting and angles.</li> <li>The computational constraints of real-time detection on small robots.</li> <li>The need for high accuracy in identifying specific objects.</li> </ul> <p>While models like YOLOv5 are popular, they can be computationally heavy and less adaptable to real-world conditions. Our approach uses the OpenAI GPT-4o-mini model for advanced image recognition, offloading computation to an external API and achieving nearly 100% accuracy.</p>"},{"location":"faq/ai/AI_Detector/#step-by-step-guide-to-implement-the-detector","title":"Step-by-Step Guide to Implement the Detector","text":""},{"location":"faq/ai/AI_Detector/#1-set-up-your-environment","title":"1. Set Up Your Environment","text":"<p>To begin, you\u2019ll need:</p> <ol> <li>OpenAI API Access: Obtain an API key from OpenAI.</li> <li>Python Environment: Install required libraries (<code>openai</code>, <code>python-dotenv</code>, <code>base64</code>).</li> </ol> <p>Install dependencies:</p> <pre><code>pip install openai python-dotenv\n</code></pre>"},{"location":"faq/ai/AI_Detector/#2-configure-your-api-key","title":"2. Configure Your API Key","text":"<p>Use the <code>dotenv</code> library to securely store and load your API key:</p> <ol> <li>Create a <code>.env</code> file in your project directory.</li> <li>Add the following to the <code>.env</code> file:    <pre><code>OPEN_API_KEY=your_openai_api_key_here\n</code></pre>    Replace <code>your_openai_api_key_here</code> with your actual OpenAI API key.</li> </ol>"},{"location":"faq/ai/AI_Detector/#3-load-the-key-in-your-script","title":"3. Load the Key in Your Script","text":"<p>Use <code>load_dotenv()</code> to securely load the API key from the <code>.env</code> file:</p> <pre><code>from dotenv import load_dotenv\nimport os\n\nload_dotenv()  # Load environment variables from the .env file\nopen_api_key = os.getenv(\"OPEN_API_KEY\")  # Retrieve the API key\n</code></pre>"},{"location":"faq/ai/AI_Detector/#3-write-the-detector-class","title":"3. Write the Detector Class","text":"<p>The <code>Detector</code> class handles plant detection via API calls to OpenAI's GPT-4o-mini model. Here's the full implementation:</p> <pre><code>import base64\nfrom openai import OpenAI\nfrom dotenv import load_dotenv\nimport os\n\n# Load environment variables\nload_dotenv()\n\n# Retrieve OpenAI API key\nopen_api_key = os.getenv(\"OPEN_API_KEY\")\n\nclass Detector:\n    def __init__(self):\n        self.client = OpenAI(api_key=open_api_key)\n        self.MAX_RETRIES = 10\n        self.plant_types = ['Cactus', 'Basil', 'Thyme', 'Parsley', 'Gatorade']\n\n    def detect_plant(self, image):\n        \"\"\"\n        Detects the type of plant or Gatorade in an image.\n\n        Args:\n            image (str): Base64 encoded string of the image.\n\n        Returns:\n            tuple: (bool, str) indicating success and identified plant type.\n        \"\"\"\n        for i in range(self.MAX_RETRIES):\n            try:\n                response = self.client.chat.completions.create(\n                    model=\"gpt-4o-mini\",\n                    messages=[\n                        {\n                            \"role\": \"user\",\n                            \"content\": [\n                                {\"type\": \"text\", \"text\": \"Output the plant type or Gatorade and only the plant type in one word: 'Cactus', 'Basil', 'Thyme', 'Parsley', or 'Gatorade' if the image's object of interest contains the plant or Gatorade\"},\n                                {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image}\"}}\n                            ],\n                        }\n                    ],\n                )\n                res = response.choices[0].message.content.strip()\n                if res in self.plant_types:\n                    return True, res\n                else:\n                    return False, None\n            except Exception as e:\n                print(f\"Failed attempt {i}: {e}\")\n\n    def is_plant(self, image):\n        \"\"\"\n        Determines whether an image contains any plant.\n\n        Args:\n            image (str): Base64 encoded string of the image.\n\n        Returns:\n            bool: True if the image contains a plant, False otherwise.\n        \"\"\"\n        for i in range(self.MAX_RETRIES):\n            try:\n                response = self.client.chat.completions.create(\n                    model=\"gpt-4o-mini\",\n                    messages=[\n                        {\n                            \"role\": \"user\",\n                            \"content\": [\n                                {\"type\": \"text\", \"text\": \"Output in one word 'true' or 'false' if the image contains any plant\"},\n                                {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image}\"}}\n                            ],\n                        }\n                    ],\n                )\n                res = response.choices[0].message.content.strip().lower()\n                return res == \"true\"\n            except Exception as e:\n                print(f\"Failed attempt {i}: {e}\")\n</code></pre>"},{"location":"faq/ai/AI_Detector/#4-how-it-works","title":"4. How It Works","text":"<ul> <li>Image Input: Convert the image to a Base64-encoded string before passing it to the <code>detect_plant</code> or <code>is_plant</code> methods.</li> <li>API Call: The OpenAI model processes the image and responds with the detected plant type or confirmation of plant presence.</li> <li>Retry Mechanism: The class retries failed API calls up to <code>MAX_RETRIES</code> to handle temporary issues.</li> </ul>"},{"location":"faq/ai/AI_Detector/#5-testing-the-detector","title":"5. Testing the Detector","text":"<p>Test the detector with a sample image:</p> <pre><code>if __name__ == \"__main__\":\n    detector = Detector()\n\n    # Convert your image to Base64 (example image path: \"plant.jpg\")\n    with open(\"plant.jpg\", \"rb\") as image_file:\n        base64_image = base64.b64encode(image_file.read()).decode('utf-8')\n\n    # Detect plant type\n    success, plant_type = detector.detect_plant(base64_image)\n    print(f\"Detection Successful: {success}, Plant Type: {plant_type}\")\n\n    # Check if the image contains any plant\n    is_plant = detector.is_plant(base64_image)\n    print(f\"Contains Plant: {is_plant}\")\n</code></pre>"},{"location":"faq/ai/AI_Detector/#why-this-matters","title":"Why This Matters","text":"<ul> <li>Efficiency: Offloading computation to the OpenAI API reduces the hardware burden, making the system suitable for resource-constrained platforms like Turtlebot.</li> <li>Accuracy: GPT-4o-mini provides reliable results even under varying environmental conditions, ensuring consistent plant detection.</li> <li>Scalability: The modular design of the <code>Detector</code> class allows for easy extension to additional detection tasks or object types.</li> </ul>"},{"location":"faq/ai/AI_Detector/#tips-for-future-students","title":"Tips for Future Students","text":"<ul> <li>Environment Setup: Always use <code>dotenv</code> to securely manage sensitive credentials like API keys.</li> <li>Error Handling: Implement a retry mechanism to ensure robust performance during temporary API failures.</li> <li>Testing: Use a diverse set of test images to validate the system\u2019s accuracy and adaptability across different scenarios.</li> </ul> <p>This guide simplifies the implementation process, allowing you to focus on building higher-level functionalities. Happy coding!</p>"},{"location":"faq/ai/chatgpt/","title":"Basic ChatGPT Connection and Publishing the Output to a Topic","text":"<p>by Kirsten Tapalla - Spring 2023  This is a quick guide to connecting to ChatGPT and getting it to generate a message that you can publish to a topic for use in other nodes. </p>"},{"location":"faq/ai/chatgpt/#necessary-imports","title":"Necessary Imports:","text":"<ul> <li><code>import requests</code>: used to connect to the internet and post the url to connect to ChatGPT</li> <li><code>import rospy</code>: used to publish the response from ChatGPT to a topic</li> <li><code>from std_msgs.msg import String</code>: used to publish the response as a ros String message </li> </ul>"},{"location":"faq/ai/chatgpt/#node-initialization-and-variablesparameters","title":"Node Initialization and Variables/Parameters:","text":"<ul> <li>Since you want to publish the message into a topic, you will have to initialize a rospy node withing your file using <code>rospy.init_node('ENTER-NODE-NAME-HERE')</code>. </li> <li>You will also want to initialze a rospy Publisher with the name of the topic you would like to publish the data to, for example: <code>text_pub = rospy.Publisher('/chatgpt_text', String, queue_size=10)</code>. </li> <li>If you would like to be able to change the prompt you are passing it when running the node, you should add a line allowing you to do this by initializing an input string to get the parameter with the name of what you would like to use to specify the input message you would like to pass it. <ul> <li>For example, do this by writing <code>input_string = rospy.get_param('~chatgpt_prompt')</code></li> <li>When setting up your launch file later on, you will want to include a line to handle this argument. This can be done by including <code>arg name=\"chatgpt_prompt\" default=\"ENTER-YOUR-DEFAULT-PROMPT-HERE\"</code> into your launch file. You can set the default to whatever default prompt you would like to be passed if you are not giving it a specific one. </li> </ul> </li> <li>You will also want to add this line into your code to to specify the URL going that will be used to connect to ChatGPT: <code>url = 'https://api.openai.com/v1/completions'</code>. Since the chat/text completions model is what we are using the get the output responses, the URL specifies 'completions' at the end.</li> </ul>"},{"location":"faq/ai/chatgpt/#chatgpt-information","title":"ChatGPT Information:","text":""},{"location":"faq/ai/chatgpt/#headers","title":"Headers:","text":"<p>To be able to access ChatGPT, you will need to include the following information in your 'header' dictionary: Content-Type and Authorization. Below is an example of what yours might look like: <code>headers = { \u00a0\u00a0\u00a0\u00a0'Content-Type': 'application/json', \u00a0\u00a0\u00a0\u00a0'Authorization': 'Bearer INSERT-YOUR-OPENAI-API-KEY-HERE', }</code></p>"},{"location":"faq/ai/chatgpt/#data","title":"Data:","text":"<p>This will include the information you will want to pass into ChatGPT. The only required field will be specifying the model you want to use, but since you are passing in a prompt, you will also want to include that as well. You will be able to specify the maximum amount of tokens you want ChatGPT to generate, but the best way to get the full output messeage from ChatGPT is to enter the maximum amount for the model you are using. For example, as you can see below I am using the 'text-davinci-003' model, and the maximum tokens that this model can generate is 2048. Furthermore, you can adjust the sampling temperature, which will determine how creative the output of ChatGPT will be. The range goes between 0-2, and higher values will cause it to be more random, while lower values will cause it to be more focused and deterministic. An example of a request body you can make is shown below: <code>data = { \u00a0\u00a0\u00a0\u00a0'model': 'text-davinci-003', \u00a0\u00a0\u00a0\u00a0'prompt': input_string, \u00a0\u00a0\u00a0\u00a0'max_tokens': 2048, \u00a0\u00a0\u00a0\u00a0'temperature': 0.5, }</code></p>"},{"location":"faq/ai/chatgpt/#getting-and-publishing-the-response","title":"Getting and Publishing the Response","text":"<p>To get the response for your input message from ChatGPT, include the following line in your code <code>response = requests.post(url, headers=headers, json=data)</code>. Note that if you are using different names for your variables, you will want to pass in those names in place of 'headers' and 'data'.  To publish your output, you will want to make sure that your request went through. If it did, you will be able to get the output from the json file that was returned in the response variable. An example of how to do this is shown below:  <code>if response.status_code == 200: \u00a0\u00a0\u00a0\u00a0generated_text = response.json()['choices'][0]['text'] \u00a0\u00a0\u00a0\u00a0text_pub.publish(generated_text)</code> By doing all of the steps above, you will be able to connect to ChatGPT, pass it a prompt, and publish its response to that prompt to a topic in ros. </p>"},{"location":"faq/ai/reinforcement_learning_imitation_learning/","title":"Reinforcement Learning & Imitation Learning in Robotics","text":""},{"location":"faq/ai/reinforcement_learning_imitation_learning/#author","title":"Author","text":"<ul> <li>Sonny George</li> <li>Dec 10 2024</li> <li>ROS version: N/A</li> </ul>"},{"location":"faq/ai/reinforcement_learning_imitation_learning/#what-is-reinforcement-learning","title":"What is reinforcement learning?","text":"<p>Reinforcment Learning (RL) is a type of machine learning where an agent learns to make decisions by interacting with an environment. The agent receives feedback in the form of rewards or penalties, which it uses to learn a policy that maximizes the cumulative reward over time.</p>"},{"location":"faq/ai/reinforcement_learning_imitation_learning/#when-is-rl-useful","title":"When is RL useful?","text":"<p>RL is useful for tasks requiring controls that are difficult to model with explicit, programmatic instructions. If a robot can be robustly controlled with reasonable hand-written instructions, it is very likely that RL should not be used.</p>"},{"location":"faq/ai/reinforcement_learning_imitation_learning/#how-does-rl-work","title":"How does RL work?","text":"<p>A \"policy\" is a mapping from environment states to actions. E.g.:</p> <pre><code>chess board state -&gt; POLICY -&gt; move to make\n</code></pre> <p>There are many different algorithms for learning policies (e.g. Q-learning, DDPG, PPO) that are suited to different types of problems.</p> <p>Of course, the agent needs to interact with the environment to get (or not get) reward signal.</p> <p>However, learning from scratch (a randomly initialized policy), it can be very sample inefficient to learn good behavior, and it can require a lot of improbable \"lucky guessing\" to ever get a reward signal to learn from. This is especially true in robotic control, where the state space is high-dimensional and the reward signal is sparse.</p>"},{"location":"faq/ai/reinforcement_learning_imitation_learning/#what-is-imitation-learning","title":"What is imitation learning?","text":"<p>Imitation learning mitigates this problem by building upon demonstrations of desired behavior provided by a human or another expert. Instead of starting from a randomly initialized policy, imitation learning initializes the agent's policy based on these demonstrations. This approach allows the agent to learn more efficiently, especially in tasks where exploration from scratch would be too costly, time-consuming, or unsafe.</p>"},{"location":"faq/ai/reinforcement_learning_imitation_learning/#types-of-imitation-learning","title":"Types of imitation learning","text":"<p>There are several methods of imitation learning, including:</p> <ol> <li> <p>Behavior Cloning (BC):    In behavior cloning, the agent learns to mimic the actions of the expert by training a supervised model on the demonstration dataset. This method works well when the expert's demonstrations are comprehensive and cover the task's state space adequately. However, it is susceptible to compounding errors if the agent encounters states not covered in the demonstrations.</p> </li> <li> <p>Inverse Reinforcement Learning (IRL):    In IRL, the agent infers the reward function that the expert appears to be optimizing and uses it to train its policy via reinforcement learning. This method is powerful because it focuses on learning the underlying intent of the behavior, better allowing the agent to generalize to states not explicitly demonstrated by the expert.</p> </li> </ol>"},{"location":"faq/ai/rl_with_gymnasium/","title":"General Impl./Workflow for RL with OpenAI Gymnasium, Gazebo, ROS, and RViz","text":"<p>By Alex Danilkovas | adanilkovas@gmail.com, Dec 10 2024</p> <p>This will be a general overview of how to implement your own Reinforcement Learning (RL) algorithm with the help of Gymnasium and a model of your choice. Before continuing, read the \"Reinforcement Learning &amp; Imitation Learning in Robotics\" FAQ by Sonny if you are not yet familiar with RL.</p>"},{"location":"faq/ai/rl_with_gymnasium/#getting-started","title":"Getting Started","text":"<p>Before starting any RL project, you must consider the following: 1. What task is my robot going to perform? 2. How can I tailor a reward function to reward the robot for performing the task correctly? 3. What RL algorithm is best suited for this problem?</p>"},{"location":"faq/ai/rl_with_gymnasium/#in-deciding-a-task","title":"In Deciding a Task","text":"<p>The task you choose for your robot defines every aspect of the RL pipeline. Consider tasks that are: * Specific: Clearly defined objectives make it easier to design the reward function. * Feasible: Ensure that the task is within the capabilities of the simulated model and the RL algorithm. * Transferable: If deploying in the real world, choose tasks that closely match scenarios the robot will encounter.</p> <p>For example: * A wheeled robot navigating through a maze. * A robotic arm placing objects into bins. * A drone stabilizing its flight in windy conditions.</p>"},{"location":"faq/ai/rl_with_gymnasium/#choosing-the-rl-algorithm","title":"Choosing the RL Algorithm","text":"<p>The algorithm depends on the complexity and nature of your task: * Discrete Action Space: Use Q-learning, Deep Q-Networks (DQN), or variants like Double DQN. * Continuous Action Space: Algorithms like Proximal Policy Optimization (PPO), Soft Actor-Critic (SAC), or Deep Deterministic Policy Gradient (DDPG) work well. * Multi-Agent Systems: Explore Multi-Agent Reinforcement Learning (MARL) frameworks.</p> <p>In all likelyhood you are going to be dealing with a continious action space, and therefore recomend I recomend looking into algorithms such as Soft Actor-Critic first. </p>"},{"location":"faq/ai/rl_with_gymnasium/#training-loop-overview","title":"Training Loop Overview","text":"<p>The training loop is the heart of the RL implementation: 1. Initialize the Gym environment and agent. 2. Start a training episode:    * Action: The agent selects an action based on its policy.    * Simulation Step: Send the action to the environment via ROS/Gazebo.    * Observe: Receive the new state, reward, and done flag.    * Learn: Update the policy using the RL algorithm.    * Reset: Obtain the initial state from the environment for the next episode after the number maximum number of steps for an episode has been reached (or an exit condition has been met). 3. Save progress weights periodically with checkpoints through a <code>checkpoint()</code> callback.</p>"},{"location":"faq/ai/rl_with_gymnasium/#reward-function-design","title":"Reward Function Design","text":"<p>The reward function is critical as it directly influences the agent's learning process. Some principles to follow: 1. Simplicity: The reward function should be straightforward and closely aligned with the task objective. 2. Clarity: Avoid ambiguity in rewards; positive rewards should correspond to progress, and penalties should signal undesirable actions. 3. Scaling: Normalize rewards to keep them within a range, ie 0 and 1.</p> <p>Example reward functions: * Navigation Task: Positive reward for moving closer to the goal; penalty for collisions. * Grasping Task: Positive reward for successfully grasping an object; penalty for dropping it.</p> <p>Consider the whether you want a linear or curved reward function, it is quite helpful to plot functions in Desmos and then transfer them into your training script.</p> <p>Below is a snipit of our Pick and Place reward function: <pre><code>python\n\ngrabber_obj_dist = np.mean(\n    [\n        grabber_base_dist,\n        grabber_base_dist,  # Weight the base distance more\n        left_finger_dist,\n        right_finger_dist,\n    ],\n    axis=0,\n)\nclipped_grabber_obj_dist = clip_value(grabber_obj_dist, limits=(0, self.GRABBER_OBJ_DIST_CLIP))\ngrabber_object_dist_reward = (self.GRABBER_OBJ_DIST_CLIP - clipped_grabber_obj_dist) / self.GRABBER_OBJ_DIST_CLIP  # Normalize to b/w 0 and 1\n\n# Exponentiate to make high rewards more potent and weak rewards less potent\ngrabber_object_dist_reward = (grabber_object_dist_reward**self.GRABBER_OBJ_DIST_REWARD_EXP_FACTOR)\n\nreward = obj_target_dist_reward + grabber_object_dist_reward\n</code></pre></p> <p>where</p> <pre><code>python\nGRABBER_OBJ_DIST_REWARD_EXP_FACTOR = 2.5\n</code></pre> <p>Here the reward is computed as the sum of the distance of the claw (more specifically, the claw and its fingers to reward the closing movement of the fingers around the block) and the amount the block has moved to a target location (in other words, if the block has been picked up and moved upwards).</p>"},{"location":"faq/ai/rl_with_gymnasium/#environment-setup","title":"Environment Setup","text":""},{"location":"faq/ai/rl_with_gymnasium/#setting-up-the-simulation","title":"Setting Up the Simulation","text":"<ol> <li>Gazebo:</li> <li>URDF to define your robot model.</li> <li>Create a world file with necessary obstacles and sensors.</li> </ol>"},{"location":"faq/ai/rl_with_gymnasium/#important-when-launching-in-a-gazebo-simulation-it-starts-in-a-paused-state-you-must-unpase-it-and-then-sleep-for-a-short-period-of-time","title":"Important: When launching in a gazebo simulation it starts in a paused state, you must unpase it and then sleep() for a short period of time.","text":"<pre><code>python\ndef unpause_gazebo():\n    rospy.wait_for_service(\"/gazebo/unpause_physics\")\n    unpause_gazebo = rospy.ServiceProxy(\"/gazebo/unpause_physics\", Empty)\n    unpause_gazebo()\n</code></pre>"},{"location":"faq/ai/rl_with_gymnasium/#important-to-have-your-simulation-run-faster-than-real-time-you-must-add-the-following-to-your-world-file","title":"Important: To have your simulation run faster than real time you must add the following to your .world file:","text":"<pre><code>xml\n&lt;physics name=\"default_physics\" type=\"ode\"&gt;\n    &lt;real_time_factor&gt;0&lt;/real_time_factor&gt;  &lt;!-- Real-time factor for simulation speed (1.0 = real time, 0 = max) --&gt;\n    &lt;real_time_update_rate&gt;0&lt;/real_time_update_rate&gt;  &lt;!-- Number of updates per second --&gt; (0 = max)\n&lt;/physics&gt;\n</code></pre> <ol> <li>Customizing the Gym Environment:</li> <li>Inherit from <code>gym.Env</code>.</li> <li>Implement <code>reset()</code> to initialize the environment begining state.</li> <li>Implement <code>step(action)</code> to execute the action, calculate rewards, and return new observations.</li> <li>Use the ROS-Gazebo bridge (publishers and subscribers) to control the robot in Gazebo and receive sensor data.</li> </ol>"},{"location":"faq/ai/rl_with_gymnasium/#visualization","title":"Visualization","text":"<ul> <li>Use Gazebo to see your robot interacting with its environment, not recommended for actual prolonged training.</li> <li>Use RViz to monitor your robots behavior during training, as it is much faster. </li> </ul> <p>Launching in an RViz state typaically required to add the gui:=false use_rviz:=true flags to your launch file.</p>"},{"location":"faq/ai/rl_with_gymnasium/#deployment-and-testing","title":"Deployment and Testing","text":"<p>Once training is complete: 1. Load the saved model 2. Observe environment with sensors (subscribers) 3. Run <code>model.predict()</code> to get the action. 4. Publish the action. 5. Repeat 2-4 until task is accomplished.</p> <p>...</p> <p>For more specific implementation details, see the PX100_Pick_and_Place_SAC project repository.</p>"},{"location":"faq/arm/claw_movement/","title":"FAQ: Using the Claw","text":"<p>This FAQ will guide you through the process of using the claw in our color sorting robot project. The applications for the claw are endless, and this guide will allow you to easily write code for use of the claw on a robot. The instructions below are written for ROS and Python.</p>"},{"location":"faq/arm/claw_movement/#how-do-i-set-up-the-robot-with-the-claw","title":"How do I set up the robot with the claw?","text":"<ol> <li>Make sure you are connected to a robot with a claw. As of now, the only robots with a claw are the platform robots in the lab.</li> </ol>"},{"location":"faq/arm/claw_movement/#how-do-i-import-the-necessary-libraries","title":"How do I import the necessary libraries?","text":"<ol> <li>Import 'Bool' from 'std_msgs.msg':<pre><code>from std_msgs.msg import Bool #Code to import Bool\n</code></pre> </li> </ol>"},{"location":"faq/arm/claw_movement/#how-do-i-create-a-publisher-for-the-claw","title":"How do I create a publisher for the claw?","text":"<ol> <li>Create a publsiher that publishes commands to the claw:<pre><code>servo_pub = rospy.Publisher('/servo', Bool, queue_size=1) #Code for publisher\n</code></pre> </li> </ol> <p>This code creates a publisher called 'servo_pub' that publishes to the '/servo' node and sends a Bool value.</p>"},{"location":"faq/arm/claw_movement/#how-do-i-write-code-to-open-or-close-the-claw","title":"How do I write code to open or close the claw?","text":"<ol> <li>Write code to open or close the claw:<pre><code>servo_pub.publish(True)  # Opens claw\nservo_pub.publish(False) # Closes claw\n</code></pre> </li> </ol>"},{"location":"faq/arm/claw_movement/#faq","title":"FAQ","text":""},{"location":"faq/arm/claw_movement/#q-can-i-control-the-speed-of-the-claw","title":"Q: Can I control the speed of the claw?","text":"<p>A: The code provided does not control the speed of the claw. You will need to modify the code and use a different message type to control the speed.</p>"},{"location":"faq/arm/claw_movement/#q-can-i-use-this-code-for-other-robots-with-a-claw","title":"Q: Can I use this code for other robots with a claw?","text":"<p>A: There are two robots as of right now with the claw attachment, both are platform robots. One of the claws is a big claw while the other one is a smaller claw. Both can be used for different applications and in both cases, the above code should work.</p>"},{"location":"faq/arm/claw_movement/#q-how-do-i-open-and-close-the-claw-at-specific-times","title":"Q: How do I open and close the claw at specific times?","text":"<p>A: As long as you have a publisher, you can publish a command to open or close a claw at any time during the main loop of your program. You can have multiple lines of codes that opens or closes the claw multiple times throughout a program or you can just write code to have the claw open once. It's up to you.</p>"},{"location":"faq/arm/joint-controllers/","title":"How to change joint control type","text":"<p>By Cole Peterson</p> <p>Each joint in ros has a type. These types determine the degrees of freedom of a joint. For example, a continuous joint can spin around a single axis, while a fixed joint has zero degrees of freedom and cannot move. At a low level, when publishing commands to a joint in ros you are telling it how you want it to move about its degrees of freedom.</p> <p>The form these commands take, however, is not determined by the joint itself, but by its joint controller. The most common joint controllers (which are provided in ros by default) are effort controllers. These controllers come in three varieties. The first is the very simple \"joint_effort_controller\" which just takes in the amount of force/torque you want the joint to exert. The second, and likely more useful type is the \"joint_position_controller\", which takes commands in the form of a desired position. In a continuous joint this would be a radian measurement for it to rotate to. The position controller uses a pid algorithm to bring the joint smoothly to the desired positon. Finally, there is the \"joint_velocity_controller\", which takes in a velocity command and once again uses a pid algorithm to maintain this speed (this is most similar to the twist commands used in class, though rotational velocity is given in radians per second. Twist commands take care of the conversion to linear velocity for you).</p> <p>In order to set up one of these joint controllers, you simply need to add a transmission block to your urdf file. This takes the following form: </p> <pre><code>&lt;transmission name=\"tran\"&gt;\n    &lt;type&gt;transmission_interface/SimpleTransmission&lt;/type&gt;\n    &lt;joint name=\"joint\"&gt;\n      &lt;hardwareInterface&gt;hardware_interface/VelocityJointInterface&lt;/hardwareInterface&gt;\n    &lt;/joint&gt;\n    &lt;actuator name=\"motor\"&gt;\n      &lt;hardwareInterface&gt;hardware_interface/VelocityJointInterface&lt;/hardwareInterface&gt;\n      &lt;mechanicalReduction&gt;1&lt;/mechanicalReduction&gt;\n    &lt;/actuator&gt;\n  &lt;/transmission&gt;\n</code></pre> <p>In this section of xml code there are a number of variables which will need to be changed to fit your environment</p> <ol> <li>transmission name. The specific name you choose doesn't matter, but it must be unique from any other transmissions in your urdf.</li> <li>joint name. This must match the name of the joint you wish to control.</li> <li>actuator name. See transmission name.</li> <li> This section must be changed to match the type of controller you wish to use.<ol> <li>note that this appears twice. Once underneath the joint name, and again under actuator name. They must match each other.</li> <li>If you want to use a position controler instead of velocity replace \"VelocityJointInterface\" with \"PositionJointInterface\" or \"EffortJointInterface\" for an effort joint.</li> </ol> <p>By making these modifications you can correctly set up a joint controller in your homemade urdf.</p>"},{"location":"faq/arm/pincer_use/","title":"Pincer Attachment","text":""},{"location":"faq/arm/pincer_use/#using-platform2-pincer-attachment","title":"Using platform2 pincer attachment","text":"<p>In order to use the pincer attachment, you must have a way publish to the servo motor.</p> <p></p>"},{"location":"faq/arm/pincer_use/#publisher","title":"Publisher","text":"<p>First, you must import Bool. True will be to open the attachment, and False will be to close the attachment. </p> <p><code>from std_msgs.msg import Bool</code></p> <p>The following publisher needs to be published to:</p> <p><code>rospy.Publisher('/servo', Bool, queue_size=1)</code></p> <p>By publishing to the servo motor, you are telling it to either open or close. If you publish True, the pincer will open. If you publish False, the pincer will close.</p> <pre><code>def open():\n    self.pub.publish(Bool(True))\n\ndef close():\n    self.pub.publish(Bool(False))\n</code></pre> <p>This has use beyond this pincer itself. By having any custom attachment with a servo motor, this provides an easy way to publish to it.</p>"},{"location":"faq/arm/pincer_use/#full-example-of-the-pincer-class","title":"Full example of the pincer class","text":"<pre><code>#!/usr/bin/env python3\n\nimport rospy\nfrom std_msgs.msg import Bool\n\nclass Pincer:\n    \"\"\" Allows pincer to open and close \"\"\"\n\n    def __init__(self):\n        self.pub = rospy.Publisher('/servo', Bool, queue_size=1)\n\n    def open(self):\n        self.pub.publish(Bool(True))\n\n    def close(self):\n        self.pub.publish(Bool(False))\n</code></pre>"},{"location":"faq/arm/px100/InterbotixPincherX100/","title":"arm setup","text":""},{"location":"faq/arm/px100/InterbotixPincherX100/#links","title":"Links","text":"<ul> <li>Arm Details</li> <li>Quickstart Guide</li> <li>Hardware setup<ul> <li>only needed to install the grippers<ul> <li>the screws would not go through initially - had to go from the other side with the screw first to ensure that the holes were completely open and the screw could be secured and then installed the grippers as the instructions said</li> </ul> </li> <li>plug in power supply first and then plug usb into computer and then plug microUSB into arm</li> </ul> </li> <li>ROS Installation Guide</li> <li>Troubleshooting</li> <li>DYNAMIXEL Software<ul> <li>after installing software and plugging arm in, scan for the arm in the dynamixel software to check that everything is working properly:<ul> <li>in options, select baudrates 57600 and 1000000</li> <li>if any link is in 57600, then change to 1000000</li> </ul> </li> <li>make sure to disconnect before running ros</li> <li>NOTE: you may not actually need to do this, but it may be good to do the first time you try to connect the arm to your computer to make sure everything is running correctly. </li> </ul> </li> <li>Arm Control<ul> <li>contains command line configs for launch file</li> </ul> </li> <li>Python-ROS Interface<ul> <li>contains details on methods that control the arm using their methods - can find basic overview at the bottom of this file. </li> </ul> </li> </ul> <p>****Installation:**** </p> <p>On Intel/AMD based processor: </p> <pre><code>sudo apt install curl\ncurl 'https://raw.githubusercontent.com/Interbotix/interbotix_ros_manipulators/main/interbotix_ros_xsarms/install/amd64/xsarm_amd64_install.sh' &gt; xsarm_amd64_install.sh\nchmod +x xsarm_amd64_install.sh\n./xsarm_amd64_install.sh -d noetic\n</code></pre> <p>Basic Commands: </p> <ul> <li>move the arm manually:<ul> <li><code>roslaunch interbotix_xsarm_control xsarm_control.launch robot_model:=px100</code></li> </ul> </li> <li>disable torque:<ul> <li><code>rosservice call /px100/torque_enable \"{cmd_type: 'group', name: 'all', enable: false}\"</code></li> </ul> </li> <li>re-enable torque to hold a pose:<ul> <li><code>rosservice call /px100/torque_enable \"{cmd_type: 'group', name: 'all', enable: true}\"</code></li> </ul> </li> <li>run with moveit:<ul> <li><code>roslaunch interbotix_xsarm_moveit xsarm_moveit.launch robot_model:=px100 use_actual:=true dof:=4</code></li> <li><code>roslaunch interbotix_xsarm_moveit xsarm_moveit.launch robot_model:=px100 use_gazebo:=true dof:=4</code></li> </ul> </li> <li>run using ROS-PYTHON API:<ul> <li><code>roslaunch interbotix_xsarm_control xsarm_control.launch robot_model:=px100 use_sim:=true</code></li> <li><code>roslaunch interbotix_xsarm_control xsarm_control.launch robot_model:=px100 use_actual:=true</code></li> </ul> </li> <li>play with joints:<ul> <li><code>roslaunch interbotix_xsarm_descriptions xsarm_description.launch robot_model:=px100 use_joint_pub_gui:=true</code></li> </ul> </li> <li>publish static transforms between two frames:<ul> <li><code>rosrun tf static_transform_publisher x y z yaw pitch roll frame_id child_frame_id period(milliseconds)</code></li> </ul> </li> </ul>"},{"location":"faq/arm/px100/InterbotixPincherX100/#interbotix-python-ros-interface","title":"Interbotix Python-ROS Interface","text":"<ul> <li> <p><code>arm.set_ee_pose_components()</code></p> <ul> <li>sets an absolute position relative to the base of the frame<ul> <li>ee_gripper_link frame with respect to the base_link frame</li> </ul> </li> <li> <p><code>arm.set_single_joint_position()</code></p> </li> <li> <p>move the specified joint</p> </li> <li>usually used for the waist to turn the robot</li> <li> <p><code>arm.set_ee_cartesian_trajectory()</code></p> </li> <li> <p>move the end effector the specified value in each direction relative to the current position</p> </li> <li>for a 4dof arm, the y and yaw values cannot be set through this</li> <li> <p><code>arm.go_to_sleep_position()</code></p> </li> <li> <p>return the arm to the sleep position</p> </li> <li> <p><code>arm.go_to_home_position()</code></p> </li> <li> <p>return the arm to the home position</p> </li> <li> <p><code>gripper.open()</code></p> </li> <li> <p>open the gripper</p> </li> <li> <p><code>gripper.close()</code></p> </li> <li> <p>close the gripper</p> </li> <li> <p><code>arm.set_trajectory_time()</code></p> </li> <li> <p>moving_time - duration in seconds it should take for all joints in the arm to complete one move.</p> </li> <li>accel_time - duration in seconds it should take for all joints in the arm to accelerate/decelerate to/from max speed.</li> </ul> </li> </ul>"},{"location":"faq/arm/px100/api-fix/","title":"Question","text":"<p>I heard that Interbotix's official API has a bug. How do I fix it? </p>"},{"location":"faq/arm/px100/api-fix/#problem","title":"Problem","text":"<p>The arm\u2019s official API is unable to process rapid sequential movement commands. That is, suppose <code>move(p)</code> represents an API command to move the arm\u2019s gripper to some point <code>p</code> within a given coordinate frame. Roughly speaking, then, the arm is unable to comply when we execute <code>move(p1)</code>, <code>move(p_2)</code>, and <code>move(p_3)</code> in rapid succession.</p>"},{"location":"faq/arm/px100/api-fix/#solution","title":"Solution","text":"<p>Modify the <code>queue_size</code> arguments of three ROS publishers in the arm's API source code.</p>"},{"location":"faq/arm/px100/api-fix/#steps","title":"Steps","text":"<ol> <li> <p>Download Interbotix's official API by following the instructions    here.</p> </li> <li> <p>Open <code>interbotix_ws/src/interbotix_ros_toolboxes/ interbotix_xs_toolbox/interbotix_xs_modules/src/interbotix_xs_modules/core. py</code></p> </li> <li> <p>Change the <code>queue_size</code> argument of the <code>pub_group</code>, <code>pub_single</code>, and    <code>pub_traj</code> publishers from <code>1</code> to <code>20</code>.</p> </li> </ol>"},{"location":"faq/arm/px100/api-fix/#why-does-this-work","title":"Why does this work?","text":"<p>If you don't have time, skip this section.</p> <p>To see why the fix works, we should first get a brief overview of how the arm\u2019s movement API behaves under the hood. </p> <p>A movement function of the arm API works by publishing command messages over one or more of three topics:</p> <ol> <li><code>JointSingleCommand</code> messages over <code>/px100/commands/joint_single</code>,</li> <li><code>JointGroupCommand</code> messages over <code>/px100/commands/joint_group</code>, or</li> <li><code>JointTrajectoryCommand</code> messages over <code>/px100/commands/joint_trajectory</code>.  </li> </ol> <p>And when a message is published over one of these three topics, the <code>/px100/xs_sdk</code> node that subscribes to the topics processes the message and orders the arm to move as the the message directs.</p> <p>So if a user calls an API movement function at a rate <code>R</code>, the API function in turn should publish command messages at a rate <code>R</code>, which the <code>/px100/xs_sdk</code> node should process at the same rate.</p> <p>But for any ROS publisher <code>p</code>, <code>p</code> works by storing the messages it is to publish in a queue, and processing them sequentially. Further, where <code>m</code> is a message, if we call <code>p.publish(m)</code> when <code>p</code>\u2019s queue is full, ROS silently drops <code>m</code> and never publishes it. </p> <p>Now the <code>queue_size</code> of the publishers that the API\u2019s movement functions used were all set to <code>1</code>. Thus, if the rate <code>R</code> at which we called an API movement function was too high, the publishers would often try to publish a message <code>m</code> when its queue was full, thus dropping <code>m</code>.  Thus, this problem is solved by simply increasing the <code>queue_size</code> of the mentioned publishers from <code>1</code> to a generous <code>20</code>.</p>"},{"location":"faq/arm/px100/arm-motion-control/","title":"Question","text":"<p>How can I effectively move the robotic arm to a desired location?</p>"},{"location":"faq/arm/px100/arm-motion-control/#answer-1","title":"Answer 1","text":"<p>To see how to move the arm with the help of fiducials and the tf2 package, refer to the README of this project.</p>"},{"location":"faq/arm/px100/arm-motion-control/#answer-2","title":"Answer 2","text":""},{"location":"faq/arm/px100/arm-motion-control/#problem","title":"Problem","text":"<p>The InterbotixPincherX100 can rotate, move up and down, and extend. To get the arm to go to a specific point in space given (x, y, z) coordinates, the x and y components must be converted to polar coordinates.</p>"},{"location":"faq/arm/px100/arm-motion-control/#solution","title":"Solution","text":"<p> Moving the arm to point (x, y), top-down view</p> <p>Since the arm can move out/back and up/down to specific points without conversion, consider only a top-down view of the arm (x-y axis). To get the rotation and extension of the arm at a specific point, consider the triangle shown above. For rotation \u03b8: $\u03b8=atan(x/y)$. For extension r: $r=y/cos(\u03b8)$. The up-down movement of the arm remains the same as the given z coordinate.</p> <p>The arm can be moved to the desired point using set_ee_cartesian_trajectory. In this method, the extension and motion up/down is relative so the current extension and vertical position of the arm must be known as current_r and current_z.</p> <pre><code>bot = InterbotixManipulatorXS(\"px100\", \"arm\", \"gripper\")\ntheta = math.atan(x/y)\ndr = y / (math.cos(theta)) - current_r\ndz = z - current_z\nbot.arm.set_single_joint_position(\"waist\", theta)\nbot.arm.set_ee_cartesian_trajectory(r = dr, z = dz)\n</code></pre>"},{"location":"faq/arm/px100/determine-arm-transforms-using-a-real-coordinate-system/","title":"Determine arm transforms using a real coordinate system","text":""},{"location":"faq/arm/px100/determine-arm-transforms-using-a-real-coordinate-system/#by-vibhu-singh","title":"By Vibhu Singh","text":""},{"location":"faq/arm/px100/determine-arm-transforms-using-a-real-coordinate-system/#a-couple-important-things-about-the-arm","title":"A couple important things about the arm","text":"<ol> <li>Moving the arm in multiple steps is not the same as moving the arm with just one transform because of the motors in the arm<ul> <li>For example, moving the arm -0.03 and then 0.01 in the x direction is not the same as moving -0.02</li> <li>The farther the arm moves in any direction, especially the y direction</li> </ul> </li> <li>The arm needs to be firmly secured on a surface, otherwise the whole apparatus will move around when the arm moves</li> <li>The arm should be returned to the sleep position frequently, one team found that the sweet spot was every 5-6 transforms. If that isn't enough, you should return the arm to the sleep position after one full cycle of transforms.</li> <li>Brush up on polar coordinates since those are the best way to think about the arm and arm transforms. The best way to think about the transforms is that the y transform is the theta and the x transform is the radius of the unit circle.</li> </ol>"},{"location":"faq/arm/px100/determine-arm-transforms-using-a-real-coordinate-system/#how-to-set-up","title":"How to set up","text":"<p>The arm set up doesn't have to be exactly as the image below, but having a setup that follows the same principles is going to be ideal. - Having the camera positioned over the arm so that the arm is at the edge of the frame will make the math that you will have to do easier - As mentioned above, the arm should be fixed on a stable surface - For reproducability of the results, you will need to have the exact same, or as close as you can get to the exact same, set up so that all the math you do works correctly.     - Depending on what you are trying to grab using the arm, a difference of even 1-2 centimeters is enough to mess up the equations</p> <p></p>"},{"location":"faq/arm/px100/determine-arm-transforms-using-a-real-coordinate-system/#how-to-get-x-transforms","title":"How to get X transforms","text":"<p>As mentioned at the start of this guide it's a good idea to brush up on polar coordinates for this project. The x transform is basically the radius portion of a polar coordinate. The arm has a very limited range of motion for the X transforms, ranging from about -0.08 to .04 and the transforms can only go to the second decimal place. The best way to find a relationship between the X transforms and the real coordinate system is to use a stepwise function. </p> <p>It is important to remember to transform the coordinates so that the origin is at. This is why it was important to position the camera so that the arm was at one edge of the frame, it would make it easier to transform the coordinates the camera gives since there will be less transforms to make.</p> <p>To get the \"bins\" for the stepwise function, the best way so far to do it is to have the arm extend or retract to all possible positions and record the coordinates, apply the transforms, and then convert to polar coordinates. What this will do is allow you to find a way to get ranges of radii that correspond with certain X transforms.</p> <p>Below is an example of a graph that the Cargo Claw team made to illustrate their X transforms for their project. There are more details available about the project in the reports section under the cargoclaw project.</p> <p></p>"},{"location":"faq/arm/px100/determine-arm-transforms-using-a-real-coordinate-system/#how-to-get-y-transforms","title":"How to get Y transforms","text":"<p>The best way to think about the Y transforms is to imagine them as angles of rotation on a unit circle. The first step to getting a good idea of the relationship between the Y transforms is to record what the coordinates are at even intervals of the Y transform locations you are trying to reach. Once you have that, apply the transforms to all the coordinates and then save the theta values. From there, it'll be easy work to graph the Y transforms against the theta values.</p> <p>From here, to get an actual equation to convert from the coordinates to transforms, it'll be a good idea make a graph of the Y transforms against the theta values so that you can see what sort of pattern the relationship follows. A cubic regression works well because it can account for some of the problems that come with the arm's movement and the inaccuracies that. You might still have to do some fine tuning in your code, for example:</p> <pre><code>    if (y_transform &gt; .5):\n        y_transform += .05\n    elif (y_transform &gt; .6):\n        y_tranfrom += .09\n</code></pre> <p>Alternatively, you can use two different cubic regressions, one for if the Y transform is positive and one for if the Y transform is negative. You can also try higher degree regressions, but you will have to decide if the additional complexity is worth it.</p> <p>Below is an example of a graph that the Cargo Claw team made to show the relationship between the theta and the y transform. There are more details available about the project in the reports section under the cargoclaw project.</p> <p></p> <p>To get an applied and working example of everything described, you can look at the code for the cargoclaw project in the box_pickup.py file within the src directory of the git.</p>"},{"location":"faq/arm/px100/general-advice/","title":"Question","text":"<p>Any general advice before I start working with the PX-100?</p>"},{"location":"faq/arm/px100/general-advice/#answer","title":"Answer","text":"<ol> <li>Fix the bug in Interbotix's source code. There's a bug in Interbotix's    source code that you should fix before you start working with the arm. Unless, that is, someone already fixed it for you. Ask Professor Salas about this, and read the entry <code>api-fix.md</code>.</li> <li>Study previous projects that used the arm. The most helpful are probably    Fiducial Pick and Place and Pnp. But this is only because they in turn referenced previous projects, like ArmCam.</li> <li>When stuck and previous projects don't provide an answer, dig into    Interbotix's source code. Most of the source code you'll need is probably here. Yes, it's a lot of code, and to make sense of it you might have to dig into other repositories that interbotix offers. But this is good practice for working with code \"in the real world\". You'll have to do it anyway, presuming you want to work in industry. Why not start now?</li> <li>Sometimes your computer just fails to detect the arm. RViz will show scary    white boxes where the robot's limbs should be. Don't worry! You (probably) didn't break the arm. Just do a hard restart of it: unplug and replug the data and power chords from the arm. That should fix the problem.</li> <li>Getting the arm to move as you want is hard. Have patience, and try to    troubleshoot in a methodical, rational manner. It's really hard! Especially if you don't have a strong math background, and have to rely on Interbotix's official API. The <code>set_ee_cartesian_trajectory</code> method, for example, tells you how much of the trajectory the API successfully calculated, but not why it failed or succeeded as much as it did. Still, don't worry: there are probably easier ways around your problem than trying to do a crash course on inverse or forward kinematics. Reference previous projects done at the lab and see how they solved their problems.</li> </ol>"},{"location":"faq/arm/px100/moveit/","title":"Question","text":"<p>I heard that MoveIt may be a good way to get acquainted with the PX-100 arm. How might I go about using it?</p>"},{"location":"faq/arm/px100/moveit/#setup","title":"Setup","text":"<p>Check if you have Interbotix's ROS1 software package installed on your computer. You can check if you do by executing</p> <pre><code>rospack list | grep interbotix\n</code></pre> <p>in your terminal. This should output the names of 20 or so packages that have <code>interbotix_</code> as a prefix. If you see such output, you're good to go!</p> <p>If your computer doesn't have Interbotix's ROS1 software package, navigate to this page, and follow the appropriate instructions to install the required software.</p>"},{"location":"faq/arm/px100/moveit/#launching-rviz-with-moveit","title":"Launching RViz with MoveIt","text":"<p>Next, execute this command to launch RViz with MoveIt:</p> <pre><code>roslaunch interbotix_xsarm_moveit_interface xsarm_moveit_interface.launch robot_model:=px100 dof:=4 use_actual:=true\n</code></pre> <p>Note the parameters: we are specifying that we want to use Interbotix's MoveIt interface with the px100 which has 4 degrees of freedom. We are also stating that we want to use MoveIt with the actual robot arm, rather than in simulation.</p> <p>Once you execute the command above, go to the <code>Displays</code> window of RViz, and change the <code>Fixed Frame</code> from <code>world</code> to <code>px100/base_link</code>, as pictured below:</p> <p> </p> <p>Next, again in the <code>Displays</code> window, click on the <code>MotionPlanning</code> tab and ensure that <code>Query Goal State</code> is checked, like so.</p> <p> </p> <p>If you do this properly, you will see a small turqoise ball appear between the robot's grippers, and arrows pointing to opposite directions on each of the x, y, and z axes.</p> <p>Now, try holding the top blue arrow with your mouse, and dragging it up, like so:</p> <p> </p> <p>Then move to the <code>Planning</code> tab of the <code>MotionPlanning</code> window, and click <code>Plan &amp; Execute</code>.</p> <p> </p> <p>If all went well, you should have witnessed the MoveIt package plan a trajectory to your arm's goal state, and move the arm accordingly!</p>"},{"location":"faq/arm/px100/moveit/#next-steps","title":"Next Steps","text":"<p>Reverse your steps to ensure the arm returns to a resting position. Otherwise, make sure you have a colleague holding the arm before you exit RViz. This is because once you exit RViz, the locks on the servo motors will shut off, and the arm will come crashing to its floor! This probably won't damage your arm permanently, but we still want to be as gentle with it as possible.</p> <p>Playing with MoveIt in the way described above allows you to get acquainted with the arm's rough workspace: what places it can reach, and what places it can't. Of course, the next steps are to control the arm's movements through code that you write. For this, see the entry on arm motion control in this faq.</p>"},{"location":"faq/aws/alexa-flask-ask/","title":"Building a skill with Alexa Flask-ASK for ROS","text":""},{"location":"faq/aws/alexa-flask-ask/#ben-soli","title":"Ben Soli","text":"<p>Before you use this tutorial, consult with the Campus Rover Packages which outline setting up ngrok with flask and getting to the Alexa Developer Console.</p> <p>The Flask Alexa Skills Kit module allows you to define advanced voice functionality with your robotic application.</p> <p>In your VNC terminal</p> <pre><code>pip3 install flask-ask\n</code></pre> <p>Then in your rospy node file</p> <pre><code>from flask_ask import Ask, statement, question, elicit_slot, confirm_slot\n</code></pre> <p>declare your Flask application as</p> <pre><code>app = Flask(__name__)\n</code></pre> <p>and connect it to the Flask ASK</p> <pre><code>ask = Ask(app, '[endpoint]')\n</code></pre> <p>So, if your ngrok subdomain is campusrover and your Alexa endpoint is /commands</p> <p>your code would be</p> <pre><code>ask = Ask(app, '/commands')\n</code></pre> <p>On the Alexa Developer Consoles define your intents and determine which slots your intents will use. Think of intents as functions, and slots as parameters you need for that function. If a slot can have multiple items in a slot mark that in the developer console. You do not need to provide the Alexa responses on the Developer Console because you will be writing them with Flask-ASK. The advantage of doing this is you can write responses to the user that take into account the robot's state and publish information to other nodes as you receive input with Alexa.</p> <p>Let's assume we have an intent called 'bring_items' that can contain multiple items to fetch. Assume that the robot needs at least one item to fulfill this intent and that the robot has a weight capacity for how much it can carry. Let's also assume we have some kind of lookup table for these items which tells us their weight. With Flask-ASK we can quickly build this.</p> <p>You are required to have a response for launching the intent marked as</p> <pre><code>@ask.launch\ndef start_skill():\n  return question('hello, what you need?)\n</code></pre> <p>This intent is not called every single time you use the skill, but its a good way to tell the user what the bot can do or tell the user what information the bot needs from them. There are a few different response types,</p> <p><code>statement(str: response)</code> which returns a voice response and closes the session.\\ <code>question(str: response)</code> which you can use to ask the user a question and keep the session open.\\ <code>elicit_slot(str: slot_name, str: response)</code> which you can use to prompt the user for a specific type of information needed to fulfill the intent.\\ <code>confirm_slot(str: slot_name, str: response)</code> which you can use to confirm with the user that Alexa heard the slot correctly.\\</p> <p>It is important to note, that to use <code>elicit_slot</code> and <code>confirm_slot</code> you must have a Dialog Model enabled on the Alexa Developer Console. The easiest way I have to found to enable this is to create an intent on the console that requires confirmation. To avoid this intent being activated, set its activation phrase to gibberish like 'asdlkfjaslfh'. You can check a switch marking that this intent requires confirmation.</p> <p>Now let's build out an intent in our flask application.</p> <p>Start with a decorator for your skill response marking which intent you are programming a response for.</p> <pre><code>@ask.intent('bring_items')\ndef bring_items():\n</code></pre> <p>First, let's assure that the user has provided the robot with some items. When you talk to Alexa, it essentially just publishes a JSON dictionary to your endpoints which your flask app can read. To get a list of the slots for this intent use the line:</p> <pre><code>  slots = request.get_json()['request']['intent']['slots']\n</code></pre> <p>Let's call our items slots 'items'. To check that the user has provided at least one item, write.</p> <pre><code>if 'value' not in slots['items'].keys() \n  return elicit_slot('items', 'what do you want me to bring?)\n</code></pre> <p>This will check that there is an item, and if there is not, prompt the user for some items. The string <code>python 'value'</code> is a key in the dictionary if and only if the user has provided the information for this slot, so this is the best way to check.</p> <p>Let's assume that our robot has a carrying capacity of 15 pounds and has been asked to carry some items that weigh more than 15 pounds. Once we've checked that the items are too heavy for the robot, you can elicit the slot again with a different response like</p> <pre><code>elicit_slot('items','those are too heavy, give me something else to bring')\n</code></pre> <p>and the user can update the order. Returning <code>elicit_slots</code> keeps the Alexa session open with that intent, so even though each call is a return statement, you are essentially making a single function call and updating a single JSON dictionary.</p> <p>Once the robot is given a list of items that it can carry, you can use a rospy publisher to send a message to another node to execute whatever robotic behavior you've implemented.</p> <p>You should also include</p> <pre><code>@ask.intent('AMAZON.FallbackIntent')\n</code></pre> <p>This is an intent you can use for any phrase that you have not assigned to an intent that your robot will use. If you do not implement a response for this, you will very likely get error messages from Alexa.</p> <p>Skills come prebuilt with some intents such as these. If a user activates one of these intents, and you don't define a response, you will get a skills response error. You should thoroughly test possible utterances a user might use to avoid errors. Skills response errors do not crash the application, but it makes for bad user-experience.</p> <p>Another default intent you will likely need is</p> <pre><code>@ask.intent('AMAZON.CancelIntent')\n</code></pre> <p>This handles negative responses from the user. An example of this is:</p> <p>User: Alexa launch campus rover\\ Alexa: hello, what do you need\\ User: nothing\\ Alexa: AMAZON.CancelIntent response\\</p> <p>I hope this tutorial has made clear the advantages of using the flask-ASK for your Alexa integration. It is a great way to rapidly develop different voice responses for your robot and quickly integrate those with robotic actions while avoiding the hassle of constantly rebuilding your Alexa skill in the Developer Console.</p>"},{"location":"faq/aws/aws-robomaker/","title":"AWS RoboMaker","text":""},{"location":"faq/aws/aws-robomaker/#introduction","title":"Introduction","text":"<p>Amazon Web Services RoboMaker is an online service which allows users to develop and test robotic applications online. Robomaker has a number of advanced features, but this notebook page will focus on developing nodes and simulating them in Gazebo. To use RoboMaker, you will also need to use AWS S3 and Cloud9.</p>"},{"location":"faq/aws/aws-robomaker/#getting-started","title":"Getting started","text":"<p>Create an AWS root account if you do not already have one. A free tier account will suffice for getting started, though make note that under a free tier membership you will be limited to 25 Simulation Units (hours) for the first twelve months.</p> <p>Once you have an AWS account, you should create an IAM for your account. AWS recommends not using your root user account when using services like RoboMaker. To learn how to set up IAM, click here. Remember the username and password of the account you create. Additionally, save the link to where you can log in with those credentials.</p> <p>Going forward, you should be logged in with your IAM account. Log into AWS with your IAM, then proceed.</p>"},{"location":"faq/aws/aws-robomaker/#amazon-documentation-and-mini-tutorial","title":"Amazon Documentation and Mini Tutorial","text":"<p>RoboMaker has a limited documentation set that can help you use the software. The \u201cgetting Started\u201d section can help familiarize yourself with the software by working with a sample application. You can find this tutorial by clicking here.</p>"},{"location":"faq/aws/aws-robomaker/#creating-an-s3-bucket","title":"Creating an S3 bucket","text":"<p>From the AWS Management Console, type \u201cS3\u201d into the \u201cfind services\u201d field and click on S3 in the autofill list below the entry box. From the S3 Management Console, click \u201cCreate Bucket\u201d</p> <ul> <li>On the first page, enter a name for your bucket. Under region, make sure that it is US East (N Virginia), NOT US East (Ohio), as RoboMaker does not work in the Ohio region.</li> <li>Skip step 2, \u201cconfigure options\u201d</li> <li>In step 3, \u201cSet Permissions\u201d, uncheck all four boxes</li> <li>Click \u201cCreate Bucket\u201d</li> </ul> <p>This bucket is where your bundled robotic applications will be stored.</p>"},{"location":"faq/aws/aws-robomaker/#creating-a-development-environment","title":"Creating a Development Environment","text":"<p>Beck at the AWS Management Console, type \u201crobomaker\u201d in to the same entry field as S3 in the last part to go to the RoboMaker Management Console. In the left hand menu, under Development, select \u201cDevelopment environments\u201d then click \u201cCreate Environment\u201d</p> <ul> <li>Give your environment a name</li> <li>Keep the instance type as its default, m4 large</li> <li>Under networking, select the default VPC and any subnet, then click \u201ccreate\u201d to finish creating your environment</li> </ul> <p>In your Cloud9 environment, use the bash command line at the bottom of the screen and follow these instructions: how to create a new RoboMaker application to create the directories needed to work with ROS.</p> <p>At the end, you will have both a robot workspace and a simulation workspace. The robot workspace (<code>robot_ws</code>) contains source files which are to be run on the robot. The simulation workspace (<code>simulation_ws</code>) contains the launch files and the world files needed to run gazebo simulations. Going forward this guide assumes that you have directories set up exactly as described in the walkthrough linked above, especially that the folders <code>robot_app</code> and <code>simulation_app</code> exist inside the <code>src</code> folders of <code>robot_ws</code> and <code>simulation_ws</code>, respectively.</p>"},{"location":"faq/aws/aws-robomaker/#adding-new-scripts-to-robot_ws","title":"Adding new scripts to <code>robot_ws</code>","text":"<p>Python ROS node files should be stored in the scripts folder inside robot_app. When you add a new node, you must also add it to the <code>CMakeLists.txt</code> inside <code>robot_app</code>. In the section labelled \u201cinstall\u201d you will see <code>scripts/rotate.py</code>, below that line is where you should list all file names that you add (with scripts/ preceding the name).</p>"},{"location":"faq/aws/aws-robomaker/#modifying-the-simulation","title":"Modifying the Simulation","text":"<p>When creating your directories, two files were put inside <code>simulation_app/launch</code>: <code>example.launch</code> and <code>spawn_turtlebot.launch</code></p> <ul> <li>Inside <code>spawn_turtlebot.launch</code>, you will see a line that looks like this (it should be on line 3): <code>&lt;arg name=\"model\" default=\"$(optenv TURTLEBOT3_MODEL waffle_pi)\" doc=\"model type [burger, waffle, waffle_pi]\"/&gt;</code> In this section: <code>default=\"$(optenv TURTLEBOT3_MODEL waffle_pi)\"</code> you can replace <code>waffle_pi</code> with <code>burger</code> or <code>waffle</code> to change the model of turtlebot3 used in the simulation</li> <li>Inside <code>example.launch</code> you will see this line (it should be on line 8): <code>&lt;arg name=\"world_name\" value=\"$(find simulation_app)/worlds/example.world\"/&gt;</code> You can replace <code>example.world</code> with the name of another world file to change the world used for the simulation. Note that the world file must be present in the folder <code>simulation_app/worlds</code>. You can copy world files from the folder <code>catkin_ws/src/turtlebot3_simulations/turtlebot3_gazebo/worlds</code> (which is on your personal machine if you have installed ROS Kinetic)</li> </ul>"},{"location":"faq/aws/aws-robomaker/#building-and-bundling-your-applications","title":"Building and Bundling your applications","text":"<p>In order to use your applications in simulation, they must first be built and bundled to a format that RoboMaker likes to use. At the top of the IDE, click: RoboMaker Run \u2192 Add or Edit configurations, then click Colcon Build.</p> <p>Give your configuration a name. I suggest it be  robot or  simulation&gt;. For working directory, select the path to either <code>robot_ws</code> or <code>simulation_ws</code> (you will have to do this twice, once for each workspace).</p> <p>Do the same, but this time for Colcon Bundle.</p> <p>You now have shortcuts to build and bundle your robot and simulation applications.</p> <p>Next, in the configuration window that you have been using, select workflow to create a shortcut which will build and bundle both applications with one click. Give your workflow a name, then put your actions in order. It is important that the builds go before the bundles.</p> <p>Once you\u2019ve made your workflow, go to: RoboMaker Run \u2192 workflow \u2192 your workflow to build and bundle your applications. This will create a bundle folder inside both <code>robot_ws</code> and <code>simulation_ws</code>. Inside <code>bundle</code>, there is a file called <code>output.tar.gz</code>. You can rename it if you like, but remember where it is.</p> <p>Finally, we will go back to the configuration window to configure a simulation launcher.</p> <ul> <li>Give it a name</li> <li>Give it a duration (in seconds)</li> <li>Select \u201cfail\u201d for failure behavior</li> <li>Select an IAM role - it doesn\u2019t necessarily matter which one, but AWSServiceRoleForRoboMaker is recommended</li> <li>Skip to the robot application section</li> <li>Give it a name</li> <li>Bundle path is the path to <code>output.tar.gz</code> (or whatever you renamed it) inside <code>robot_ws/bundle</code></li> <li>S3 bucket should be the bucket you created at the beginning of this guide</li> <li>Launch package name is <code>robot_app</code></li> <li>Launch file is the launch file you wish to use</li> </ul> <p>NOTE: the name of the robot application and the launch file should related in some way The simulation application section is much of the same, except everything that was \u201crobot\u201d should be replaced with \u201csimulation.\u201d</p> <p>OPTIONAL: Once your simulation launch configuration has been saved, you can add it as the final action of the workflow you made earlier.</p>"},{"location":"faq/aws/aws-robomaker/#running-a-simulation","title":"Running a simulation","text":"<p>These are the steps that have been found to work when you want to run a simulation in RoboMaker. They are Kludgy, and perhaps a more elegant solution exists, but for now this is what has been found:</p> <ol> <li>Make sure your applications have been built and bundled. Then from the IDE, go to RoboMaker Run \u2192 Simulation Launch \u2192 your simulation config. This will upload your application bundles to the S3 bucket you specified, then try to start the simulation. IT WILL PROBABLY FAIL. This is okay, the main goal of this step was to upload the bundles.</li> <li>Go back to the RoboMaker Management Console, and in the left menu Select Simulations \u2192 Simulation Jobs, then click \u201cCreate simulation job\u201d</li> <li>Now we will configure the simulation job again:</li> <li>Set the failure behavior to fail</li> <li>For IAM role, select \u201ccreate new role\u201d, then give your role a name. Each simulation job will have its own IAM role, so make the name related to the simulation job.</li> <li>Click next</li> <li>For robot application, select the name you gave when you configured the simulation in the IDE. The launch package name and launch file will be the same too, but you must type those in manually.</li> <li>Click next, the next page is for configuring the simulation application, the same rules apply here as the robot application</li> <li>Click next, review the configuration then click create.</li> <li>RoboMaker will begin preparing to launch your simulation, in a few minutes it will be ready and start automatically. Once it is running, you will be able to monitor it through gazebo, rqt, rviz and the terminal.</li> <li>Be sure that once you are done with your simulation (if it is before the duration expires) to cancel the simulation job under the action menu in the upper right of the simulation job management screen.</li> </ol>"},{"location":"faq/aws/aws-world-gazebo-map/","title":"AWS Robotics Maps Integration","text":""},{"location":"faq/aws/aws-world-gazebo-map/#introduction","title":"Introduction","text":"<p>The AWS Robotics GitHub repository provides a variety of prebuilt Gazebo maps, including environments like hospitals and warehouses. These maps can be used for simulation purposes with ROS Noetic and TurtleBot3. This guide focuses on using these maps without AWS services, running simulations locally or on a lab VNC.</p> <p>I recommend you try to build a local ROS environment to learn ROS, especially now ROS2 is more and more popular. Also, I recommend you to learn how to build a gazebo map, that is really interesting!</p>"},{"location":"faq/aws/aws-world-gazebo-map/#getting-started","title":"Getting Started","text":"<p>Clone the AWS Robotics map repository and prepare your workspace to integrate these maps into your simulation environment.</p>"},{"location":"faq/aws/aws-world-gazebo-map/#cloning-the-repository","title":"Cloning the Repository","text":"<ol> <li>Open a terminal and navigate to your desired workspace directory.    <pre><code>cd ~/catkin_ws/src\n</code></pre></li> <li>Clone the AWS Robotics map repository:    <pre><code>git clone https://github.com/aws-robotics/aws-robomaker-sample-application-maps.git\n</code></pre></li> <li>Navigate to the <code>maps</code> directory to view the available maps:    <pre><code>cd aws-robomaker-sample-application-maps/maps\nls\n</code></pre></li> </ol> <p>You should see <code>.world</code> files like <code>hospital.world</code> and <code>warehouse.world</code>.</p>"},{"location":"faq/aws/aws-world-gazebo-map/#using-the-maps-in-gazebo","title":"Using the Maps in Gazebo","text":""},{"location":"faq/aws/aws-world-gazebo-map/#launching-maps-in-gazebo","title":"Launching Maps in Gazebo","text":"<ol> <li>Open Gazebo and load a map directly. For example, to load the <code>hospital.world</code> map:    <pre><code>gazebo ~/catkin_ws/src/aws-robomaker-sample-application-maps/maps/hospital.world\n</code></pre></li> <li>Verify that the map loads correctly in Gazebo. If adjustments are needed, edit the <code>.world</code> file with a text editor.</li> </ol>"},{"location":"faq/aws/aws-world-gazebo-map/#using-the-maps-with-turtlebot3","title":"Using the Maps with TurtleBot3","text":""},{"location":"faq/aws/aws-world-gazebo-map/#modifying-launch-files","title":"Modifying Launch Files","text":"<p>To integrate AWS Robotics maps with TurtleBot3, modify the <code>turtlebot3_world.launch</code> file to use the desired map:</p> <ol> <li>Locate the <code>turtlebot3_world.launch</code> file:    <pre><code>cd ~/catkin_ws/src/turtlebot3/turtlebot3_simulations/turtlebot3_gazebo/launch\n</code></pre></li> <li>Open the file in a text editor:    <pre><code>nano turtlebot3_world.launch\n</code></pre></li> <li>Replace the default world file with the AWS Robotics map path. For example:    <pre><code>&lt;arg name=\"world_file\" default=\"$(find aws-robomaker-sample-application-maps)/maps/hospital.world\"/&gt;\n</code></pre></li> <li>Save and close the file.</li> </ol>"},{"location":"faq/aws/aws-world-gazebo-map/#launching-the-simulation","title":"Launching the Simulation","text":"<ol> <li>Start the simulation with the modified launch file:    <pre><code>roslaunch turtlebot3_gazebo turtlebot3_world.launch\n</code></pre></li> <li>Use the keyboard teleoperation node to control the TurtleBot3:    <pre><code>roslaunch turtlebot3_teleop turtlebot3_teleop_key.launch\n</code></pre></li> </ol>"},{"location":"faq/aws/aws-world-gazebo-map/#slam-and-navigation-with-aws-maps","title":"SLAM and Navigation with AWS Maps","text":""},{"location":"faq/aws/aws-world-gazebo-map/#running-slam","title":"Running SLAM","text":"<ol> <li>Launch the SLAM node:    <pre><code>roslaunch turtlebot3_slam turtlebot3_slam.launch\n</code></pre></li> <li>Visualize the map building process in RViz:    <pre><code>rosrun rviz rviz\n</code></pre></li> <li>Control the TurtleBot3 to explore the map.</li> </ol>"},{"location":"faq/aws/aws-world-gazebo-map/#navigation","title":"Navigation","text":"<p>To use AWS Robotics maps for navigation:</p> <ol> <li>Convert the <code>.world</code> file to <code>.pgm</code> and <code>.yaml</code> formats. This can be done using Gazebo and the map_server package.</li> <li>Launch the navigation stack with the converted map:    <pre><code>roslaunch turtlebot3_navigation turtlebot3_navigation.launch map_file:=/path/to/your_map.yaml\n</code></pre></li> <li>Set navigation goals in RViz to test path planning and obstacle avoidance.</li> </ol>"},{"location":"faq/aws/aws-world-gazebo-map/#notes-and-tips","title":"Notes and Tips","text":"<ul> <li>Map Compatibility: Ensure that the AWS Robotics maps are compatible with your Gazebo version. If models are missing, download them from the Gazebo Model Database.</li> <li>Performance Optimization: For complex maps, lower Gazebo\u2019s simulation frequency to improve performance.</li> <li>Path Planning: Use the <code>move_base</code> node or custom algorithms to test navigation in realistic environments provided by AWS Robotics maps.</li> </ul> <p>By following these steps, you can effectively use AWS Robotics maps with ROS Noetic and TurtleBot3 to simulate and test robotic applications.</p>"},{"location":"faq/aws/ros_and_aws_integration/","title":"ROS and Amazon Web Service Integration (boto3)","text":""},{"location":"faq/aws/ros_and_aws_integration/#by-nazari-tuyo","title":"By Nazari Tuyo","text":""},{"location":"faq/aws/ros_and_aws_integration/#step-1-install-the-aws-python-cli","title":"Step 1: Install the AWS python CLI","text":"<p>Follow the steps at the link below for linux:</p> <p>https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html</p>"},{"location":"faq/aws/ros_and_aws_integration/#step-2-run-the-following-commands-on-your-vnc-to-install-boto3","title":"Step 2: Run the following commands on your VNC to install boto3","text":"<ol> <li><code>pip install boto3</code></li> <li><code>pip freeze</code></li> <li>check that <code>boto3</code> is actually there!</li> <li><code>python -m pip install --user boto3</code></li> </ol>"},{"location":"faq/aws/ros_and_aws_integration/#step-3-integrating-boto3-clients-into-your-code","title":"Step 3: Integrating boto3 clients into your code","text":"<ol> <li>Create a file in your package to store your AWS credentials</li> <li>Sign into AWS, and create an access key</li> <li>Create an Access Policy for your services</li> <li> <p>Create a class that will hold your credentials, for example:</p> <pre><code>class Credentials:\n    AWS_ACCESS_KEY_ID = ''\n    SECRET_ACCESS_KEY = ''\n    OWNER_ID = ''\n</code></pre> <p>and any other information you may need to store for the service you choose to use.</p> </li> <li> <p>Based on the service you\u2019d like to use, create the appropriate client. If you\u2019re using dynamoDB, it may look like this:</p> <pre><code>dynamodb = boto3.resource('dynamodb')\n</code></pre> <p>Please refer to the documentation for of the service you\u2019d like to use and the request format</p> <p>https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/index.html</p> </li> <li> <p>There are several helpful methods included with each service, here are some of dynamoDB\u2019s methods for example:</p> <pre><code>delete_item()\ndelete_table()\nput_item()\nquery()\nupdate_item()\nupdate_table()\n</code></pre> <p>Each method  will have a unique request, so be sure to check before implementing it into your code.</p> </li> <li> <p>This can be used within your node, but keep in mind how many requests you are making if you place your request in a loop.</p> </li> </ol>"},{"location":"faq/batteries/batteries-and-chargers/","title":"Batteries and Chargers","text":""},{"location":"faq/batteries/batteries-and-chargers/#danger","title":"Danger","text":"<p>LiPo batteries are dangerous. When mistreated they can cause a major fire. This is a serious concern and we ask you to be careful and take this seriously.</p>"},{"location":"faq/batteries/batteries-and-chargers/#fire-extinguisher","title":"Fire Extinguisher","text":"<p>There is a fire extinguisher in the lab. It is near the main door, on the left as you enter the lab.</p>"},{"location":"faq/batteries/batteries-and-chargers/#danger-signs","title":"Danger signs","text":"<ul> <li>When charging, LiPo batteries will become warm. As long as you can put your palm on it and keep it there for 10 seconds you are more or less ok.</li> <li>If a LiPo battery becomes hot to the touch, immediately unplug it and put it into one of the safety bags.</li> <li>If a LiPo battery starts emitting smoke or sparks, this is a fire emergency. Luckily we've not experienced this ever.</li> <li>If a LiPo battery deforms in any way, that is bulges out on one of the sides, that battery is defective and should not be used any more.</li> </ul>"},{"location":"faq/batteries/batteries-and-chargers/#chargers","title":"Chargers","text":"<p>We have many different types of chargers. </p>"},{"location":"faq/batteries/batteries-and-chargers/#technical-information","title":"Technical Information","text":"<ul> <li>Battery Charging Instructions for Brandeis Robotics Lab</li> <li>Battery Technical Specs</li> </ul>"},{"location":"faq/camera/astra_pro_depth_camera_setup/","title":"Astra Pro Depth Camera Setup","text":"<p>by Veronika Belkina</p> <p>This is a guide to installing the Astra Pro Depth Camera onto a robot and the various problems and workarounds that were experienced along the way. </p>"},{"location":"faq/camera/astra_pro_depth_camera_setup/#setup","title":"Setup","text":"<p>To start, try to follow the instructions given on the Astra github.</p> <p>If this goes well, then you're pretty much all set and should skip down to the usb_cam section. If this doesn't go well, then keep reading to see if any of the errors that you received can be resolved here. </p>"},{"location":"faq/camera/astra_pro_depth_camera_setup/#possible-errors-and-ways-to-solve-them","title":"Possible errors and ways to solve them","text":"<ul> <li>make sure to run <code>sudo apt update</code> on the robot </li> <li>if you are getting an error that mentions a lock when you are installing dependencies, try to reboot the robot: <code>sudo reboot now</code></li> </ul> <p>If you have tried to install the dependencies using this:  <pre><code>sudo apt install ros-$ROS_DISTRO-rgbd-launch ros-$ROS_DISTRO-libuvc ros-$ROS_DISTRO-libuvc-camera ros-$ROS_DISTRO-libuvc-ros\n</code></pre> and this is not working, then here is an alternative to try:  <pre><code>sudo apt install libuvc\nsudo apt install ros-*-rgbd-launch\n</code></pre> If this is successful, then within ~/catkin_ws/src, <code>git clone https://github.com/orbbec/ros_astra_camera</code> and run the following commands: </p> <pre><code>roscd astra_camera\nchmod 777 /scripts/create_udev_rules\n./scripts/create_udev_rules\n</code></pre> <p>After this, run <code>catkin_make</code> on the robot. This might freeze on you. Restart the robot and run <code>catkin_make -j1</code> to run on a single core. This will be slower, but will finish. </p> <p>If you are getting a publishing checksum error, try to update the firmware on the robot using commands from TB3_image or run these commands, line by line: <pre><code>export OPENCR_PORT=/dev/ttyACM0\nexport OPENCR_MODEL=burger_noetic# or waffle_noetic if you have a waffle tb3\nrm -rf ./opencr_update.tar.bz2\nwgethttps://github.com/ROBOTIS-GIT/OpenCR-Binaries/raw/master/turtlebot3/ROS1/latest/opencr_update.tar.bz2\ntar -xvf opencr_update.tar.bz2\ncd ./opencr_update\n./update.sh $OPENCR_PORT $OPENCR_MODEL.opencr\n</code></pre></p> <p>At this point, hopefully, all the errors have been resolved and you are all set with the main astra_camera package installation. </p> <p>There is one more step that needs to be done. </p>"},{"location":"faq/camera/astra_pro_depth_camera_setup/#usb_cam","title":"usb_cam","text":"<p>The Astra Pro camera doesn't have an RGB camera that's integrated with OpenNI2. Instead, it has a regular Video4Linux webcam. This means that from ROS's point of view, there are two completely separate devices.To resolve this, you can install another package onto the robot called usb_cam following these instructions: <pre><code>cd ~/catkin_ws/src\ngit clone https://github.com/bosch-ros-pkg/usb_cam.git\ncd ..\ncatkin_make\nsource ~/catkin-ws/devel/setup.bash\n</code></pre></p> <p>Test it by running <code>rosrun usb_cam usb_cam_node</code> </p> <p>Afterwards, when you need to run the depth camera and need the rgb stream as well, you will need to run the following instructions onboard the robot:  <pre><code>bringup \nroslaunch astra_camera astra.launch\nrosrun usb_cam usb_cam_node\n</code></pre></p> <p>If this is something that you will be needing often, then it might be worth it to add the usb_cam node into the astra launch file as you do need to ssh onto the robot for each instruction. The usb_cam_node publishes to the topic <code>/usb_cam/image_raw</code>. You can check <code>rostopic list</code> to see which one suits your needs. </p> <p>If you want to just explore the depth camera part of the camera, then just run the astra.launch file. </p> <p>Then there will be a view topics that the camera publishes: </p> <pre><code>/camera/depth/camera_info\n/camera/depth/image_raw\n/camera/depth/points\n/camera/depth_registered/points\n/camera/extrinsic/depth_to_color\n/camera/ir/camera_info\n/camera/ir/image_raw\n/camera/reset_device\n</code></pre> <p>If you open rviz, click add, go to the by topic tab and open the PointCloud2 topic under /camera/depth/points: </p> <p></p> <p>If you\u2019re working with just the camera, you might need to fix the frame and can just pick any random one for now other than map. A point cloud of what\u2019s in front of you should appear: </p> <p></p> <p>If you're looking through rqt, then you might see something like this: </p> <p></p> <p>From there, you could use colour detection, object detection, or whatever other detector you want to get the pixels of your object and connect them with the pixels in the point cloud. It should output a distance from the camera to the object. However, I can\u2019t speak for how reliable it is. It can\u2019t see objects right in front of it - for example when I tried to wave my hand in front of it, it wouldn\u2019t detect it until it was around 40 or so cm away.</p>"},{"location":"faq/camera/camera-performance-notes/","title":"Notes on Camera Performance","text":"<p>Author: Pito Salas, Date: April 2023</p> <p>I posted a question to the world about improving camera performance. For posterity I am saving the rought answers here. Over time I will edit these intomore specific, tested instructions.</p>"},{"location":"faq/camera/camera-performance-notes/#original-question","title":"Original Question","text":"<p>My robot has:</p> <ul> <li>ROS1 Notetic </li> <li>Rasberry Pi 4 Raspberry</li> <li>Pi Camera V2 Running raspicam node (https://github.com/UbiquityRobotics/r...)</li> <li>My \"remote\" computer is running</li> <li>Ubuntu 20.04 on a cluster which is located in my lab</li> <li>They are communicating via Wifi.</li> <li>roscore is running on the robot</li> <li>The raspicam node is publishing images to it's variety of topics.</li> </ul> <p>I have two nodes on my remote computer, each is processing the images in a different way. One of them is looking for fiducially and one of them is doing some simple image processing with opencv.</p> <p>Performance is not good. Specifically it seems like the robot itself is not moving smoothly, and there is too much of a delay before the image gets to the remote computer. I have not measured this so this is just a subjective impression.</p> <p>I hypothesize that the problem is that the image data is big and causing one of a number of problems (or all).</p> <p>a. Transmitting it iis too much for the Pi  b. The wifi is not fast enough  c. Because there are several nodes on the remote computer that are subscribing to the images, the images are being sent redunantly to the remote computer</p> <p>I have some ideas on how to try to fix this: </p> <p>a.  Reduce the image size at the raspicam node  b.  Do some of the imate processing onboard the Pi  c. Change the image to black and white d. Turn off any displays of the image on the remote computer (i.e. rviz and debugging windows)</p>"},{"location":"faq/camera/camera-performance-notes/#answer","title":"Answer","text":"<p>If you have more nodes receiving from 1 raspberry ROS node, image is sent many times over wifi. One clear improvement would be having one node that communicates to raspberry over wifi and publishes to subscribers over ethernet and offloads raspberry. Maybe use something else than ROS node to async copy image towards listeners so you dont need to first wait for whole picture before you can send data forward to reduce latency.</p> <p>Not sure what is the exact problem, but there are different speeds that different wifi versions support. Base raspberry has 1 pcb antenna that is not so great. Compute node can support better antenna, some embedded support m.2 slot where you can plug better stuff. Mimo wlan with 2 antennas and 40mhz channel can provide much more bandwidth. But also AP need to support similar modes. Faster transmission is also reduced latency.</p> <p>If in urban area, wifi channels can have more traffic. There are tools like iptraf and more to show you the traffic. And analyzers for wifi, too.</p> <p>And raspberry might not be the most powerful platform, see that you minimize image format changes on raspberry side or use hw acceleration for video to do that (if available).</p>"},{"location":"faq/camera/camera-performance-notes/#answer_1","title":"Answer","text":"<p>I'd suggest experimenting with drone FPV camera, 5.8 GHz USB receiver connected to desktop, which would process video. No need to use Wifi for video stream. I posted here before on my use of it, and the optimal setup.</p>"},{"location":"faq/camera/camera-performance-notes/#answer_2","title":"Answer","text":"<p>I have a similar setup on a recent robot, except that I'm using a USB camera instead of the Raspberry Pi camera. I was initially using OpenCV to capture the images and then send over WiFi. The first limitation I found was that Image messages are too large for a decent frame rate over WiFi, so I initially had the code on the robot compress to JPEG and send ImageCompressed. That improved things but there was still a considerable delay in receiving the images (&gt; 1.5 sec delay to the node on the remote computer doing object detection). I was only able to get about 2 frames/sec transferred using this method, too, and suspected that the JPEG compression in OpenCV was taking too much time.</p> <p>So I changed the capture to use V4L2 instead of OpenCV, in order to get JPEG images directly from an MJPEG stream from the camera. With this change in place I can get over 10 frames/sec (as long as lighting is good enough - the camera drops the frame rate if the lighting is low), and the delay is only a fraction of a second, good enough for my further processing. This is with a 1280x720 image from a fisheye camera. If I drop the resolution to 640x360 I believe I can get 20 frames/sec, but that's likely overkill for my application, and I'd rather keep the full resolution.</p> <p>(Another difference from your setup that probably doesn't matter: My robot is running on a Beaglebone Blue rather than Raspberry Pi, and does not run ROS. Instead, I use ZeroMQ to serve the images to a ROS2 node on my remote computer.)</p> <p>What Sampsa said about ROS messages to multiple subscribers is also salient: unless the underlying DDS implementation is configured to use UDP multicasting you'll end up with multiple copies of the image being transferred. With the setup above, using a bridging node on the remote computer as the only recipient, only one copy of the image is transferred over WiFi.</p>"},{"location":"faq/camera/camera-performance-notes/#answer_3","title":"Answer","text":"<p>Sergei's idea to use analog video removes the delay completely but them you need an analog to digital conversion at the server.   The video quality is as good as you are willing to pay for.     Analog video seems to many people a radical solution as they are used to computers and networks.</p> <p>An even more radical solution is to move the camera off the robots and place them on the walls.  Navigation is easier but of course the robots is confined to where you have cameras.  Then with fixed cameras you can use wires to transmit the signals and have zero delay.  You can us standed security cameras that use \"POE\".</p> <p>But if the camera must be digital and must be on the robot them it is best to process as much as you cam on the robot, compress it and send it to one node on the server, that node then redistributes the data.</p> <p>With ROS2 the best configuration is many CPU cores and large memory rather then a networked cluster.  Because ROS2 can do \"zero copy\" message passing where the data states on the same RAM location and pointers are passed to nodes.  The data never moves and it is very fast.</p>"},{"location":"faq/camera/camera-performance-notes/#answer_4","title":"Answer","text":"<p>Quick comment: as with everything wireless: make sure to first base benchmark wireless throughput/performance (using something like iperf). Compare result to desired transfer rate (ie: image_raw bw). If achieved_bw &lt; desired_bw (or very close to), things will not work (smoothly). Note that desired_bw could be multiple times the bw of a single image_raw subscription, as you write you have multiple subscribers.</p> <p>In all cases though: transmitting image_raw (or their rectified variants) topics over a limited bw link is not going to work. I'd suggest looking into image_transport and configure a compressed transport. There are lossless plugins available, which would be important if you're looking to use the images for image processing tasks). One example would be swri-robotics/imagezero_transport. It takes some CPU, but reduces bw significantly.</p> <p>See also #q413068 for a recent discussion about a similar topic.</p>"},{"location":"faq/camera/camera_calibration/","title":"Question","text":"<p>I want to run a computer vision algorithm on my robot, but I'm told that I need to calibrate my camera(s) first. What is camera calibration, and how can I do it?</p>"},{"location":"faq/camera/camera_calibration/#what-is-camera-calibration","title":"What is Camera Calibration?","text":"<p>Camera calibration is the act of determining the intrinsic parameters of your camera. Roughly speaking, the intrinsic parameters of your camera are constants that, in a mathematical model of your camera, describe how your camera (via its interior mechanisms) converts a 3D point in the world coordinate frame to a 2D point on its image plane.</p> <p>Intrinsic parameters are distinct from extrinsic parameters, which describe where your camera is in the world frame.</p> <p>So, since calibration deals with the intrinsic parameters of your camera, it practically doesn't matter where you place your camera during calibration.</p> <p>To hear more about the basics of camera calibration, watch the following 5-minute videos by Cyrill Stachniss in order:</p> <ol> <li>Camera Intrinsics and Extrinsics</li> <li>Mapping the 3D World to an Image</li> <li>Intrinsic Camera Calibration</li> </ol> <p>This video, also by Cyrill Stachniss, is a deep dive into Zhang's method, which is what the <code>camera_calibration</code> package we discuss below uses under the hood.</p>"},{"location":"faq/camera/camera_calibration/#how-can-i-calibrate-my-camera","title":"How Can I Calibrate my Camera?","text":"<p>This note describes two ways you can calibrate your camera. The first is by using the <code>camera_calibration</code> ROS package. This is the easier approach, since it basically does almost all of the work for you. The second is by using OpenCV's library directly, and writing your own calibration code (or using one in circulation).</p>"},{"location":"faq/camera/camera_calibration/#the-camera_calibration-package","title":"The <code>camera_calibration</code> Package","text":"<p>This guide assumes you've already got your camera working on ROS, and that you're able to publish <code>camera_info</code> and <code>image_raw</code> topics for the camera. If you need to set up a new usb camera, see this entry in our lab notebook.</p> <p>First, let's install the package:</p> <pre><code>sudo apt-get update\nsudo apt-get install ros-noetic-camera-calibration\n</code></pre> <p>Second, print out this checkerboard on a letter-sized piece of paper.</p> <p>Third, tape the corners of the paper to a firm, flat surface, like the surface of a piece of cardboard.</p> <p>Fourth, measure a side of a single square, convert your measurement to millimeters, and divide the result by <code>1000</code>. Let's call your result, <code>RESULT</code>.</p> <p>Now, let the number of rows of your checkerboard be <code>M</code> and its number of columns <code>N</code>. Finally, let's say your camera node's name is <code>CAM</code>, such that, when you connect it with ROS, it publishes the <code>/CAM/camera_info</code> and <code>/CAM/image_raw</code> topics. Now, after ensuring that these two topics are being published, execute:</p> <pre><code>rosrun camera_calibration cameracalibrator.py --size MxN --square\nRESULT image:=/CAM/image_raw camera:=CAM\n</code></pre> <p>Next, follow the instructions under section 4 \"Moving the Checkboard\", and section 5 Calibration Results of the official camera_calibration tutorial.</p> <p>WARNING The two sections stated above are the only ones you actually want to follow in the official tutorial. Much of the rest of the material there is outdated or misleading.</p> <p>Here's a video of what a successful calibration process might look like.</p>"},{"location":"faq/camera/camera_calibration/#opencv","title":"OpenCV","text":"<p>Sometimes, you might want to use object detection or use certain algorithms that require a camera such as VSLAM. These algorithms usually require a very good calibration of the camera to work properly. The calibration fixes things like distortion by determining the camera\u2019s true parameters such as focal length, format size, principal point, and lens distortion. If you see lines that are curved but are supposed to be straight, then you should probably calibrate your camera. </p> <p>Usually this is done with some kind of checkerboard pattern. This can be a normal checkerboard or a Charuco/Aruco board which has some patterns that look like fiducials or QR codes on it to further help with calibration. In this tutorial, we\u2019ll be using a 7x9 checkerboard with 20x20mm squares: checkerboard pdf. </p> <p>The most ideal way to do is to print the checkerboard on a large matte and sturdy piece of paper so that the checkerboard is completely flat and no reflections can be seen on it. However, it\u2019s okay to just print it on a normal piece of paper as well and put it on a flat surface. Then, take at least ten photos with your camera from a variety of angles and positions so that the checkerboard is in all corners of the photos. Make sure the whole checkerboard is seen in each picture. Save those photos in an easy to find place and use the following to get your intrinsic calibration matrix. </p> <p>The code I used was this opencv calibration. It also has more notes and information about what the information you are getting is.</p> <p>Step by step: </p> <ul> <li>print out checkerboard pattern</li> <li>take at least 10 photos of the checkerboard at a variety of angles   and positions (see image 1 for examples) and save in an easy to access place</li> <li>download/copy the opencv calibration code and run it after changing   the folder path</li> <li>get the intrinsic matrix and distortion and enter it into whatever you need</li> </ul> <p></p> <p>Image 1: some examples of having the checkerboard cover all corners of the image</p> <p>Potential intrinsic matrix:</p> <p>[[688.00030953   0.         307.66412893]</p> <p>[  0.         689.47629485 274.053018  ]</p> <p>[  0.           0.           1.        ]]</p> <p>Pastable into python: </p> <p>fx, fy, cx, cy = (688.00030953, 689.47629485, 307.66412893, 274.053018)</p> <p>Distortion coefficients: </p> <p>[9.39260444e-03, 4.90535732e-01, 1.48962564e-02, 4.68503188e-04, -1.77954077e+00]</p>"},{"location":"faq/camera/camera_pitch/","title":"Change Camera Pitch","text":""},{"location":"faq/camera/camera_pitch/#jacqueline-zhou","title":"Jacqueline Zhou","text":""},{"location":"faq/camera/camera_pitch/#summary","title":"Summary","text":"<p>This is a quick tip on how to change turtlebot3_waffle's camera pitch, so that the camera (in simulation) can look down or up</p>"},{"location":"faq/camera/camera_pitch/#make-change-in-urdf","title":"Make change in urdf","text":"<p>Find this section of \"Vision Camera\" in turtlebot's urdf: <pre><code>&lt;joint name=\"camera_joint\" type=\"fixed\"&gt;\n      &lt;origin xyz=\"0 0 ${1+base_height}\" rpy=\"0 ${-cam_pitch} 0\" /&gt;\n      &lt;parent link=\"base_link\"/&gt;\n      &lt;child link=\"camera_link\" /&gt;\n    &lt;/joint&gt;\n</code></pre> And you can modify ${-cam_pitch} to any angle you want.</p>"},{"location":"faq/camera/fixing-camera-problems/","title":"Camera fails to initialize:","text":"<p>Ensure that the camera is enabled and there is enough GPU memory, Note that there isa config.txt in /boot/firmware when ubuntu is running but editing that seems to not do anything. Instead shut down the raspberry pi and pull the MicroSD card and plug it into your laptop so you can view it without ubuntu running.</p> <ol> <li> <p>Locate the file config.txt in the boot microsd when viewing it on your laptop.</p> </li> <li> <p>Add the following lines to the <code>config.txt</code> file if they are not already there.</p> </li> </ol> <pre><code>start_x=1\ngpu_mem=128\n</code></pre> <p>And then reboot. Note that the config.txt file will say that you should not modify it and instead make the changes in a file called userconfig.txt. However I found that the userconfig.txt file is not invoked. So I added the lines above directly to config.txt.</p> <ol> <li>Check the amount of gpu_memory</li> </ol> <p><pre><code>vcgencmd get_mem gpu\n</code></pre> It should show 256 or whatever number you put there.</p> <ol> <li>Check that the camera is supported and detected:</li> </ol> <pre><code>vcgencmd get_camera\n</code></pre> <ol> <li>Snap a picture to a jpeg <pre><code>raspistill -o output.jpg\n</code></pre></li> </ol>"},{"location":"faq/camera/follow_partial_line/","title":"Follow Partial Line","text":"<p>Author: Eric Hurchey Date: Dec 2024  </p>"},{"location":"faq/camera/follow_partial_line/#summary","title":"Summary","text":"<p>This project involved building a Line Follower robot that could follow part of a line, stop at a specific destination, and transition to the next task.  </p>"},{"location":"faq/camera/follow_partial_line/#details","title":"Details","text":""},{"location":"faq/camera/follow_partial_line/#image_cb-line-detection","title":"<code>image_cb</code> - Line Detection","text":"<p>The <code>image_cb</code> function processes camera input to detect the line. It: 1. Converts the image to the HSV color space to isolate a specific color range (orange). 2. Masks the lower part of the image to focus on the area where the line is most likely visible. 3. Identifies the largest contour of the line and calculates the error between the line's center and the image's center. 4. Passes the error to a PID controller for movement adjustments.  </p>"},{"location":"faq/camera/follow_partial_line/#example-of-error-calculation","title":"Example of error calculation","text":"<p>cx = int(M['m10'] / M['m00'])  # Center x-coordinate of the line err = cx - w // 2  # Error relative to the image center</p>"},{"location":"faq/camera/follow_partial_line/#movement-control","title":"Movement Control","text":"<p>The movement function uses the error value to adjust the robot's linear and angular velocities. A PID controller ensures the robot stays aligned with the line:  </p>"},{"location":"faq/camera/follow_partial_line/#pid-controller-example","title":"PID controller example","text":"<p>self.control_signal = (KP * err) + (KD * derivative) self.twist.linear.x = MAX_SPEED self.twist.angular.z = -self.control_signal</p>"},{"location":"faq/camera/follow_partial_line/#behavior","title":"Behavior","text":"<ul> <li>Line Following: The robot follows the detected line, continuously adjusting its path.  </li> <li>Stopping at Destination: Once the robot reaches a predefined destination, it stops following the line.  </li> <li>Task Transition: After stopping, the robot transitions to the next task, such as approaching a target or turning around.  </li> </ul>"},{"location":"faq/camera/follow_partial_line/#key-features","title":"Key Features","text":"<ul> <li>Uses OpenCV for image processing and line detection.  </li> <li>PID control for smooth and responsive movement.  </li> <li>State-based logic for task transitions.  </li> </ul>"},{"location":"faq/camera/follow_partial_line/#improvements","title":"Improvements","text":"<ul> <li>Fine-tuning the mask area to handle different lighting and line conditions.  </li> <li>Enhancing PID parameters to reduce overshooting or oscillations.  </li> <li>Adding robustness for handling line gaps or sharp turns.  </li> </ul>"},{"location":"faq/camera/hsv_values_finder/","title":"Finding the HSV Values for any Color","text":"<p>by Sampada Pokharel</p> <p>This is a quick guide to finding the HSV values for any color that you may need for you project. We found this particularly helpful for following a line.</p>"},{"location":"faq/camera/hsv_values_finder/#prerequisite","title":"Prerequisite","text":"<ul> <li>A robot with a camera</li> <li>VNC</li> </ul>"},{"location":"faq/camera/hsv_values_finder/#basic-steps","title":"Basic Steps","text":"<ol> <li>Connect your robot. You can find the guide  here </li> <li>Once your robot is connected, open vnc and run cvexample.py file in the terminal.</li> </ol> <pre><code>rosrun prrexamples cvexample.py\n</code></pre> <ol> <li>In a seperate terminal, run rqt.</li> </ol> <pre><code>rqt\n</code></pre> <ol> <li> <p>In rqt window, click Plugning -&gt; Dynamic Reconfigure</p> </li> <li> <p>Click cvexample and the hsv slides should pop up.</p> </li> <li> <p>Adjust the sliders to find the hsv values for your desired colors.</p> </li> </ol> <p> In a seperate terminal, run rqt_image_view to get a larger frame of the camera image. This is optional.</p> <pre><code>rqt_image_view\n</code></pre> <p></p>"},{"location":"faq/camera/ribbon_cable_fixing/","title":"Fixing Camera Failure","text":"<p>It is a common issue for the camera to fail to initialize when bringing up a robot. While there can be a number of reasons for this, in my experience 9 out of 10 times this error is due to the camera's ribbon cable being incorectly attached. The goal of this lab notebook entry is to give detailed instructions how how to troubleshoot this problem and re-attach the ribbon cable correctly.</p>"},{"location":"faq/camera/ribbon_cable_fixing/#troubleshooting-and-fixing-steps","title":"Troubleshooting and fixing steps:","text":""},{"location":"faq/camera/ribbon_cable_fixing/#step-1","title":"Step 1:","text":"<p>Upon receiving a \"failed to initialze\" error for the camera on bringup, locate both ends of the ribbon cable (the end attached to the camera and the end attached to the Raspberry Pi).</p>"},{"location":"faq/camera/ribbon_cable_fixing/#step-2","title":"Step 2:","text":"<p>Check that the ribbon cable is correctly attached to both ends. (Both ends of the cable should be attached in the same way). 1. The outlet where the ribbon cable is plugged in is incut slightly, the side of the ribbon cable with a blue tip should be faced away from the side that is incut (it should be facing the wall of the outlet). 2. Ensure that the white tab on top of the outlet is pushed all the way down and that the ribbon cable is not loose. It is often the case, especially with the Turtlebots, that the ribbon cable looks attached, but that one side of the outlet is not locked in place. See the image below for an example, notice how the white tab on the right side is not pushed in fully:     </p>"},{"location":"faq/camera/ribbon_cable_fixing/#step-3","title":"Step 3:","text":"<p>If the cable is not correctly inserted pop out the white tab on the desired outlet by using your fingers to lift up the small knobs on both sides of that outlet. (Note it will only lift up a small amount).</p>"},{"location":"faq/camera/ribbon_cable_fixing/#step-4","title":"Step 4:","text":"<p>Carefully insert the ribbon cable (with the correct orientation as described in part 1 of step 2), making sure that the cable is pushed all the way down into the outlet. It won't go very deep or click, so don't use too much force.</p>"},{"location":"faq/camera/ribbon_cable_fixing/#step-5","title":"Step 5:","text":"<p>Once the ribbon cable has been inserted correctly into the outlet, snap both sides of the white tab back into place. This step can be a bit finicky as one side of the ribbon cable can become loose while snapping the other side into place, so ensure the cable doesn't become disloged during this step and that both tabs are locked in place. To verify that the cable is secure you can give it a slight tug to ensure that the cable is locked into the outlet.</p>"},{"location":"faq/camera/ribbon_cable_fixing/#step-6","title":"Step 6:","text":"<p>Check your work to make sure the installed ribbon cable looks like the two images below, and try to bring the robot up, as well as verify that the image topics are publishing.  </p>"},{"location":"faq/camera/ribbon_cable_fixing/#step-7","title":"Step 7:","text":"<p>If you are still having trouble, it may be a software issue and I would recommend trying the steps found in this lab notebook entry</p>"},{"location":"faq/camera/set_camera_parameters/","title":"Setting the Camera Parameters","text":"<p>by Zixin Jiang</p> <p>This is a quick guide to set the camera parameters such as contrast or brightness. This is particularly helpful for improve callback image quality.</p>"},{"location":"faq/camera/set_camera_parameters/#prerequisite","title":"Prerequisite","text":"<ul> <li>A robot with a camera</li> <li>VNC</li> </ul>"},{"location":"faq/camera/set_camera_parameters/#basic-steps","title":"Basic Steps","text":""},{"location":"faq/camera/set_camera_parameters/#get-the-appropriate-value","title":"Get the appropriate value","text":"<ol> <li>Connect and launch your robot. </li> <li>Once your robot is ready, open vnc and run <code>rqt</code>. You can adjust the parameters with sliders in this window.</li> <li>In a seperate terminal, run <code>rqt_image_view</code>. This will shows the camera call back image with any updated parameters. </li> <li>However, any modification on parameters using this method is not going to save to robot. Thus, all the changes will be set back to default when you launch the robot again.</li> <li>Record all the changes you make, including the parameter name and value, such as \"contrast\" and \"73\".</li> </ol>"},{"location":"faq/camera/set_camera_parameters/#store-the-change-to-launch-file","title":"Store the change to launch file.","text":"<ol> <li>Connect to your desired robot, but don't bring it up. Locate to the launch file directory in the onboad terminal with following command: <pre><code>roscd turtlebot3_bringup/launch\n</code></pre></li> <li>Copy the .launch file we usually use for bringup the robot with comment 'cp'.</li> <li>Open the new .launch file (let's call it new.launch), and modify the following lines: Add <code>&lt;arg name=\u201dcontrast\u201d value=\u201d73\u201d/&gt;</code> between <code>&lt;include&gt;</code> block. Change the contrast and 73 to the parameter and value you want. <pre><code>&lt;include file=\"$(find raspicam_node)/launch/camerav2_410x308_30fps.launch\"&gt;\n    &lt;arg name=\u201dcontrast\u201d value=\u201d73\u201d/&gt; \n&lt;/include&gt;\n</code></pre></li> <li>Save the .launch file.</li> <li>Nevigate to camera launch folder with the follow command <pre><code>roscd raspicam_node/launch/\n</code></pre></li> <li>Since from the previous launch file we find that the camera is launched with file \"camerav2_410x308_30fps.launch\", we open this file and add the following change: <pre><code>&lt;arg name=\"contrast\" default=\"50\"/&gt;\n</code></pre></li> <li>And under the <code>&lt;node type=\"raspicam_node\"&gt;</code>, we add the following line: <pre><code>&lt;param name=\"contrast\" value=\"$(arg contrast)\"/&gt;\n</code></pre></li> <li>Finally, launch the robot with the new launch file instead of the default one. <pre><code>roslauch turtlebot3_bringup new.launch\n</code></pre></li> </ol>"},{"location":"faq/camera/usb-cam-setup/","title":"USB Camera Setup","text":""},{"location":"faq/camera/usb-cam-setup/#author-ken-kirio","title":"Author: Ken Kirio","text":"<p>This guide will show how to set up an external camera by connecting it to a computer running ROS. This guide assumes your computer is running ROS on Ubuntu natively, not via the VNC, in order to access the computer's USB port. (The lab has computers with Ubuntu preinstalled if you need one.)</p> <ol> <li> <p>Installation</p> <ul> <li>Install the ros package usb_cam: <code>sudo apt install ros-noetic-usb-cam</code></li> <li>Install guvcview for the setup: <code>sudo apt install guvcview</code></li> </ul> </li> <li> <p>Edit the launch file, <code>usb_cam-test.launch</code></p> <ul> <li>Find the location of the file inside the usb_cam package: <code>roscd usb_cam</code></li> <li>Set <code>video_device</code> parameter to the port of your camera <ul> <li>Check which ports are in use: <code>ls /dev</code> and look for video0, video1, etc.</li> <li>Check the output of each port: <code>guvcview /dev/&lt;port&gt;</code></li> <li>If you unplug the camera between uses or restart your computer, the camera may be on a different port. Check every time!</li> </ul> </li> </ul> </li> <li> <p>Run the node! <code>roslaunch usb_cam usb_cam-test.launch</code></p> </li> </ol>"},{"location":"faq/client/localStorage/","title":"Working with localStorage in React for web clients","text":"<p>Author: James Kong Emails: jameskong@brandeis.edu, jameskong098@gmail.com GitHub: jameskong098 LinkedIn: James Deming Kong</p>"},{"location":"faq/client/localStorage/#what-is-localstorage","title":"What is localStorage?","text":"<p>localStorage is a web storage object that enables developers to store key-value pairs in a web browser. It ensures that this data survives all page refreshes, even when a user closes or restarts a browser. It does not have an expiration date.</p>"},{"location":"faq/client/localStorage/#why-is-it-useful","title":"Why is it useful?","text":"<p>localStorage can be used for settings pages, such as the one used in my team's project Command &amp; Control. While storing setting configurations within an online database like MongoDB is an option, it is not always necessary, especially when a user account system is not required. Saving settings in localStorage allows the website to function properly regardless of where it is accessed.</p>"},{"location":"faq/client/localStorage/#approach-for-using-localstorage-for-a-settings-page","title":"Approach for using localStorage for a settings page","text":"<p>It is recommended to create a default configuration file that lists a set of default values in case no setting configurations have been stored in localStorage. Below is an example of a config file used in our project:</p> <pre><code>let ros_config = {\n    ROSBRIDGE_SERVER_IP: \"127.0.0.1\",\n    ROSBRIDGE_SERVER_PORT: \"9090\",\n    RECONNECTION_TIMEOUT: 3000,\n    CHECK_IMAGE_CONFIG: 3000,\n    ROSBRIDGE_BATTERY_STATE_THROTTLE: 5000,\n    ROSBRIDGE_CMD_VEL: \"/cmd_vel\",\n    ROSBRIDGE_ODOM: \"/odom\",\n    ROSBRIDGE_CAMERA_TOPIC: \"/camera/rgb/image_raw/compressed\",\n    ROSBRIDGE_RASPICAM_TOPIC: \"/raspicam_node/image_res/compressed\",\n    ROSBRIDGE_IMAGE_CONFIGS: \"/image_configs\",\n    ROSBRIDGE_ROSTOPICS_LIST: \"/rostopic_list\",\n    ROSBRIDGE_IMAGE_WIDTH: \"426\",\n    ROSBRIDGE_IMAGE_HEIGHT: \"240\",\n    ROSBRIDGE_FRAME_WIDTH: 426,\n    ROSBRIDGE_FRAME_HEIGHT: 240,\n    ROSBRIDGE_BATTERY_TOPIC: \"/battery_state\",\n    ROSBRIDGE_MANUAL_TELEOP: false,\n    ROSBRIDGE_BATTERY_STATUS: true,\n}\n\nexport default ros_config\n</code></pre> <p>In the example below, we use a default value that is defined beforehand from the ros_config file. The logic states that if no value exists within local storage, then use the value from the ros_config value. You can just name the local storage value any name just make sure it stays consistent when you try to update it later on.</p> <pre><code>this.state = {\n    ...\n    rosbridgeServerIP: localStorage.getItem('rosbridgeServerIP') || ros_config.ROSBRIDGE_SERVER_IP,\n    rosbridgeServerPort: localStorage.getItem('rosbridgeServerPort') || ros_config.ROSBRIDGE_SERVER_PORT,\n    imageWidth: localStorage.getItem('imageWidth') || ros_config.ROSBRIDGE_IMAGE_WIDTH,\n    imageHeight: localStorage.getItem('imageHeight') || ros_config.ROSBRIDGE_IMAGE_HEIGHT,\n    frameWidth: localStorage.getItem('frameWidth') || ros_config.ROSBRIDGE_FRAME_WIDTH,\n    frameHeight: localStorage.getItem('frameHeight') || ros_config.ROSBRIDGE_FRAME_HEIGHT,\n    batteryStatus: localStorage.getItem('batteryStatus') !== null ? localStorage.getItem('batteryStatus') === \"true\" : ros_config.ROSBRIDGE_BATTERY_STATUS,\n    manualTeleop: localStorage.getItem('manualTeleop') !== null ? localStorage.getItem('manualTeleop') === \"true\" : ros_config.ROSBRIDGE_MANUAL_TELEOP,\n    ...\n}\n</code></pre> <p>After you define the logic for determining what value to use upon boot, you will want to also know how to change and/or clear the values from local storage for maybe implementing a save button or a clear button.</p> <p>To update the local storage value, you will want to use the following function (with the first parameter being the name of the local storage value and the second being the value you want to be used for replacing the local storage value):</p> <pre><code>localStorage.setItem('rosbridgeServerIP', this.state.rosbridgeServerIP);\n</code></pre> <p>To clear the values within local storage, you will want to use the following function: <pre><code>localStorage.clear();\n</code></pre></p>"},{"location":"faq/client/tkinter-ROS-improved/","title":"Creating a Tkinter GUI and communicating w/ ROS nodes","text":"<p>Brendon Lu and Benjamin Blinder</p> <p>Make sure you have the following packages imported: <code>tkinter</code> and <code>rospy</code>. The tkinter module is a basic and simple, yet effective way of implementing a usable GUI.</p> <pre><code>import tkinter as tk\nimport rospy\n</code></pre> <p>Because tkinter has a hard time recognizing functions created at strange times, you should next create any functions you want to use for your node. For this example, I recommend standard functions to publish very simple movement commands.</p> <pre><code>def turn_function():\n    t=Twist()\n    t.angular.z=0.5\n    cmd_vel.publish(t)\n\ndef stop_function():\n    t=Twist()\n    t.angular.z=0\n    cmd_vel.publish(t)\n</code></pre> <p>You still need to initialize the ros node, as well as any publishers and subscribers, which should be the next step in your code.</p> <pre><code>rospy.init_node(\"tkinter_example\")\ncmd_vel = rospy.Publisher('cmd_vel', Twist, queue_size=1)\n</code></pre> <p>Now it is time to create a basic window for your GUI. First, write <code>[window name] = tk.Tk()</code> to create a window, then set the title and size of the window. Not declaring a window size will create a window that adapts automatically to the size of whatever widgets you create on the window.</p> <pre><code>root=tk.Tk()\nroot.wm_title(\"Test TKinter Window\")\nroot.geometry(\"250x250\") #set size of window\n</code></pre> <p>The step is to populate your window with the actual GUI elements, which tkinter calls \"Widgets\". Here we will be making two basic buttons, but there are other common widget types such as the canvas, entry, label, and frame widgets. <pre><code>turn_button=tk.Button(\n    root,\n    text=\"turn\",\n    bg=\"grey\",\n    command=turn_function\n)\nstop_button=tk.Button(\n    root,\n    text=\"stop\",\n    bg=\"grey\",\n    command=stop_function\n)\n</code></pre></p> <p>And now that you have created widgets, you will notice that if you run your code, it is still blank. This is because the widgets need to be added to the window. You can use \"grid\", \"place\", or \"pack\" to put the widget on the screen, each of which have their own strengths and weaknesses. For this example, I will be using \"pack\".</p> <pre><code>turn_button.pack()\nstop_button.pack()\n</code></pre> <p>And now finally, you are going to run the tkinter mainloop. Please note that you cannot run a tkinter loop and the rospy loop in the same node, as they will conflict with each other.</p> <pre><code>root.mainloop()\n</code></pre> <p>To run the node we have created here, you should have your robot already running either in the simulator or in real life, and then simply use rosrun to run your node.  Here is the code for the example tkinter node I created, with some more notes on what different parts of the code does</p> <p><pre><code>#!/usr/bin/env python\nimport tkinter as tk #import tkinter\nfrom geometry_msgs.msg import Twist\nimport rospy\n\n#Put callbacks here\n\n#Put all standard functions here\n\n#Put functions that are called by pressing GUI buttons here\ndef turn_function():\n    t=Twist() #creates a twist object\n    t.angular.z=0.5 #sets the angular velocity of that object so the robot will turn\n    cmd_vel.publish(t) #publishes the twist\n\ndef stop_function():\n    t=Twist()\n    t.angular.z=0\n    cmd_vel.publish(t)\n\n\n#initialize rospy node, publishers, and subscribers\nrospy.init_node(\"tkinter\")\ncmd_vel = rospy.Publisher('cmd_vel', Twist, queue_size=1) #standard publisher to control movement\n\n\n#initialize tkinter window\nroot=tk.Tk() #initialize window\nroot.wm_title(\"Test TKinter Window\") #set window name\nroot.geometry(\"250x250\") #set size of window\n\n#create widgets\nturn_button=tk.Button( #creates a button\n    root, #sets the button to be on the root, but this could also be a frame or canvas if you want\n    text=\"turn\", #The text on the button\n    bg=\"green\", #The background of the button, tkinter lets you write out color names for many standard colors, but you can also use hex colors or rgb values\n    command=turn_function #command will tie a previously defined function to your button. You must define this function earlier in the code.\n)\nstop_button=tk.Button(\n    root,\n    text=\"stop\",\n    bg=\"red\",\n    command=stop_function\n)\n\n#adding the buttons to the window\nturn_button.pack() #pack will simply stack each widget on the screen in the order they were packed, but that can be changed with various arguments in the pack method. Please check the tkinter documentation to see more options.\nstop_button.pack()\n\n#Tkinter Mainloop\nroot.mainloop()\n</code></pre> Although this code does technically move the robot and with some serious work it could run a much more advanced node, I do not recommend doing this. I would recommend that you create two nodes: A GUI node and a robot node. In the GUI node, create a custom publisher such as <code>command_pub=rospy.Publisher('command', Twist, queue_size=1)</code> and use this to send messages for movement to the robot node. This way, the robot node can handle things like LiDAR or odometry without issues, since the tkinter update loop will not handle those kinds of messages very efficiently.</p> <p>Overall, tkinter is an industry staple for creating simple GUIs in Python, being fast, easy to implement, versatile, and flexible, all with an intuitive syntax. For more information, check out the links below.</p> <p>basic tkinter tutorial</p> <p>basic tkinter video tutorial</p> <p>documentation</p> <p>variable text</p> <p>images using photoimage</p>"},{"location":"faq/client/tkinter-ROS/","title":"Creating a Tkinter GUI and communicating w/ ROS nodes","text":"<p>Brendon Lu</p> <p>Make sure you have the following packages imported: ==tkinter==, ==rospy==. The tkinter module is a basic and simple, yet effective way of implementing a usable GUI.</p> <pre><code>import tkinter as tk\nimport rospy\n</code></pre> <p>Immediately after importing the necessary packages, initialize a rospy node and publishers/subscribers, which can be used to communicate and pass userinput data to other nodes through respective topics. </p> <pre><code>rospy.init_node(\"tkinter\")\nspeed_pub = rospy.Publisher('[topic]', [datatype], queue_size = [queue size])\nheat_sub = rospy.Subscriber('[topic]', [datatype], [callback fxn])\n</code></pre> <p>To create a basic gui, first run <code>[root name] = tk.Tk()</code> in order to create a root window. In order to add elements to your gui/window, create a canvas and grid using </p> <pre><code>[canvas name] == tk.Canvas([root name], width = [width], height = [height])\n[canvas name].grid(columnspan = [# of columns]\n</code></pre> <p>Some other basic functions to get you started include: <code>tk.Label([root name] ... )</code> for showing text or variables, and <code>tk.PhotoImage(file = [img PATH])</code>. Note that to add an image, first you need to pass it into tk using PhotoImage, then run Label in order to show it. Overall, tkinter is an industry staple for creating simple GUIs in Python, being fast, easy to implement, versatile, and flexible, all with an intuitive syntax. For more information, check out the links below.</p> <p>basic tkinter tutorial</p> <p>documentation</p> <p>variable text</p> <p>images using photoimage</p>"},{"location":"faq/cv/color-for-line-following/","title":"How to get correct color for line following in the lab by Rongzi Xie.","text":"<p>Line follower may work well and easy to be done in gazebo because the color is preset and you don't need to consider real life effect. However, if you ever try this in the lab, you'll find that many factors will influence the result of your color.</p>"},{"location":"faq/cv/color-for-line-following/#real-life-influencer","title":"Real Life Influencer:","text":"<ol> <li>Light: the color of the tage can reflect in different angles and rate at different time of a day depend on the weather condition at that day. </li> <li>Shadow: The shadow on the tape can cause error of color recognization</li> <li>type of the tape: The paper tage is very unstable for line following, the color of such tape will not be recognized correctly. The duct tape can solve most problem since the color that be recognized by the camera will not be influenced much by the light and weather that day. Also it is tougher and easy to clean compare to other tape.</li> <li>color in other object: In the real life, there are not only the lines you put on the floor but also other objects. Sometimes robots will love the color on the floor since it is kind of a bright white color and is easy to be included in the range. The size of range of color is a trade off. If the range is too small, then the color recognization will not be that stable, but if it is too big, robot will recognize other color too. if you are running multiple robots, it might be a good idea to use electric tape to cover the red wire in the battery and robot to avoid recognizing robot as red line.</li> </ol> <p><pre><code>OpenCV and HSV color:\nOpencv use hsv to recognize color, but it use different scale than normal.\nHere is a comparison of scale:\nnormal use  H: 0-360, S: 0-100, V: 0-100\nOpencv use  H: 0-179, S: 0-255, V: 0-255\n</code></pre> So if we use color pick we find online, we may need to rescale it to opencv's scale.</p>"},{"location":"faq/cv/computer-vision-suggestion/","title":"How to approach computer vision","text":""},{"location":"faq/cv/computer-vision-suggestion/#how-to-approach-computer-vision","title":"How to Approach Computer Vision","text":"<p>Using machine learning models for object detection/ classification) might be harder than you expect, from our own groups\u2019 experience, as machine learning models are usually uninterpretable and yield stochastic results.  Therefore, if you don\u2019t have a solid understanding of how to build/train a model or if you\u2019re not confident that you know what to do when the model JUST doesn\u2019t work as expected, I recommend you to start from more interpretable and stable computer vision techniques to avoid confusion/frustration, and also try to simplify the object you want to detect/classify \u2013 try to make them easily differentiable with other background objects from color and shape.  For examples, several handy functions from OpenCV that I found useful and convenient-to-use include color masking (cv2.inRange), contour detection (cv2.findContours), indices extraction (cv2.convexHul) etc..  These functions suffice for easy object detection (like colored circles, colored balls, arrows, cans, etc.); and you can use cv2.imshow to easily see the transformation from each step -- this would help you debug faster and have something functional built first. </p> <p>For one example (this is what our team used for traffic sign classification): Using this piece of code:  You can detect the contour of the arrow </p> <p>After which you can find the tip of the arrow, and then determine the direction of the arrow </p> <p>The algorithm of finding the tip might be a little complicated to understand (it uses the \"convexity defects\" of the \"hull\"). But the point I'm making here is that with these few lines of code, you can achieve probably more than you expected. So do start with these \"seamingly easy\" techniques first before you use something more powerful but confusing.</p>"},{"location":"faq/cv/computer-vision-with-model-Yolo8/","title":"Computer vision with pretrained model Yolo8","text":"<p>Yolo8 is a cutting-edge, state-of-the-art (SOTA) model that builds upon the success of previous YOLO versions and introduces new features and improvements to further boost performance and flexibility. YOLOv8 is designed to be fast, accurate, and easy to use, making it an excellent choice for a wide range of object detection and tracking, instance segmentation, image classification and pose estimation tasks. It is a powerful model that can be used to detect multiple objects in an image. </p> <p>It has been wrapped into a user-friendly python package Ultralytics (https://docs.ultralytics.com/). To detect objects of interest, the pre-trained model yolo8 can be used. Or one can customize the yolo8 model by training it with provided train image data. Here the website Roboflow (https://roboflow.com/) has a variety of object datasets, e.g. traffic sign dataset (https://universe.roboflow.com/usmanchaudhry622-gmail-com/traffic-and-road-signs/model/1). Once the dataset is downloaded and the Ultralytics package is installed, the yolo8 model can be trained easily: <pre><code>yolo detect train model=yolov8n.pt data=traffic_sign.yaml\n</code></pre> Where the traffic_sign.yaml is the path to the yaml file inside your downloaded dataset from roboflow. You have the options to use various yolo8 models. See Detection Docs for usage examples with these models.</p> Model size<sup>(pixels) mAP<sup>val50-95 Speed<sup>CPU ONNX(ms) Speed<sup>A100 TensorRT(ms) params<sup>(M) FLOPs<sup>(B) YOLOv8n 640 37.3 80.4 0.99 3.2 8.7 YOLOv8s 640 44.9 128.4 1.20 11.2 28.6 YOLOv8m 640 50.2 234.7 1.83 25.9 78.9 YOLOv8l 640 52.9 375.2 2.39 43.7 165.2 YOLOv8x 640 53.9 479.1 3.53 68.2 257.8 <p>The larger the model is, the higher the latency will present in the ros node that holds the model. Therefore, the smaller model should be used as long as the model works for the objects of interest.</p> <p>This is a quick ann simply way to train a customized model that is powerful in objects detection of robot vision.</p>"},{"location":"faq/cv/computer_vision_tips/","title":"Computer Vision","text":""},{"location":"faq/cv/computer_vision_tips/#attention","title":"Attention","text":"<p>For new contributors of Percetion_CV team, please first create your own branch and make sure all your work is done within your branch. Do PR (pull request) only if your team leader asks you to do so.   </p> <p>For new team leaders of Perception_CV, the <code>master branch</code> should only contain stable code that has been confirmed working. Master branch will be the source we use for integration with other teams when the time is ready.</p>"},{"location":"faq/cv/computer_vision_tips/#introduction","title":"Introduction","text":"<p>Full CV Repo here: https://github.com/campusrover/Robotics_Computer_Vision</p> <p>This repo is originally forked from https://github.com/ultralytics/yolov3 but heavily modified for our own use. The purpose of this repo is to achieve custom object detection for Brandeis Autonomous Robotics Course. Changes were made based on our object of deploying CV on ROS.   To download our most recent best trained weights, please go to https://drive.google.com/file/d/1DquRwpNDaXgkON2gj9Oks8N2ZWgN2Z9q/view?usp=sharing   Then unzip the file and copy <code>coco</code> and <code>weights</code> directory in this repo and replace everything.</p> <p>Notes:  I've put a low of useful tools inside the <code>./utils</code> directory, please feel free to use them whenever you need it.   </p> <ul> <li><code>./utils/vid_2_frm.py</code> : The python script that extracts all frames out of a video, you can control the extracting rate by reading the comment and do small modification. This script will also tell you the fps of the source video which will be useful for later converting frames back to video. </li> <li><code>./utils/frm_2_vid.py</code> : The python script that is converting frames by its name into a video, you better know the original/target video's fps to get the optimal output. </li> <li><code>./utils/xml_2_txt</code> : The repo that converts .xml format annotation into our desired .txt format (Yolo format), read and follow the README file inside. </li> <li><code>./utils/labelimg</code> : The repo that we use for labelling images, great tool! Detailed README inside. </li> <li><code>./utils/check_missing_label.py</code> : The python script that can be used for checking if there's any missing label in the annotation/image mixed directory. </li> <li><code>./utils/rename_dataset.py</code> : The python script doing mass rename in case different datasets' images names and annotations are the same and need to be distinguished. </li> <li><code>./list_img_path.py</code> : The python script that splits the datasets (images with its corresponding annotations) into training set and validation set in the ratio of 6:1 (you can modify the ratio). </li> <li><code>./utils/img_2_gif.py</code> : The python script that converts images to gif. </li> <li><code>./coco/dataset_clean.py</code> : The python script that cleans the uneven images and labels that is going to be trained and make sure they are perfectly parallel. </li> <li><code>./utils/video_recorder_pi.py</code> : The python script that records videos on pi camera. This script should be located in the robot and run under SSH </li> </ul> <p>Here are links to download our datasets (images and annotations) by certain class:</p> <p>Doorplate Recognition:  </p> <ul> <li>custom_volen(provided by Haofan): https://drive.google.com/file/d/1A9yI5PdLeAlKEVQww2NJgQLRDxq9tcOJ/view?usp=sharing </li> <li>custom_doorplate(provided by Haofan): https://drive.google.com/file/d/1jITWceHYYFXjUyaJ1bp4Wdb_tKhtylQJ/view?usp=sharing </li> </ul> <p>Facial Recognition:  </p> <ul> <li>Abhishek: https://drive.google.com/file/d/1Z3ICrLEVt50ia1C07ZCxE_Na105aRjsE/view?usp=sharing </li> <li>Haofan: https://drive.google.com/file/d/1nDcGb0QGSzLJaQL1ewMWVXtvXXjSwWC0/view?usp=sharing </li> <li>Yuchen: https://drive.google.com/file/d/1PomjuCvcJ25_d_EaQwqE9l1wuZ5z5Zz3/view?usp=sharing </li> <li>Huaigu: https://drive.google.com/file/d/1QNKtvanc58PoQZCg6htQpcImd00toYby/view?usp=sharing </li> <li>Eli: https://drive.google.com/file/d/14qII9t4tyDsYqj_bxxCdxSyIip0CRwQT/view?usp=sharing </li> <li>Nate: https://drive.google.com/file/d/1KE0UVu7dalip4mDVoVGpBgr1uyyhoHQB/view?usp=sharing </li> <li>Cody: https://drive.google.com/file/d/1Yb4RmYWXWCBO3nb_Di--3tRh0LdiRZBn/view?usp=sharing </li> <li>Pito: https://drive.google.com/file/d/1NZ4SBfv1Y5zuGpRQLebOlK-duG_p_0pg/view?usp=sharing </li> <li>Sibo: https://drive.google.com/file/d/1c7ZcMN-LcMAjmO62oS_C3y2hpA6IUgvP/view?usp=sharing </li> <li>Arjun: https://drive.google.com/file/d/10NnfTU150Pis5ugOWLzxVvwcesi873LY/view?usp=sharing </li> <li>Charlie: https://drive.google.com/file/d/1UmCUl-uLPwwOub2ZsTNpQHK9Q_rdVScI/view?usp=sharing </li> </ul>"},{"location":"faq/cv/computer_vision_tips/#cv-subscriber-publisher","title":"CV Subscriber &amp; Publisher","text":"<p>All the CV subscriber and publisher are located at <code>./utils/</code> directory, they are:</p> <ul> <li><code>./utils/image_subscriber.py</code> : The python script that subscribe image from <code>raspicam_node/image</code> rostopic. </li> <li><code>./utils/string_publisher.py</code> : The python script that publishes a string on rostopic of <code>/mutant/face_detection</code> which is generated from <code>detect.py</code>, the format is explained below:  </li> </ul> <p>CV Publisher example: \"['sibo', -4.34, 1.63]\"   </p> <p>[   &lt;\"class name\"&gt;,   &lt;\"angle of target to front in degree (negative -&gt; left, positive -&gt; right\")&gt;,   &lt;\"rough distance in meter\"&gt;   ]   </p>"},{"location":"faq/cv/computer_vision_tips/#cheat-sheet-for-raspberry-pi-camera","title":"Cheat Sheet For Raspberry Pi Camera","text":"<p>Detailed official user guide here: http://emanual.robotis.com/docs/en/platform/turtlebot3/appendix_raspi_cam/ </p> <p>Some useful commands:   </p> <ul> <li><code>raspivid -vf -hf -t 30000 -w 640 -h 480 -fps 25 -b 1200000 -p 0,0,640,480 -o pivideo.h264</code>   recording 30 seconds video on 25 fps.  </li> <li><code>MP4Box -add pivideo.h264 pivideo.mp4</code>   converting .h264 video to .mp4  </li> <li><code>scp donatello@129.64.243.61:~/pivideo.mp4 ~/Downloads/</code>   downloading video from ssh to local machine  </li> <li><code>rqt_image_view</code>   getting vision from camera, requires bringup which is conflict to the video recording function  </li> <li><code>rosrun rqt_reconfigure rqt_reconfigure</code>   edit camera configuration  </li> </ul> <p>Pipeline of recording video on <code>DONATELLO</code>:</p> <ul> <li>ssh <code>donatello@129.64.243.61</code></li> <li>If you want to see preview images, <code>roslaunch turtlebot3_bringup turtlebot3_rpicamera.launch</code>, then on remote computer, do <code>rqt_image_view</code></li> <li>when you recording video, shut down the <code>rpicamera bringup</code> in advance</li> <li>Do <code>raspivid -vf -hf -t 30000 -w 640 -h 480 -fps 25 -b 1200000 -p 0,0,640,480 -o pivideo.h264</code> on <code>DONATELLO</code> to record video</li> </ul>"},{"location":"faq/cv/computer_vision_tips/#cheat-sheet-for-usb-web-camera","title":"Cheat Sheet For USB Web-Camera","text":"<p>Get image_view</p> <ul> <li>ssh &lt;Robot_name_space&gt;@  </li> <li>plug in the USB camera  </li> <li>On slave, do <code>lsusb</code> and <code>ls /dev |grep video</code> to check if camera was recognized by system  </li> <li>On slave, install usb_cam ROS node <code>sudo apt install ros-kinetic-usb-cam</code> </li> <li>On slave, check the usb camera launch file <code>cat /opt/ros/kinetic/share/usb_cam/launch/usb_cam-test.launch</code> </li> <li>(Optional) On local client machine (master machine), run <code>roscore</code> (Usually it's constantly running on the desktop of Robotics Lab so you won't need to do this line)  </li> <li>On slave, start usb_cam node <code>roslaunch usb_cam usb_cam-test.launch</code> </li> <li>(Optional) On slave, bring running process to background with <code>CTRL+Z</code> and execute <code>bg</code> command to continue execution it in background  </li> <li>(Optional) On slave, check the topic of usb camera <code>rostopic list</code> </li> <li>(Optional) On master, check the topics in GUI <code>rqt_graph</code> </li> <li>On master, read camera data with image_view <code>rosrun image_view image_view image:=/&lt;name_space&gt;/usb_cam/image_raw</code> </li> <li>On slave, to bring background task to foreground <code>fg</code> </li> </ul> <p>Web Streaming</p> <ul> <li>On slave, install web-video-server ROS node <code>sudo apt install ros-kinetic-web-video-server</code> </li> <li>On slave, to make it right, create catkin workspace for our custom launch file <code>mkdir -p ~/rosvid_ws/src</code> </li> <li>On slave,<code>cd ~/rosvid_ws</code> </li> <li>On slave, <code>catkin_make</code> </li> <li>On slave, <code>source devel/setup.bash</code> </li> <li>On slave, create ROS package <code>cd src</code> then <code>catkin_create_pkg vidsrv std_msgs rospy roscpp</code> </li> <li>On slave, create launch file using nano, vim, etc <code>mkdir -p vidsrv/launch</code> then <code>nano vidsrv/launch/vidsrv.launch</code>. Then copy and paste the code below  </li> </ul> <p>https://github.com/campusrover/Perception_CV/blob/master/utils/vidsrv.launch </p> <ul> <li>On slave, build package <code>cd..</code> then <code>catkin_make</code> </li> <li>On master, Make sure <code>roscore</code> is running  </li> <li>On slave, run created launch file <code>roslaunch vidsrv vidsrv.launch</code> </li> <li>On your client machine, open web browser and go to <code>&lt;Robot IP address&gt;:8080</code> . Under <code>/usb_cam/</code> categoryand and click <code>image_raw</code> .  </li> <li>Enjoy the web streaming  </li> </ul>"},{"location":"faq/cv/computer_vision_tips/#description","title":"Description","text":"<p>The https://github.com/ultralytics/yolov3 repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux, MacOS and Windows. Training is done on the COCO dataset by default: https://cocodataset.org/#home. Credit to Joseph Redmon for YOLO: https://pjreddie.com/darknet/yolo/.</p>"},{"location":"faq/cv/computer_vision_tips/#requirements","title":"Requirements","text":"<p>Python 3.7 or later with the following <code>pip3 install -U -r requirements.txt</code> packages:</p> <ul> <li><code>numpy</code></li> <li><code>torch &gt;= 1.0.0</code></li> <li><code>opencv-python</code></li> <li><code>tqdm</code></li> </ul>"},{"location":"faq/cv/computer_vision_tips/#tutorials","title":"Tutorials","text":"<ul> <li>GCP Quickstart</li> <li>Transfer Learning</li> <li>Train Single Image</li> <li>Train Single Class</li> <li>Train Custom Data</li> </ul>"},{"location":"faq/cv/computer_vision_tips/#training","title":"Training","text":"<p>Start Training: Run <code>train.py</code> to begin training after downloading COCO data with <code>data/get_coco_dataset.sh</code>.</p> <p>Resume Training: Run <code>train.py --resume</code> resumes training from the latest checkpoint <code>weights/latest.pt</code>.</p> <p>Each epoch trains on 117,263 images from the train and validate COCO sets, and tests on 5000 images from the COCO validate set. Default training settings produce loss plots below, with training speed of 0.6 s/batch on a 1080 Ti (18 epochs/day) or 0.45 s/batch on a 2080 Ti.</p> <p>Here we see training results from <code>coco_1img.data</code>, <code>coco_10img.data</code> and <code>coco_100img.data</code>, 3 example files available in the <code>data/</code> folder, which train and test on the first 1, 10 and 100 images of the coco2014 trainval dataset.</p> <p><code>from utils import utils; utils.plot_results()</code> </p>"},{"location":"faq/cv/computer_vision_tips/#image-augmentation","title":"Image Augmentation","text":"<p><code>datasets.py</code> applies random OpenCV-powered (https://opencv.org/) augmentation to the input images in accordance with the following specifications. Augmentation is applied only during training, not during inference. Bounding boxes are automatically tracked and updated with the images. 416 x 416 examples pictured below.</p> Augmentation Description Translation +/- 10% (vertical and horizontal) Rotation +/- 5 degrees Shear +/- 2 degrees (vertical and horizontal) Scale +/- 10% Reflection 50% probability (horizontal-only) HSV Saturation +/- 50% HSV Intensity +/- 50% <p></p>"},{"location":"faq/cv/computer_vision_tips/#speed","title":"Speed","text":"<p>https://cloud.google.com/deep-learning-vm/ Machine type: n1-standard-8 (8 vCPUs, 30 GB memory) CPU platform: Intel Skylake GPUs: K80 ($0.198/hr), P4 ($0.279/hr), T4 ($0.353/hr), P100 ($0.493/hr), V100 ($0.803/hr) HDD: 100 GB SSD Dataset: COCO train 2014</p> GPUs <code>batch_size</code> batch time epoch time epoch cost (images) (s/batch) 1 K80 16 1.43s 175min $0.58 1 P4 8 0.51s 125min $0.58 1 T4 16 0.78s 94min $0.55 1 P100 16 0.39s 48min $0.39 2 P100 32 0.48s 29min $0.47 4 P100 64 0.65s 20min $0.65 1 V100 16 0.25s 31min $0.41 2 V100 32 0.29s 18min $0.48 4 V100 64 0.41s 13min $0.70 8 V100 128 0.49s 7min $0.80"},{"location":"faq/cv/computer_vision_tips/#inference","title":"Inference","text":"<p>Run <code>detect.py</code> to apply trained weights to an image, such as <code>zidane.jpg</code> from the <code>data/samples</code> folder:</p> <p>YOLOv3: <code>python3 detect.py --cfg cfg/yolov3.cfg --weights weights/yolov3.weights</code> </p> <p>YOLOv3-tiny: <code>python3 detect.py --cfg cfg/yolov3-tiny.cfg --weights weights/yolov3-tiny.weights</code> </p> <p>YOLOv3-SPP: <code>python3 detect.py --cfg cfg/yolov3-spp.cfg --weights weights/yolov3-spp.weights</code> </p>"},{"location":"faq/cv/computer_vision_tips/#webcam","title":"Webcam","text":"<p>Run <code>detect.py</code> with <code>webcam=True</code> to show a live webcam feed.</p>"},{"location":"faq/cv/computer_vision_tips/#pretrained-weights","title":"Pretrained Weights","text":"<ul> <li>Darknet <code>*.weights</code> format: https://pjreddie.com/media/files/yolov3.weights</li> <li>PyTorch <code>*.pt</code> format: https://drive.google.com/drive/folders/1uxgUBemJVw9wZsdpboYbzUN4bcRhsuAI</li> </ul>"},{"location":"faq/cv/computer_vision_tips/#map","title":"mAP","text":"<ul> <li>Use <code>test.py --weights weights/yolov3.weights</code> to test the official YOLOv3 weights.</li> <li>Use <code>test.py --weights weights/latest.pt</code> to test the latest training results.</li> <li>Compare to darknet published results https://arxiv.org/abs/1804.02767.</li> </ul> ultralytics/yolov3 darknet <code>YOLOv3 320</code> 51.8 51.5 <code>YOLOv3 416</code> 55.4 55.3 <code>YOLOv3 608</code> 58.2 57.9 <code>YOLOv3-spp 320</code> 52.4 - <code>YOLOv3-spp 416</code> 56.5 - <code>YOLOv3-spp 608</code> 60.7 60.6 <pre><code>git clone https://github.com/ultralytics/yolov3\n# bash yolov3/data/get_coco_dataset.sh\ngit clone https://github.com/cocodataset/cocoapi &amp;&amp; cd cocoapi/PythonAPI &amp;&amp; make &amp;&amp; cd ../.. &amp;&amp; cp -r cocoapi/PythonAPI/pycocotools yolov3\ncd yolov3\n\npython3 test.py --save-json --img-size 416\nNamespace(batch_size=32, cfg='cfg/yolov3-spp.cfg', conf_thres=0.001, data_cfg='data/coco.data', img_size=416, iou_thres=0.5, nms_thres=0.5, save_json=True, weights='weights/yolov3-spp.weights')\nUsing CUDA device0 _CudaDeviceProperties(name='Tesla V100-SXM2-16GB', total_memory=16130MB)\n               Class    Images   Targets         P         R       mAP        F1\nCalculating mAP: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 157/157 [05:59&lt;00:00,  1.71s/it]\n                 all     5e+03  3.58e+04     0.109     0.773      0.57     0.186\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.335\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.565\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.349\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.151\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.360\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.493\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.280\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.432\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.458\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.255\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.494\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.620\n\npython3 test.py --save-json --img-size 608 --batch-size 16\nNamespace(batch_size=16, cfg='cfg/yolov3-spp.cfg', conf_thres=0.001, data_cfg='data/coco.data', img_size=608, iou_thres=0.5, nms_thres=0.5, save_json=True, weights='weights/yolov3-spp.weights')\nUsing CUDA device0 _CudaDeviceProperties(name='Tesla V100-SXM2-16GB', total_memory=16130MB)\n               Class    Images   Targets         P         R       mAP        F1\nComputing mAP: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 313/313 [06:11&lt;00:00,  1.01it/s]\n                 all     5e+03  3.58e+04      0.12      0.81     0.611     0.203\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.366\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.607\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.386\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.207\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.391\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.485\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.296\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.464\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.494\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.331\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.517\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.618\n</code></pre>"},{"location":"faq/cv/computer_vision_tips/#citation","title":"Citation","text":""},{"location":"faq/cv/computer_vision_tips/#contact","title":"Contact","text":"<p>Issues should be raised directly in the repository. For additional questions or comments please contact your CV Team Leader or Sibo Zhu at siboz1995@gmail.com</p>"},{"location":"faq/cv/edgeDetection/","title":"Detection of Edges","text":""},{"location":"faq/cv/edgeDetection/#by-harris-ripp","title":"By Harris Ripp","text":"<p>The detection of edges in image processing is very important when needing to find straight lines in pictures using a camera. One of the most popular ways to do so is using an algorithm called a Canny Edge Detector. This algorithm was developed by John F. Canny in 1986 and there are 5 main steps to using it. Below are examples of the algorithm in use:</p> <p> </p> <p>The second image displays the result of using canny edge detection on the first image.</p>"},{"location":"faq/cv/edgeDetection/#steps-to-edge-detection","title":"Steps to Edge Detection:","text":"<ul> <li>Apply Gaussian filter to smooth image</li> <li>Find intensity gradients of image</li> <li>Apply gradient magnitude thresholding or lower bound cut-off suppression to remove false results</li> <li>Track edge by surprisessing weak edges so only strong ones appear</li> </ul>"},{"location":"faq/cv/edgeDetection/#application-of-canny-edge-detection","title":"Application of Canny Edge Detection","text":"<p>The below function demonstrates how to use this algorithm:</p> <pre><code>def img_callback(self, msg):\n\n        # Canny Edge Detection\n        # Blurs image and find intensity gradients\n        img = self.bridge.imgmsg_to_cv2(msg, 'bgr8')\n\n        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n\n        kernelSize = 31\n</code></pre> <p>First, the image is converting into something usable by cv. It is then grayed and the intensity gradient for the kernel is found</p> <pre><code>grayBlur = cv2.GaussianBlur(gray, (kernelSize, kernelSize), 0)\n</code></pre> <p>The image is then blurred for canny preparation. </p> <pre><code>lowEnd = 30\n        highEnd = 100\n        edges = cv2.Canny(grayBlur, lowEnd, highEnd)\n\n        # Publish for use in finding centroid \n        self.img_pub.publish(self.bridge.cv2_to_imgmsg(edges))\n</code></pre> <p>The lower and upper bounds are decided and the Canny algorithm is run on the image. In the case of this function, the new image is then published to a topic called \"canny mask\" for use by another node. </p> <p>The above code was created for use in a project completed by myself and fellow student Adam Ring</p>"},{"location":"faq/cv/gesture-recognition/","title":"Using OpenCV and Neural Networks for hand landmarking with Google MediaPipe","text":""},{"location":"faq/cv/gesture-recognition/#leo-gao","title":"Leo Gao","text":"<p>Here is the process of utilizing raspicam's images to get recognized as gestures. </p>"},{"location":"faq/cv/gesture-recognition/#summary","title":"Summary","text":""},{"location":"faq/cv/gesture-recognition/#after-you-have-properly-subscribed-to-the-camera","title":"After you have properly subscribed to the camera:","text":"<pre><code>rospy.Subscriber('/raspicam_node/image/compressed', CompressedImage, self.image_cb)\n</code></pre>"},{"location":"faq/cv/gesture-recognition/#this-would-be-how-you-would-convert-a-compressed-image-message-into-an-opencv-readable-format","title":"This would be how you would convert a compressed image message into an OpenCV-readable format:","text":"<pre><code>def image_cb(self, msg):\n        \"\"\"Process incoming images from the camera.\"\"\"\n        try:\n            np_arr = np.frombuffer(msg.data, np.uint8)\n            self.cv_image = cv.imdecode(np_arr, cv.IMREAD_COLOR)\n        except Exception as e:\n            rospy.logerr(f\"Image conversion failed: {e}\")\n</code></pre>"},{"location":"faq/cv/gesture-recognition/#for-each-detected-hand-calculate-the-pixel-coordinates-of-the-landmarks-relative-to-the-image-dimensions-keep-in-mind-that-it-is-one-hand-at-a-time","title":"For each detected hand, calculate the pixel coordinates of the landmarks relative to the image dimensions, keep in mind that it is one hand at a time:","text":"<pre><code>def calc_landmark_list(self, image, hand_landmarks):\n    image_width, image_height = image.shape[1], image.shape[0]\n    return [\n        [\n            int(landmark.x * image_width),\n            int(landmark.y * image_height),\n        ]\n        for landmark in hand_landmarks.landmark\n    ]\n</code></pre>"},{"location":"faq/cv/gesture-recognition/#normalize-the-coordinates-relative-to-the-wrist-first-landmark-and-scale-them","title":"Normalize the coordinates relative to the wrist (first landmark) and scale them:","text":"<pre><code>def preprocess_landmarks(self, landmark_list):\n    base_x, base_y = landmark_list[0]\n    for landmark in landmark_list:\n        landmark[0] -= base_x\n        landmark[1] -= base_y\n\n    flattened = list(itertools.chain.from_iterable(landmark_list))\n    max_value = max(map(abs, flattened))\n    normalized = [n / max_value for n in flattened]\n\n    return normalized\n</code></pre>"},{"location":"faq/cv/gesture-recognition/#draw-landmarks-for-the-detected-hand","title":"Draw landmarks for the detected hand :","text":"<pre><code>def draw_landmarks(self, image, landmark_list):\n    for point in landmark_list:\n        cv.circle(image, tuple(point), 5, (255, 0, 0), -1)\n</code></pre>"},{"location":"faq/cv/gesture-recognition/#combining-it-together-by-first-tidying-up-the-image-by-flipping-it-and-then-converting-from-bgr-to-rgb-for-mediapipe-you-would-be-able-to-achieve-your-processed-gesture","title":"Combining it together by first tidying up the image by flipping it and then converting from BGR to RGB for mediapipe you would be able to achieve your processed gesture:","text":"<pre><code>def process_frame(self):\n        image = cv.flip(self.cv_image, 1)  # Mirror image for convenience\n        debug_image = copy.deepcopy(image)\n\n        # Convert image to RGB for MediaPipe\n        rgb_image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n        results = self.hands.process(rgb_image)\n\n        if results.multi_hand_landmarks:\n            for hand_landmarks in results.multi_hand_landmarks:\n                # Process hand landmarks\n                landmark_list = self.calc_landmark_list(image, hand_landmarks)\n                preprocessed_landmarks = self.preprocess_landmarks(landmark_list)\n\n                # Classify gesture\n                hand_sign_id = self.keypoint_classifier(preprocessed_landmarks)\n                hand_sign_label = self.keypoint_classifier_labels[hand_sign_id]\n\n                # Annotate image\n                brect = self.calc_bounding_rect(image, hand_landmarks)\n                debug_image = self.draw_landmarks(debug_image, landmark_list)\n                debug_image = self.draw_bounding_rect(debug_image, brect)\n                debug_image = self.draw_info_text(debug_image, brect, hand_sign_label)\n\n        cv.imshow(\"Gesture Recognition\", debug_image)\n        cv.waitKey(1)\n</code></pre>"},{"location":"faq/cv/gesture-recognition/#these-would-be-the-outputs-to-the-neural-network-which-would-have-been-pre-trained-by-you","title":"These would be the outputs to the neural network which would have been pre-trained by you.","text":"<p><pre><code>        hand_sign_id = self.keypoint_classifier(preprocessed_landmarks)\n        hand_sign_label = self.keypoint_classifier_labels[hand_sign_id]\n</code></pre> preprocessed_landmarks:  A normalized list of hand landmark coordinates is passed to the KeyPointClassifier. These landmarks are detected using MediaPipe and processed to remove variations caused by hand size, orientation, and position.</p> <p>self.keypoint_classifier:</p> <p>This is a neural network thatwas trained on a dataset of hand landmarks corresponding to various gestures.  Input: Normalized landmarks (e.g., [x1, y1, x2, y2, ..., x21, y21]). Output: Gesture ID (e.g., 0 for \"Fist\").</p> <p>When called, the neural network takes the normalized coordinates as input and outputs a gesture class ID (an integer representing a specific gesture) based on the input landmarks.</p>"},{"location":"faq/cv/gesture-recognition/#the-run-method-continuously-checks-for-new-frames-in-selfcv_image-and-calls-process_frame-when-available","title":"The run method continuously checks for new frames in self.cv_image and calls process_frame when available:","text":"<pre><code>def run(self):\n    rospy.loginfo(\"Gesture recognizer is running...\")\n    while not rospy.is_shutdown():\n        if self.cv_image is not None:\n            self.process_frame()\n        else:\n            rospy.loginfo_once(\"Waiting for camera feed...\")\n        rospy.sleep(0.01)\n</code></pre>"},{"location":"faq/cv/how_to_get_correct_color_for_line_following/","title":"How to get correct color for line following","text":"<p>How to get correct color for line following in the lab by Rongzi Xie. Line follower may work well and easy to be done in gazebo because the color is preset and you don't need to consider real life effect. However, if you ever try this in the lab, you'll find that many factors will influence the result of your color.</p> <p>Real Life Influencer: 1. Light: the color of the tage can reflect in different angles and rate at different time of a day depend on the weather condition at that day.  2. Shadow: The shadow on the tape can cause error of color recognization 3. type of the tape: The paper tage is very unstable for line following, the color of such tape will not be recognized correctly. The duct tape can solve most problem since the color that be recognized by the camera will not be influenced much by the light and weather that day. Also it is tougher and easy to clean compare to other tape. 4. color in other object: In the real life, there are not only the lines you put on the floor but also other objects. Sometimes robots will love the color on the floor since it is kind of a bright white color and is easy to be included in the range. The size of range of color is a trade off. If the range is too small, then the color recognization will not be that stable, but if it is too big, robot will recognize other color too. if you are running multiple robots, it might be a good idea to use electric tape to cover the red wire in the battery and robot to avoid recognizing robot as red line.</p> <p>OpenCV and HSV color: Opencv use hsv to recognize color, but it use different scale than normal. Here is a comparison of scale: normal use  H: 0-360, S: 0-100, V: 0-100 Opencv use  H: 0-179, S: 0-255, V: 0-255</p> <p>So if we use color pick we find online, we may need to rescale it to opencv's scale.</p>"},{"location":"faq/cv/smooth_navigation_towards_colored_object/","title":"How to Smoothly Navigate Towards a Colored Object Using ROS and OpenCV","text":""},{"location":"faq/cv/smooth_navigation_towards_colored_object/#author","title":"Author","text":"<ul> <li>Vedanshi Shah</li> <li>Dec 10 2024</li> </ul>"},{"location":"faq/cv/smooth_navigation_towards_colored_object/#summary","title":"Summary","text":"<p>This FAQ explains how to smoothly navigate a robot towards a colored object using ROS and OpenCV. It demonstrates how to apply exponential smoothing to the detected object's position to reduce noise and create smoother movements. The guide includes steps for object detection using color, applying smoothing to the object's position, and generating velocity commands for the robot's movement. It also covers visualizing the results to ensure the smoothing works as expected. This method helps stabilize the robot's navigation towards a target, avoiding jerky motions and ensuring smoother interaction with the environment.</p>"},{"location":"faq/cv/smooth_navigation_towards_colored_object/#how-to-smoothly-navigate-towards-a-colored-object-using-ros-and-opencv","title":"How to Smoothly Navigate Towards a Colored Object Using ROS and OpenCV","text":"<p>This FAQ entry focuses on one specific task: smoothing robot navigation towards a colored object. It highlights how to use exponential smoothing to stabilize the robot\u2019s movement when tracking the object's position in real-time.</p>"},{"location":"faq/cv/smooth_navigation_towards_colored_object/#prerequisites","title":"Prerequisites","text":"<p>Before starting, ensure you have the following: 1. A robot running ROS (e.g., TurtleBot, custom platform). 2. A camera publishing image data to a ROS topic (e.g., <code>/cv_camera/image_raw</code>). 3. Python with OpenCV and ROS packages installed (e.g., <code>cv_bridge</code>, <code>sensor_msgs</code>, <code>geometry_msgs</code>).</p>"},{"location":"faq/cv/smooth_navigation_towards_colored_object/#overview","title":"Overview","text":"<p>When a robot detects a colored object, it may experience jerky movements due to small variations in the object's detected position. Exponential smoothing helps mitigate this by creating a more stable center for the robot to track.</p>"},{"location":"faq/cv/smooth_navigation_towards_colored_object/#key-components","title":"Key Components:","text":"<ul> <li>Object Detection: Detect a colored object and calculate its position.</li> <li>Exponential Smoothing: Apply smoothing to stabilize the detected position.</li> <li>Robot Movement: Use the smoothed position to guide the robot smoothly.</li> </ul>"},{"location":"faq/cv/smooth_navigation_towards_colored_object/#step-by-step-instructions","title":"Step-by-Step Instructions","text":""},{"location":"faq/cv/smooth_navigation_towards_colored_object/#1-object-detection","title":"1. Object Detection","text":"<p>To detect objects based on color, the camera image is converted from RGB to HSV (Hue, Saturation, Value) format. A mask isolates the target color, and contours are used to find the object's bounding box.</p>"},{"location":"faq/cv/smooth_navigation_towards_colored_object/#code-snippet-for-color-detection","title":"Code Snippet for Color Detection:","text":"<pre><code># Convert the image to HSV color space\nhsv_image = cv2.cvtColor(cv_image, cv2.COLOR_BGR2HSV)\n\n# Define color ranges for the target color\ncolor_ranges = {\n    \"red\": ((0, 100, 100), (10, 255, 255)),\n    \"blue\": ((100, 150, 0), (140, 255, 255)),\n    \"green\": ((35, 40, 40), (85, 255, 255))\n}\n\n# Get the HSV range for the target color\nlower, upper = color_ranges.get(self.target_color, ((0, 0, 0), (0, 0, 0)))\n\n# Create a mask to isolate the target color\nmask = cv2.inRange(hsv_image, lower, upper)\n\n# Find contours in the mask\ncontours, _ = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n\n# Select the largest contour (assumed to be the object)\nif contours:\n    largest_contour = max(contours, key=cv2.contourArea)\n    x, y, w, h = cv2.boundingRect(largest_contour)\n    block_center_x = x + w / 2\n    block_center_y = y + h / 2\n</code></pre>"},{"location":"faq/cv/smooth_navigation_towards_colored_object/#2-smoothing-the-detected-position","title":"2. Smoothing the Detected Position","text":"<p>Exponential smoothing is applied to the detected object's center to reduce noise and sudden changes.</p>"},{"location":"faq/cv/smooth_navigation_towards_colored_object/#code-snippet-for-smoothing","title":"Code Snippet for Smoothing:","text":"<pre><code># Initialize smoothed center (defaults to image center)\nself.smoothed_center_x = 320\nself.smoothed_center_y = 240\n\n# Apply exponential smoothing\nalpha = 0.5  # Smoothing factor (0.0 = no smoothing, 1.0 = instant response)\ndef smooth_center(block_center_x, block_center_y):\n    self.smoothed_center_x = alpha * block_center_x + (1 - alpha) * self.smoothed_center_x\n    self.smoothed_center_y = alpha * block_center_y + (1 - alpha) * self.smoothed_center_y\n    return self.smoothed_center_x, self.smoothed_center_y\n\nsmoothed_x, smoothed_y = smooth_center(block_center_x, block_center_y)\n</code></pre>"},{"location":"faq/cv/smooth_navigation_towards_colored_object/#3-using-smoothed-position-for-navigation","title":"3. Using Smoothed Position for Navigation","text":"<p>The smoothed position is used to generate velocity commands for the robot, resulting in smoother navigation towards the object.</p>"},{"location":"faq/cv/smooth_navigation_towards_colored_object/#code-snippet-for-navigation","title":"Code Snippet for Navigation:","text":"<pre><code># Define a Twist message for velocity control\nmove_command = Twist()\n\n# Adjust angular velocity to center the object horizontally\nif smoothed_x &lt; image_center_x - tolerance_x:\n    move_command.angular.z = 0.5  # Turn left\nelif smoothed_x &gt; image_center_x + tolerance_x:\n    move_command.angular.z = -0.5  # Turn right\nelse:\n    move_command.angular.z = 0  # Stop turning\n\n# Adjust linear velocity to approach the object\nif smoothed_y &gt; image_center_y + tolerance_y:\n    move_command.linear.x = 0.5  # Move forward\nelse:\n    move_command.linear.x = 0  # Stop moving\n\n# Publish the velocity command\nself.velocity_pub.publish(move_command)\n</code></pre>"},{"location":"faq/cv/smooth_navigation_towards_colored_object/#4-visualizing-results","title":"4. Visualizing Results","text":"<p>Use OpenCV to display the detected object and bounding box, verifying that the smoothing works as intended.</p> <pre><code>cv2.rectangle(cv_image, (x, y), (x + w, y + h), (0, 255, 0), 2)\ncv2.imshow(\"Detected Block\", cv_image)\ncv2.waitKey(1)\n</code></pre>"},{"location":"faq/cv/smooth_navigation_towards_colored_object/#final-notes","title":"Final Notes","text":"<p>Exponential smoothing is a simple but effective technique for stabilizing noisy sensor data in real-time applications. By applying it to the detected object's position, the robot can navigate smoothly and reliably. Future improvements could include dynamic adjustment of the smoothing factor or integration with advanced filtering techniques like Kalman filters.</p>"},{"location":"faq/cv/tb3-camera-cv2-tips/","title":"Some tips for using OpenCV and Turtlebot3 Camera","text":"<p>Junhao Wang</p>"},{"location":"faq/cv/tb3-camera-cv2-tips/#read-compressedimage-type","title":"Read CompressedImage type","text":"<p>When using Turtlebot in the lab, it only publishs the CompressedImage from '/raspicam_node/image/compressed'. Here's how to read CompressedImage and Raw if you need. <pre><code>from cv_bridge import CvBridge\ndef __init__(self):\n   self.bridge = CvBridge()\ndef image_callback(self, msg):\n        # get raw image\n        # image = self.bridge.imgmsg_to_cv2(msg)\n\n        # get compressed image\n        np_arr = np.frombuffer(msg.data, np.uint8)\n        img_np = cv2.imdecode(np_arr, cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(img_np, cv2.COLOR_BGR2HSV)\n</code></pre></p>"},{"location":"faq/cv/tb3-camera-cv2-tips/#limit-frame-rate","title":"Limit frame rate","text":"<p>When the computation resource and the bandwidtn of robot are restricted, you need limit the frame rate.</p> <pre><code>def __init__(self):\n   self.counter = 1\ndef image_callback(self, msg):\n        # set frame rate 1/3\n        if self.counter % 3 != 0:\n            self.counter += 1\n            return\n        else:\n            self.counter = 1\n</code></pre>"},{"location":"faq/cv/tb3-camera-cv2-tips/#republish-the-image","title":"Republish the image","text":"<p>Again, when you have multiple nodes need image from robot, you can republish it to save bandwidth.</p> <pre><code>#!/usr/bin/env python\n\nimport rospy\nfrom sensor_msgs.msg import CompressedImage\nfrom cv_bridge import CvBridge\n\ndef image_callback(msg):\n    # Convert the compressed image message to a cv2 image\n    bridge = CvBridge()\n    cv2_image = bridge.compressed_imgmsg_to_cv2(msg, desired_encoding='passthrough')\n\n    # Process the image (e.g., resize, blur, etc.)\n    processed_image = process_image(cv2_image)\n\n    # Convert the processed cv2 image back to a compressed image message\n    compressed_image_msg = bridge.cv2_to_compressed_imgmsg(processed_image)\n\n    # Publish the processed image on the new topic\n    processed_image_pub.publish(compressed_image_msg)\n\ndef process_image(cv2_image):\n    # Perform image processing (e.g., resize, blur, etc.) and return the processed image\n    # Example: resized_image = cv2.resize(cv2_image, (new_width, new_height))\n    return cv2_image\n\nif __name__ == '__main__':\n    rospy.init_node('image_processing_node')\n    rospy.Subscriber('/robot/camera_topic', CompressedImage, image_callback)\n    processed_image_pub = rospy.Publisher('/processed_image_topic', CompressedImage, queue_size=1)\n\n    rospy.spin()\n</code></pre>"},{"location":"faq/fiducials/apriltags_setup/","title":"setting up apriltags","text":"<p>Setting up apriltags to work with your program is pretty simple. </p> <p>You can run the following two lines to install apriltags and its ros version onto your ubuntu: </p> <p><code>sudo apt install ros-noetic-apriltag</code></p> <p><code>sudo apt install ros-noetic-apriltag-ros</code></p> <p>Afterwards, connect your camera</p> <p><code>roslaunch usb_cam usb_cam-test.launch</code></p> <ul> <li>changed the video_device to /dev/video2 (or whichever video device you figure out it is) to take from usbcam</li> <li>created head_camera.yaml file from this example and added the calibration information:</li> </ul> <pre><code>cam0:\n  camera_model: pinhole\n  intrinsics: [684.9223702799867, 686.2362451089259, 307.24291147440965, 257.2964284210188]\n  distortion_model: plumb_bob\n  distortion_coeffs: [9.39260444e-03, 4.90535732e-01, 1.48962564e-02, 4.68503188e-04, -1.77954077e+00]\n  resolution: [640, 480]\n  tagtopic: /cam0\n</code></pre> <p><code>roslaunch apriltag_ros continuous_detection.launch publish_tag_detections_image:=true</code></p> <ul> <li>can remap input topics to what you need:<ul> <li>image_rect:=usb_cam/image_raw</li> <li>camera_info:=usb_cam/camera_info</li> </ul> </li> </ul>"},{"location":"faq/fiducials/fiducial-tips/","title":"General Tips When Using Fiducials","text":"<ul> <li>Calibrate the camera</li> </ul> <p>I have found that performing camera calibration helps fiducial recognition a lot. Realize that each robot has a slightly different camera and so the calibration file you find on the robot, conceivably, may be incorrect. When you get down to the wire, perform the calibration on your robot. It will help.</p> <ul> <li>Making Fiducials</li> </ul> <p>There are  * Two types of Fiducials - Aruco and April Tags * How to recognize Fiducials * TF of Camera * TF of Fiducial * Static Transforms</p>"},{"location":"faq/fiducials/fiducial_follows/","title":"Working with Fiducials","text":"<p>UNDER DEVELOMENT</p> <p>This document will not be explaining fiducials or localization. It is meant to help you get the software up and running on your robot. We are working here purely with ROS1 based robots, running on a Raspberry Pi with a Rasberry Pi Camera. Also we are working purely with Aruco Fiducials and the Ubiquity aruco_detect library. There are many other variations which are going to be ignored.</p>"},{"location":"faq/fiducials/fiducial_follows/#building-blocks","title":"Building Blocks","text":"<p>There are several new components which come into play in order to use fiducials </p> <ol> <li>Camera - Which converts light into a published topic with an image</li> <li>Fiducial \"signs\" - Which you print out and place within view of the robot</li> <li><code>aruco_detect</code> package - which analyzes the images and locates fiducials in them, publishing a tf for the relative position between the camera and the fiducial</li> </ol>"},{"location":"faq/fiducials/fiducial_follows/#packages-necessary","title":"Packages necessary","text":""},{"location":"faq/fiducials/fiducial_follows/#raspicam_node-main-camera-node","title":"raspicam_node - Main Camera Node","text":"<p>This package, by Ubiquity Robotics enables the raspberry Pi camera. It should be installed on your robot already. Raspi cameras have quite a lot of configuration parameters and setting them up as well as possible requires 'calibration'.</p>"},{"location":"faq/fiducials/fiducial_follows/#aruco_detect-reconition-software","title":"aruco_detect - Reconition software","text":"<p>This package is also from Ubituity contains the components that go into recognizing the fiducials and working with them.</p>"},{"location":"faq/fiducials/fiducial_follows/#fiducials","title":"Fiducials","text":"<p>Fiducials as you know are black and white images. There is software that recognizes them and more than that, are able to compute precisely the relative positions between the camera and the fiducials. </p>"},{"location":"faq/fiducials/fiducial_follows/#fiducial-dictionaries","title":"Fiducial Dictionaries","text":"<p>All fiducials are not created equally. Simply because it looks like a black and white image doesn't mean that it will work. The algorithm depends on the images coming from a standardized set of possibiltities. An individual Aruco fiducial comes from one of a small set of predefined Dictionaries and has a specific index therein. You need to know what you are dealing with.  Therefore you must make sure the fiducials you are using match the software</p>"},{"location":"faq/fiducials/fiducial_follows/#print-out-some-fiducials","title":"Print out some fiducials","text":"<p>Use this web site: Aruco Markers Generator and print out one or more fiducials. Pick \"Original Aruco Dictionary\" which corresponds to \"16\" in the code. If you print more than one give them different IDs. Later on that ID will come through in the code to allow you to tell one from the other. Tape the fiducial within sight of the camera on your robot.</p>"},{"location":"faq/fiducials/fiducial_follows/#running-the-software","title":"Running the software`","text":""},{"location":"faq/fiducials/fiducial_follows/#enable-the-camera","title":"Enable the camera","text":"<p>First you have to enable the camera. On the robot (<code>onboard</code>) run the following, either by itself or as part of another launch file. </p> <p><code>roslaunch raspicam_node camerav2_410x308_30fps.launch</code></p> <p>You can view the image of the camera within Rviz by subscribing to <code>/raspicam_node/image</code> or <code>rqt_image</code>. If the image happens to be upside down then. If the image is upsdide down get help to change the VFlip default variable.</p>"},{"location":"faq/fiducials/fiducial_follows/#other-launch-options","title":"Other launch options","text":"<p><code>roslaunch raspicam_node camerav2_1280x960_10fps.launch</code> # for a comppressed image, a good default <code>roslaunch raspicam_node camerav2_1280x960_10fps.launch enable_raw:=true</code> # for an uncompressed image</p>"},{"location":"faq/fiducials/fiducial_follows/#topics","title":"Topics","text":"<p>You will see that when you enable the camera it will begin publishing on one of several topics, for example:</p> <p><code>raspicam_node/camera/compressed</code></p> <p>Note that the last part of the topic (as you would see it in <code>rostopic list</code>) is not actually used when you subscribe to it. In fact the topic you subscribe to is <code>raspicam_node/camera</code> and the <code>/compressed</code> is used to designate that the data is a compressed image. This is confusing and unusual.</p>"},{"location":"faq/fiducials/fiducial_follows/#detect-fiducials","title":"Detect Fiducials","text":""},{"location":"faq/fiducials/fiducial_follows/#basic","title":"Basic","text":"<p>Make sure first that your camera is pointed at a Fiducial that you printed earlier. Now run (on your vnc)</p> <p><code>roslaunch aruco_detect aruco_detect.launch vis_msgs:=false dictionary:=16 fiducial_len:=0.10</code></p> <p>If detect sees the tag and identifies it you should see a large number of new topics (<code>rostopic list</code>). One that you can check is `/fiducial_images/. View it with rqt_image or rviz. If it is working and the fiducial is in view, you will see an colored outline around the fiducial. aruco_detect does have numerous parameters that in the future you can look at tweaking.</p>"},{"location":"faq/fiducials/fiducial_follows/#transforms","title":"Transforms","text":"<p>When you have just one fiducial things are simpler. <code>aruco_detect</code> will create a tf between that fiducial and the robot itself. In theory that means that as the robot moves, as long as it can see the fiducial, you can use the TF to determin the robot's location relative to the tf.</p> <p>You can see this by running rviz, looking at the tf tree. When you display the tf tree you will see the tf and the robot. Depending on which of the two you make the rviz \"fixed frame\".</p>"},{"location":"faq/fiducials/fiducial_follows/#reference-links","title":"Reference Links","text":"<p>Ubiquity Overview of Fiducials Fiducial Slam Project Report</p>"},{"location":"faq/gazebo/Spawning_Animated_Human/","title":"How do I spawn an animated Human?","text":""},{"location":"faq/gazebo/Spawning_Animated_Human/#nathan-cai","title":"Nathan Cai","text":"<p>This FAQ section assumes understanding of creating a basic Gazebo world and how to manipulate a XML file. This tutorial relies on the assets native to the Gazebo ecosystem.</p>"},{"location":"faq/gazebo/Spawning_Animated_Human/#setting-up-empty-gazebo-world","title":"Setting up empty Gazebo world","text":"<p>By Nathan Cai</p> <p>(If you have a prexisting Gazebo world you want to place an actor you can skip this part) Empty Gazebo worlds often lack a proper ground plane so it must be added in manually. You can directly paste this code into the world file.</p> <pre><code>&lt;?xml version=\"1.0\" ?&gt;\n&lt;sdf version=\"1.6\"&gt;\n   &lt;world name=\"default\"&gt;\n      &lt;!-- A ground plane --&gt;\n      &lt;include&gt;\n         &lt;uri&gt;model://ground_plane&lt;/uri&gt;\n      &lt;/include&gt;\n      &lt;!-- A global light source --&gt;\n      &lt;include&gt;\n         &lt;uri&gt;model://sun&lt;/uri&gt;\n      &lt;/include&gt;\n</code></pre>"},{"location":"faq/gazebo/Spawning_Animated_Human/#placing-an-actor-in-the-world","title":"Placing an actor in the world","text":""},{"location":"faq/gazebo/Spawning_Animated_Human/#tldr-quick-setup","title":"TL:DR Quick Setup","text":"<p>Here is the quick setup of everything, one can simply copy and paste this code and change the values to suit the need:</p> <p>(If you do not have a Plugin for the model, please delete the Plugin section)</p> <pre><code>&lt;actor name=\"actor1\"&gt;\n    &lt;pose&gt;0 0 0 0 0 0&lt;/pose&gt;\n    &lt;skin&gt;\n        &lt;filename&gt;PERSON_MESH.dae&lt;/filename&gt;\n        &lt;scale&gt;1.0&lt;/scale&gt;\n    &lt;/skin&gt;\n    &lt;animation name=\"NAME\"&gt;\n        &lt;filename&gt;ANIMATION_FILE_NAME.dae&lt;/filename&gt;\n        &lt;scale&gt;1.000000&lt;/scale&gt;\n        &lt;interpolate_x&gt;true&lt;/interpolate_x&gt;\n    &lt;/animation&gt;\n    &lt;plugin name=\"PLUGIN NAME\" filename=\"PLUGIN_FILE_NAME.so\"&gt;\n        ...\n        ...\n        ...\n    &lt;/plugin&gt;\n    &lt;script&gt;\n        &lt;trajectory id=\"0\" type=\"NAME\"&gt;\n            &lt;waypoint&gt;\n                &lt;time&gt;0&lt;/time&gt;\n                &lt;pose&gt;0 2 0 0 0 -1.57&lt;/pose&gt;\n            &lt;/waypoint&gt;\n            &lt;waypoint&gt;\n                &lt;time&gt;2&lt;/time&gt;\n                &lt;pose&gt;0 -2 0 0 0 -1.57&lt;/pose&gt;\n            &lt;/waypoint&gt;\n            ...\n                ...\n                ...\n            ...\n        &lt;/trajectory&gt;\n      &lt;/script&gt;\n&lt;/actor&gt;\n</code></pre>"},{"location":"faq/gazebo/Spawning_Animated_Human/#defining-an-actor","title":"Defining an actor","text":"<p>Human or animated models in Gazebo are called actors, which contains all the information of an actor. The information can include: <code>pose</code>, <code>skin</code>, <code>animation</code>, or any <code>plugins</code>. Each actor needs a unique name. It uses the syntax: <pre><code>&lt;actor name=\"actor\"&gt;\n    ...\n    ...\n    ...\n&lt;/actor&gt;\n</code></pre></p>"},{"location":"faq/gazebo/Spawning_Animated_Human/#change-the-actor-pose","title":"Change the actor pose","text":"<p>The pose of the is determined using the pose parameter of an actor. The syntax is: (x_pos, y_pos, z_pos, x_rot, y_rot, z_rot)</p> <pre><code>&lt;actor name=\"actor\"&gt;\n    &lt;pose&gt;0 0 0 0 0 0&lt;/pose&gt;\n    ...\n    ...\n&lt;/actor&gt;\n</code></pre>"},{"location":"faq/gazebo/Spawning_Animated_Human/#add-in-skins","title":"Add in Skins","text":"<p>The skin is the mesh of the actor or model that you want to place into the world. it is placed in the actor group and takes in the input of the filename. The syntax is:</p> <p><pre><code>&lt;actor name=\"actor\"&gt;\n    &lt;skin&gt;\n        &lt;filename&gt;moonwalk.dae&lt;/filename&gt;\n        &lt;scale&gt;1.0&lt;/scale&gt;\n    &lt;/skin&gt;\n&lt;/actor&gt;\n</code></pre> The mesh scale is also adjustable by changing the scale parameter.</p>"},{"location":"faq/gazebo/Spawning_Animated_Human/#add-in-animations","title":"Add in Animations","text":"<p>Though the actor can operate without animations, it is preferable for you to add one, especially if the model is to move, as it would make the enviorment more interesting and realistic.</p> <p>To add an animation to the actor, all it needs is a name fore the animation, and the file that contains the animation. The syntax for this is: NOTE: THE FILE BECOMES BUGGY OR WILL NOT WORK IF THERE IS NO SKIN.</p>"},{"location":"faq/gazebo/Spawning_Animated_Human/#important-in-order-for-the-animation-to-work-the-skeleton-of-the-skin-must-be-compatable-with-the-animation","title":"IMPORTANT: IN ORDER FOR THE ANIMATION TO WORK, THE SKELETON OF THE SKIN MUST BE COMPATABLE WITH THE ANIMATION!!!!","text":"<pre><code>&lt;actor name=\"actor\"&gt;\n    &lt;skin&gt;\n        &lt;filename&gt;moonwalk.dae&lt;/filename&gt;\n        &lt;scale&gt;1.0&lt;/scale&gt;\n    &lt;/skin&gt;\n    &lt;animation name=\"walking\"&gt;\n        &lt;filename&gt;walk.dae&lt;/filename&gt;\n        &lt;scale&gt;1.000000&lt;/scale&gt;\n    &lt;/animation&gt;\n&lt;/actor&gt;\n</code></pre> <p>The animation can also be scaled.</p>"},{"location":"faq/gazebo/Spawning_Animated_Human/#scripts","title":"Scripts","text":"<p>Scripts are tasks that you can assign an actor to do, in this case it is to make the actor walk around to specific points at specific times. The syntax for this is:</p> <p><pre><code>&lt;actor name=\"actor\"&gt;\n    &lt;skin&gt;\n        &lt;filename&gt;moonwalk.dae&lt;/filename&gt;\n        &lt;scale&gt;1.0&lt;/scale&gt;\n    &lt;/skin&gt;\n    &lt;animation name=\"walking\"&gt;\n        &lt;filename&gt;walk.dae&lt;/filename&gt;\n        &lt;scale&gt;1.000000&lt;/scale&gt;\n    &lt;/animation&gt;\n    &lt;script&gt;\n        &lt;trajectory id=\"0\" type=\"NAME\"&gt;\n            &lt;waypoint&gt;\n                &lt;time&gt;0&lt;/time&gt;\n                &lt;pose&gt;0 2 0 0 0 -1.57&lt;/pose&gt;\n            &lt;/waypoint&gt;\n            &lt;waypoint&gt;\n                &lt;time&gt;2&lt;/time&gt;\n                &lt;pose&gt;0 -2 0 0 0 -1.57&lt;/pose&gt;\n            &lt;/waypoint&gt;\n            ...\n                ...\n                ...\n            ...\n        &lt;/trajectory&gt;\n    &lt;/script&gt;\n&lt;/actor&gt;\n</code></pre> You can add as many waypoitns as you want so long as they are at different times. The actor will navigate directly to that point at the specified time of arrive in <code>&lt;time&gt;0&lt;/time&gt;</code> and pose using <code>&lt;pose&gt;0 0 0 0 0 0&lt;/pose&gt;</code>.</p>"},{"location":"faq/gazebo/Spawning_Animated_Human/#plugin-addons","title":"Plugin addons","text":"<p>The actor can also take on plugins such as obstacle avoidance, random navigation, and potentially teleop. The parameters for each plugin may be different, but the general syntax to give an actor a plugin is: </p> <pre><code>&lt;actor name=\"actor\"&gt;\n    &lt;skin&gt;\n        &lt;filename&gt;moonwalk.dae&lt;/filename&gt;\n        &lt;scale&gt;1.0&lt;/scale&gt;\n    &lt;/skin&gt;\n    &lt;animation name=\"walking\"&gt;\n        &lt;filename&gt;walk.dae&lt;/filename&gt;\n        &lt;scale&gt;1.000000&lt;/scale&gt;\n    &lt;/animation&gt;\n    &lt;plugin name=\"PLUGIN_NAME\" filename=\"NAME_OF_PLUGIN_FILE\"&gt;\n        ...\n    &lt;/plugin&gt;\n&lt;/actor&gt;\n</code></pre> <p>With all of this you should be able to place a human or any model of actor within any Gazebo world. For reference, you can refer to the Gazebo actor tutorial for demonstration material.</p>"},{"location":"faq/gazebo/Spawning_Animated_Human/#references","title":"References","text":"<ul> <li>Make an animated model (actor)</li> </ul>"},{"location":"faq/gazebo/change_model_color/","title":"Change model color","text":""},{"location":"faq/gazebo/change_model_color/#author-yifei-han","title":"Author: Yifei Han","text":""},{"location":"faq/gazebo/change_model_color/#intro-pre-built-models-in-gazebo","title":"Intro - Pre-built models in Gazebo","text":"<p>A lot of times we may use the simple pre-built models within Gazebo to simulate real world objects without putting much effort in mimicing the details of the objects. At the top tool bar, you may click and drag either a cube, cylinder or sphere into the model. You can easily size the object by changing its configuration by selecting  it and entering information on the left tool bar but you may wonder how to actually change the visual of such objects.</p>"},{"location":"faq/gazebo/change_model_color/#changing-the-color-of-basic-model-in-gazebo","title":"Changing the color of basic model in Gazebo","text":"<p>Although this simple tutorial will help you learn how to change the visual of the object, you may follow similar stepes to alter other physical properties without the  need to actually dig into the verbose XML files.</p> <p>First, you would need to highlight the obejct and right click. Select the option <code>Edit Model</code>. You have now entered the model editor. Select the object again and right click the option <code>Link Inspector</code>. You should see something like this:</p> <p></p> <p>Click on Visual tab and scroll down until you see three options for RGB input - Ambient, Diffuse and Specular. Ambient refers to the color you would see if no light is directly pointing at the object, which is the color you would see if the object is in shadow. Diffuse refers to the color you would see if there is a pure white light pointing at the object, which is the color you would see if the object is in direct sunlight. Specular deals with the color intensity of the reflection, something we may not be interested in for the sake of changing color of the object. </p> <p>Now you have the option to enter the RGB range or you can click on the three dots to the right and bring up the color panel for easy color selection.</p> <p></p> <p>The diffuse will change automatically with ambient color. This may be problematic sometimes if you are trying to create a mask for camera image as the HSV value would change for the object depending on the angle. To solve this problem you can manually change the ambient to match with diffuse.</p> <p>Save the model and you would see the color of your cube changed! Now keep doing your OpenCV masks!</p> <p></p>"},{"location":"faq/gazebo/create-gazebo.world/","title":"Create Gazebo Maps","text":"<p>This tutorial is largely based on what I have learnt here: Building a world. Please refer to this official tutorial if you need more details.</p>"},{"location":"faq/gazebo/create-gazebo.world/#1-open-a-gazebo-simulation","title":"1. Open a Gazebo simulation","text":"<p>First, open Gazebo - either search for gazebo in the Unity Launcher GUI or simply type <code>gazebo</code> onto the terminal. Click on <code>Edit</code> --&gt; <code>Building Editor</code> and you should see the following page. Note there are three areas:</p> <ul> <li>Platte: You can choose models that you wish to add into the map here.</li> <li>2D View: The only place you make changes to the map.</li> <li>3D View: View only.</li> </ul> <p></p>"},{"location":"faq/gazebo/create-gazebo.world/#2-import-a-floor-plan","title":"2. Import a floor plan","text":"<p>You may create a scene from scratch, or use an existing image as a template to trace over. On the Platte, click on <code>import</code> and selet a 2D map plan image in the shown prompt and click on <code>next</code>.</p> <p></p> <p>To make sure the walls you trace over the image come up in the correct scale, you must set the image's resolution in pixels per meter (px/m). To do so, click/release on one end of the wall. As you move the mouse, an orange line will appear as shown below. Click/release at the end of the wall to complete the line. Once you successfully set the resolution, click on <code>Ok</code> and the 2D map plan image you selected should show up in the 2D-View area.</p> <p></p>"},{"location":"faq/gazebo/create-gazebo.world/#3-add-edit-walls","title":"3. Add &amp; Edit walls","text":"<ul> <li>Select Wall from Platte.</li> <li>On the 2D View, click/release anywhere to start the wall. As you move the mouse, the wall's length is displayed.</li> <li>Click again to end the current wall and start an adjacent wall.</li> <li>Double-click to finish a wall without starting a new one.</li> <li>Double-clicking on an existing wall allows you to modify it.</li> </ul> <p>You can manipulate other models likewise. For more detailed instructions, please refer to http://gazebosim.org/tutorials?tut=build_world for more details</p> <p></p>"},{"location":"faq/gazebo/create-gazebo.world/#4-prepare-a-package","title":"4. Prepare a package","text":"<p>You need to create a package for your Gazebo world so that you can use <code>roslaunch</code> to launch your it later.</p> <ul> <li>Go to your catkin workspace</li> </ul> <p><code>$ cd ~/catkin_ws/src</code></p> <ul> <li>Create a package using the following command.</li> </ul> <p><code>$ catkin_create_pkg ${your_package_name}</code></p> <ul> <li>Go to your package and create three folders launch, worlds and models.</li> </ul> <pre><code>cd ${your_package_name}\nmkdir launch}\nmkdir worlds\nmkdir models\n</code></pre>"},{"location":"faq/gazebo/create-gazebo.world/#5save-your-map","title":"5.Save your map","text":"<p>Once you finish editing the map, give a name to your model on the top on the Platte and click on <code>File</code> -&gt; <code>Save As</code> to save the model you just created into <code>../${your_package_name}/models</code>.</p> <p></p> <p>Click on <code>File</code> -&gt; <code>Exit Building Editor</code> to exit. Please note that once you exit the editor, you are no longer able to make changes to the model. Click on <code>File</code> -&gt; <code>Save World As</code> into <code>../${your_package_name}/worlds</code>.</p> <p>I will refer to this world file as <code>${your_world_file_name}.world</code> from now on.</p>"},{"location":"faq/gazebo/create-gazebo.world/#6create-a-launch-file-for-your-gazebo-map","title":"6.Create a launch file for your gazebo map","text":"<p>Go to <code>../${your_package_name}/launch</code> and make a new file <code>${your_launch_file}</code> Copy and paste the following code into your launch file and substitute <code>${your_package_name}</code> and <code>{your_world_file_name}</code> with their actual names.</p> <pre><code> &lt;launch&gt;\n    &lt;arg name=\"model\" default=\"$(env TURTLEBOT3_MODEL)\" doc=\"model type [burger, waffle, waffle_pi]\"/&gt;\n    &lt;arg name=\"x_pos\" default=\"0.0\"/&gt;\n    &lt;arg name=\"y_pos\" default=\"0.0\"/&gt;\n    &lt;arg name=\"z_pos\" default=\"0.0\"/&gt;\n    &lt;include file=\"$(find gazebo_ros)/launch/empty_world.launch\"&gt;\n        &lt;arg name=\"world_name\" value=\"$(find ${your_package_name})/worlds/${your_world_file_name}.world\"/&gt;\n\n        &lt;arg name=\"paused\" value=\"false\"/&gt;\n        &lt;arg name=\"use_sim_time\" value=\"true\"/&gt;\n        &lt;arg name=\"gui\" value=\"true\"/&gt;\n        &lt;arg name=\"headless\" value=\"false\"/&gt;\n        &lt;arg name=\"debug\" value=\"false\"/&gt;\n    &lt;/include&gt;\n\n     &lt;param name=\"robot_description\" command=\"$(find xacro)/xacro --inorder $(find turtlebot3_description)/urdf/turtlebot3_$(arg model).urdf.xacro\" /&gt;\n\n     &lt;node name=\"spawn_urdf\" pkg=\"gazebo_ros\" type=\"spawn_model\" args=\"-urdf -model turtlebot3_$(arg model) -x $(arg x_pos) -y $(arg y_pos) -z $(arg z_pos) -param robot_description\" /&gt;\n&lt;/launch&gt;\n</code></pre>"},{"location":"faq/gazebo/create-gazebo.world/#7-test","title":"7. Test","text":"<p>Go to the workspace where your new package was created e.g. <code>cd ~/catkin_ws</code></p> <p>run <code>catkin_make</code> and then <code>roslaunch ${your_package_name} ${your_launch_file}</code></p> <p>You should see the Gazebo map you just created along with a turtlebot loaded.</p>"},{"location":"faq/gazebo/create-gazebo.world/#using-the-model-editor-instead","title":"Using the Model Editor instead","text":"<p>The building editor is a faster, easier to use tool than the model editor, as it can create a map in mere minutes. With the model editor, you have more technical control over the world, with the trade off being a more tedious process. The model editor can help make more detailed worlds, as you can import .obj files that can be found on the internet or made in 3d modeling software such as Blender. For the purposes of use in this class, USE THE BUILDING EDITOR For your own recreational robotic experimentation purposes, of course, do whatever you like.</p> <p>If you do wish to use the model editor, here are two tips that will help you to get started making basic, serviceable worlds.</p>"},{"location":"faq/gazebo/create-gazebo.world/#1-change-the-color","title":"1. Change the Color","text":"<p>The basic shapes that gazebo has are a greyish-black by default- which is difficult to see on gazebo's greyish-black background. To change the color, follow these steps: 1. Right click on the model 1. select \"open link inspector\" 1. go to the \"visual\" tab 1. scroll down to \"material\" and open that section 1. use the RGB values labeled \"ambient\" to alter the color - set them all to 1 to make it white.</p>"},{"location":"faq/gazebo/create-gazebo.world/#2-alter-the-shape","title":"2. Alter the shape","text":"<p>use the shortcut s to open the scaling tool - grab the three axis to stretch the shape. Hold ctrl to snap it to the grid. use the shortcut t to switch to the translation tool - this moves the model around. Hold ctrl to snap it to the grid. use the shortcut r to open the rotation tool. grab the rings to rotate the object.</p>"},{"location":"faq/gazebo/create-gazebo.world/#3-make-it-static","title":"3. Make it Static","text":"<p>If an object isn't static, it will fall over/ obey the laws of physics if the robot collides with it - to avoid this, click the object in the left hand menu and click the is_static field.</p>"},{"location":"faq/gazebo/create-gazebo.world/#4-use-the-building-editor-instead","title":"4. Use the Building Editor instead","text":"<p>Does the model editor seem like a hassle already? Then just use the building editor.</p>"},{"location":"faq/gazebo/diy-gazebo-world/","title":"Building Your Own Custom Gazebo World with DIY models from Blender","text":""},{"location":"faq/gazebo/diy-gazebo-world/#by-al-colon","title":"by Al Colon","text":"<p>FOR YOUR CONSIDERATION: This tutorial is for .dae files. It would probably work with .stl or .obj files but since I was using Blender, it was the best mesh format as it was really easy to transfer, update and adjust. It also includes material within itself so you don't have to worry about adding colors through gazebo. If you are looking for a solid, rigid object, I definitely recommend using Blender as it is very flexible. There are plenty of tutorials to make pretty much any shape so be creative with your inquiries and you should be able to create whatever you need.</p>"},{"location":"faq/gazebo/diy-gazebo-world/#steps","title":"Steps!","text":""},{"location":"faq/gazebo/diy-gazebo-world/#step-1-creating-your-model","title":"Step 1! Creating your model","text":"<p>Making a model can be difficult if you are not sure where to start. My advice is to go as simple as possible. Not only will it make gazebo simulation simpler, it will save you time when you inevitable need to make changes.</p> <p>Make sure that the object you are designing looks exactly like you want it to in Gazebo. Obviously, this speaks to overall design, but almost more important than that, make sure that your orientation is correct. This means making sure that your model is centralized the way you want it on the x, y and z\u2013axis. Trying to update the pose though code never worked and it is one adjustment away through editing your model. Note: you can add collisions but if you want to create a surface for the robots to drive on, it will be easier to just make it as thin as possible.</p>"},{"location":"faq/gazebo/diy-gazebo-world/#step-2-importing-your-model","title":"Step 2! Importing your model","text":"<p>*Learning from my mistakes | * It might be tempting to use the Add function via the Custom Shapes but this was probably the most frustrating part of my project. What I suggest, as it would have save me weeks of work, it would behoove you to add the mesh (the 3d model files) to a world file so that you can get you measurement and adjustments.</p> <p>The best tutorial that I found was http://gazebosim.org/tutorials/?tut=import_mesh It is fairly simple and the most reliable way to get your model to show up. From here, however, save a new world from this file as it will have a more complete .world file. Robots should be launched from roslaunch but double check how they interact by adding the model via the Insert tab. It will be white and more boxy but it will help you with positioning, interation and scale.</p>"},{"location":"faq/gazebo/diy-gazebo-world/#step-3-setup-your-files","title":"Step 3! Setup your files","text":"<p>It is important to be incredibly vigilant while doing this part as you will be editing some files in the gazebo-9 folder. You do not want to accidental delete or edit the wrong file.</p> <p>Using one of the models in gazebo-9 or my_ros_data/.gazebo/models as a template, change the information in the model.sdf and model.config files. The hierarchy should look something like:</p> <p><pre><code>&lt;your_model_name&gt;\n    &lt;material&gt;\n        &lt;material1.png&gt;\n        &lt;material2.png&gt;\n        &lt;etc...&gt;\n    &lt;meshes&gt;\n        &lt;your_model.dae&gt;\n    &lt;model.sdf&gt;\n    &lt;model.config&gt;\n</code></pre> According to your model, you might need to edit more but, for me, it sufficed to edit the mesh uri with my dae file, update the .sdf and .config files and add png versions of the colors I used in blender.</p> <p>In order for Gazebo to open files through roslaunch, they must be in the proper spot. You can double check where exactly Gazebo is looking by checking the $GAZEBO_RESOURCE_PATH and $GAZEBO_MODEL_PATH in the setup.sh in the gazebo-9 folder. It will probably be something like usr/share/gazebo-9 but just copy and paste the entire file into this folder !! after !! you !! have !! confirmed !! that !! it !! works !! in !! your !! world !! file !! (To check without a roslaunch, navigate to the directory that holds your .world file AND .dae file and in the terminal $ gazebo your_world.world)</p> <p>You can also add your world file to the ~/gazebo-9/worlds.</p>"},{"location":"faq/gazebo/diy-gazebo-world/#step-3-roslaunch","title":"Step 3! ROSLAUNCH","text":"<p>Launch files are pretty straightforward actually. Having an understanding about how args work is really key and a quick google search will probably get a better explanation for your needs. You can also take a gander at launch files you have already used for other assignments.</p> <p>KEY</p> <ol> <li> <p>Make sure the launch file is looking in the right places for your models.</p> </li> <li> <p>Debug as often as you can</p> </li> <li> <p>Reference other launch files to know what you need for your file.</p> </li> <li>Update arg fields as needed</li> </ol>"},{"location":"faq/gazebo/gazebo_tf/","title":"TF in Gazebo","text":"<p>MinJun Song</p>"},{"location":"faq/gazebo/gazebo_tf/#introduction","title":"Introduction","text":"<p>When we manipulate a robot in the gazebo world, there are times when the odometry of the robot does not correspond correctly with the actual position of the robot.  This might be due to the robot running into objects which leads to mismatch between the rotation of the wheels and the calculation for the location derived from it.  So it will be useful to get the absolute position of the robot that is given by the gazebo environment.</p>"},{"location":"faq/gazebo/gazebo_tf/#getting-started","title":"Getting Started","text":"<p>For the robot to navigate autonomously, it has to know its exact location. To manifest this process of localization, the simplest method is to receive values from the wheel encoder that calculates the odometry of the robot from the starting position.</p> <p>In terms of mobile robots like turtlebot that we use for most of the projects, localization is impossible when the robot collides with other objects such as walls, obstacles, or other mobile robots.</p> <p>So in this case, we can use the <code>p3d_base_controller</code> plugin that gazebo provides to receive the correct odometry data of the robot to localize in the environment.</p>"},{"location":"faq/gazebo/gazebo_tf/#code","title":"Code","text":"<p>What I needed was relative positions of the robots.  Originally we need to get the odometry data of the two robots and use rotation, translation matrix to find the relative coordinate values and yaw by ourselves.</p> <p>In <code>gazebo.xacro</code> file paste the following commands:</p> <pre><code>&lt;gazebo&gt;\n    &lt;plugin name=\"p3d_base_controller\" filename=\"libgazebo_ros_p3d.so\"&gt;\n    &lt;alwaysOn&gt;true&lt;/alwaysOn&gt;\n    &lt;updateRate&gt;50.0&lt;/updateRate&gt;\n    &lt;bodyName&gt;base_footprint&lt;/bodyName&gt;\n    &lt;topicName&gt;base_footprint_odom&lt;/topicName&gt;\n    &lt;gaussianNoise&gt;0.01&lt;/gaussianNoise&gt;\n    &lt;frameName&gt;world&lt;/frameName&gt;\n    &lt;xyzOffsets&gt;0 0 0&lt;/xyzOffsets&gt;\n    &lt;rpyOffsets&gt;0 0 0&lt;/rpyOffsets&gt;\n    &lt;/plugin&gt;\n&lt;/gazebo&gt;\n</code></pre> <p>Explaination of the Code</p> <ul> <li>bodyName: the name of the body that we want the location of</li> <li>topicName: the name of odometry topic</li> </ul> <p>With the code above, we can represent two robot\u2019s location.  To find robot 1\u2019s location relative to robot 2\u2019s location, we can use the following code:</p> <pre><code>try:\n    robot_trans = tfBuffer.lookup_transform(\u2018robot1\u2019, \u2018robot2\u2019, rospy.Time())\nexcept (tf2_ros.LookupException, tf2_ros.ConnectivityException, tf2_ros.ExtrapolationException):\n    continue\n</code></pre>"},{"location":"faq/gazebo/gazebo_world/","title":"Gazebo world","text":""},{"location":"faq/gazebo/gazebo_world/#creating-a-gazebo-world","title":"Creating a Gazebo World","text":""},{"location":"faq/gazebo/gazebo_world/#author-mahima-devanahalli","title":"Author: Mahima Devanahalli","text":"<p>To create a gazebo world you can first create new models on gazebo, mannually and save those as sdf files. You can also use premade gazebo models. Once you do this, you can make a world file which incorporates all the models you need. </p>"},{"location":"faq/gazebo/gazebo_world/#what-is-the-difference-between-a-sdf-file-and-world-file","title":"What is the difference between a .sdf file and .world file?","text":"<p>Both these files share the same format. However, the .sdf files define models that you can create in gazebo. When these models are in their own file, they can be reused in multiple worlds. A .world file describes an entire scene with objects and various models. You can copy the contents of an sdf file into your world file to incorporate that model into your world. </p> <p>When you create your own models on gazebo and save them, they automatically save as sdf files which can then be used as mentioned above.  To do this:</p> <p>Open an empty world in gazebo Click edit -&gt; building editor or model editor -&gt; then save it in your desired location</p>"},{"location":"faq/gazebo/gazebo_world/#helpful-links","title":"Helpful Links","text":"<p>https://www.youtube.com/watch?v=3YhW04wIjEc http://gazebosim.org/tutorials?cat=build_world https://nlamprian.me/blog/software/ros/2019/10/06/gazebo-virtual-worlds/</p>"},{"location":"faq/gazebo/gazebo_world_builder/","title":"Creating Gazebo world from scratch","text":""},{"location":"faq/gazebo/gazebo_world_builder/#muthhukumar-malaiiyyappan-malai","title":"Muthhukumar Malaiiyyappan (Malai)","text":"<p>Building a gazebo world might be a little daunting of a task when you are getting started. One might want to edit existing gazebo worlds but I will save you the trouble and state that its not going to work.</p> <ol> <li>Open a vnc terminal</li> </ol> <p><pre><code>gazebo\n</code></pre> This would open a gazebo terminal and once you get in there you would want to bring your cursor to the top left hand corner and find the Building Editor in the Edit tab.</p> <p>Once in the building editor click on the wall to create the boundaries on the top half of the editor. Left click to get out of the building mode. If you would like to create walls without standard increments, press shift while dragging the wall. </p> <p>If you would like to increase or decrease the thickness of the wall. Click on the walls you would like to change and it will open up a modal with options to change. </p> <p>After you are satisfied with their boundaries, save it as a model into your models folder within your project.</p> <p>When the models have been saved. You would be brought back to gazebo with the model that you have built.</p> <ol> <li> <p>Change the pose of the model if need be according to your needs then save the world.</p> </li> <li> <p>Upload your new model into this part of the launch file</p> </li> </ol> <pre><code>    &lt;include file=\"$(find gazebo_ros)/launch/empty_world.launch\"&gt;\n        &lt;arg name=\"world_name\" value=\"$(find {project_name})/worlds/{world_name}.world\"/&gt;\n\n        &lt;arg name=\"paused\" value=\"false\"/&gt;\n        &lt;arg name=\"use_sim_time\" value=\"true\"/&gt;\n        &lt;arg name=\"gui\" value=\"true\"/&gt;\n        &lt;arg name=\"headless\" value=\"false\"/&gt;\n        &lt;arg name=\"debug\" value=\"false\"/&gt;\n    &lt;/include&gt;\n</code></pre> <p>If you find that your robot is not in the right place. Open the launch file, make the changes to the model accordingly and save it again as the world again.</p> <p>Thats how you can build a world from scratch, hope this helped. </p>"},{"location":"faq/gazebo/move_base-in-gazebo/","title":"Using move_base in gazebo","text":"<p>move_base is a ROS navigation package that allows a robot to navigate through an environment. It provides path planning and obstacle avoidance in the navigation. The move_base node creates a server that accepts navigation goals and attempts to guide the robot from its current position to the specified target position while avoiding obstacles. Using move_base, you can also create a map of the environment. To send a goal to move_base, you can use RViz or a script, specifying a target pose in the environment.</p> <p>To launch move_base you can use: Open the gazebo world you want to navigate <code>roslaunch turtlebot3_gazebo turtlebot3_&lt;gazebo_world&gt;.launch</code></p> <p>Start the mapping algorithm with  <code>roslaunch turtlebot3_slam turtlebot3_gmapping.launch</code></p> <p>Start RViz <code>roslaunch turtlebot3_gazebo turtlebot3_gazebo_rviz.launch</code> Make sure to add the map display in RViz</p> <p>To create a goal using RViz, use the 2D Nav Goal button at the top of the screen, you can also use another script or use teleop to move the robot around. When the robot moves around, the map will be visibly filled out. Impassible walls will appear as black lines.</p> <p>To save a created map run: <code>rosrun map_server map_saver -f &lt;filename&gt;</code></p> <p>To open up gazebo with a created map run <code>roslaunch turtlebot3_navigation turtlebot3_navigation.launch map_file:=\"&lt;filepath.yaml&gt;\"</code></p> <p>The yaml file created when saving a map will look something like this: <pre><code>    image: test_map.pgm\n    resolution: 0.050000\n    origin: [-10.000000, -10.000000, 0.000000] \n    negate: 0\n    occupied_thresh: 0.65\n    free_thresh: 0.196\n</code></pre> Changing the origin will change where the robot centers the map. The occupied and free thresholds determine whether a cell on the map will be considered impassible or not.</p>"},{"location":"faq/gazebo/reset-world-gazebo/","title":"Resetting your Gazebo World in Python Files","text":""},{"location":"faq/gazebo/reset-world-gazebo/#author-seho-kim","title":"Author: Seho Kim","text":"<p>When you are making games or programs with python using ROS, you may need to reset your world at certain point in your code. The easiest way to reset your Gazebo world would be ctrl+R. However, this may cause unwanted error such as killing one of your node or losing connection.</p> <p></p> <p>To prevent this, you can open up a new terminal and call service:</p> <pre><code>rosservice call /gazebo/reset_world\n</code></pre> <p>However, you may not want users to manually call service above each time you are required to reset.</p>"},{"location":"faq/gazebo/reset-world-gazebo/#to-programmatically-reset-your-gazebo","title":"To Programmatically Reset Your Gazebo","text":"<p>Sometimes, your program should reset your Gazebo when certain conditions are met. In order to achieve this, implementing code below will do its job.</p> <pre><code>import rospy\nfrom std_srvs.srv import Empty\n\nrospy.wait_for_service('/gazebo/reset_world')\nreset_world = rospy.ServiceProxy('/gazebo/reset_world', Empty)\nreset_world()\n</code></pre>"},{"location":"faq/gazebo/reset-world-gazebo/#empty","title":"Empty","text":"<p>We import Empty from std_srvs.srv to send signal to correct services. Empty type is one of the most common service pattern for sending signal to ROS Node. Empty service does not exchange any actual data between service and a client.</p>"},{"location":"faq/gazebo/reset-world-gazebo/#rospywait_for_servicegazeboreset_world","title":"rospy.wait_for_service('/gazebo/reset_world')","text":"<p>Line above allows code to wait until specific service is active. </p>"},{"location":"faq/gazebo/reset-world-gazebo/#reset_world-rospyserviceproxygazeboreset_world-empty","title":"reset_world = rospy.ServiceProxy('/gazebo/reset_world', Empty)","text":"<p>Here we declare reset_world as service definition. First parameter is service name we want to call. In our case, we call '/gazebo/reset_world' to reset our Gazebo world. Second parameter is for srv which usually contain a request message and a response message. For resetting Gazebo world, we do not need any request message nor response message. For resetting Gazebo world, second parameter should be 'Empty'</p>"},{"location":"faq/gazebo/rviz-markers/","title":"Rviz markers","text":""},{"location":"faq/gazebo/rviz-markers/#using-rviz-markers","title":"Using Rviz Markers","text":""},{"location":"faq/gazebo/rviz-markers/#author-jonah-jakab","title":"Author: Jonah Jakab","text":"<p>Rviz markers are a useful tool for displaying custom information to the user of a program. This tutorial teaches you how to use them.</p>"},{"location":"faq/gazebo/rviz-markers/#what-are-rviz-markers","title":"What are Rviz markers?","text":"<p>Markers are visual features that can be added to an Rviz display. They can take many forms - cubes, spheres, lines, points, etc.</p> <p>Rviz markers are published as a message of the Marker type. First, you want to create a publisher. It can publish on any topic, but the message type should be Marker. Next, you should fill out the fields of the Marker message type. Notably, you need to define the frame_id. You can either use the default \"map\" frame, or construct a new one that fits your requirements.</p> <p>Now, you can publish the Marker message. Run your project, then open a new terminal tab and open Rviz. You should see an 'Add' option at the bottom left of the window. Click it, then scroll down and select Marker and select 'Ok'. Your marker should appear on the display. If you used a new frame as the frame_id, you need to select it under the Marker you added before it will show up. </p>"},{"location":"faq/gazebo/rviz-markers/#helpful-links","title":"Helpful Links","text":"<p>http://docs.ros.org/en/melodic/api/visualization_msgs/html/msg/Marker.html</p>"},{"location":"faq/git/git-command-line/","title":"Why can't I modify git from command line when I have permission?","text":""},{"location":"faq/git/git-command-line/#background","title":"Background","text":"<p>Ever wonder why sometimes when you try to make changes to a git you KNOW you have permissions on doesn't work via command line? Whenever you try, this error occurs:</p> <p><code>remote: Permission to &lt;git name&gt;.git denied to &lt;username&gt;.</code></p> <p>Well, fear not. I have the perfect solution for you, so you don't have to worry about logging out and logging in again over and over to try and fix the issue! The answer? Personal Access Tokens! With these, you can make changes to your git from anywhere with just one line! Below are the steps needed to do so:</p>"},{"location":"faq/git/git-command-line/#getting-your-personal-access-token","title":"Getting your Personal Access Token","text":"<p>First, you need to copy an active Personal Access Token from git, or if you don't have one, you can create one. </p> <ol> <li>Go to github.com, and make sure you are logged in</li> <li>Click your profile photo in the top right, and click settings in the drop down mene</li> <li>Scroll down to the bottom of the menu on your left and click on Developer Settings</li> <li>Click the Personal access tokens drop down menu, and click Tokens (classic)</li> <li>On the top right, click Generate new token</li> <li>Give your new token a name, set the expiration to as long as you will be working on your project, and make sure all of the checkboxes underneath repo are checked</li> <li>Click Generate token</li> </ol> <p>Voila! You have created a fresh new token. Copy this and you can use it in your terminal or command line.</p>"},{"location":"faq/git/git-command-line/#using-your-token","title":"Using your token","text":"<p>Now that you have your token go into a terminal or command line of your choice, and navigate to your git directory using the <code>cd</code> command.</p> <p>Once there, run the following line:</p> <p><code>git remote set-url origin https://&lt;username&gt;:&lt;token&gt;@github.com/&lt;your github repository name&gt;.git</code></p> <p>And fill in the &lt;&gt; as needed. And thats it. Now just push your changes and your done!</p> <p><code>git add .</code> <code>git commit -m \"message\"</code> <code>git push</code></p>"},{"location":"faq/git/ssh-github/","title":"Setting github for ssh","text":"<p>Note</p> <p>This only applies to repos that you have permission to access, that is, to do a git push or pull</p>"},{"location":"faq/git/ssh-github/#background","title":"Background","text":"<p>When you access a github repo remotely, you have to login so that github can determine that you are allowed to access it.</p>"},{"location":"faq/gps/GPS-knowledge/","title":"GPS Knowledge","text":""},{"location":"faq/gps/GPS-knowledge/#august-soderberg-2021-2022","title":"August Soderberg / 2021 - 2022","text":""},{"location":"faq/gps/GPS-knowledge/#this-is-a-collection-all-current-knowledge-i-have-found-in-my-gps-research-some-sections-take-a-very-high-level-approach-and-others-are-quite-technical-please-read-prior-sections-if-any-sections-are-difficult-to-understand","title":"This is a collection all current knowledge I have found in my GPS research. Some sections take a very high level approach and others are quite technical. Please read prior sections if any sections are difficult to understand.","text":""},{"location":"faq/gps/GPS-knowledge/#how-does-gps-work","title":"How does GPS work?","text":"<p>The high level of how GPS functions relies on several GPS satellites sending out a radio signals which encode the time at which the signal was sent and the position of the satellite at that time. You should imagine this radio signal as a sphere of information expanding out at the speed of light with the satellite, which emitted the signal, at the center.</p> <p>If I were just looking at a single satellite, I would receive this radio signal and be able to calculate the difference in time between the moment I received it, and the time the signal left the satellite which again is encoded in the radio signal. Let\u2019s say I calculate that the signal traveled 10,000 miles before I received it. That would indicate to me that I could be in any position in space exactly 10,000 miles away from the satellite which sent the signal. Notice that this is a giant sphere of radius 10,000 miles; I could be standing anywhere on this massive imaginary sphere.  Thus, GPS is not very useful with a single satellite.</p> <p>Now let\u2019s say I receive a signal from 2 satellites, I know their positions when they sent their messages and the time it took each message to reach me. Each satellite defines a sphere on which I could be standing, however, with both spheres I now know I must be somewhere on the intersection of these two spheres. As you may be able to picture, the intersection of two spheres is a circle in space, this means with 2 satellites I could be standing anywhere on this circle in space, still not very useful.</p> <p>Now if I manage to receive a signal from 3 satellites, I suddenly have three spheres of possible locations which all intersect. Because 3 spheres will intersect in a single point, I now have my exact point in space where I must be standing.</p> <p>This is how GPS works. The name of the game is calculating how far I am from several satellites at once, and finding the intersection; luckily for us, people do all of these calculations for us.</p>"},{"location":"faq/gps/GPS-knowledge/#how-many-gps-satellites-are-there","title":"How many GPS satellites are there?","text":"<p>This is a bit of a trick question since technically GPS refers specifically to the global positioning satellites which the United States have put in orbit. The general term here is Global Navigation Satellite System (GNSS) which encompasses all satellite constellations owned and operated by any country. This includes the US\u2019s GPS, Russia's GLONASS, Europe's Galileo, and China's BeiDou. Any GPS sensor I reference here is actually using all of these satellites to localize itself, not just the GPS constellation.</p>"},{"location":"faq/gps/GPS-knowledge/#why-is-gps-inaccurate","title":"Why is GPS inaccurate?","text":"<p>There is a big problem with GPS. The problem lies in the fact that the radio signals are not traveling in a vacuum to us, they are passing through our atmosphere! The changing layers of our atmosphere will change the amount of time it takes the radio signal to travel to us. This is a huge problem when we rely on the time it took the signal to reach us to calculate our distance from each satellite. If we look at a really cheap GPS sensor (there is one in the lab, I will mention it later) you will find that our location usually has an error of up to 20 meters. This is why people will \u201ccorrect\u201d their GPS signals.</p>"},{"location":"faq/gps/GPS-knowledge/#broad-hardware-overview","title":"Broad Hardware overview","text":"<p>There are two parts to every GPS sensor: the antenna and the module. The GPS antenna is responsible for receiving the radio signals from the satellites and passing them to the module which will perform all of the complex calculations and output a position. High quality antennas and modules are almost always purchased separately and can be quite expensive. There are also cheap all in one sensors which combine the antenna and module into something as small as a USB drive.</p>"},{"location":"faq/gps/GPS-knowledge/#multi-band-vs-single-band-antenna","title":"Multi-band vs single-band antenna","text":"<p>Satellites transmit multiple radio frequencies to help the GPS module account for the timing errors created by our atmosphere. Multi-band GPS means that you are listening to multiple radio frequencies (L1 and L2 refer to these different radio frequencies emitted by a single satellite) which can improve accuracy overall, single-band GPS is not as good. Keep in mind that both the antenna and the module will need to be multi-band and both units will be significantly more expensive because of that.</p>"},{"location":"faq/gps/GPS-knowledge/#plugging-a-gps-sensor-into-a-linux-computer","title":"Plugging a GPS sensor into a linux computer","text":"<p>When you plug any kind of GPS sensor into a linux computer using a USB cable you will see the device appear as a serial port. This can be located in the directory <code>/dev</code> most of the time it will be called <code>/dev/ttyACM0</code> or <code>/dev/ttyACM1</code>. If you run <code>$ cat /dev/ttyACM0</code> in your terminal you will see all of the raw information coming off of the GPS sensor. Usually this is just NMEA messages which are what show you the location of the GPS as well as lots of other information.</p>"},{"location":"faq/gps/GPS-knowledge/#nmea-messages","title":"NMEA messages","text":"<p>NMEA messages provide almost all of the information about our GPS that we could desire. There are several types of NMEA messages all with 5 character names. They will all begin GP or GN depending on whether they are using just the GPS constellation or all satellite constellations, respectively. Sometimes the first 2 characters are noted Gx for simplicity. The last 3 characters will show you the type of message. Here is a complete list of their definitions. The most important message for us is the GxGGA message, the complete definition of which you can view in the previous link. It will include the latitude, latitude direction (north or south), longitude, and longitude direction (east or west). There are several different ways of writing latitude and longitude values but online converters can convert between any of them, and they can always be viewed on Google Maps for verification. The other important piece of information in the GxGGA message is the \"fix quality\". This value tells you what mode your GPS is currently operating in. 0 indicates no valid position and 1 indicates uncorrected position (this is the standard GPS mode). 2, 4, and 5 are only present when you are correcting your GPS data, more on what this means later. You can use this Python script to read all of the NMEA messages and print relevant data to console. Obviously this can be edited to do much more complex things with this data.</p>"},{"location":"faq/gps/GPS-knowledge/#gps-correction","title":"GPS Correction","text":""},{"location":"faq/gps/GPS-knowledge/#non-corrected-gps-data","title":"Non-corrected GPS data","text":"<p>For less than $20 you can buy a cheap plug and play GPS sensor which does not attempt to correct its GPS data in any way. We purchased the G72 G-Mouse USB GPS Dongle to take some preliminary results. Below we see 5 minutes of continuous GPS data (in blue) taken from a fixed GPS location (in red). I will note it was a cloudy rainy day when the data was recorded and the true GPS location was under a large concrete overhang outdoors near other buildings. This is a particularly difficult situation which lead to the larger than normal maximum inaccuracy of ~25 meters. </p> <p></p> <p>NOTE: no matter how fancy or expensive your GPS sensor is, if it is not being corrected by some kind of secondary remote device, you will not see good accuracy. This is confusing because a lot of GPS sensors tout their \"centimeter accuracy in seconds\" which would imply you could just plug it in and achieve that accuracy. </p> <p>There are no shortcuts, for centimeter accuracy you need to correct your GPS data with an outside source.</p>"},{"location":"faq/gps/GPS-knowledge/#how-to-correct-gps-data","title":"How to correct GPS data","text":"<p>The most common and most accurate way to correct GPS data is by utilizing two GPS sensors in a process called Differential GPS (DGPS). There are three ways to achieve a differential GPS system according to RACELOGIC:</p> <ul> <li>SBAS \u2013 Correction messages are sent from Geostationary Satellites, for example, EGNOS or WASS.</li> <li>RTCMv2 \u2013 Correction messages are sent from a static base station, giving 40 \u2013 80 cm accuracy.</li> <li>RTK \u2013 Correction messages are sent from a static base station signal giving &lt;2cm accuracy on RTK enabled units. </li> </ul> <p>We will ignore SBAS and RTCMv2 (see above source for more detail) and focus entirely on RTK correction because it is the most common and most accurate differential GPS correction method.</p> <p>RTK stands for Real-Time Kinematic and provides ~1cm accuracy when it is set up properly and operated in reasonable conditions. This is our ticket to highly accurate GPS data.</p>"},{"location":"faq/gps/GPS-knowledge/#how-rtk-works","title":"How RTK works","text":"<p>RTK correction relies on two GPS sensors to provide our ~1cm accuracy. One sensor is part of the \"base station\" and the other is a \"rover\".</p>"},{"location":"faq/gps/GPS-knowledge/#base-station","title":"Base station","text":"<p>The base station consists of a GPS sensor in an accurately known, fixed location on earth which is continually reading in the radio signals from the satellites. The goal of the base station GPS is to compute, in real time, the error in the amount of time it takes the radio signal from each satellite to reach the base station. This is an incredibly complex calculation to figure out which timing errors each individual radio signal is experiencing. We cannot simply say that the measurement is off by 4 meters and assume that all nearby GPS sensors will experience the same 4 meter error vector. The base station computer must look at each satellite signal it is using to calculate location, look at the total error in location, and then reverse engineer the timing error that each radio signal exhibits. (Accurate GPS requires 3-4 different satellites to determine location, our calculation will thus produce at least 3-4 timing error values, one for each satellite).</p> <p>The base station will then send these timing errors in the form of an RTCM message (this is the standard RTK error message) to the \"rover\" so that the rover can perform its own calculations based on which satellites it is currently using. This will ultimately achieve the ~1cm accuracy.</p> <p>To summarize, RTK correction requires a fixed base station to determine the error in the amount of time it takes each radio signal from all satellites in view to reach the sensor. It then sends this list of errors to the rover GPS. The rover GPS will look at all of the radio signals it is using to calculate its position, adjust each time value by the error sent from the base station, and calculate a very accurate position.</p>"},{"location":"faq/gps/GPS-knowledge/#base-station-types","title":"Base station types","text":"<p>There are two ways to acquire RTK corrections. You can either set up a local base station or you can utilize RTK corrections from various public or subscription based base stations around the country.</p>"},{"location":"faq/gps/GPS-knowledge/#why-you-would-want-to-set-up-a-local-base-station","title":"Why you would want to set up a local base station","text":"<p>The subscription based base stations are often quite expensive and difficult to find, the good news is that most states have state owned public base stations you can receive RTK correction data from, here is the Massachusetts public base stations site. and here is a national Continuously Operating Reference Stations map The problem is that these base stations are often old and not very high quality. They often use solely single-band antenna which means that to have accurate RTK correction you need be within 10km of the public base station and the correction values get drastically better the closer you are. If you set up your own base station you will be able to use multi-band signals for higher accuracy, you will be much closer, and this is where you will see ~1cm accuracies. That being said, Olin College of Engineering uses public base stations for their work.</p>"},{"location":"faq/gps/GPS-knowledge/#setting-up-a-local-base-station","title":"Setting up a local base station","text":"<p>This will take you through the process of setting up a local Brandeis base station.</p>"},{"location":"faq/gps/GPS-knowledge/#gps-module","title":"GPS module","text":"<p>The GPS module is what processes all of the information read in from the antenna and gives you actual usable data. For cost and performance reasons we have selected the ZED-F9P module from u-blox. More specifically, we have selected the developer kit from SparkFun which is called the SparkFun GPS-RTK2 Board - ZED-F9P which is a board containing the ZED-F9P module and has convenient ports attached for everything you will need to plug in.</p> <p>Important information about this board: * A GPS-RTK2 board and a GPS-RTK board are not the same! Do not mix RTK2 and RTK, it will not work. * The board must be connected to an antenna (more below) to receive the radio signals. * This is a multi-band module which will allow us to have much more accurate data even if our rover goes near building, under trees, or far away from the base station. * The board plugs into a Raspberry Pi with a USB cable and is powered and sends data through that single cable.</p> <p>We will require two of these boards, one for the base station and one for the rover.</p>"},{"location":"faq/gps/GPS-knowledge/#antenna","title":"Antenna","text":"<p>We need a quality multi-band antenna to receive the multi-band signal, these can get very fancy and very expensive, we will be using this ArduSimple antenna.</p> <p>If you use the same antenna on your base station and your rover it will marginally improve accuracy since the noise characteristics will be very similar. </p>"},{"location":"faq/gps/GPS-knowledge/#communication-between-raspberry-pi-and-gps-module","title":"Communication between Raspberry Pi and GPS module","text":"<p>The GPS module will send data through USB to the Raspberry Pi and appear as serial port. You can watch this video to see what these GPS communications look like to the Raspberry Pi and how to process them in a python script.</p>"},{"location":"faq/gps/GPS-knowledge/#configuring-the-gps-module","title":"Configuring the GPS module","text":"<p>All configuration of the GPS module can be done while connected to a Windows computer running u-center which is a u-blox application. Sadly u-center only runs on Windows. This configuration is important because it will establish whether your GPS module is a rover or a base station and will allow you to set the base station's known location etc.</p>"},{"location":"faq/gps/GPS-knowledge/#physical-location-of-the-base-station","title":"Physical location of the base station","text":"<p>The base station will need to have a very precise known location for the antenna. This should be as close to your rover on average as possible. To find the exact location, you can use the Survey-in mode on the GPS or use a fixed location determined by Google Maps, the configuration video will cover this.</p>"},{"location":"faq/gps/GPS-knowledge/#sending-rtk-corrections-from-base-station-to-the-rover","title":"Sending RTK corrections from base station to the rover","text":"<p>Your base station will output RTCM messages which are the standard RTK correction messages which a rover type GPS module will be able to use to correct its own GPS data. These RTCM messages will be outputted over the serial port to the base station Raspberry Pi and you will need to set up some kind of messaging protocol to get these messages from the base station to the rover. I recommend using rtk2go.com to handle this message passing. More on this in the configuration video.</p>"},{"location":"faq/gps/GPS-knowledge/#receiving-rtk-corrections","title":"Receiving RTK corrections","text":"<p>Once the rover u-blox module is continually receiving RTK corrections as RTCM messages, it will use these messages to perform calculations and in turn output over serial port the ~1cm accurate GPS data in the form of an NMEA message. These NMEA messages are simple to parse and will clearly provide latitude and longitude data as well as a lot more information for more complex applications. The Raspberry Pi will be able to read these messages (as described in the video above) and now you have incredibly accurate GPS data to do with as you wish.</p>"},{"location":"faq/gps/GPS-knowledge/#credentials","title":"Credentials","text":"<p>For rtk2go.com, username: asoderberg@brandeis.edu password: [the standard lab password]</p> <p>For macorsrtk.massdot.state.ma.us, username: asod614 password: mzrP8idxpiU9UWC</p>"},{"location":"faq/gps/GPS-knowledge/#ntrip-client-script","title":"NTRIP Client script","text":"<p><code>/home/rover/RTKLIB/app/str2str/gcc/str2str -in ntrip://rover:ROSlab134@rtk2go.com:2101/brandeis -out serial://ttyACM0:115200</code></p>"},{"location":"faq/gps/GPS-knowledge/#configuration-and-hardware-overview-video","title":"Configuration and hardware overview video","text":"<p>https://youtu.be/qZ2at1xV8DY</p>"},{"location":"faq/gps/gps-data/","title":"GPS Data with iPhone (GPS2IP)","text":""},{"location":"faq/gps/gps-data/#brandon-j-lacy","title":"Brandon J. Lacy","text":""},{"location":"faq/gps/gps-data/#overview","title":"Overview","text":"<p>The utilization of GPS data on a robot is a common requirement within projects. However, a majority of hardware components that can be configured with the robot produce lackluster results. The iPhone uses sophisticated technology to produce more accurate GPS data, which makes it a prime candidatee for situation in which a robot is in need of accurate information. The application, GPS2IP, uses the technology of the iPhone and communicates it over the internet. Through this application and the iPhone technology an accurate vehicle to produce GPS data is obtained.</p>"},{"location":"faq/gps/gps-data/#gps2ip","title":"GPS2IP","text":"<p>The application is solely available on iOS. No research was conducted on applications on Android that produce similar functionality. There are two versions of the application on the App Store: GPS2IP ($8.99) and GPS2IP Lite (Free). The free version only allows transmission for 4 minutes before automatically turning off. The paid version has no restrictions. </p>"},{"location":"faq/gps/gps-data/#walkthrough","title":"Walkthrough","text":""},{"location":"faq/gps/gps-data/#iphone-configuration","title":"iPhone Configuration","text":""},{"location":"faq/gps/gps-data/#turn-off-auto-lock","title":"Turn Off Auto-Lock","text":"<p>Settings &gt; Display &amp; Brightness &gt; Auto-Lock &gt; Never</p>"},{"location":"faq/gps/gps-data/#gps2ip-configuration","title":"GPS2IP Configuration","text":""},{"location":"faq/gps/gps-data/#enable-gps2ip","title":"Enable GPS2IP","text":"<p>Open GPS2IP &gt; Toggle On \"Enable GPS2IP\" Switch</p>"},{"location":"faq/gps/gps-data/#nmea-message-type","title":"NMEA Message Type","text":"<p>Open GPS2IP &gt; Settings &gt; NMEA Messages to Send &gt; Only Toggle On \"GLL\" Switch</p>"},{"location":"faq/gps/gps-data/#python-code","title":"Python Code","text":"<pre><code>#!/usr/bin/env python\n\n'''\nA module with a GPS node.\n\nGPS2IP: http://www.capsicumdreams.com/gps2ip/\n'''\n\nimport json\nimport re\nimport rospy\nimport socket\n\nfrom std_msgs.msg import String\n\nclass GPS:\n    '''A node which listens to GPS2IP Lite through a socket and publishes a GPS topic.'''\n    def __init__(self):\n        '''Initialize the publisher and instance variables.'''\n        # Instance Variables\n        self.HOST = rospy.get_param('~HOST', '172.20.38.175')\n        self.PORT = rospy.get_param('~PORT', 11123)\n\n        # Publisher\n        self.publisher = rospy.Publisher('/gps', String, queue_size=1)\n\n    def get_coords(self):\n        '''A method to receive the GPS coordinates from GPS2IP Lite.'''\n        # Instantiate a client object\n        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n            s.connect((self.HOST, self.PORT))\n            # The data is received in the RMC data format\n            gps_data = s.recv(1024)\n\n        # Transform data into dictionary\n        gps_keys = [\"message_id\", \"latitude\", \"ns_indicator\", \"longitude\", \"ew_indicator\"]\n        gps_values = re.split(',|\\*', gps_data.decode())[:5]\n        gps_dict = dict(zip(gps_keys, gps_values))\n\n        # Cleanse the coordinate data\n        for key in ['latitude', 'longitude']:\n            # Identify the presence of a negative number indicator\n            neg_num = False\n\n            # The GPS2IP application transmits a negative coordinate with a zero prepended\n            if gps_dict[key][0] == '0':\n                neg_num = True\n\n            # Transform the longitude and latitude into a format that can be utilized by the front-end web-client\n            gps_dict[key] = float(gps_dict[key]) / 100\n\n            # Apply the negative if the clause was triggered\n            if neg_num:\n                gps_dict[key] = -1 * gps_dict[key]\n\n        # Publish the decoded GPS data\n        self.publisher.publish(json.dumps(gps_dict))\n\n\nif __name__ == '__main__':\n    # Initialize a ROS node named GPS\n    rospy.init_node(\"gps\")\n\n    # Initialize a GPS instance with the HOST and PORT\n    gps_node = GPS()\n\n    # Continuously publish coordinated until shut down\n    while not rospy.is_shutdown():\n        gps_node.get_coords()\n</code></pre>"},{"location":"faq/gps/graph-nav/","title":"Graph-Based Mapping for Navigation","text":""},{"location":"faq/gps/graph-nav/#overview","title":"Overview","text":"<p>Graph derived navigation is a method where the environment is represented as a graph, consisting of nodes (representing locations) and edges (representing not necessarily paths but instead the interconnectivity between the nodes).  This method is ideal for scenarios where the robot needs to navigate from one point to another while not necessarily following a pre-defined path.</p> <p>In this example, the robot uses Breadth-First Search (BFS) to find the shortest path between two points on a map. This pathfinding technique is particularly suitable for unweighted graphs, like those used in basic navigation tasks.</p>"},{"location":"faq/gps/graph-nav/#how-does-ths-searching-work-work","title":"How does ths searching work work?","text":"<p>BFS is a graph traversal algorithm that explores nodes layer by layer, starting from a starting node. BFS is commonly used for pathfinding in graphs because it guarantees finding the shortest path in an unweighted graph, i.e., all edges have no weight. BFS looks at all the neighbors of a node before moving to the next level of nodes. This is ideal for pathfinding between two nodes because it systematically expands outward from the start.</p> <ol> <li>Initialization: BFS starts by visiting the starting node and marking it as visited. This node is then added to the queue.</li> <li>Exploring Closest Nodes Outward: The algorithm dequeues a node and explores all of its unvisited nearby nodes. Each unvisited node is added to the queue and marked as visited.</li> <li>Path Creation: BFS tracks the path taken to each node. Once the target node is eventually reached, the algorithm reconstructs the path by backtracking thanks to the queue. </li> <li>Termination: The algorithm terminates when it finds the destination node once more. We are guaranteed to find the shortest path, because it explores nodes in order of how many epochs it takes to reach them. </li> </ol>"},{"location":"faq/gps/graph-nav/#how-does-the-graph-work","title":"How does the graph work?","text":"<p>The nodes are saved as a list of tuples containing latitude and longitute points. The edges are stored as a dictionary of edges, mapping the node to list of connected nodes. It is also convenient to make a separate dict with the node information, with keys containing easy to remember names. In the below example, the edges are not directed or weighted.</p>"},{"location":"faq/gps/graph-nav/#basic-implementation","title":"Basic Implementation","text":"<p>```python from collections import deque</p> <p>class Graph:     def init(self):         self.nodes = []  # List of nodes (lat, lon)         self.edges = {}</p> <pre><code>def add_node(self, lat, lon):\n    \"\"\"Add a new node to the graph.\"\"\"\n    self.nodes.append((lat, lon))\n    self.edges[(lat, lon)] = []\n\ndef add_edge(self, node1, node2):\n    \"\"\"Add an undirected edge between two nodes. This can be modified to have directions as well.\"\"\"\n    if node1 in self.edges and node2 in self.edges:\n        self.edges[node1].append(node2)\n        self.edges[node2].append(node1)\n    else:\n        print(\"One of the nodes does not exist in the graph!\")\n\ndef bfs(self, start_node, end_node):\n    \"\"\"Find the shortest path using BFS from start_node to end_node.\"\"\"\n    queue = deque([(start_node, [start_node])])  # Store nodes with the path taken\n    visited = set()  # Set to track visited nodes\n\n    while queue:\n        current_node, path = queue.popleft()\n\n        # If we reach the target node, return the path\n        if current_node == end_node:\n            return path\n\n        visited.add(current_node)  # Mark the node as visited\n\n        # Explore the neighbors (connected nodes)\n        for close_alt in self.edges.get(current_node, []):\n            if close_alt not in visited:\n                queue.append((close_alt, path + [close_alt]))  # Add node with new path\n</code></pre>"},{"location":"faq/hardware/bldc-motor-guide/","title":"BLDC motor guide","text":"<p>{% hint style=\"info\" %} Don't know how to read a BLDC motor's spec sheet, check out this section</p> <p>Authors: Julian Ho, Cass Wang</p>"},{"location":"faq/hardware/bldc-motor-guide/#what-is-a-bldc-motor","title":"What is a BLDC motor?","text":"<p>BLDC motor stands for Brushless DC motor, as their name implies, brushless DC motors do not use brushes. With brushed motors, the brushes deliver current through the commutator into the coils on the rotor.</p> <p></p> <p>With a BLDC motor, it is the permanent magnet that rotates; rotation is achieved by changing the direction of the magnetic fields generated by the surrounding stationary coils. To control the rotation, you adjust the magnitude and direction of the current into these coils.</p> <p></p> <p>By switching on/off each pairs of stators really quickly, the BLDC motor can achieve a high rotational speed.</p> <p>This is a simple table comparing a brushed DC motor, AC induction motor, and a BLDC motor:</p> <p></p> <p>BLDC motors are commonly found in drones, electric cars, even robots!</p>"},{"location":"faq/hardware/bldc-motor-guide/#types-of-bldc-motor","title":"Types of BLDC motor","text":"<p>There are a couple different types of BLDC motor on the market for different applications. Here are some examples,</p>"},{"location":"faq/hardware/bldc-motor-guide/#small-motor","title":"Small motor","text":"<ul> <li>&lt;150g weight</li> <li>&lt;5cm diameter</li> <li>11.1-22.2v operational voltage</li> <li>~0.3 NM torque</li> <li>application: small drones</li> </ul>"},{"location":"faq/hardware/bldc-motor-guide/#mid-size-motor","title":"Mid-size motor","text":"<ul> <li>400-1000g weight</li> <li>5-10cm diameter</li> <li>22.2-44.4v operational voltage</li> <li>~4 NM torque</li> <li>application: RC cars, electric skateboard, robot actuator</li> </ul>"},{"location":"faq/hardware/bldc-motor-guide/#stepper-motor","title":"Stepper motor","text":"<ul> <li>~400g weight</li> <li>5-8cm diameter</li> <li>11.1-22.2v operational voltage</li> <li>~0.9 NM torque</li> <li>application: 3D printers, servos</li> </ul>"},{"location":"faq/hardware/bldc-motor-guide/#bldc-motor-lingos","title":"BLDC motor lingos","text":"<p>When shopping for a BLDC motor, there are a couple motor specific terms to consider.</p> <ul> <li>Max RPM (KV - RPM per volt)</li> <li>2200KV @ 10v = <code>KV x V</code> = 22,000 RPM max speed</li> <li>Max Torque (NM - newton-meter)</li> <li>1 NM = able to lift 1 KG weight attached to the end of a 1 meter long stick</li> <li>Max Power (W - Watts)</li> <li>835w @ 10v = <code>W/V</code> = 83.5Amp max power draw</li> <li>Motor Efficiency (%)</li> <li>90% efficiency = 90% of theoretical power</li> <li>Input Volt (S - li-ion Cells)</li> <li>6S = <code>S x 3.7V</code> = 22.2v</li> <li>Max Current (A - Amps)</li> <li>50A @ 10v = <code>A x V</code> = 500W max power</li> <li>Motor poles (N-P)</li> <li>24N40P = 24 stator poles, 40 permanent magnet poles</li> <li>Outrunner/Inrunner</li> <li>Outrunner = motor body spin with output shaft</li> <li>Inrunner = only output shaft will spin, body is stationary</li> <li>Motor numbering</li> <li>6355 = 63mm diameter, 55mm length</li> </ul>"},{"location":"faq/hardware/bldc-motor-guide/#bldc-electric-speed-controllers-esc","title":"BLDC electric speed controllers (ESC)","text":"<p>To drive a BLDC motor, you need a dedicated speed controller (ESC) to control it. Here are different types of ESC for different applications. These ESCs (like the motors above) are considered hobbyist-use, but they are quite sufficient for building small/mid-size robots.</p>"},{"location":"faq/hardware/bldc-motor-guide/#drone-esc","title":"Drone ESC","text":"<ul> <li>very light ~9g</li> <li>very small footprint (size of a dollar coin)</li> <li>1-6S input voltage</li> <li>~40A max continuous current</li> <li>cheap</li> <li>application: small drone, small fighter robot, RC helicopter</li> <li>downside: cannot handle big motors, heat up very quickly, only simple motor control algorithms available</li> </ul>"},{"location":"faq/hardware/bldc-motor-guide/#rc-car-esc","title":"RC car ESC","text":"<ul> <li>3-12S input voltage</li> <li>~50A max continuous current</li> <li>can handle medium size motors</li> <li>have active cooling</li> <li>affordable</li> <li>application: RC car, RC boat, electric skateboard</li> <li>downside: limited control protocol (PWM only), only simple motor control algorithms available</li> </ul>"},{"location":"faq/hardware/bldc-motor-guide/#robot-esc","title":"Robot ESC","text":"<ul> <li>commonly used in robotic arm, actuator control</li> <li>more expensive</li> <li>~120A max continuous current</li> <li>can handle large motors</li> <li>offer fine positional control of motor</li> <li>offer programmatic control (serial/USB/CANbus)</li> <li>application: robot, robotic arm</li> <li>downside: quite pricey, not plug-and-play, need to tune the motor manually before use</li> </ul>"},{"location":"faq/hardware/bldc-motor-guide/#bldc-control-algorithms","title":"BLDC control algorithms","text":"<p>There are 2 most common motor control algorithm used in hobbyist ESCs.</p> <ul> <li>Sensorless BLDC Motor Control</li> <li>Quick video</li> <li>Advantage: No need for dedicated encoder on the motor</li> <li>Downside: Weak low speed control, less speed less torque</li> <li>Field Oriented Control (FOC)</li> <li>Quick video</li> <li>Advantage: Full torque at any speed</li> <li>Downside: Require fine motor tuning (PID), and dedicated encoder</li> </ul>"},{"location":"faq/hardware/bluetooth/","title":"Bluetooth Headset","text":""},{"location":"faq/hardware/bluetooth/#the-linux-bluetooth-struggle","title":"The linux bluetooth struggle","text":"<p>There are some troubles that you may run into if you are trying to connect to bluetooth using linux or raspberry pi, so this is a guide to try and overcome those difficulties. Hopefully, it is helpful. </p>"},{"location":"faq/hardware/bluetooth/#install-pipewire","title":"Install Pipewire","text":"<p>Run the following commands to install Pipewire and disable PulseAudio. </p> <pre><code>sudo add-apt-repository ppa:pipewire-debian/pipewire-upstream\nsudo apt update\nsudo apt install pipewire\nsudo apt install libspa-0.2-bluetooth\nsudo apt install pipewire-audio-client-libraries\nsystemctl --user daemon-reload\nsystemctl --user --now disable pulseaudio.service pulseaudio.socket\nsystemctl --user mask pulseaudio\nsystemctl --user --now enable pipewire-media-session.service\n</code></pre> <p>To check that Pipewire is properly installed, run </p> <pre><code>pactl info\n</code></pre> <p>If this doesn't work, try restarting Pipewire or your computer: </p> <pre><code>systemctl --user restart pipewire\n</code></pre> <p>If you get the error:   <code>Connection failure: Connection refused</code></p> <pre><code>systemctl --user restart pipewire-pulse\n</code></pre> <p>Source for procedure</p>"},{"location":"faq/hardware/bluetooth/#steps-taken-to-get-bluetooth-headset-to-run","title":"Steps taken to get bluetooth headset to run","text":"<p>Check the status of your bluetooth: </p> <pre><code>sudo systemctl status bluetooth\n</code></pre> <p>To connect your bluetooth device, run the following commands: </p> <pre><code>sudo bluetoothctl\nagent on\ndefault-agent\nscan on\npair XX:XX:XX:XX\nconnect XX:XX:XX:XX\ntrust XX:XX:XX:XX\n</code></pre>"},{"location":"faq/hardware/bluetooth/#set-profile","title":"Set profile","text":"<p>After this, run: </p> <pre><code>pactl list\n</code></pre> <p>You'll get a list of devices and the bluetooth device will be in the form of <code>bluez_card.84_6B_45_98_FD_8E</code></p> <p>From what I understand, most bluetooth headsets have two different profiles: ad2p and headset-head-unit. To use the microphone, you will need to set the card profile of your bluetooth device to <code>headset-head-unit</code></p> <pre><code>pactl set-card-profile bluez_card.84_6B_45_98_FD_8E headset-head-unit\n</code></pre> <p>Then, test whether or not the device is recording and playing properly:</p> <pre><code>parec -d bluez_card.84_6B_45_98_FD_8E.headset-head-unit | lame -r -V0 - out.mp3\nmpg321 out.mp3\n</code></pre>"},{"location":"faq/hardware/bluetooth/#change-default","title":"Change default","text":"<p>You can set the default input and output devices using the following commands. </p> <p>First check what sources are available: </p> <pre><code>pactl list short sources\npactl list short sinks\n</code></pre> <p>Then set the default source and sink devices: </p> <pre><code>pactl set-default-source &lt;device_name_output&gt;\npactl set-default-sink &lt;device_name_input&gt;\n</code></pre>"},{"location":"faq/hardware/communicate-with-rosserial/","title":"Communicating over Rosserial","text":"<p>Rosserial is a libaray of ROS tools that allow a ROS node to run on devices like Arduinos and other non-linux devices. Because Arduino and clones like Tivac are programmed usinc C++, writing a Rosserial node is very similar to writing a ROScpp node. For you, the programmer, you can program your rosserial node to publish and subscribe in the same way as a normal ROS node. The topics are transferred across the USB serial port using the Rosserial_python serial_node on whichever device the Rosserial device is tethered to - so to be clear, a Rosserial node cannot properly communicate with the rest of ROS without the serial_node to act as a middle man to pass the messages between ROS and the microcontroller.</p> <p>This page assumes you have followed the rosserial installation instructions for your device and IDE.</p> <p>So, you've written a node on your Tiva C and You'd like to make sure that it is working. Upload your code to the board, then proceed.</p> <p>First, enable your machine to read from the board's serial connection. Run this command in the terminal Only if you have not enabled this device in udev rules:</p> <pre><code>sudo chmod a+rw /dev/ttyACM0\n</code></pre> <p>Now, start up a <code>roscore</code>, then run this command:</p> <pre><code>rosrun rosserial_python serial_node.py _port=\"/dev/ttyACM0\" _baud=57600\n</code></pre> <p>This tells the serial node to look for ros topics on port <code>/dev/ttyACM0</code> and to communicate at a rate of 57600. If the baudrate is defined as a diferrent value in your embedded source code, then you must use that value instead!</p> <ul> <li>there is also a rosserial_server</li> </ul> <p>If there is no red error text printing in the terminal, then you should be good to go! Open a <code>rostopic list</code> to see if your topics from the board are appearing, and <code>rostopic echo</code> your topics to ensure that the correct information is being published.</p>"},{"location":"faq/hardware/external_actuator_control/","title":"Controlling an External Actuator with a ROS Publisher","text":""},{"location":"faq/hardware/external_actuator_control/#requirements","title":"Requirements","text":"<ul> <li>Arduino (optional)</li> <li>Raspberry Pi</li> <li>Relay</li> <li>External actuator (such as a battery-powered sprayer)</li> </ul>"},{"location":"faq/hardware/external_actuator_control/#how-it-works","title":"How It Works","text":"<p>To control an actuator, a relay is used to handle the switching of the circuit loop, enabling it to turn on and off. A logic board, such as a microcontroller, is required to control the relay, providing the necessary signals to power the actuator on and off.</p> <p>There are two ways to control the relay with the logic board:  </p>"},{"location":"faq/hardware/external_actuator_control/#1-controlling-the-relay-directly-with-a-raspberry-pi","title":"1. Controlling the Relay Directly with a Raspberry Pi","text":"<p>In this approach, the relay is connected directly to the GPIO pins of the Raspberry Pi, which controls the relay based on ROS messages received.</p>"},{"location":"faq/hardware/external_actuator_control/#pros","title":"Pros:","text":"<ul> <li>Simplicity: Fewer components are needed since the relay is connected directly to the Raspberry Pi.</li> <li>Reduced Latency: Direct control minimizes delay between receiving a message and activating the actuator.</li> <li>Compact Design: Eliminates the need for additional hardware like an Arduino.</li> </ul>"},{"location":"faq/hardware/external_actuator_control/#cons","title":"Cons:","text":"<ul> <li>Limited GPIO Pins: The Raspberry Pi has a limited number of GPIO pins, which may restrict additional functionality.</li> <li>Current and Voltage Handling: GPIO pins on the Raspberry Pi may not handle higher current/voltage requirements for some relays, necessitating the use of relay modules with optoisolation.</li> </ul>"},{"location":"faq/hardware/external_actuator_control/#2-controlling-the-relay-with-an-arduino","title":"2. Controlling the Relay with an Arduino","text":"<p>In this approach, the relay is connected to an Arduino, which receives commands from the Raspberry Pi via serial communication. The Arduino processes the commands and controls the relay accordingly.</p>"},{"location":"faq/hardware/external_actuator_control/#pros_1","title":"Pros:","text":"<ul> <li>Increased Flexibility: The Arduino can offload tasks from the Raspberry Pi, allowing it to handle other processes efficiently.</li> <li>Higher Voltage/Current Capability: The Arduino can interface more easily with relays that require higher voltage or current than the Raspberry Pi GPIO pins can provide.</li> <li>Modularity: Easier to troubleshoot or replace components since the Raspberry Pi and Arduino work independently.</li> </ul>"},{"location":"faq/hardware/external_actuator_control/#cons_1","title":"Cons:","text":"<ul> <li>Increased Complexity: Adds another component to the system, requiring more programming and wiring.</li> <li>Latency: Introduces a small delay due to the additional communication step between the Raspberry Pi and Arduino.</li> <li>Cost and Size: Slightly increases the overall cost and size of the setup.</li> </ul>"},{"location":"faq/hardware/external_actuator_control/#which-option-to-choose","title":"Which Option to Choose?","text":"<ul> <li>Use Option 1 (Direct Control with Raspberry Pi) if simplicity, compactness, and minimal cost are priorities, and the relay can operate within the Raspberry Pi's GPIO pin specifications.  </li> <li>Use Option 2 (Control with Arduino) if you require additional flexibility, need to handle higher power relays, or want to distribute the control logic between the Raspberry Pi and Arduino.</li> </ul>"},{"location":"faq/hardware/external_actuator_control/#implementation-examples","title":"Implementation Examples","text":"<p>Control with Arduino 1. Connect the relay module to the Arduino digital pin (e.g., pin 7). 2. Program the Arduino to receive serial commands (<code>ACTUATOR_ON</code>/<code>ACTUATOR_OFF</code>) from the Raspberry Pi and toggle the relay. <pre><code>const int relayPin = 7;  // Pin connected to the relay\nbool isRelayOn = false; \n\nvoid setup() {\n  pinMode(relayPin, OUTPUT);       // Set relay pin as output\n  digitalWrite(relayPin, HIGH);     // Ensure relay starts OFF\n  Serial.begin(9600);             // Start serial communication\n}\n\nvoid loop() {\n  if (Serial.available() &gt; 0) {  // Check if data is available to read\n    String command = Serial.readStringUntil('\\n');  // Read the incoming message\n    command.trim();  // Remove any extra whitespace or newline characters\n\n    if (command == \"RELAY_ON\" &amp;&amp; !isRelayOn) {\n      digitalWrite(relayPin, LOW);  // Turn the relay ON\n      isRelayOn = true;\n      Serial.println(\"Relay is ON\");\n    } else if (command == \"RELAY_OFF\" &amp;&amp; isRelayOn) {\n      digitalWrite(relayPin, HIGH);   // Turn the relay OFF\n      isRelayOn = false;\n      Serial.println(\"Relay is OFF\");\n    } else {\n      Serial.println(\"Unknown command received\");\n    }\n  }\n}\n</code></pre> 4. Write a ROS subscriber node on the Raspberry Pi that sends these serial commands to the Arduino. <pre><code>#!/usr/bin/env python3\nimport rospy\nfrom std_msgs.msg import String\nimport serial\nimport time\n\nclass ArduinoController:\n    def __init__(self):\n        # Set up the serial connection to the Arduino\n        self.arduino = serial.Serial('/dev/arduino-uno', 9600, timeout=1)  # Replace with your port\n        time.sleep(2)  # Allow time for the connection to initialize\n\n        # Initialize the ROS subscriber\n        rospy.init_node('relay_control_subscriber', anonymous=True)\n        rospy.Subscriber('/relay_control', String, self.callback)\n        rospy.loginfo(\"Subscriber initialized and listening to /relay_control\")\n\n    def callback(self, msg):\n        # Send the received command to the Arduino\n        command = msg.data.strip()\n        rospy.loginfo(f\"Received: {command}\")\n        if command in [\"RELAY_ON\", \"RELAY_OFF\"]:\n            self.arduino.write(f\"{command}\\n\".encode())\n            rospy.loginfo(f\"Sent to Arduino: {command}\")\n        else:\n            rospy.logwarn(f\"Unknown command received: {command}\")\n\n    def run(self):\n        rospy.spin()\n\nif __name__ == '__main__':\n    try:\n        controller = ArduinoController()\n        controller.run()\n    except rospy.ROSInterruptException:\n        pass\n</code></pre></p>"},{"location":"faq/hardware/performance/","title":"Ubuntu Performance","text":"<p>The following are a few techniques that have been known to speed up ubuntu on Raspberry Pi. These are not all verified or guaranteed.</p>"},{"location":"faq/hardware/performance/#figure-out-why-boot-sequence-is-so-slow","title":"Figure out why  boot sequence is so slow","text":"<p>figure out what each step during boot takes <pre><code>systemd-analyze blame\n</code></pre></p>"},{"location":"faq/hardware/performance/#check-out-snapd-services","title":"Check out snapd services","text":"<pre><code>snap list\nSnap services\n</code></pre>"},{"location":"faq/hardware/performance/#remove-multipath-service","title":"Remove multipath-service","text":"<p>This is safe on a raspberry Pi</p> <pre><code>sudo systemctl disable multipathd.service\n</code></pre>"},{"location":"faq/hardware/performance/#disable-cloud-services","title":"Disable cloud services","text":"<p>Which we dont need <pre><code>sudo systemctl disable cloud-init.service\nsudo systemctl disable cloud-init-local.service\nsudo systemctl disable cloud-config.service\nsudo systemctl disable cloud-final.service\n</code></pre></p> <p>Disable motd: /etc/default/motd-news https://docs.vultr.com/working-with-the-ubuntu-message-of-the-day-motd-service</p> <pre><code>sudo systemctl disable apt-daily.service\nsudo systemctl disable apt-daily.timer\n\nsudo systemctl disable apt-daily-upgrade.timer\nsudo systemctl disable apt-daily-upgrade.service\n</code></pre>"},{"location":"faq/hardware/raspi_version/","title":"Figuring out Raspberry Pi Version","text":"<ol> <li>Use the <code>cat /proc/device-tree/model</code> command: If you have a Raspberry Pi OS installed and running, you can open a terminal and run the command cat /proc/device-tree/model. This will display the exact model name of your Raspberry Pi.</li> <li>Check the cpuinfo file: Another option is to run <code>cat /proc/cpuinfo</code> in the terminal. Look for the \"model name\" line, which will indicate the specific Raspberry Pi model.</li> </ol>"},{"location":"faq/hardware/teensy_hardware_test/","title":"Teensy Hardware Test","text":"<p>All of our homemade robots are based on the Linorobot code base. They define specifically what motors, motor drivers, imus, and lidars are supported. Stick to those exact ones. Don't get clever and assume that another model looks the same so it must be compatible!</p> <p>We always use the Teensy Microcontroller. When building a new one it is highly desireable to test it out without ROS with only the teensy.</p>"},{"location":"faq/hardware/teensy_hardware_test/#setup","title":"Setup","text":"<p>The teensy has a usb port that normally is connected to the Rasberry Pi on the robot. To do the testing you will connect the teensy to your mac or pc.</p>"},{"location":"faq/hardware/teensy_hardware_test/#software-setup","title":"Software Setup","text":"<p>Use the Teensyduino software package to run the Arduino IDE with Teensy specific configuration. Install the software on your computer, connect the USB, and verify that you have a good set up by installing and running the simplest code, which is the famous blink sketch. Just follow these instructions.</p>"},{"location":"faq/hardware/teensy_hardware_test/#what-to-test","title":"What to test","text":"<p>You want to verify:</p> <ol> <li>The motors are responding correctly. In our software, MOTOR1 is on the let (\"driver side\") and MOTOR2 is on the right (\"passenger side.\"). Test left and right for forward and backward at different speeds, as well as stopped. Forward motion of the motor is when the robot, on a surface would drive forward (this may be obvious)</li> <li>Encoders are responding correctly. Disconnect power from the motors and manually turn them and see that the encoders are counting up and down based on the direction of motion.</li> <li>i2c is connected correctly. Run the i2c detection script and make sure that it detects the right number of i2c devices. All we can do is check detection. Correctness has to wait for actually running ROS.</li> </ol>"},{"location":"faq/hardware/teensy_hardware_test/#test-scripts","title":"Test Scripts","text":"<p>Under development! Teensy Scripts to test Branbot hardware</p>"},{"location":"faq/hardware/teensy_hardware_test/#pins","title":"Pins","text":"<p>You will need to have the right pin numbers to run the tests. As of now, here are the pins.</p> <pre><code>#define MOTOR1_ENCODER_A 14\n#define MOTOR1_ENCODER_B 15\n#define MOTOR2_ENCODER_A 12\n#define MOTOR2_ENCODER_B 11\n\n#define MOTOR_DRIVER L298\n#define MOTOR1_IN_A 20\n#define MOTOR1_IN_B 1\n#define MOTOR1_PWM 22\n#define MOTOR2_IN_A 6\n#define MOTOR2_IN_B 8\n#define MOTOR2_PWM 5\n</code></pre>"},{"location":"faq/imu/calibrating_and_using_a_magnetometer/","title":"Calibrating and Using a Magnetometer","text":""},{"location":"faq/imu/calibrating_and_using_a_magnetometer/#author","title":"Author","text":"<ul> <li>Artem Lavrov</li> <li>Dec 9 2024</li> <li>ROS 1 (noetic)</li> </ul>"},{"location":"faq/imu/calibrating_and_using_a_magnetometer/#why-is-this-in-the-imu-folder","title":"Why is this in the imu folder?","text":"<p>This is in the imu folder because the idea that the imu is one sensor is a lie. An imu usually consists of 3 separate sensors, each of which do different things. Two of these sensors are relative sensors (they give data relative to the robot's state when it was started - it's position and orientation - and this data may not necessarily be consistent between) and one of them is absolute (it gives its measurement based on the robot's position and orientation relative to the earth). The two relative sensors are the gyroscope and accelerometer. We will not be focusing on those two sensors in this article. Our focus is on the absolute sensor: the magnetometer.</p>"},{"location":"faq/imu/calibrating_and_using_a_magnetometer/#what-is-a-magnetometer","title":"What is a Magnetometer","text":"<p>The magnetometer is essentially an electronic compass. It detects the magnetic field of the earth and publishes that data. More specifically, it measures the strength of the magnetic field along each of its axes. That's it. It seems very incredibly simple, yet it is deceptively tricky. The reason for this is because the earth is not the only thing that has a magnetic field. Most electronic components have magnetic fields, and also magnetic fields can be warped by materials, for example steel and a lot of other metals. If you've been paying attention you can see the major issue here. These are all abundant on a robot, so most likely, there will be a lot of interference with the magnetometer. To account for this interference, you need to calibrate the magnetometer, which I will teach you how to do in this article.</p>"},{"location":"faq/imu/calibrating_and_using_a_magnetometer/#types-of-interference","title":"Types of Interference","text":"<p>This isn't super important to understand unless you are making your own calibration script, but there are two types of interference. Hard iron interference is pretty straight forward, it comes from other magnets and results in your data being offset by a specific amount due to the pull of that magnetic field. Soft iron interference is a little more tricky: it comes from matetials (mostly metals) that warp magnetic fields, but do not generate a magnetic field. This can still be calibrated for, but requires a lot more math that I will not be explaining in this article.</p>"},{"location":"faq/imu/calibrating_and_using_a_magnetometer/#gathering-data","title":"Gathering Data","text":"<p>In order to calibrate your magnetometer, you first want to collect a bunch of readings in every direction. I would do this by creating a script to log these values into a csv file. I mean literally just pull the x,y, and z values and write them to a csv. Here's a quick example of how to do just that.</p> <p>Copy the python script below: <pre><code>#!/usr/bin/env python\nimport rospy\nfrom sensor_msgs.msg import MagneticField\nfrom lino_msgs.msg import Imu\nimport math\nimport matplotlib as mpl\n\n\nclass MagneticComparer():\n\n    def magnetic_callback(self, msg):\n        self.mag_x = msg.magnetic_field.x\n        self.mag_y = msg.magnetic_field.y\n        self.mag_z = msg.magnetic_field.z\n\n    def __init__(self):\n        rospy.init_node('magnetic_orientation', anonymous=True)\n        self.mag_sub = rospy.Subscriber(\n            \"/imu/mag\", MagneticField, self.magnetic_callback)\n        self.imu_sub = rospy.Subscriber(\"/raw_imu\", Imu, self.imu_cb)\n\n        self.mag_x = 0\n        self.mag_y = 0\n        self.mag_z = 0\n\n    def run(self):\n        print(f\"x,y,z\")\n\n        while rospy.is_shutdown is False and self.mag_x == 0:\n            continue\n        while rospy.is_shutdown() is False:\n            print(f\"{self.mag_x}, {self.mag_y}, {self.mag_z}\")\n            rospy.Rate(30).sleep()\n\nif __name__ == '__main__':\n    try:\n        MagneticComparer().run()\n    except rospy.ROSInterruptException:\n        pass\n</code></pre></p> <p>and then dumping the output into a csv file like so <pre><code>python3 mag_logger.py &gt; magnetometer.csv\n</code></pre></p> <p>Of course, you can do this any way you like.</p>"},{"location":"faq/imu/calibrating_and_using_a_magnetometer/#getting-the-calibration-parameters","title":"Getting the Calibration Parameters","text":"<p>I do not recommend trying to figure out how to account for the two different types of interference yourself, but it is handy to know how to recognize the different types of interference.</p> <p>Perfect data should look roughly like a circle around the origin. This would indicate no interference.</p> <p>Hard iron interference will cause your data to be offset and not be centered around the origin.</p> <p>Soft iron interference will cause your data to not look like a circle. It could make it more ellipsoid, or if it is really bad, it could just make it look completely random.</p> <p>I would use someone else's script to calibrate these values, as the math for soft iron interference can be difficult. However it is not impossible. All you really have to do is fit the data to a sphere, find out the center of the sphere, and figure out the offset of the center of this sphere from the origin. If you don't want to do this yourself here is a nice script that does this for you.</p>"},{"location":"faq/imu/calibrating_and_using_a_magnetometer/#using-the-calibration-parameters","title":"Using the Calibration Parameters","text":"<p>Assuming you're using the script linked to above, correcting the raw mag data once you have the correct calibration parameters should be pretty straightforward. First add or subtract your hard iron offsets to make sure your data is centered around the origin. Then use your soft iron values to fit your data to a circle. Here is an example based on the parameters of the script above (this assumes you are using ros): <pre><code>#! /usr/bin/python3\nimport numpy as np\nfrom sensor_msgs.msg import MagneticField\nfrom geometry_msgs.msg import Vector3\nimport rospy\nfrom collections import deque\n\n'''Calibration Params'''\n# Replace with the offset values you get from calibration_plotter.py\nOFFSETS = [-13, -33, -33]\n# Replace with the scale values you get from calibration_plotter.py\nSCALES = [1.083333, 0.962963, 0.962963]\n\n''' Median Filter Params '''\n# Adjust this to increase the buffer size for the median filter.\n# Increasing will reduce noise, decreasing will increase noise.\nWINDOW_SIZE = 20\n\n# Create buffers for median filter\nx_buffer = deque(maxlen=WINDOW_SIZE)\ny_buffer = deque(maxlen=WINDOW_SIZE)\nz_buffer = deque(maxlen=WINDOW_SIZE)\n\n\ndef apply_calibration(raw_mag_data, offsets, scales):\n    '''\n        Apply soft and hard iron correction to the raw data and return the \n        corrected data\n    '''\n    # Apply hard-iron correction\n    corrected_data = np.array(raw_mag_data) - np.array(offsets)\n\n    # Apply soft-iron correction\n    corrected_data = corrected_data / np.array(scales)\n\n    return corrected_data\n\n\ndef mag_callback(msg):\n    '''\n        ROS Subscriber callback to apply calibration and publish corrected data\n    '''\n    # Extract magnetometer data from the message\n    raw_mag_data = np.array(\n        [msg.magnetic_field.x, msg.magnetic_field.y, msg.magnetic_field.z])\n\n    corrected_mag_data = apply_calibration(raw_mag_data, OFFSETS, SCALES)\n\n    x_buffer.append(corrected_mag_data[0])\n    y_buffer.append(corrected_mag_data[1])\n    z_buffer.append(corrected_mag_data[2])\n\n    filtered_data = (np.median(x_buffer), np.median(\n        y_buffer), np.median(z_buffer))\n\n    # Create a new message to publish the corrected data\n    corrected_mag_msg = MagneticField()\n    corrected_mag_msg.magnetic_field = Vector3(*filtered_data)\n    corrected_mag_msg.header = msg.header\n\n    # Publish the corrected data\n    mag_pub.publish(corrected_mag_msg)\n\n\n# Initialize ROS node\nrospy.init_node('magnetometer_calibration')\n\n# Create a publisher for the corrected magnetometer data\nmag_pub = rospy.Publisher('/imu/mag_corrected', MagneticField, queue_size=10)\n\n# Subscribe to the original magnetometer topic\nrospy.Subscriber('/imu/mag', MagneticField, mag_callback)\n\n# Spin to keep the node running\nrospy.spin()\n</code></pre></p> <p>Now you should be publishing corrected mag data! Congrats! You can fuse this wiht your other imu data or use it to calculate your absolute heading (if you do go this route, make sure to account for magnetic declination). Thank you for reading and I hope this was useful to you in some way.</p>"},{"location":"faq/launchfiles/namespacing-tfs/","title":"Namespaces and Namespacing TFs","text":""},{"location":"faq/launchfiles/namespacing-tfs/#evalyn-berleant-kelly-duan","title":"Evalyn Berleant, Kelly Duan","text":""},{"location":"faq/launchfiles/namespacing-tfs/#tf-prefixes","title":"TF prefixes","text":"<p>The tf prefix is a parameter that determines the prefix that will go before the name of a tf frame from broadcasters such as gazebo and the robot state publisher, similar to namespacing with nodes that publish to topics. It is important to include</p> <pre><code>&lt;param name=\"tf_prefix\" value=\"$(arg ns)\" /&gt;\n</code></pre> <p>inside the namespace spawning the robots.</p> <p>However, not everything uses tf_prefix as it is deprecated in tf2. In most cases, the tf prefix will likely remain the same as the namespace in order to be compatible with most packages that rely on topics and tf trees, such as gmapping, which disregard the use of tf prefixes and use namespace for tf frames.</p>"},{"location":"faq/launchfiles/namespacing-tfs/#getting-the-namespace","title":"Getting the namespace","text":"<p>To retrieve the current namespace with rospy, use <code>rospy.get_namespace()</code>. This will return the namespace with <code>/</code> before and after the name. To remove the <code>/</code>s, use <code>rospy.get_namespace()[1:-1]</code>.</p>"},{"location":"faq/launchfiles/namespacing-tfs/#references","title":"References","text":"<ul> <li>ROS wiki official tf2 migration and decline of the tf prefix</li> <li>spawning robots with namespaces in gazebo and rviz - read bottom answer</li> <li>robot_state_publisher and tf prefixes</li> </ul>"},{"location":"faq/launchfiles/using-args-params-roslaunch/","title":"How do I use Parameters and Arguments in ROS?","text":""},{"location":"faq/launchfiles/using-args-params-roslaunch/#evalyn-berleant-kelly-duan","title":"Evalyn Berleant, Kelly Duan","text":"<p>Arguments and parameters are important tags for roslaunch files that are similar, but not quite the same.</p>"},{"location":"faq/launchfiles/using-args-params-roslaunch/#what-are-parameters","title":"What are parameters?","text":"<p>Parameters are either set within a launch file or taken from the command line and passed to the launch file, and then used within scripts themselves.</p>"},{"location":"faq/launchfiles/using-args-params-roslaunch/#getting-parameters","title":"Getting parameters","text":"<p>Parameters can be called inside their nodes by doing</p> <p><pre><code># get a global parameter\nrospy.get_param('/global_param_name')\n\n# get a parameter from our parent namespace\nrospy.get_param('param_name')\n\n# get a parameter from our private namespace\nrospy.get_param('~private_param_name')\n</code></pre> Example from ROS parameter tutorial.</p>"},{"location":"faq/launchfiles/using-args-params-roslaunch/#adding-parameters-to-launch-files","title":"Adding parameters to launch files","text":""},{"location":"faq/launchfiles/using-args-params-roslaunch/#setting-parameters","title":"Setting Parameters","text":"<p>Parameters can be set inside nodes like such (python):</p> <pre><code>rospy.set_param('some_numbers', [1., 2., 3., 4.])\nrospy.set_param('truth', True)\nrospy.set_param('~private_bar', 1+2)\n</code></pre> <p>For instance, if you wanted to generate a random number for some parameter, you could do as follows:</p> <p><pre><code>&lt;param name=\"robot_position\" command=\"$(find some_package)/scripts/generate_random_position.py\"/&gt;\n</code></pre> which would generate a random position for the parameter.</p> <p>Be careful that if you are setting parameters in more than one place that they are set in order correctly, or one file may overwrite the parameter\u2019s value set by another file. (See links in resources for more detail).</p>"},{"location":"faq/launchfiles/using-args-params-roslaunch/#what-are-arguments","title":"What are arguments?","text":"<p>While parameters can pass values from a launch file into a node, arguments (that look like <code>&lt;arg name=\u201dname\u201d/&gt;</code> in the launch file) are passed from the terminal to the launch file, or from launch file to launch file. You can put arguments directly into the launch file like such and give it a value (or in this case a default value):</p> <pre><code>&lt;launch&gt;\n  &lt;arg name=\"x_pos\" default=\"0.0\" /&gt;\n  &lt;arg name=\"y_pos\" default=\"0.0\" /&gt;\n  &lt;arg name=\"z_pos\" default=\"0.0\" /&gt;\n...\n</code></pre> <p>Or you can pass arguments into \u201cincluded\u201d files (launch files included in other launch files that will run): <pre><code>&lt;!-- start world and launch gazebo --&gt;\n  &lt;include file=\"$(find gazebo_ros)/launch/empty_world.launch\"&gt;\n    &lt;arg name=\"world_name\" value=\"$(find swarmbots)/worlds/$(arg world).world\"/&gt;\n    &lt;arg name=\"paused\" value=\"true\"/&gt;\n    &lt;arg name=\"use_sim_time\" value=\"true\"/&gt;\n    &lt;arg name=\"gui\" value=\"true\"/&gt;\n    &lt;arg name=\"headless\" value=\"false\"/&gt;\n    &lt;arg name=\"debug\" value=\"false\"/&gt;\n  &lt;/include&gt;\n</code></pre></p>"},{"location":"faq/launchfiles/using-args-params-roslaunch/#substitution-args","title":"Substitution args","text":"<p>Substitution args, recognized by the <code>$</code> and parentheses surrounding the value, are used to pass values between arguments. Setting the value of a parameter or argument as <code>value=\u201d$(arg argument_name)\u201d</code> will get the value of argument_name in the same launch file. Using <code>$(eval some_expression)</code> will set the value to what the python expression at <code>some_expression</code> evaluates to. Using <code>$(find pkg)</code> will get the location of a package recognized by the catkin workspace (very often used).</p> <p>The <code>if</code> attribute can be used on the group tag, node tag, or include tag and work like an if statement that will execute what is inside the tag if true. By using <code>eval</code> and <code>if</code> together, it is possible to create loops to run files recursively. For example, running a launch file an arbitrary number of times can be done by specifying the number of times to be run in the launch file, including the launch file within itself, and decrementing the number of times to be run for each recursive <code>include</code> launch, stopping at some value checked by the <code>if</code> attribute. Here is an example of a recursive launch file called <code>follower.launch</code> to spawn in robots.</p> <pre><code>&lt;launch&gt;\n  &lt;arg name=\"followers\" /&gt;\n  &lt;arg name=\"ns\" /&gt;\n\n  &lt;!-- BEGIN robot[#] --&gt;\n  &lt;group ns=\"$(arg ns)\"&gt;\n\n    &lt;param name=\"tf_prefix\" value=\"$(arg ns)\" /&gt;\n    &lt;include file=\"$(find swarmbots)/launch/one_robot.launch\"&gt;\n      &lt;arg name=\"robot_name\" value=\"$(arg ns)\" /&gt;\n      &lt;arg name=\"followers\" value=\"$(arg followers)\" /&gt;\n    &lt;/include&gt;\n  &lt;/group&gt;\n\n  &lt;!-- recursively start robot[#-1] --&gt;\n  &lt;arg name=\"new_followers\" value=\"$(eval arg('followers') - 1)\" /&gt;\n  &lt;include file=\"$(find swarmbots)/launch/follower.launch\" if=\"$(eval arg('new_followers') &gt;= 0)\"&gt;\n    &lt;arg name=\"followers\" value=\"$(arg new_followers)\" /&gt;\n    &lt;arg name=\"ns\" value=\"robot$(arg new_followers)\" /&gt;\n  &lt;/include&gt;\n&lt;/launch&gt;\n</code></pre> <p><code>followers</code> here will impact the number of times the launch file is recursively called. <code>$(eval arg('followers') - 1)</code> will decrement the value of <code>followers</code> inside each recursive launch, and the <code>if</code> attribute</p> <pre><code> if=\"$(eval arg('new_followers') &gt;= 0)\"\n</code></pre> <p>checks that once the new number is below 0, it will not call the launch file again.</p>"},{"location":"faq/launchfiles/using-args-params-roslaunch/#differences-between-arguments-and-parameters-important","title":"Differences between arguments and parameters (important!)","text":"<p>Both arguments and parameters can make use of substitution args. However, arguments cannot be changed by nodes like parameters are with <code>rospy.set_param()</code>. Because of the limits of substitution, you cannot take the value of a parameter and bring it to an argument. If you want to use the same value between two params that require generating a specific value with <code>rospy.set_param()</code> then you should create another node that sets both parameters at once.</p> <p>For example, this script</p> <pre><code>#!/usr/bin/env python\nimport random, rospy, roslib\n\nrospy.init_node(\"generate_random_x\")\npos_range = float(rospy.get_param('pos_range', 3))\n\nx_pos = random.uniform(-pos_range / 2, pos_range / 2)\nrospy.set_param('map_merge/init_pose_x',x_pos)\n\nprint(x_pos)\n</code></pre> <p>is called within a parameter using the <code>command</code> attribute.</p> <pre><code>&lt;param name=\"x_pos\" command=\"$(find swarmbots)/src/generate_random_x.py\" /&gt;\n</code></pre> <p>The command attribute sets the value of the parameter to whatever is printed by <code>stdout</code> in the script. In this case, the script generates a random number for <code>x_pos</code>. In the same file, <code>rospy.setparam</code> is called to set another parameter to the same value of <code>x_pos</code>. In that way, both parameters can be set at once.</p>"},{"location":"faq/launchfiles/using-args-params-roslaunch/#too-many-parameters-use-rosparam","title":"Too many parameters? Use rosparam!","text":"<p>If you have too many parameters and/or groups of parameters, not only is it inefficient to write them into a launch file, but is also prone to many more errors. That is when a rosparam file comes in handy--a rosparam file is a YAML file that stores parameters in an easier-to-read format. A good example of the utility of rosparam is the parameters for move_base, which uses the command <pre><code>&lt;rosparam file=\"$(find turtlebot3_navigation)/param/local_costmap_params.yaml\" command=\"load\" /&gt;\n</code></pre> which loads the parameters from the yaml file here: <pre><code>local_costmap:\n  global_frame: odom\n  robot_base_frame: base_footprint\n\n  update_frequency: 10.0\n  publish_frequency: 10.0\n  transform_tolerance: 0.5  \n\n  static_map: false  \n  rolling_window: true\n  width: 3\n  height: 3\n  resolution: 0.05\n</code></pre></p>"},{"location":"faq/launchfiles/using-args-params-roslaunch/#references","title":"References","text":"<ul> <li>Ros Parameters Tutorial</li> <li>Substitution args</li> <li>Order of launch files</li> </ul>"},{"location":"faq/launchfiles/using-conditionals-in-roslaunch/","title":"Using the Group Tag to Make Conditional Statements Within Launch Files","text":""},{"location":"faq/launchfiles/using-conditionals-in-roslaunch/#author-lisandro-mayancela","title":"Author: Lisandro Mayancela","text":"<p>This tutorial assumes the reader knows how to use/access args and parameters in a launch file.</p> <p>When making launch files you may sometimes want aspects of your launch (Such as the urdf file that is used) to be dependent on certain conditions</p> <p></p> <p>In order to have this functionality you can use the group tag with an if parameter like so:</p> <pre><code>&lt;group if=\"condition goes here\"&gt;\n    If statement body\n&lt;/group&gt;\n</code></pre>"},{"location":"faq/launchfiles/using-conditionals-in-roslaunch/#examples","title":"Examples","text":"<p>For a better example let's look at a launch file which spawns a robot into a gazebo world:</p> <pre><code>&lt;launch&gt;\n    &lt;arg name=\"robot_name\"/&gt;\n    &lt;arg name=\"init_pose\"/&gt;\n    &lt;arg name=\"team\"/&gt;\n    &lt;arg name=\"type\"/&gt;\n\n    &lt;group if=\"$(eval team == 'Red')\"&gt;\n        &lt;group if=\"$(eval type == 'painter')\"&gt;\n            &lt;param name=\"robot_description\" \n                command=\"$(find xacro)/xacro.py $(find robopaint)/urdf/red/painterbot_red_burger.urdf.xacro\" /&gt;\n        &lt;/group&gt;\n        &lt;group if=\"$(eval type == 'attacker')\"&gt;\n            &lt;param name=\"robot_description\" \n                command=\"$(find xacro)/xacro.py $(find robopaint)/urdf/red/attackerbot_red_burger.urdf.xacro\" /&gt;\n        &lt;/group&gt;\n    &lt;/group&gt;\n\n    &lt;group if=\"$(eval team == 'Blue')\"&gt;\n        &lt;group if=\"$(eval type == 'painter')\"&gt;\n            &lt;param name=\"robot_description\" \n                command=\"$(find xacro)/xacro.py $(find robopaint)/urdf/blue/painterbot_blue_burger.urdf.xacro\" /&gt;\n        &lt;/group&gt;\n        &lt;group if=\"$(eval type == 'attacker')\"&gt;\n            &lt;param name=\"robot_description\" \n                command=\"$(find xacro)/xacro.py $(find robopaint)/urdf/blue/attackerbot_blue_burger.urdf.xacro\" /&gt;\n        &lt;/group&gt;\n    &lt;/group&gt;\n\n    node name=\"spawn_minibot_model\" pkg=\"gazebo_ros\" type=\"spawn_model\"\n     args=\"$(arg init_pose) -urdf -param robot_description -model $(arg robot_name)\"\n     respawn=\"false\" output=\"screen\" /&gt;\n\n    &lt;node pkg=\"robot_state_publisher\" type=\"robot_state_publisher\" \n          name=\"robot_state_publisher\" output=\"screen\"/&gt;\n&lt;/launch&gt;\n</code></pre> <p>In lines 7-27 we see a chain of these if statements which then end up deciding one out of four urdf files to load as the robot_description which then gets passed into the spawn_minibot_model node.</p>"},{"location":"faq/mapping/gmapping_tips/","title":"gmapping Parameters Recommendation","text":""},{"location":"faq/mapping/gmapping_tips/#author","title":"Author","text":"<ul> <li>Chao An</li> <li>Dec 7 2024</li> <li>ROS 1</li> </ul>"},{"location":"faq/mapping/gmapping_tips/#summary","title":"Summary","text":"<p>Recommendations for gmapping parameters and location of it.</p>"},{"location":"faq/mapping/gmapping_tips/#details","title":"Details","text":"<p>gmapping is the most beginner friendly SLAM algorithm that is provided in ROS1. The limitation of gmapping is that it must used with robots that provide /scan rostopic (Lidar) and /odom rostopic (Odom). There's other options like Karto SLAM and Hector SLAM, but gmapping would be the one I'm choosing for my project.</p> <p>But gmapping with default parameters might not be appropriate for all environment like maze. Here's my recommend parameters for small environment. </p>"},{"location":"faq/mapping/gmapping_tips/#instructions","title":"Instructions","text":"<p>To change the parameters, you can modify:  <pre><code>turtlebot3/turtlebot3_slam/config/gmapping_params.yaml .\n</code></pre> It would be called by command like:  <pre><code>roslaunch turtlebot3_slam turtlebot3_slam.launch slam_methods:=gmapping\n</code></pre></p>"},{"location":"faq/mapping/gmapping_tips/#parameters","title":"Parameters","text":"<p>Here's the parameters:  <pre><code>map_update_interval: 2.0\nmaxUrange: 3.0\nsigma: 0.05\nkernelSize: 1\nlstep: 0.05\nastep: 0.05\niterations: 5\nlsigma: 0.075\nogain: 3.0\nlskip: 10 # Somebody on forus use 0 or 10\nminimumScore: 10 # Options: 10, 50(default)\nsrr: 0.01 # 0.01 , 0.1 (default)\nsrt: 0.02 # 0.02, 0.2 (default)\nstr: 0.01 # 0.01, 0.1 (default)\nstt: 0.02 # 0.02, 0.2 (default)\nlinearUpdate: 0.05 # 0.05, 0.1 (default)\nangularUpdate: 0.2 # 0.2, 0.1 (default)\ntemporalUpdate: 0.5\nresampleThreshold: 0.5\nparticles: 100\nxmin: -10.0\nymin: -10.0\nxmax: 10.0\nymax: 10.0\ndelta: 0.01 # 0.01, 0.05 (default)\nllsamplerange: 0.01\nllsamplestep: 0.01\nlasamplerange: 0.005\nlasamplestep: 0.005\n</code></pre></p>"},{"location":"faq/mapping/gmapping_tips/#clarification","title":"Clarification","text":"<p>Hopefully these parameters recommendation can help you, but it is important to modify it based on your using condition. Remember, tunning mapping is closer to art than science, which means you need luck sometimes.</p>"},{"location":"faq/math/PID-guide/","title":"PID-guide.md","text":"<p>PID is a common and powerful programming tool that can be used to maintain stability and accuracy in many tasks that give feedback. This guide will walk through how to de-mystify PID and teach you to become a confident PID user.</p> <p>PID stands for _P_roportional, _I_ntegral and _D_erivative. All three mathematical techniques are used in a PID controller.</p> <p>First, let's think of a PID application. There is a wheel on a motor with a quadrature shaft encoder that you desire to spin at a certain RPM. In this example, imagine you have already written the code to get the speed of the motor by reading the encoder data.</p> <p>The goal is for the wheel to spin at 60 RPM. so 60 is our Setpoint Variable. the actual speed that the motor encoders report is the Process Variable. By subtracting the Process Variable PV from the Setpoint Variable SP, we get the error</p> <pre><code>double error = target_speed - get_wheel_speed(); // assume this function has been written and returns motor speed in RPM\n</code></pre> <p>We will use this error three ways, and combine them using weights (called Gains) to acheive PID control. The three components answer three questions:</p> <ol> <li>How big is my error?</li> <li>How much error have I accumulated?</li> <li>Is my error getting bigger or smaller?</li> </ol>"},{"location":"faq/math/PID-guide/#proportional","title":"Proportional","text":"<p>Proportional control could not be simpler: multiply the error by some constant Kp</p> <pre><code>double p = error * Ki;\n</code></pre>"},{"location":"faq/math/PID-guide/#integral","title":"Integral","text":"<p>If you know about Integrals then you know that they represent the area under a curve, which is a sum of all the points on said curve. Therefore, we can calculate the Integral component by summating the total error over time, like this:</p> <pre><code>double i;\n...\ni += Ki * error * change_in_time();\n</code></pre> <p>Of course, Ki is the Integral Gain constant, similar to Kp. Error is multiplied by the change in time because remember, the integral is the area under the curve. So if we plot error on the Y axis, time would likely be plotted on the x axis, thus area is error * time.</p>"},{"location":"faq/math/PID-guide/#derivative","title":"Derivative","text":"<p>Derivatives are the rate at which things change, so naturally a simple way to get the Derivative component is to compare the current error to the pervious error, and then account for time, like this:</p> <pre><code>double d = Kd * (error - pervious_error) / change_in_time();\nprevious_error = error;\n</code></pre> <p>Again, Kd is the derivative gain constant, and we divide by the change in time because if the change is 4 units in half a second, then the rate of change is 8 units/second (4 / 0.5)</p>"},{"location":"faq/math/PID-guide/#what-to-do-with-p-i-and-d","title":"What to do with P, I and D","text":"<p>Add them up. This is an adjusted representation of your control system's error. Therefore, you can apply it as a change to your previous command to try to get closer to the Setpoint Variable</p> <pre><code>double pid = p + i + d;\ncommand = previous_command + pid;\nprevious_command = command;\n</code></pre> <p>Then your command can be sent to your actuator. Please note that some additional arithmatic may be required, this is a bare-bones simple example and does by no means serve as a copy+paste solution to all PID applications.</p>"},{"location":"faq/math/PID-guide/#tuning","title":"Tuning","text":"<p>A PID system must be tuned with the proper values of Kp, Ki and Kd in order to acheive smooth control. The main goals of tuning are to minimize error and over shooting the Setpoint Goal. There are plenty of guides and theories as to how to tune, so here is the Google Search to get you started</p>"},{"location":"faq/math/quaternions/","title":"quaternions.md","text":"<p>\ufffd\ufffd#\ufffd \ufffdQ\ufffdu\ufffda\ufffdt\ufffde\ufffdr\ufffdn\ufffdi\ufffdo\ufffdn\ufffds\ufffd \ufffda\ufffdn\ufffdd\ufffd \ufffdR\ufffdo\ufffdt\ufffda\ufffdt\ufffdi\ufffdo\ufffdn\ufffd \ufffdC\ufffdh\ufffde\ufffda\ufffdt\ufffd \ufffdS\ufffdh\ufffde\ufffde\ufffdt\ufffd \ufffd \ufffd \ufffd \ufffdQ\ufffdu\ufffda\ufffdt\ufffde\ufffdr\ufffdn\ufffdi\ufffdo\ufffdn\ufffds\ufffd \ufffda\ufffdr\ufffde\ufffd \ufffdp\ufffdo\ufffdi\ufffdn\ufffdt\ufffds\ufffd \ufffdo\ufffdn\ufffd \ufffdt\ufffdh\ufffde\ufffd \ufffd \ufffd4\ufffdD\ufffd \ufffdu\ufffdn\ufffdi\ufffdt\ufffd \ufffdh\ufffdy\ufffdp\ufffde\ufffdr\ufffds\ufffdp\ufffdh\ufffde\ufffdr\ufffde\ufffd.\ufffd\ufffd\ufffd \ufffdF\ufffdo\ufffdu\ufffdr\ufffd-\ufffdd\ufffdi\ufffdm\ufffde\ufffdn\ufffds\ufffdi\ufffdo\ufffdn\ufffda\ufffdl\ufffd \ufffdc\ufffdo\ufffdm\ufffdp\ufffdl\ufffde\ufffdx\ufffd \ufffdn\ufffdu\ufffdm\ufffdb\ufffde\ufffdr\ufffds\ufffd \ufffda\ufffdr\ufffde\ufffd \ufffda\ufffdl\ufffdw\ufffda\ufffdy\ufffds\ufffd \ufffdo\ufffdf\ufffd \ufffdt\ufffdh\ufffde\ufffd \ufffdf\ufffdo\ufffdr\ufffdm\ufffd:\ufffd \ufffd \ufffd \ufffd \ufffd#\ufffd#\ufffd \ufffda\ufffd \ufffd+\ufffd \ufffdb\ufffd \ufffd\ufffdi\ufffd\ufffd \ufffd+\ufffd \ufffdc\ufffd \ufffd\ufffdj\ufffd\ufffd \ufffd+\ufffd \ufffdd\ufffd \ufffd\ufffdk\ufffd\ufffd \ufffd \ufffd \ufffd \ufffd&amp; w\ufffdi\ufffdt\ufffdh\ufffd \ufffdo\ufffdn\ufffde\ufffd \ufffdr\ufffde\ufffda\ufffdl\ufffd \ufffdp\ufffda\ufffdr\ufffdt\ufffd \ufffd\u0018 a\ufffd\u0019 ,\ufffd \ufffda\ufffdn\ufffdd\ufffd \ufffd3\ufffd \ufffd \ufffd\ufffd\ufffdi\ufffdm\ufffda\ufffdg\ufffdi\ufffdn\ufffda\ufffdr\ufffdy\ufffd\ufffd\ufffd \ufffdo\ufffdr\ufffd \ufffd\ufffd\ufffdv\ufffde\ufffdc\ufffdt\ufffdo\ufffdr\ufffd\ufffd\ufffd \ufffdp\ufffda\ufffdr\ufffdt\ufffds\ufffd \ufffd\u0018 b\ufffd\u0019 ,\ufffd \ufffd\u0018 c\ufffd\u0019 ,\ufffd \ufffda\ufffdn\ufffdd\ufffd \ufffd\u0018 d\ufffd\u0019 .\ufffd\ufffd\ufffd \ufffdS\ufffdi\ufffdn\ufffdc\ufffde\ufffd \ufffda\ufffdl\ufffdl\ufffd \ufffdq\ufffdu\ufffda\ufffdt\ufffde\ufffdr\ufffdn\ufffdi\ufffdo\ufffdn\ufffds\ufffd \ufffdf\ufffda\ufffdl\ufffdl\ufffd \ufffdo\ufffdn\ufffd \ufffdt\ufffdh\ufffde\ufffd \ufffdu\ufffdn\ufffdi\ufffdt\ufffd \ufffdh\ufffdy\ufffdp\ufffde\ufffdr\ufffds\ufffdp\ufffdh\ufffde\ufffdr\ufffde\ufffd,\ufffd \ufffdi\ufffdt\ufffd \ufffdw\ufffdi\ufffdl\ufffdl\ufffd \ufffda\ufffdl\ufffdw\ufffda\ufffdy\ufffds\ufffd \ufffdh\ufffda\ufffdv\ufffde\ufffd \ufffda\ufffd \ufffdd\ufffdi\ufffds\ufffdt\ufffda\ufffdn\ufffdc\ufffde\ufffd \ufffd1\ufffd \ufffdf\ufffdr\ufffdo\ufffdm\ufffd \ufffdt\ufffdh\ufffde\ufffd \ufffdo\ufffdr\ufffdi\ufffdg\ufffdi\ufffdn\ufffd.\ufffd \ufffdT\ufffdh\ufffde\ufffdy\ufffd \ufffdt\ufffdh\ufffde\ufffdr\ufffde\ufffdf\ufffdo\ufffdr\ufffde\ufffd \ufffdm\ufffda\ufffdi\ufffdn\ufffdt\ufffda\ufffdi\ufffdn\ufffd \ufffdt\ufffdh\ufffde\ufffd \ufffdf\ufffdo\ufffdl\ufffdl\ufffdo\ufffdw\ufffdi\ufffdn\ufffdg\ufffd \ufffdr\ufffde\ufffdl\ufffda\ufffdt\ufffdi\ufffdo\ufffdn\ufffds\ufffdh\ufffdi\ufffdp\ufffd:\ufffd \ufffd \ufffd \ufffd \ufffd#\ufffd#\ufffd \ufffda\ufffd^\ufffd2\ufffd \ufffd+\ufffd \ufffdb\ufffd^\ufffd2\ufffd \ufffd+\ufffd \ufffdc\ufffd^\ufffd2\ufffd \ufffd+\ufffd \ufffdd\ufffd^\ufffd2\ufffd \ufffd=\ufffd \ufffd1\ufffd \ufffd \ufffd \ufffd \ufffdI\ufffdf\ufffd \ufffdX\ufffd \ufffda\ufffdn\ufffdd\ufffd \ufffdY\ufffd \ufffda\ufffdr\ufffde\ufffd \ufffdt\ufffdw\ufffdo\ufffd \ufffdq\ufffdu\ufffda\ufffdt\ufffde\ufffdr\ufffdn\ufffdi\ufffdo\ufffdn\ufffds\ufffd \ufffdt\ufffdh\ufffda\ufffdt\ufffd \ufffds\ufffda\ufffdt\ufffdi\ufffds\ufffdf\ufffdy\ufffd \ufffdt\ufffdh\ufffde\ufffd \ufffda\ufffdb\ufffdo\ufffdv\ufffde\ufffd \ufffdr\ufffdu\ufffdl\ufffde\ufffd,\ufffd \ufffdX\ufffdY\ufffd \ufffdw\ufffdi\ufffdl\ufffdl\ufffd \ufffda\ufffdl\ufffds\ufffdo\ufffd \ufffds\ufffda\ufffdt\ufffdi\ufffds\ufffdf\ufffdy\ufffd \ufffdi\ufffdt\ufffd.\ufffd \ufffd \ufffd \ufffd \ufffdS\ufffdi\ufffdn\ufffdc\ufffde\ufffd \ufffdq\ufffdu\ufffda\ufffdt\ufffde\ufffdr\ufffdn\ufffdi\ufffdo\ufffdn\ufffds\ufffd \ufffda\ufffdr\ufffde\ufffd \ufffde\ufffdx\ufffdt\ufffde\ufffdn\ufffds\ufffdi\ufffdo\ufffdn\ufffds\ufffd \ufffdo\ufffdf\ufffd \ufffdc\ufffdo\ufffdm\ufffdp\ufffdl\ufffde\ufffdx\ufffd \ufffdn\ufffdu\ufffdm\ufffdb\ufffde\ufffdr\ufffds\ufffd,\ufffd \ufffdw\ufffde\ufffd \ufffdc\ufffda\ufffdn\ufffd \ufffdm\ufffdu\ufffdl\ufffdt\ufffdi\ufffdp\ufffdl\ufffdy\ufffd \ufffdt\ufffdh\ufffde\ufffdm\ufffd \ufffdb\ufffdy\ufffd \ufffd \ufffdd\ufffdi\ufffds\ufffdt\ufffdr\ufffdi\ufffdb\ufffdu\ufffdt\ufffdi\ufffdo\ufffdn\ufffd,\ufffd \ufffdb\ufffdu\ufffdt\ufffd \ufffdt\ufffdh\ufffdi\ufffds\ufffd \ufffdr\ufffde\ufffdq\ufffdu\ufffdi\ufffdr\ufffde\ufffds\ufffd \ufffds\ufffdt\ufffdr\ufffdo\ufffdn\ufffdg\ufffde\ufffdr\ufffd \ufffdd\ufffde\ufffdf\ufffdi\ufffdn\ufffdi\ufffdt\ufffdi\ufffdo\ufffdn\ufffds\ufffd \ufffdo\ufffdf\ufffd \ufffdi\ufffd,\ufffd \ufffdj\ufffd,\ufffd \ufffda\ufffdn\ufffdd\ufffd \ufffdk\ufffd \ufffda\ufffdn\ufffdd\ufffd \ufffdt\ufffdh\ufffde\ufffdi\ufffdr\ufffd \ufffd \ufffdm\ufffdu\ufffdl\ufffdt\ufffdi\ufffdp\ufffdl\ufffdi\ufffdc\ufffda\ufffdt\ufffdi\ufffdo\ufffdn\ufffd.\ufffd\ufffd\ufffd \ufffdT\ufffdh\ufffde\ufffds\ufffde\ufffd \ufffdr\ufffde\ufffdl\ufffda\ufffdt\ufffdi\ufffdo\ufffdn\ufffds\ufffdh\ufffdi\ufffdp\ufffds\ufffd \ufffdc\ufffda\ufffdn\ufffd \ufffdb\ufffde\ufffd \ufffde\ufffda\ufffds\ufffdi\ufffdl\ufffdy\ufffd \ufffdr\ufffde\ufffdm\ufffde\ufffdm\ufffdb\ufffde\ufffdr\ufffde\ufffdd\ufffd \ufffdw\ufffdi\ufffdt\ufffdh\ufffd \ufffdt\ufffdh\ufffde\ufffd \ufffd \ufffdr\ufffdi\ufffdg\ufffdh\ufffdt\ufffd-\ufffdh\ufffda\ufffdn\ufffdd\ufffd-\ufffdr\ufffdu\ufffdl\ufffde\ufffd.\ufffd \ufffd \ufffd \ufffd \ufffd!\ufffd[\ufffdQ\ufffdu\ufffda\ufffdt\ufffd4\ufffd]\ufffd(\ufffd.\ufffd.\ufffd/\ufffdi\ufffdm\ufffda\ufffdg\ufffde\ufffds\ufffd/\ufffdq\ufffdu\ufffda\ufffdt\ufffd/\ufffdq\ufffdu\ufffda\ufffdt\ufffd0\ufffd4\ufffd.\ufffdp\ufffdn\ufffdg\ufffd)\ufffd \ufffd \ufffd \ufffd \ufffdN\ufffdo\ufffdw\ufffd \ufffdw\ufffde\ufffd \ufffdc\ufffda\ufffdn\ufffd \ufffdm\ufffdu\ufffdl\ufffdt\ufffdi\ufffdp\ufffdl\ufffdy\ufffd \ufffdt\ufffdh\ufffde\ufffd \ufffdq\ufffdu\ufffda\ufffdt\ufffde\ufffdr\ufffdn\ufffdi\ufffdo\ufffdn\ufffds\ufffd \ufffdb\ufffdy\ufffd \ufffdd\ufffdi\ufffds\ufffdt\ufffdr\ufffdi\ufffdb\ufffdu\ufffdt\ufffdi\ufffdo\ufffdn\ufffd.\ufffd\ufffd\ufffd \ufffd \ufffdT\ufffdh\ufffdi\ufffds\ufffd \ufffdc\ufffda\ufffdn\ufffd \ufffdb\ufffde\ufffd \ufffds\ufffdi\ufffdm\ufffdp\ufffdl\ufffdi\ufffdf\ufffdi\ufffde\ufffdd\ufffd \ufffdt\ufffdo\ufffd \ufffdt\ufffdh\ufffde\ufffd \ufffdf\ufffdo\ufffdl\ufffdl\ufffdo\ufffdw\ufffdi\ufffdn\ufffdg\ufffd \ufffde\ufffdq\ufffdu\ufffda\ufffdt\ufffdi\ufffdo\ufffdn\ufffd,\ufffd \ufffdk\ufffdn\ufffdo\ufffdw\ufffdn\ufffd \ufffda\ufffds\ufffd \ufffdt\ufffdh\ufffde\ufffd \ufffd\ufffdH\ufffda\ufffdm\ufffdi\ufffdl\ufffdt\ufffdo\ufffdn\ufffd \ufffd \ufffdp\ufffdr\ufffdo\ufffdd\ufffdu\ufffdc\ufffdt\ufffd\ufffd:\ufffd \ufffd \ufffd \ufffd \ufffd#\ufffd#\ufffd \ufffd(\ufffda\ufffd&lt;\ufffds\ufffdu\ufffdb\ufffd&gt;\ufffd1\ufffd&lt;\ufffd/\ufffds\ufffdu\ufffdb\ufffd&gt;\ufffd \ufffd+\ufffd \ufffdb\ufffd&lt;\ufffds\ufffdu\ufffdb\ufffd&gt;\ufffd1\ufffd&lt;\ufffd/\ufffds\ufffdu\ufffdb\ufffd&gt;\ufffd\ufffdi\ufffd\ufffd \ufffd+\ufffd \ufffdc\ufffd&lt;\ufffds\ufffdu\ufffdb\ufffd&gt;\ufffd1\ufffd&lt;\ufffd/\ufffds\ufffdu\ufffdb\ufffd&gt;\ufffd\ufffdj\ufffd\ufffd \ufffd+\ufffd \ufffdd\ufffd&lt;\ufffds\ufffdu\ufffdb\ufffd&gt;\ufffd1\ufffd&lt;\ufffd/\ufffds\ufffdu\ufffdb\ufffd&gt;\ufffd\ufffdk\ufffd\ufffd)\ufffd \ufffd\ufffd \ufffd(\ufffda\ufffd&lt;\ufffds\ufffdu\ufffdb\ufffd&gt;\ufffd2\ufffd&lt;\ufffd/\ufffds\ufffdu\ufffdb\ufffd&gt;\ufffd \ufffd+\ufffd \ufffdb\ufffd&lt;\ufffds\ufffdu\ufffdb\ufffd&gt;\ufffd2\ufffd&lt;\ufffd/\ufffds\ufffdu\ufffdb\ufffd&gt;\ufffd\ufffdi\ufffd\ufffd+\ufffd \ufffdc\ufffd&lt;\ufffds\ufffdu\ufffdb\ufffd&gt;\ufffd2\ufffd&lt;\ufffd/\ufffds\ufffdu\ufffdb\ufffd&gt;\ufffd\ufffdj\ufffd\ufffd \ufffd+\ufffd \ufffdd\ufffd&lt;\ufffds\ufffdu\ufffdb\ufffd&gt;\ufffd2\ufffd&lt;\ufffd/\ufffds\ufffdu\ufffdb\ufffd&gt;\ufffd\ufffdk\ufffd\ufffd)\ufffd \ufffd=\ufffd \ufffd \ufffd \ufffd \ufffd!\ufffd[\ufffdQ\ufffdu\ufffda\ufffdt\ufffd1\ufffd]\ufffd(\ufffd.\ufffd.\ufffd/\ufffdi\ufffdm\ufffda\ufffdg\ufffde\ufffds\ufffd/\ufffdq\ufffdu\ufffda\ufffdt\ufffd/\ufffdq\ufffdu\ufffda\ufffdt\ufffd0\ufffd7\ufffd.\ufffdp\ufffdn\ufffdg\ufffd)\ufffd \ufffd \ufffd \ufffd \ufffdI\ufffdn\ufffd \ufffdo\ufffdr\ufffdd\ufffde\ufffdr\ufffd \ufffdt\ufffdo\ufffd \ufffdf\ufffdo\ufffdr\ufffdm\ufffd \ufffda\ufffd \ufffd3\ufffdD\ufffd \ufffdr\ufffde\ufffdp\ufffdr\ufffde\ufffds\ufffde\ufffdn\ufffdt\ufffda\ufffdt\ufffdi\ufffdo\ufffdn\ufffd \ufffdo\ufffdf\ufffd \ufffdo\ufffdu\ufffdr\ufffd \ufffd4\ufffdD\ufffd \ufffdq\ufffdu\ufffda\ufffdt\ufffde\ufffdr\ufffdn\ufffdi\ufffdo\ufffdn\ufffd,\ufffd \ufffdw\ufffde\ufffd \ufffdu\ufffds\ufffde\ufffd \ufffda\ufffd \ufffd\ufffds\ufffdt\ufffde\ufffdr\ufffde\ufffdo\ufffdg\ufffdr\ufffda\ufffdp\ufffdh\ufffdi\ufffdc\ufffd \ufffdp\ufffdr\ufffdo\ufffdj\ufffde\ufffdc\ufffdt\ufffdi\ufffdo\ufffdn\ufffd\ufffd,\ufffd \ufffd \ufffdw\ufffdh\ufffdi\ufffdc\ufffdh\ufffd \ufffdd\ufffdr\ufffda\ufffdw\ufffds\ufffd \ufffdl\ufffdi\ufffdn\ufffde\ufffds\ufffd \ufffdt\ufffdh\ufffdr\ufffdo\ufffdu\ufffdg\ufffdh\ufffd \ufffdt\ufffdh\ufffde\ufffd \ufffdp\ufffdo\ufffdi\ufffdn\ufffdt\ufffd \ufffd(\ufffd-\ufffd1\ufffd,\ufffd \ufffd0\ufffd,\ufffd \ufffd0\ufffd,\ufffd \ufffd0\ufffd)\ufffd \ufffda\ufffdn\ufffdd\ufffd \ufffde\ufffdv\ufffde\ufffdr\ufffdy\ufffd \ufffdo\ufffdt\ufffdh\ufffde\ufffdr\ufffd \ufffdo\ufffdn\ufffde\ufffd \ufffdo\ufffdn\ufffd \ufffdt\ufffdh\ufffde\ufffd \ufffd \ufffdh\ufffdy\ufffdp\ufffde\ufffdr\ufffds\ufffdp\ufffdh\ufffde\ufffdr\ufffde\ufffd.\ufffd\ufffd\ufffd \ufffdW\ufffdh\ufffde\ufffdr\ufffde\ufffdv\ufffde\ufffdr\ufffd \ufffdt\ufffdh\ufffde\ufffds\ufffde\ufffd \ufffdl\ufffdi\ufffdn\ufffde\ufffds\ufffd \ufffdi\ufffdn\ufffdt\ufffde\ufffdr\ufffds\ufffde\ufffdc\ufffdt\ufffd \ufffdt\ufffdh\ufffde\ufffd \ufffd3\ufffdD\ufffd \ufffds\ufffdp\ufffda\ufffdc\ufffde\ufffd \ufffdi\ufffds\ufffd \ufffdt\ufffdh\ufffde\ufffdi\ufffdr\ufffd \ufffdp\ufffdr\ufffdo\ufffdj\ufffde\ufffdc\ufffdt\ufffdi\ufffdo\ufffdn\ufffd \ufffd \ufffdo\ufffdn\ufffdt\ufffdo\ufffd \ufffdi\ufffdt\ufffd \ufffd(\ufffd2\ufffdD\ufffd \ufffdp\ufffdr\ufffdo\ufffdj\ufffde\ufffdc\ufffdt\ufffdi\ufffdo\ufffdn\ufffd \ufffdi\ufffdn\ufffdt\ufffdo\ufffd \ufffda\ufffd \ufffd1\ufffdD\ufffd \ufffds\ufffdp\ufffda\ufffdc\ufffde\ufffd \ufffds\ufffdh\ufffdo\ufffdw\ufffdn\ufffd \ufffdb\ufffde\ufffdl\ufffdo\ufffdw\ufffd)\ufffd:\ufffd \ufffd \ufffd \ufffd \ufffdY\ufffde\ufffdl\ufffdl\ufffdo\ufffdw\ufffd \ufffdl\ufffdi\ufffdn\ufffde\ufffds\ufffd \ufffda\ufffdr\ufffde\ufffd \ufffdd\ufffdr\ufffda\ufffdw\ufffdn\ufffd \ufffdo\ufffdr\ufffdi\ufffdg\ufffdi\ufffdn\ufffda\ufffdt\ufffdi\ufffdn\ufffdg\ufffd \ufffda\ufffdt\ufffd \ufffd-\ufffd1\ufffd \ufffd+\ufffd \ufffd0\ufffd\ufffdi\ufffd\ufffd \ufffda\ufffdn\ufffdd\ufffd \ufffdi\ufffdn\ufffdt\ufffde\ufffdr\ufffds\ufffde\ufffdc\ufffdt\ufffd \ufffdw\ufffdi\ufffdt\ufffdh\ufffd \ufffde\ufffdv\ufffde\ufffdr\ufffdy\ufffd \ufffdp\ufffdo\ufffdi\ufffdn\ufffdt\ufffd \ufffdo\ufffdn\ufffd \ufffdt\ufffdh\ufffde\ufffd \ufffdu\ufffdn\ufffdi\ufffdt\ufffd \ufffd \ufffdc\ufffdi\ufffdr\ufffdc\ufffdl\ufffde\ufffd.\ufffd\ufffd\ufffd\ufffd\ufffd \ufffdT\ufffdh\ufffde\ufffd \ufffdp\ufffdo\ufffdi\ufffdn\ufffdt\ufffd \ufffda\ufffdt\ufffd \ufffdw\ufffdh\ufffdi\ufffdc\ufffdh\ufffd \ufffdt\ufffdh\ufffde\ufffd \ufffdl\ufffdi\ufffdn\ufffde\ufffd \ufffdi\ufffdn\ufffdt\ufffde\ufffdr\ufffds\ufffde\ufffdc\ufffdt\ufffds\ufffd \ufffdt\ufffdh\ufffde\ufffd \ufffd\ufffdi\ufffd\ufffd-\ufffdl\ufffdi\ufffdn\ufffde\ufffd \ufffdi\ufffds\ufffd \ufffdw\ufffdh\ufffde\ufffdr\ufffde\ufffd \ufffdt\ufffdh\ufffde\ufffd \ufffd \ufffdp\ufffdo\ufffdi\ufffdn\ufffdt\ufffd \ufffdi\ufffds\ufffd \ufffdp\ufffdr\ufffdo\ufffdj\ufffde\ufffdc\ufffdt\ufffde\ufffdd\ufffd \ufffdo\ufffdn\ufffdt\ufffdo\ufffd \ufffdt\ufffdh\ufffde\ufffd \ufffd1\ufffdD\ufffd \ufffdl\ufffdi\ufffdn\ufffde\ufffd.\ufffd\ufffd\ufffd \ufffdH\ufffde\ufffdr\ufffde\ufffd,\ufffd \ufffdy\ufffdo\ufffdu\ufffd \ufffdc\ufffda\ufffdn\ufffd \ufffds\ufffde\ufffde\ufffd \ufffdt\ufffdh\ufffde\ufffd \ufffd2\ufffdD\ufffd \ufffdp\ufffdo\ufffdi\ufffdn\ufffdt\ufffd \ufffd0\ufffd.\ufffd7\ufffd9\ufffd \ufffd+\ufffd \ufffd \ufffd0\ufffd.\ufffd6\ufffd1\ufffd\ufffdi\ufffd\ufffd \ufffdo\ufffdn\ufffdt\ufffdo\ufffd \ufffdt\ufffdh\ufffde\ufffd \ufffdp\ufffdo\ufffdi\ufffdn\ufffdt\ufffd \ufffd\\~\ufffd0\ufffd.\ufffd4\ufffd.\ufffd \ufffd \ufffd \ufffd \ufffdA\ufffdl\ufffdl\ufffd \ufffdt\ufffdh\ufffde\ufffds\ufffde\ufffd \ufffde\ufffdl\ufffde\ufffdm\ufffde\ufffdn\ufffdt\ufffds\ufffd \ufffdc\ufffdo\ufffdm\ufffdb\ufffdi\ufffdn\ufffde\ufffdd\ufffd \ufffda\ufffdl\ufffdl\ufffdo\ufffdw\ufffd \ufffdu\ufffds\ufffd \ufffdt\ufffdo\ufffd \ufffdu\ufffds\ufffde\ufffd \ufffdq\ufffdu\ufffda\ufffdt\ufffde\ufffdr\ufffdn\ufffdi\ufffdo\ufffdn\ufffds\ufffd \ufffdt\ufffdo\ufffd \ufffdd\ufffde\ufffdf\ufffdi\ufffdn\ufffde\ufffd \ufffda\ufffd \ufffdr\ufffdo\ufffdb\ufffdo\ufffdt\ufffd\u0019 s\ufffd \ufffd(\ufffdo\ufffdr\ufffd \ufffda\ufffdn\ufffdy\ufffd \ufffdo\ufffdt\ufffdh\ufffde\ufffdr\ufffd \ufffd3\ufffdD\ufffd \ufffdo\ufffdb\ufffdj\ufffde\ufffdc\ufffdt\ufffd\u0019 s\ufffd)\ufffd \ufffdo\ufffdr\ufffdi\ufffde\ufffdn\ufffdt\ufffda\ufffdt\ufffdi\ufffdo\ufffdn\ufffd \ufffdi\ufffdn\ufffd \ufffd3\ufffdD\ufffd \ufffds\ufffdp\ufffda\ufffdc\ufffde\ufffd.\ufffd\ufffd\ufffd \ufffdI\ufffdn\ufffds\ufffdt\ufffde\ufffda\ufffdd\ufffd \ufffdo\ufffdf\ufffd \ufffda\ufffdd\ufffdd\ufffdi\ufffdn\ufffdg\ufffd \ufffdr\ufffdo\ufffdt\ufffda\ufffdt\ufffdi\ufffdo\ufffdn\ufffds\ufffd,\ufffd \ufffdw\ufffde\ufffd \ufffdu\ufffds\ufffde\ufffd \ufffdt\ufffdh\ufffde\ufffd \ufffd\ufffdH\ufffda\ufffdm\ufffdi\ufffdl\ufffdt\ufffdo\ufffdn\ufffd \ufffdp\ufffdr\ufffdo\ufffdd\ufffdu\ufffdc\ufffdt\ufffd\ufffd \ufffdt\ufffdo\ufffd \ufffdc\ufffdo\ufffdm\ufffdb\ufffdi\ufffdn\ufffde\ufffd \ufffdq\ufffdu\ufffda\ufffdt\ufffde\ufffdr\ufffdn\ufffdi\ufffdo\ufffdn\ufffds\ufffd \ufffd(\ufffds\ufffdi\ufffdn\ufffdc\ufffde\ufffd \ufffdw\ufffde\ufffd \ufffda\ufffdr\ufffde\ufffd \ufffdw\ufffdo\ufffdr\ufffdk\ufffdi\ufffdn\ufffdg\ufffd \ufffdo\ufffdn\ufffde\ufffd \ufffdd\ufffdi\ufffdm\ufffde\ufffdn\ufffds\ufffdi\ufffdo\ufffdn\ufffd \ufffdu\ufffdp\ufffd)\ufffd.\ufffd \ufffd \ufffdT\ufffdh\ufffde\ufffd \ufffdd\ufffdi\ufffdm\ufffde\ufffdn\ufffds\ufffdi\ufffdo\ufffdn\ufffda\ufffdl\ufffdi\ufffdt\ufffdy\ufffd \ufffdo\ufffdf\ufffd \ufffdt\ufffdh\ufffde\ufffd \ufffdr\ufffdo\ufffdt\ufffda\ufffdt\ufffdi\ufffdo\ufffdn\ufffd \ufffdi\ufffds\ufffd \ufffdb\ufffde\ufffds\ufffdt\ufffd \ufffdv\ufffdi\ufffds\ufffdu\ufffda\ufffdl\ufffdi\ufffdz\ufffde\ufffdd\ufffd \ufffda\ufffds\ufffd \ufffda\ufffd \ufffdr\ufffdo\ufffdt\ufffda\ufffdt\ufffdi\ufffdo\ufffdn\ufffd \ufffdQ\ufffd \ufffda\ufffdr\ufffdo\ufffdu\ufffdn\ufffdd\ufffd \ufffda\ufffd \ufffd\ufffdE\ufffdu\ufffdl\ufffde\ufffdr\ufffd \ufffda\ufffdx\ufffdi\ufffds\ufffd\ufffd \ufffd(\ufffda\ufffd \ufffd3\ufffdD\ufffd \ufffdu\ufffdn\ufffdi\ufffdt\ufffd \ufffdv\ufffde\ufffdc\ufffdt\ufffdo\ufffdr\ufffd)\ufffd.\ufffd\ufffd\ufffd \ufffdA\ufffd \ufffdq\ufffdu\ufffda\ufffdt\ufffde\ufffdr\ufffdn\ufffdi\ufffdo\ufffdn\ufffd \ufffd\ufffdq\ufffd\ufffd,\ufffd \ufffdw\ufffdh\ufffdi\ufffdc\ufffdh\ufffd \ufffdd\ufffde\ufffds\ufffdc\ufffdr\ufffdi\ufffdb\ufffde\ufffds\ufffd \ufffda\ufffd \ufffdr\ufffdo\ufffdt\ufffda\ufffdt\ufffdi\ufffdo\ufffdn\ufffd \ufffdQ\ufffd \ufffda\ufffdr\ufffdo\ufffdu\ufffdn\ufffdd\ufffd \ufffdt\ufffdh\ufffde\ufffd \ufffdu\ufffdn\ufffdi\ufffdt\ufffd \ufffdv\ufffde\ufffdc\ufffdt\ufffdo\ufffdr\ufffd \ufffd\ufffdu\ufffd\ufffd,\ufffd \ufffd \ufffdi\ufffds\ufffd \ufffdg\ufffdi\ufffdv\ufffde\ufffdn\ufffd \ufffda\ufffds\ufffd:\ufffd \ufffd \ufffd \ufffd \ufffd!\ufffd[\ufffdq\ufffdu\ufffda\ufffdt\ufffd6\ufffd]\ufffd(\ufffd.\ufffd.\ufffd/\ufffdi\ufffdm\ufffda\ufffdg\ufffde\ufffds\ufffd/\ufffdq\ufffdu\ufffda\ufffdt\ufffd/\ufffdq\ufffdu\ufffda\ufffdt\ufffd0\ufffd6\ufffd.\ufffdp\ufffdn\ufffdg\ufffd)\ufffd \ufffd \ufffd \ufffd \ufffd#\ufffd#\ufffd#\ufffd \ufffdU\ufffds\ufffde\ufffdf\ufffdu\ufffdl\ufffd \ufffdL\ufffdi\ufffdn\ufffdk\ufffds\ufffd \ufffd \ufffd \ufffd \ufffd\ufffd \ufffd[\ufffde\ufffda\ufffdt\ufffde\ufffdr\ufffd.\ufffdn\ufffde\ufffdt\ufffd \ufffdQ\ufffdu\ufffda\ufffdt\ufffde\ufffdr\ufffdn\ufffdi\ufffdo\ufffdn\ufffds\ufffd]\ufffd(\ufffdh\ufffdt\ufffdt\ufffdp\ufffds\ufffd:\ufffd/\ufffd/\ufffde\ufffda\ufffdt\ufffde\ufffdr\ufffd.\ufffdn\ufffde\ufffdt\ufffd/\ufffdq\ufffdu\ufffda\ufffdt\ufffde\ufffdr\ufffdn\ufffdi\ufffdo\ufffdn\ufffds\ufffd)\ufffd \ufffd \ufffd\ufffd \ufffd[\ufffdW\ufffdi\ufffdk\ufffdi\ufffdp\ufffde\ufffdd\ufffdi\ufffda\ufffd \ufffdQ\ufffdu\ufffda\ufffdt\ufffde\ufffdr\ufffdn\ufffdi\ufffdo\ufffdn\ufffds\ufffd]\ufffd(\ufffdh\ufffdt\ufffdt\ufffdp\ufffds\ufffd:\ufffd/\ufffd/\ufffde\ufffdn\ufffd.\ufffdw\ufffdi\ufffdk\ufffdi\ufffdp\ufffde\ufffdd\ufffdi\ufffda\ufffd.\ufffdo\ufffdr\ufffdg\ufffd/\ufffdw\ufffdi\ufffdk\ufffdi\ufffd/\ufffdQ\ufffdu\ufffda\ufffdt\ufffde\ufffdr\ufffdn\ufffdi\ufffdo\ufffdn\ufffd)\ufffd \ufffd \ufffd\ufffd \ufffd[\ufffdW\ufffdi\ufffdk\ufffdi\ufffdp\ufffde\ufffdd\ufffdi\ufffda\ufffd \ufffdQ\ufffdu\ufffda\ufffdt\ufffde\ufffdr\ufffdn\ufffdi\ufffdn\ufffdi\ufffdo\ufffdn\ufffds\ufffd \ufffda\ufffdn\ufffdd\ufffd \ufffdS\ufffdp\ufffda\ufffdt\ufffdi\ufffda\ufffdl\ufffd \ufffdR\ufffdo\ufffdt\ufffda\ufffdt\ufffdi\ufffdo\ufffdn\ufffds\ufffd]\ufffd(\ufffdh\ufffdt\ufffdt\ufffdp\ufffds\ufffd:\ufffd/\ufffd/\ufffde\ufffdn\ufffd.\ufffdw\ufffdi\ufffdk\ufffdi\ufffdp\ufffde\ufffdd\ufffdi\ufffda\ufffd.\ufffdo\ufffdr\ufffdg\ufffd/\ufffdw\ufffdi\ufffdk\ufffdi\ufffd/\ufffdQ\ufffdu\ufffda\ufffdt\ufffde\ufffdr\ufffdn\ufffdi\ufffdo\ufffdn\ufffds\ufffd_\ufffda\ufffdn\ufffdd\ufffd_\ufffds\ufffdp\ufffda\ufffdt\ufffdi\ufffda\ufffdl\ufffd_\ufffdr\ufffdo\ufffdt\ufffda\ufffdt\ufffdi\ufffdo\ufffdn\ufffd)\ufffd \ufffd \ufffd*\ufffd \ufffd[\ufffdW\ufffdi\ufffdk\ufffdi\ufffdp\ufffde\ufffdd\ufffdi\ufffda\ufffd \ufffdS\ufffdt\ufffde\ufffdr\ufffde\ufffdo\ufffdg\ufffdr\ufffda\ufffdp\ufffdh\ufffdi\ufffdc\ufffd \ufffdP\ufffdr\ufffdo\ufffdj\ufffde\ufffdc\ufffdt\ufffdi\ufffdo\ufffdn\ufffds\ufffd]\ufffd(\ufffdh\ufffdt\ufffdt\ufffdp\ufffds\ufffd:\ufffd/\ufffd/\ufffde\ufffdn\ufffd.\ufffdw\ufffdi\ufffdk\ufffdi\ufffdp\ufffde\ufffdd\ufffdi\ufffda\ufffd.\ufffdo\ufffdr\ufffdg\ufffd/\ufffdw\ufffdi\ufffdk\ufffdi\ufffd/\ufffdS\ufffdt\ufffde\ufffdr\ufffde\ufffdo\ufffdg\ufffdr\ufffda\ufffdp\ufffdh\ufffdi\ufffdc\ufffd_\ufffdp\ufffdr\ufffdo\ufffdj\ufffde\ufffdc\ufffdt\ufffdi\ufffdo\ufffdn\ufffd)\ufffd \ufffd \ufffd</p>"},{"location":"faq/math/sigmoid-instead-of-pid/","title":"How do I use a sigmoid function instead of a PID","text":"<p>To use a sigmoid function instead of a PID controller, you need to replace the PID control algorithm with a sigmoid-based control algorithm. Here is a step-by-step guide to do this:</p> <ol> <li>Understand the sigmoid function: A sigmoid function is an S-shaped curve, mathematically defined as:    f(x) = 1 / (1 + exp(-k * x))</li> </ol> <p>where x is the input, k is the steepness factor, and exp() is the exponential function. The sigmoid function maps any input value to a range between 0 and 1.</p> <ol> <li> <p>Determine the error: Just like in a PID controller, you need to calculate the error between the desired setpoint and the current value (process variable). The error can be calculated as:    error = setpoint - process_variable</p> </li> <li> <p>Apply the sigmoid function: Use the sigmoid function to map the error to a value between 0 and 1. You can adjust the steepness factor (k) to control the responsiveness of the system:    sigmoid_output = 1 / (1 + exp(-k * error))</p> </li> <li> <p>Scale the output: Since the sigmoid function maps the error to a range between 0 and 1, you need to scale the output to match the actual range of your control signal (e.g., motor speed or actuator position). You can do this by multiplying the sigmoid_output by the maximum control signal value:    control_signal = sigmoid_output * max_control_signal</p> </li> <li> <p>Apply the control signal: Send the control_signal to your system (e.g., motor or actuator) to adjust its behavior based on the error.</p> </li> </ol> <p>Note that a sigmoid function-based controller may not provide the same level of performance as a well-tuned PID controller, especially in terms of overshoot and settling time. However, it can be useful in certain applications where a smooth, non-linear control response is desired.</p>"},{"location":"faq/math/smart_rotation/","title":"Smart Rotation for Navigation","text":""},{"location":"faq/math/smart_rotation/#author-eyal-cohen","title":"Author - Eyal Cohen","text":""},{"location":"faq/math/smart_rotation/#given-two-coordinates-in-a-2-dimensional-plane-and-your-robots-current-direction-the-smart-rotation-algorithm-will-calculate-your-target-angle-and-return-whether-your-robot-should-to-turn-left-turn-right-or-go-straight-ahead-to-reach-its-navigation-goal","title":"Given two coordinates in a 2-dimensional plane and your robots' current direction, the smart rotation algorithm will calculate your target angle and return whether your robot should to turn left, turn right, or go straight ahead to reach its navigation goal.","text":""},{"location":"faq/math/smart_rotation/#how-it-works","title":"How It Works:","text":"<p>Inputs: <code>posex1</code> is a float that represents the  x-axis coordinate of your robot, <code>posey1</code> is a float that represents your robot's y-axis coordinate. <code>posex2</code> and <code>posey2</code> are floats that represent the x and y of your robot's goal. Lastly, <code>theta</code> represents your robot's current pose angle.</p> <pre><code>from math import atan2\nimport math\n\n\ndef smartRotate(posex1, posey1, posex2, posey2, theta):\n    inc_x = posex2 -posex1\n    inc_y = posey2 -posey1\n    angle_goal = atan2(inc_y, inc_x)\n    # if angle_to_goal &lt; 0:\n    #     angle_to_goal = (2* math.pi) + angle_to_goal\n    print(\"angle goal = \",angle_goal)\n    goal_range = theta + math.pi\n    wrapped = goal_range - (2 * math.pi)\n    if abs(angle_goal - theta) &gt; 0.1:\n        print(theta)\n        print(\"goal_range = \",goal_range)\n        if (goal_range) &gt; (2 * math.pi) and (theta &lt; angle_goal or angle_goal &lt; wrapped):\n            print(\"go left\")\n        elif (goal_range) &lt; (2 * math.pi) and (theta &lt; angle_goal and angle_goal &lt; goal_range):\n            print(\"go left\")\n        else:\n            print(\"go right\")\n\n    else:\n        print(\"go straight\")\n</code></pre> <p>Firstly, the <code>angle_goal</code> from your robot's coordinate (not taking its current angle into account) is calculated by finding the arc tangent of the difference between the robot's coordinates and the goal coordinates. </p> <p>In order to decide whether your robot should go left or right, we must determine where the <code>angle_goal</code> is relative to its current rotational direction. If the <code>angle_goal</code> is on the robot's left rotational hemisphere, the robot should rotate left, otherwise it should rotate right. Since we are working in Radians, \u03c0 is equivilant to 180 degrees.  To check whether the <code>angle_goal</code> is within the left hemisphere of the robot, we must add \u03c0 to <code>theta</code> (the robot's current direction) to get the upperbound of the range of values we want to check the target may be included in. If the <code>angle_goal</code> is between <code>theta</code> and that upper bound, then the robot must turn in that direction to most efficiently reach its goal.</p>"},{"location":"faq/math/smart_rotation/#consider-this-example","title":"Consider This Example:","text":"<pre><code>    smartRotation(0,0,2,2,0)\n</code></pre> <p>If your robot is at (0,0), its rotational direction is 0, and it's target is at (2,2), then its <code>angle_goal</code> would equal = 0.785. First we check whether its current angle's deviation from the <code>angle_goal</code> is significant by finding the difference and seeing if its larger than 0.1. If the difference between the angles is insignificant the robot should go straight towards its goal. In this case however, <code>angle_goal</code> - <code>theta</code> (0.785 - 0) is greater than 0.1, so we know that we must turn left or right to near our <code>angle_goal</code>. To find out whether this angle is to the left or the right of the robot's current angle, we must add \u03c0 to its current angle to discover the point between its left and right hemispheres. In this case, if the <code>angle_goal</code> is between <code>theta</code> and its goal_range, 3.14 (0(<code>theta</code>) + \u03c0), then we would know that the robot must turn left to reach its goal. </p> <p>However, if <code>theta</code> (your robot's current direction) + \u03c0 is greater than 2\u03c0 (maximum radians in a circle) then the left hemisphere of your robot is partially across the 0 radian point of the circle. To account for that case, we must calculate how far the goal range wraps around the circle passed the origin. If there is a remainder, we check whether the <code>angle_goal</code> is between <code>theta</code> and 2\u03c0 or if the <code>angle_goal</code> is present within the remainder of the range that wraps around the origin. If either of these conditions are met then we know that your robot should turn left to most efficiently arrive at its goal, otherwise it should turn right.</p>"},{"location":"faq/ml/Reinforcement_Learning/","title":"Reinforcement Learning and its Applications","text":"<p>Authors: Rachel Lese, Tiff Lyman</p> <p>A lot of people are intimidated by the idea of machine learning, which is fair. However, machine learning doesn't have to be some complicated multi-layered network. In fact, you can implement a machine learning algorithm with just a dictionary in ROS. Specifically, you can run a reinforcement learning algorithm that replaces PID to steer a robot and have it follow a line.</p>"},{"location":"faq/ml/Reinforcement_Learning/#what-is-reinforcement-learning","title":"What is Reinforcement Learning?","text":"<p>Reinforcement learning is exactly what it sounds like, learning by reinforcing behavior. Desired outcomes are rewarded in a way that makes them more likely to occur down the road (no pun intended). There are a few key components to a reinforcement learning algorithm and they are as follows:   - A set of possible behaviors in response to the reinforcement   - A quantifiable observation/state that can be evaluated repeatedly   - A reward function that scores behaviors based on the evaluation</p> <p>With this, the algorithm takes the highest scoring behavior and applies it.</p>"},{"location":"faq/ml/Reinforcement_Learning/#application-line-follower","title":"Application: Line Follower","text":"<p>Lets look at what those required traits would be in a Line Follower. </p>"},{"location":"faq/ml/Reinforcement_Learning/#behaviors","title":"Behaviors","text":"<p>In this case would be our possible linear and angular movements. At the start we want everything to be equally likely, so we get the following dictionaries. <pre><code>angular_states_prob =  { -.5: .5, -.45: .5, -.4: .5, -.35: .5, -.3: .5, -.25: .5, -.2: .5, -.15: .5, -.1: .5, -.05: .5, 0 : .5, .05 : .5, .1 : .5, .15 : .5, .2: .5, .25: .5, .3: .5, .35: .5, .4: .5, .45: .5, .5: .5 }\nlinear_states_prob = { 0.1 : .5, 0.25 : .5, 0.4 : .5 }\n</code></pre> Here the key is the possible behavior (in radians/sec or m/sec) and the value is the associated weight. </p>"},{"location":"faq/ml/Reinforcement_Learning/#observation-to-evaluate","title":"Observation to Evaluate","text":"<p>As for the observation that we evaluate at regular intervals, we have camera data in CV callback. Just like with the original line follower, we create a mask for the camera data so that we see just the line and have everything else black. With this we can get the \"center\" of the line and see how far it is from being at the center of the camera. We do this by getting the center x and y coordinates as shown below. <pre><code>moments = cv2.moments(mask_yellow)\n        if moments['m00'] &gt; 0:\n            cx = int(moments['m10']/moments['m00'])\n            cy = int(moments['m01']/moments['m00'])\n</code></pre> Here cx and cy represent pixel indecies for the center, so if we know the resolution of the camera we can use this to establish preferable behaviors. To do this, we want our angular movement to be proportional to how far we deviate from the camera's center. For the Waffle model in ROS, we determined that it's roughly 1570x1000, meaning that cx has a range of (0,1570) and cy has a range of (0,1000).</p>"},{"location":"faq/ml/Reinforcement_Learning/#reward-function","title":"Reward Function","text":"<p>This next part might also be intimidating, only because it involves some math. What we want to do is convert our pixel range to the range of possible outcomes and reward velocities based on how close they are to the converted index. For example, right turns are negative and further right means greater cx, so the line of code below does the transformation (0,1570) =&gt; (0.5, -0.5) for cx: <pre><code>angle = (float(cx)/1570.0 - 0.5) * -1.0\n</code></pre> With this, we check which keys are close to our value angle and add weight to those values as follows: <pre><code>for value in angular_states_prob.keys():\n            angleSum += angular_states_prob[value]\n            if abs(value - angle) &lt; 0.05:\n                angular_states_prob[value] = angular_states_prob[value]*5\n            elif abs(value - angle) &lt; 0.1:\n                angular_states_prob[value] = angular_states_prob[value]*2\n</code></pre> So if the key is closest to our angle we multiply the weight by 5, and if it's somewhat close we multiply it by 2. To make sure weights don't go off to infinity, we have the value angleSum which keeps track of the sum of the weights. If it exceeds a certain limit, we scale all of the weights down. Similarly, we dont scale a weight down if it is less than 0.05 so that they don't become infinitesimally small.</p>"},{"location":"faq/ml/Reinforcement_Learning/#conclusion","title":"Conclusion","text":"<p>This is just one example of reinforcement learning, but it's remarkably ubiquitous. Hopefully this demonstrates that reinforcement learning is a straightforward introduction to machine learning. </p>"},{"location":"faq/ml/convert-imagenet-to-darknet/","title":"Annotation Format Conversion","text":"<p>ImageNet file xml format to Darknet text format.</p> <p>Full Repo here: https://github.com/campusrover/Robotics_Computer_Vision/tree/master/utils/xml_2_txt</p>"},{"location":"faq/ml/convert-imagenet-to-darknet/#installation","title":"Installation","text":"<pre><code>sudo pip install -r requirements.txt\n</code></pre>"},{"location":"faq/ml/convert-imagenet-to-darknet/#usage","title":"Usage","text":"<pre><code>python xmltotxt.py -xml xml -out out\n</code></pre>"},{"location":"faq/ml/convert-imagenet-to-darknet/#example","title":"Example","text":"<p>Input xml file.</p> <pre><code>&lt;annotation&gt;\n    &lt;filename&gt;image-0000016.jpg&lt;/filename&gt;\n    &lt;size&gt;\n        &lt;width&gt;1920&lt;/width&gt;\n        &lt;height&gt;1080&lt;/height&gt;\n    &lt;/size&gt;\n    &lt;object&gt;\n        &lt;name&gt;sedan&lt;/name&gt;\n        &lt;bndbox&gt;\n            &lt;xmin&gt;75&lt;/xmin&gt;\n            &lt;ymin&gt;190&lt;/ymin&gt;\n            &lt;xmax&gt;125&lt;/xmax&gt;\n            &lt;ymax&gt;210&lt;/ymax&gt;\n        &lt;/bndbox&gt;\n    &lt;/object&gt;\n&lt;/annotation&gt;\n</code></pre> <p>Output text file.</p> <pre><code>4 0.052083 0.185185 0.026042 0.018519\n</code></pre>"},{"location":"faq/ml/convert-imagenet-to-darknet/#motivation","title":"Motivation","text":"<p>I used Darknet for real-time object detection and classification. Sometimes you need to collect your own trainig dataset for train your model. I collected training dataset images and fine awesome tool for labeling images. But it generates xml files. So I needed to implement tool which translates from ImageNet xml format to Darknet text format.</p>"},{"location":"faq/ml/monocular-depth-estimation/","title":"Monocular Depth Estimation (No Depth Camera, One Camera Only)","text":""},{"location":"faq/ml/monocular-depth-estimation/#by-jeffrey-wang","title":"By Jeffrey Wang","text":"<p>Let's suppose you don't have/don't want to use a depth camera, and only have one camera at your disposal. How could you possibly find the depth of an image? Shockingly, there exist machine learning models that can not only perform this task but can also do so with relative accuracy and speed. </p>"},{"location":"faq/ml/monocular-depth-estimation/#tutorial","title":"Tutorial","text":"<p>We will use the MiDaS model for this tutorial for pytorch. </p> <p>In your code, do the following: <pre><code>midas = torch.hub.load(\"intel-isl/MiDaS\", \"MiDaS_small\")\nmidas_transforms = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\")\n\ndevice = torch.device(\"cpu\")\nmidas.to(device)\nmidas.eval()\n\ntransform = midas_transforms.small_transform\n</code></pre></p> <p>This loads the model and necessary image transformers. This is recommended to be put into the top of a class, so that it only needs to be loaded once.  It is not necessary to use MiDaS_small, you can choose whatever model you want. However, this one worked particularly well and without significant lag.</p> <p>The device MUST be cpu, unless you are running this on a device with a Nvidia GPU.</p> <p>Next, we apply transforms to our image before predicting: <pre><code>input_batch = transform(cv_image).to(self.device)\n\nwith torch.no_grad():\n    prediction = midas(input_batch)\n\n    prediction = torch.nn.functional.interpolate(\n        prediction.unsqueeze(1),\n        size=cv_image.shape[:2],\n        mode=\"bicubic\",\n        align_corners=False,\n    ).squeeze()\n</code></pre></p> <p>Note that the image must be a CV image, so don't forget to do  <code>cv_bridge.imgmsg_to_cv2(image)</code> on the image you are analyzing.</p> <p>Save your prediction with  <code>output = prediction.cpu().numpy()</code></p> <p>The model outputs a 2D array, where the element at the jth row, ith column represents the pixel at (i, j), if the image is starting from top left coordinates. When displaying the image, don't forget to normalize the pixels because it will display as grayscale unless you convert to RGB. The easiest way to do this is to divide the whole array by 1200, which is simply <code>output_array / 1200</code> because it is a numpy array.</p>"},{"location":"faq/ml/monocular-depth-estimation/#details","title":"Details","text":"<p>This is a disparity map, not a depth map. This means the \"depth\" values at each pixel are NOT accurate and DO NOT accurately represent space in the real world, but rather represent relative depths to other objects. This means an object 10 inches away in real life may either be at \"1000 units\" or \"400 units\" depending on how close or far away other objects are. There are many claims of real-time monocular depth estimation with metric depth (meaning accurate, real life measurements) but they have all been tested by us to either be extremely laggy or not accurate. </p>"},{"location":"faq/ml/object_detection_yolo_setup/","title":"Setting Up Object Detection Using yolo and darknet_ros","text":""},{"location":"faq/ml/object_detection_yolo_setup/#by-peter-zhao","title":"By Peter Zhao","text":""},{"location":"faq/ml/object_detection_yolo_setup/#introduction","title":"Introduction","text":"<p>The FAQ section describes how to integrate YOLO with ros that allows the turtlebot to be able to detect object using its camera. YOLO (You only look once) is a widely used computer vision algorith that makes use of convolutional neural networks (CNN) to detect and label an object. Typically before a machine learning algorithm can be used, one needs to train the algorithm with a large data set. Fortunately, YOLO provides many pre-trained model that we can use. For example, yolov2-tiny weight can classify around 80 objects including person, car, bus, bir, cat, dog, and so on.</p>"},{"location":"faq/ml/object_detection_yolo_setup/#darknet_ros","title":"darknet_ros","text":"<p>In order to integrate YOLO with ROS easily, one can use this third party package known as darknet_ros, which uses darknet (a neural network library written in C) to run YOLO and operates as a ros node. The node basically subscribes to the topics that has Image message type, and publishes bounding box information as BoundingBoxes message type to the /darknet_ros/bounding_boxes topic. </p>"},{"location":"faq/ml/object_detection_yolo_setup/#how-to-use-darknet_ros","title":"How to use darknet_ros","text":"<p>To use darknet_ros, first clone the github repo, and place this directory somewhere inside your catkin_workspace so that when you run catkin_make, this can be built:</p> <pre><code>cd ~/catkin_ws/src/\ngit clone https://github.com/leggedrobotics/darknet_ros\n</code></pre> <p>Afterward, run catkin_make to build the darknet_ros package <pre><code>cd ~/catkin_ws\ncatkin_make\n</code></pre></p> <p>You are now ready to run darknet_ros!</p>"},{"location":"faq/ml/object_detection_yolo_setup/#how-to-run-darknet_ros","title":"How to run darknet_ros","text":"<p>darknet_ros can be run aloneside with your other nodes. In your launch file include the following lines</p> <pre><code>&lt;include file=\"$(find darknet_ros)/launch/darknet_ros.launch\"&gt;\n         &lt;arg name=\"image\" value=\"rapicam_node/image/raw\"/&gt;\n&lt;/include&gt;\n</code></pre> <p>Next you need to download the pretrained weight or put the weight you've trained yourself into the darknet_ros/darknet_ros/yolo_network_config/ folder. To download a pretrained weight, follow the insturction given here. Essentially run the following commands</p> <pre><code>cd DIRECTORY_TO_DARKNET_ROS/darknet_ros/yolo_network_config/weights\nwget http://pjreddie.com/media/files/yolov2-tiny.weights\n</code></pre> <p>Then run this launch file</p> <pre><code>roslaunch [your_package_name] [your_launch_file.launch]\n</code></pre> <p>You should be able to see the darknet_ros node running with a window that displays the image with bounding boxes placed around objects. If you don't see any GUI window, try to check if the topic passed in in the \"image\" arg is publishing a valid image.</p>"},{"location":"faq/ml/object_detection_yolo_setup/#subscribing-to-darknet_ross-topics","title":"Subscribing to darknet_ros's topics","text":"<p>To receive information from the darknet_ros topics, we need to subscribes to the topic it is publishing. Here is an example usage:</p> <pre><code>from darknet_ros_msgs.msg import BoundingBoxes, BoundingBox\n\nclass ObjectRecognizer\n\n    def __init__(self):\n        self.boxes = []\n        self.box_sub = box_sub = rospy.Subscriber(topics.BOUNDING_BOXES, BoundingBoxes, self.get_bounding_box_cb())\n\n    def get_bounding_box_cb(self)\n\n        def cb(msg: BoundingBoxes):\n            self.boxes = msg.bounding_boxes\n            for box in self.boxes:\n                print(\"box_class: {}\".format(box.Class))\n                print(\"box_x_min: {}\".format(box.xmin))\n                print(\"box_x_max: {}\".format(box.xmax))\n                print(\"box_y_min: {}\".format(box.ymin))\n                print(\"box_y_max: {}\".format(box.ymax))\n                print()\n</code></pre> <p>The detailed description of the message types can be found in darknet_ros/darknet_ros_msg/msg folder. </p>"},{"location":"faq/ml/object_detection_yolo_setup/#working-with-compressedimage","title":"Working with CompressedImage","text":"<p>You might have noticed that darknet_ros expects to get raw image from the topic it subscribes to. Sometimes, we may want to use CompressedImage in order to reduce network latency so that the image can be published at a higher frame rate. This can be done by slightly modifying the source code of darknet_ros.</p> <p>I have forked the original darknet_ros repository and made the modification myself. If you wish to use it, you can simply clone this repo. Now you can modify the launch file to ensure that the darknet_ros subscirbes to a topic that publishes CompressedImage message type:</p> <pre><code>&lt;include file=\"$(find darknet_ros)/launch/darknet_ros.launch\"&gt;\n         &lt;arg name=\"image\" value=\"rapicam_node/image/compressed\"/&gt;\n&lt;/include&gt;\n</code></pre> <p>The changes I've made can be found in darknet_ros/darknet_ros/include/darknet_rosYoloObjectDetector.hpp and darknet_ros/darknet_ros/src/YoloObjectDetector.cpp.</p> <p>Essentially, you need to change the </p> <pre><code>void YoloObjectDetector::cameraCallback(const sensor_msgs::CompressedImageConstPtr&amp; msg) {\n  ROS_DEBUG(\"[YoloObjectDetector] USB image received.\");\n</code></pre> <p>method to accept msg of type sensor_msgs::ImageConstPtr&amp; instead of sensor_msgs::CompressedImageConstPtr&amp;. You also need to change the corresponding header file so that the method signature matches.</p>"},{"location":"faq/ml/object_recognition_based_on_color_and_size/","title":"Masking and Image Preparation","text":"<p>To start off, you need to process the image by removing all the sections that don't contain your target color. To do this we use opencv's inRange() function.</p> <p>An example of how to do this for red using a ROS compressed image message is</p> <p>` image = self.bridge.imgmsg_to_cv2(imagemsg)</p> <p>hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)</p> <p>lower_red = np.array([170,80,80]) upper_red = np.array([190,255,255])</p> <p>mask = cv2.inRange(hsv, lower_red, upper_red) output = cv2.bitwise_and(image, image, mask=mask) `</p> <p>An important note is that the Hue in the hsv array for the inRange() function is interpreted on a scale of 0-180, while saturation and value are on a scale of 0-255.</p> <p>This is important when comparing to a site like this colorpicker. Which measures hue from 0-360 and saturation and value from 0-100.</p>"},{"location":"faq/ml/object_recognition_based_on_color_and_size/#identifying-and-using-contours","title":"Identifying and using Contours","text":"<p>The following code will mark the contours and identify the largest one by area and then outline it with a rectangle. ` ret,thresh = cv2.threshold(mask, 40, 255, 0)         if (int(cv2.version[0]) &gt; 3):             contours, hierarchy = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)         else:             im2, contours, hierarchy = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)</p> <pre><code>    if len(contours) != 0:\n        # draw in blue the contours that were founded\n        cv2.drawContours(output, contours, -1, 255, 3)\n\n        # find the biggest countour (c) by the area\n        c = max(contours, key = cv2.contourArea)\n        x,y,w,h = cv2.boundingRect(c)\n\n        cx = x + (w/2)\n        cy = y + (h/2)\n\n        # draw the biggest contour (c) in green\n        cv2.rectangle(output,(x,y),(x+w,y+h),(0,255,0),2)\n</code></pre> <p>` cx and cy are the center point of the rectangle. You can use them or something else like a centroid to integrate this code into your program, such as to drive towards an object or avoid it.</p>"},{"location":"faq/ml/object_recognition_based_on_color_and_size/#example-image","title":"Example Image","text":"<p>In this image, the bottom left window is the original image, </p> <p>the second from the bottom is the mask of the target color (green in this example), </p> <p>the top right is the blue contour around the target color with a green rectangle identifying it as the largest contour,</p> <p>and the top right is the original image with a center point (calculated from the rectangle) that I used for navigation in my project</p> <p></p>"},{"location":"faq/ml/pretrained-model-deployment/","title":"Deploying a Pretrained Pytorch Model in Ubuntu 18.04 Virtual Environment","text":"<p>By Adam Ring</p> <p>Using a pre-trained deep learning model from a framework such as Pytorch has myriad applications in robotics, from computer vision to speech recognition, and many places inbetween. Sometimes you have a model that you want to train on another system with more powerful hardware, and then deploy the model elsewhere on a less powerful system. For this task, it is extremely useful to be able to transfer the weights of your trained model into another system, such as a virtual machine running Ubuntu 18.04. These methods for model transfer will also run on any machine with pytorch installed.</p>"},{"location":"faq/ml/pretrained-model-deployment/#note","title":"Note","text":"<p>It is extremely discouraged to mix versions of Pytorch between training and deployment. If you train your model on Pytorch 1.8.9, and then try to load it using Pytorch 1.4.0, you  may encounter some errors due to differences in the modules between versions. For this reason it is encouraged that you load your Pytorch model using the same version that is was trained on.</p>"},{"location":"faq/ml/pretrained-model-deployment/#saving-and-loading-a-trained-model","title":"Saving and loading a trained model","text":"<p>Let's assume that you have your model fully trained and loaded with all of the necessary weights.</p> <p><code>model = MyModel()</code></p> <p><code>model.train()</code></p> <p>For instructions on how to train a machine learning model, see this section on training a model in the lab notebook. There are multiple ways to save this model, and I will be covering just a few in this tutorial.</p>"},{"location":"faq/ml/pretrained-model-deployment/#saving-the-state_dict","title":"Saving the <code>state_dict</code>","text":"<p>This is reccommended as the best way to save the weights of your model as its <code>state_dict</code>, however it does require some dependencies to work. Once you have your model, you must specify a <code>PATH</code> to the directory in which you want to save your model. This is where you can name the file used to store your model.</p> <p><code>PATH = \"path/to/directory/my_model_state_dict.pt\"</code></p> <p>or</p> <p><code>PATH = \"path/to/directory/my_model_state_dict.pth\"</code></p> <p>You can either specify that the <code>state_dict</code> be saved using <code>.pt</code> or <code>.pth</code> format.</p> <p>Then, to save the model to a path, simply call this line of code.</p> <p><code>torch.save(model.state_dict(), PATH)</code></p>"},{"location":"faq/ml/pretrained-model-deployment/#loading-the-state_dict","title":"Loading the <code>state_dict</code>","text":"<p>Download the <code>my_model_state_dict.pt/pth</code> into the environment in which you plan on deploying it. Note the path that the state dict is placed in. In order to load the model weights from the <code>state_dict</code> file, you must first initialize an untrained istance of your model.</p> <p><code>loaded_model = MyModel()</code></p> <p>Keep in mind that this step requires you to have your model architecture defined in the environment in which you are deploying your model.</p> <p>Next, you can simply load your model weights from the state dict using this line of code.</p> <p><code>loaded_model.load_state_dict(torch.load(\"path/to/state/dict/my_model_state_dict.pt/pth\"))</code></p> <p>The trained weights of the model are now loaded into the untrained model, and you are ready to use the model as if it is pre-trained.</p>"},{"location":"faq/ml/pretrained-model-deployment/#saving-and-loading-the-model-using-torchscript","title":"Saving and loading the model using TorchScript","text":"<p>TorchScript is a framework built into Pytorch which is used for model deployment in many different types of environments without having the model defined in the  deployment environment. The effect of this is that you can save a model using tracing and load it from a file generated by tracing it.</p> <p>What tracing does is follow the operations done on an input tensor that is run through your model. Note that if your model has conditionals such as <code>if</code> statements or external dependencies, then the tracing will not record these. Your model must only work on tensors as well.</p>"},{"location":"faq/ml/pretrained-model-deployment/#saving-the-trace-of-a-model","title":"Saving the trace of a model","text":"<p>In order to trace your trained model and save the trace to a file, you may run the following lines of code.</p> <p><code>PATH = \"path/to/traced/model/traced_model.pt/pth\"</code> <code>dummy_input = torch.ones(typical_input_size, dtype=dype_of_typical_input)</code> <code>traced_model = torch.jit.trace(model, dummy_input)</code></p> <p><code>torch.jit.save(traced_model, PATH)</code></p> <p>The <code>dummy_input</code> can simply be a bare tensor that is the same size as a typical input for your model. You may also use one of the training or test inputs. The content of the dummy input does not matter, as long as it is the correct size.</p>"},{"location":"faq/ml/pretrained-model-deployment/#loading-the-trace-of-a-model","title":"Loading the trace of a model","text":"<p>In order to load the trace of a model, you must download the traced model <code>.pt</code> or <code>.pth</code> file into your deployment environment and note the path to it.</p> <p>All you need to do to load a traced model for deployment in Pytorch is use the following line of code.</p> <p><code>loaded_model = torch.jit.load(\"path/to/traced/model/traced_model.pt/pth\")</code></p> <p>Keep in mind that the traced version of your model will only work for torch tensors, and will not mimic the behavior of any conditional statements that you may have in your model.</p>"},{"location":"faq/ml/pretrained-model-deployment/#data-annotation","title":"Data Annotation","text":"<p>Please see the full tutorial in the repo: https://github.com/campusrover/Robotics_Computer_Vision/tree/master/utils/labelImg</p>"},{"location":"faq/ml/setup-coral-tpu/","title":"Detailed steps for using the Coral TPU","text":"<p>The Google Coral TPU is a USB machine learning accelerator which can plugged into a computer to increase machine learning performance on Tensorflow Lite models. The following documentation will detail how to install the relevant libraries and use the provided PyCoral library in Python to make use of the Coral TPU.</p>"},{"location":"faq/ml/setup-coral-tpu/#requirements","title":"Requirements","text":"<ul> <li>Linux Debian 10, or a derivative thereof (such as Ubuntu 18.04)</li> <li>A system architecture of either x86-64, Armv7 (32-bit), or Armv8 (64-bit), Raspberry Pi 3b+ or later</li> <li>One available USB port (for the best performance, use a USB 3.0 port)</li> <li>Python 3.6-3.9</li> <li>Note: For use with ROS, you will want to have ROS Noetic installed on Ubuntu 20.04.</li> </ul> <p>For details on how to set up for Windows or Mac OS, see Get started with the USB Accelerator on the Coral website.</p>"},{"location":"faq/ml/setup-coral-tpu/#installing-required-packages","title":"Installing Required Packages","text":"<p>Follow the following steps in order to get your environment configured for running models on the Coral TPU 1. Open up a terminal window and run the following commands:     - <code>echo \"deb https://packages.cloud.google.com/apt coral-edgetpu-stable main\" | sudo tee /etc/apt/sources.list.d/coral-edgetpu.list</code>     - <code>curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -</code>     - <code>sudo apt-get update</code> 2. Plug in your Coral TPU USB Accelerator into a USB 3.0 port on your computer. If you already have it plugged in, unplug it and then plug it back in. 3. Install one of the following Edge TPU Runtime libraries:</p> <pre><code>To install the reduced clock-speed library, run the following command:\n- `sudo apt-get install libedgetpu1-std`\n\nOr run this command to install the maximum clock-speed library:\n- `sudo apt-get install libedgetpu1-max`\n\n*Note:* If you choose to install the maximum clock-speed library, an excessive heat warning message will display in your terminal. To close this window, simply use the down arrow to select `OK` and press enter and your installation will continue.\n\nIn order to switch runtime libraries, just run the command corresponding to the runtime library that you wish to install. Your previous runtime installation will be deleted automatically.\n</code></pre> <ol> <li>Install the PyCoral Python library with the following command:<ul> <li><code>sudo apt-get install python3-pycoral</code></li> </ul> </li> </ol> <p>You are now ready to begin using the PyCoral TPU to run Tensorflow Lite models.</p>"},{"location":"faq/ml/setup-coral-tpu/#running-a-pretrained-tflite-model","title":"Running a pretrained TFLite model","text":"<p>The following section will detail how to download and execute a TFLite model that has been compiled for the Edge TPU</p>"},{"location":"faq/ml/setup-coral-tpu/#downloading-a-model","title":"Downloading a model","text":"<p>Pretrained TFLite models that have been precompiled for the Coral TPU can be found on the models section of the Coral website.</p> <ul> <li>Once you have downloaded your model, place it into a folder within your workspace.</li> </ul>"},{"location":"faq/multi-robot/change_robot_ip/","title":"How to change robot ip","text":"<p>The Project runs multiple robots at the same time. By signing one robot to another robot\u2019s IP, we can control two robots at the same time. However, due to environmental limitations, we can not run more than two robots since we don\u2019t want to interrupt others by corrupting the robot when we sign it to another robot.</p> <p>Step1: </p> <p>make sure robot 1 is on board through ssh</p> <p>then type</p> <pre><code>nano ~/.bashrc\n</code></pre> <p>Step2: </p> <p>change the IP address to robot2</p> <p>Then</p> <pre><code>source ~/.bashrc\n</code></pre> <p>save bashrc changes</p> <p>Then you should be able to launch the robot1 using</p> <pre><code>roslaunch turtlebot3_bringup turtlebot3_multi_robot.launch\n</code></pre>"},{"location":"faq/multi-robot/connect-multiple-robots/","title":"How to connect to multiple robots","text":"<p>Step 1: switch the .bashrc to be running in sim mode.    Step 1.1: Go into .bashrc file and uncomment the simulation mode as shown below:</p> <p># Setting for simulation mode   # $(bru mode sim)   # $(bru name roba -m $(myvpnip))</p> <p>Step 1.2: Comment out real mode/robot ip addresses For Example:   # Settings for donatello   # $(bru mode real)   # $(bru name donatello -m 100.106.194.39)</p> <p>Step 2: run roscore on vnc. To do this type \"roscore\" into the terminal</p> <p>Step 3: Now in the terminal do these steps   Step 3.1: get vpn ip address: In the terminal type \"myvpnipaddress\"    Step 3.2: Type \"$(bru mode real)\"   Step 3.3: Type \"$(bru name robotname -m \"vpn ip address from step 4\")\"   Step 3.4: type\"multibringup\" in each robot terminal</p> <p>Step 4: Repeat step 3 in a second tab for the other robot(s)</p>"},{"location":"faq/multi-robot/multi-robot-infrastructure/","title":"Multi Robot Setup","text":""},{"location":"faq/multi-robot/multi-robot-infrastructure/#how-to-namespace-multiple-robots-and-have-them-on-the-same-roscore","title":"How to namespace multiple robots and have them on the same roscore","text":"<p>To have multiple robots on the same ROS core and each of them listen to a separate node, namespacing would be an easy and efficient way to help.</p>"},{"location":"faq/multi-robot/multi-robot-infrastructure/#set-namespace-on-robots-onboard-computers-with-environment-variable","title":"Set namespace on robots' onboard computers with environment variable","text":"<ul> <li>Boot up the robot and ssh into it</li> <li>On the robot's onboard computer, open the terminal and type in</li> </ul> <pre><code>nano ~/.bashrc\n</code></pre> <p>Then add the following line to the end of the file, using the robots name as the namespace</p> <pre><code>export ROS_NAMESPACE={namespace_you_choose}\n</code></pre> <p>Also make the following other changes:</p> <pre><code>alias bu='roslaunch turtlebot3_bringup turtlebot3_robot.launch'\nexport IP=\"$(ifconfig | sed -En 's/127.0.0.1//;s/.*inet (addr:)?(([0-9]*\\.){3}[0-9]*).*/\\2/p')\"\nexport ROS_IP=$IP\nexport ROS_MASTER_URI=http://roscore1.cs.brandeis.edu:11311\nexport ROS_NAMESPACE=roba\nexport TB3_MODEL=burger\nexport TURTLEBOT3_MODEL=burger\n</code></pre> <ul> <li>Save the file and don't forget to do <code>source ~/.bashrc</code></li> </ul>"},{"location":"faq/multi-robot/multi-robot-infrastructure/#set-namespace-on-your-laptop-with-environment-variable","title":"Set namespace on your laptop with environment variable","text":"<ul> <li>Now that the robot is configured properly with its own unique name space, how do we talk to it?</li> <li>There are three ways:</li> <li>Configure your laptop to be permanently associated with the same name space</li> <li>Set a temporary environment variable that specifies the name space</li> <li>Add the namespace to a launch file</li> <li>Use the __ns parameter for roslaunch or rosrun (not recommended)</li> </ul>"},{"location":"faq/multi-robot/multi-robot-infrastructure/#permanently-associate-your-laptop-with-the-name-space","title":"Permanently associate your laptop with the name space","text":"<ul> <li>Use the same steps above. Make sure your namespace is exactly the same as the namespace of the robot you want to control.</li> <li>From now on, whenever you do, e.g. a cmd_vel, it will be directed just to your robot.</li> </ul>"},{"location":"faq/multi-robot/multi-robot-infrastructure/#use-an-environment-variable","title":"Use an environment variable","text":"<ul> <li>Set namespace for a termimal with temporary environment variable.</li> <li>To set a namespace temporarily for a terminal, which will be gone when you close the termial, just type in <code>export ROS_NAMESPACE={namespace_you_choose}</code> directly in your terminal window.</li> <li>You can use <code>echo $ROS_NAMESPACE</code> to check it.</li> </ul>"},{"location":"faq/multi-robot/multi-robot-infrastructure/#use-the-launch-file","title":"Use the .launch file","text":"<ul> <li>To set namespace for a node in launch file, Use the attribute <code>ns</code>, for example:</li> </ul> <p><code>&lt;node name=\"listener1\" pkg=\"rospy_tutorials\" type=\"listener.py\" ns=\"{namespace_you_choose}\" /&gt;</code></p>"},{"location":"faq/multi-robot/multi-robot-infrastructure/#not-recommended-add-__ns-to-the-run-or-launch-command","title":"Not recommended: add __ns to the run or launch command","text":"<ol> <li>Launch/Run a file/node with namespace in terminal. Add a special key <code>__ns</code> (double underscore here!) to your command, for example: <code>roslaunch turtlebot3_teleop turtlebot3_teleop_key.launch __ns:={namespace_you_choose}</code> However,</li> <li>Use of this keyword is generally not encouraged as it is provided for special cases where environment variables cannot be set. (http://wiki.ros.org/Nodes)</li> </ol>"},{"location":"faq/multi-robot/multi-robot-infrastructure/#publishingsubsribing-topics-in-other-namespace","title":"Publishing/Subsribing topics in other namespace","text":"<p>Do a <code>rostopic list</code>, you will find out topics under a namespace will be listed as <code>/{namespace}/{topic_name}</code></p>"},{"location":"faq/multi-robot/multi-robot-infrastructure/#make-changes-to-turtlebot3_navigation-package-on-your-laptop","title":"Make changes to turtlebot3_navigation package on your laptop","text":"<ul> <li>Type <code>roscd turtlebot3_navigation</code> to go to the package directory.</li> <li>Type <code>cd launch</code> to go to the folder that stores .launch files.</li> <li>Open the <code>turtlebot3_navigation.launch</code> file with nano or your favorite code editor.</li> <li>You will see the scripts for launching move base and rviz:</li> </ul> <pre><code>  &lt;!-- move_base --&gt;\n  &lt;include file=\"$(find turtlebot3_navigation)/launch/move_base.launch\"&gt;\n    &lt;arg name=\"model\" value=\"$(arg model)\" /&gt;\n    &lt;arg name=\"move_forward_only\" value=\"$(arg move_forward_only)\"/&gt;\n  &lt;/include&gt;\n\n  &lt;!-- rviz --&gt;\n  &lt;group if=\"$(arg open_rviz)\"&gt;\n    &lt;node pkg=\"rviz\" type=\"rviz\" name=\"rviz\" required=\"true\"\n          args=\"-d $(find turtlebot3_navigation)/rviz/turtlebot3_navigation.rviz\"/&gt;\n  &lt;/group&gt;\n</code></pre>"},{"location":"faq/multi-robot/multi-robot-infrastructure/#change-move-base-arguments","title":"Change move base arguments","text":"<ul> <li>Add another argument <code>cmd_vel_topic</code> to the four arguments in the <code>&lt;!-- Arguments --&gt;</code> section:</li> </ul> <pre><code>  &lt;!-- Arguments --&gt;\n  &lt;arg name=\"model\" default=\"$(env TURTLEBOT3_MODEL)\" doc=\"model type [burger, waffle, waffle_pi]\"/&gt;\n  &lt;arg name=\"map_file\" default=\"$(find turtlebot3_navigation)/maps/map.yaml\"/&gt;\n  &lt;arg name=\"open_rviz\" default=\"true\"/&gt;\n  &lt;arg name=\"move_forward_only\" default=\"false\"/&gt;\n  &lt;arg name=\"cmd_vel_topic\" default=\"/{namespace_you_choose}/cmd_vel\"/&gt;\n</code></pre> <ul> <li> <p>Pass the new argument to move base launch file:</p> <pre><code>&lt;!-- move_base --&gt;\n&lt;include file=\"$(find turtlebot3_navigation)/launch/move_base.launch\"&gt;\n &lt;arg name=\"model\" value=\"$(arg model)\" /&gt;\n &lt;arg name=\"move_forward_only\" value=\"$(arg move_forward_only)\"/&gt;\n &lt;arg name=\"cmd_vel_topic\" value=\"$(arg cmd_vel_topic)\"/&gt;\n&lt;/include&gt;\n</code></pre> </li> <li> <p>These changes will make move base publish to <code>/{namespace_you_choose}/cmd_vel</code> instead of <code>/cmd_vel</code>. Open the <code>move_base.launch</code> file in the launch folder, you will see why it worked.</p> </li> </ul>"},{"location":"faq/multi-robot/multi-robot-infrastructure/#change-rviz-arguments","title":"Change rviz arguments","text":"<ul> <li>Type <code>roscd turtlebot3_navigation</code> to go to the package directory.</li> <li>Type <code>cd rviz</code> to go to the folder that has <code>turtlebot3_navigation.rviz</code>. It stores all the arguments for rviz.</li> <li>Open <code>turtlebot3_navigation.rviz</code> with VSCode or other realiable code editors, since this file is long and a bit messy.</li> <li>In the .rviz file, search for all the lines that has the part <code>Topic:</code></li> <li>Add your namespace to the topics you found. For example, change</li> </ul> <pre><code>  Topic: /move_base/local_costmap/footprint\n  `\n</code></pre> <pre><code>to\n</code></pre> <pre><code>  Topic: /roba/move_base/local_costmap/footprint\n</code></pre> <ul> <li>These changes will make rviz subsribe to topics in your namespace.</li> </ul>"},{"location":"faq/multi-robot/multi-robot-one-core/","title":"Running multiple robots on a single roscore","text":"<p>Joshua Liu</p>"},{"location":"faq/multi-robot/multi-robot-one-core/#normal-behavior","title":"Normal behavior","text":"<p>Normally, running the bringup launch file will create a new roscore running on that robot. This, of course, is not desired.</p>"},{"location":"faq/multi-robot/multi-robot-one-core/#step-0-decide-on-your-desired-structure","title":"Step 0: Decide on your desired structure","text":"<p>These configurations should both be possible:</p> <ul> <li> <p>Run roscore on one robot </p> </li> <li> <p>Run roscore on a seperate computer (probably your VNC environment)</p> </li> </ul> <p>We will assume the first configuration here.      I have not actually testes the second one, but I see no reason it wouldn't work.</p>"},{"location":"faq/multi-robot/multi-robot-one-core/#setp-1-edit-bashrc-file-of-your-auxiliary-robots","title":"Setp 1: Edit .bashrc file of your \"auxiliary\" robot(s)","text":"<p>SSH into your auxiliary robot (We will use \"auxiliary\" here to mean \"not running roscore\").</p> <p>Open the .bashrc file in your editor of choice</p> <p>Navigate to where it says</p> <pre><code>$(bru name ROBOTNAME -m 127.0.0.1)\n</code></pre> <p>Change the ROBOTNAME to the name of your computer (robot is computer too) that will run roscore. Change the IP address to the IP address of your core robot.   Save your changes.</p>"},{"location":"faq/multi-robot/multi-robot-one-core/#step-2-copy-and-edit-the-bringup-launch-file","title":"Step 2: Copy and edit the bringup launch file","text":"<p>Do this for each robot, even your \"core\" (running roscore) one.</p> <p>While still SSH'ed into that robot, run <code>roscd bringup</code>  Navigate to the <code>launch</code> directory   Run <code>cp turtlebot3_robot.launch turtlebot3_multi_robot.launch</code> to copy the file. Please do this.    </p> <p>Open the newly created <code>turtlebot3_multi_robot.launch</code> file in your editor of choice.    </p> <p>At the top, there should be two <code>&lt;arg&gt;</code> tags.</p> <p>After them, put on a new line: <code>&lt;group ns=\"$(arg multi_robot_name)\"&gt;</code></p> <p>Then, right before the very bottom <code>&lt;/launch&gt;</code>, add a closing <code>&lt;/group&gt;</code> tag.   The result should look something like:</p> <pre><code>&lt;launch&gt;\n    &lt;arg&gt; lorem ipsum dolor sit amet ... &lt;/launch&gt;\n    &lt;arg&gt; lorem ipsum dolor sit amet ... &lt;/launch&gt;\n    &lt;group ns=\"$(arg multi_robot_name)\"&gt;\n        &lt;stuff&gt; &lt;/stuff&gt;\n        &lt;stuff&gt; &lt;/stuff&gt;\n        &lt;stuff&gt; &lt;/stuff&gt;\n        &lt;stuff&gt; &lt;/stuff&gt;\n    &lt;/group&gt;\n&lt;/launch&gt;\n</code></pre> <p>Repeat the two above steps on all your auxiliary robots.</p> <p>What does this do? While it is not necessary, this will prefix all the nodes created by bringup with the robot's name.    If you don't do this, there will be many problems.</p>"},{"location":"faq/multi-robot/multi-robot-one-core/#launching-the-robots","title":"Launching the robots","text":"<p>When SSH'ed into a robot, you can't just run <code>bringup</code> anymore. Use this command instead:</p> <p><code>roslaunch turtlebot3_bringup turtlebot3_robot.launch multi_robot_name:=THEROBOTNAME</code></p> <p>This will cause the bringup nodes to be properly prefixed with the robot's name.   For example, <code>raspicam_node/images/compressed</code> becomes <code>name/raspicam_node/images/compressed</code>.   Additionally, the robot will also subscribe to some different nodes.   The only one I can know of so far is <code>cmd_vel</code> which becomes <code>name/cmd_vel</code>. Keep this in mind if your robots don't move.</p> <p>As long as you launch the core robot first, this should work.</p> <p>See what it looks like by running <code>rostopic list</code> from your (properly configured to point at the core computer) vnc environment.</p>"},{"location":"faq/multi-robot/multi-robot-one-core/#acknowledgments","title":"Acknowledgments","text":"<p>Very many thanks to August Soderberg, who figured most of this out.</p>"},{"location":"faq/multi-robot/multi_robot_gazebo_real/","title":"Running Multi Robot in Gazebo and Real Robot","text":"Launching on Gazebo  <p>If you are interested in launching on the real turtlebot3, you are going to have to ssh into it and then once you have that ssh then you will be able to all bringup on it. There is more detail about this in other FAQs that can be searched up. When you are running multirobots, be aware that it can be a quite bit slow because of concurrency issues.</p> <p>These 3 files are needed to run multiple robots on Gazebo. In the object.launch that is what you will be running roslaunch. Within the robots you need to spawn multiple one_robot and give the position and naming of it.</p> <p></p> <p>Within the object.launch of line 5, it spawns an empty world. Then when you have launched it you want to throw in the guard_world which is the one with the multiple different colors and an object to project in the middle. Then you want to include the file of robots.launch because that is going to be spawning the robots. </p> <p></p> <p>For each robot, tell it to spawn. We need to say that it takes in a robot name and the init_pose. And then we would specify what node that it uses.</p> <p></p> <p>Within the robots.launch, we are going to have it spawn with specified position and name.  </p>  Launching Real  <p>Make sure your bashrc looks something like this:</p> <pre><code>\n# export ROS_MASTER_URI=http://100.74.60.34:11311\nexport ROS_MASTER_URI=http://100.66.118.56:11311\n# export ROS_IP=100.66.118.56\nexport ROS_IP=100.74.60.34\n# Settings for a physical robot \n$(bru mode real)\n# $(bru name robb -m 100.99.186.125)\n# $(bru name robc -m 100.117.252.97)\n$(bru name robc -m $(myvpnip))\n# $(bru name robb -m $(myvpnip))\n</code></pre> <p>Then you need to ssh into the robots to start up the connection. You will need to know the ip of the robot and you can get this by doing a tailscale grep on the the robot of interest, and that can be roba, robb, robc, rafael, or donanotella as only these turtlebot are set up for that. </p> <p>Then you will need to then call on (bru mode real) and (bru name -m ) and multibringup. The multibringup is a script that Adam Rings wrote so that these connectiosn can set up this behavior."},{"location":"faq/multi-robot/multirobot-map-merge/","title":"Multirobot map merge","text":""},{"location":"faq/multi-robot/multirobot-map-merge/#evalyn-berleant-kelly-duan","title":"Evalyn Berleant, Kelly Duan","text":"<p>This FAQ section assumes understanding of creating TF listeners/broadcasters, using the TF tree, namespacing, and launching of multiple robots. This tutorial also relies on the ROS gmapping package although different SLAM methods can be substituted.</p>"},{"location":"faq/multi-robot/multirobot-map-merge/#setting-up-launch-files-for-gmapping","title":"Setting up launch files for gmapping","text":"<p>Gmapping will require namespaces for each robot. If you want to use the launch file <code>turtlebot3_gmapping.launch</code> from the package <code>turtlebot3_slam</code>, be sure to pass values to the arguments <code>set_base_frame</code>, <code>set_odom_frame</code>, and <code>set_map_frame</code> to use the robot namespaces. For instance, if the default value is <code>base_footprint</code>, the namespaced value will be <code>$(arg ns)/base_footprint</code>. You can also directly call the node for slam_gmapping inside the namespace (below it is <code>$(aarg ns)</code>) with</p> <pre><code>&lt;node pkg=\"gmapping\" type=\"slam_gmapping\" name=\"turtlebot3_slam_gmapping\" output=\"log\"&gt;\n    &lt;param name=\"base_frame\" value=\"$(arg ns)/base_footprint\"/&gt;\n    &lt;param name=\"odom_frame\" value=\"$(arg ns)/odom\"/&gt;\n    &lt;param name=\"map_frame\"  value=\"$(arg ns)/map\"/&gt;\n&lt;/node&gt;\n</code></pre>"},{"location":"faq/multi-robot/multirobot-map-merge/#using-multirobot_map_merge-package","title":"Using multirobot_map_merge package","text":"<ol> <li>First navigate to the ros documentation for multirobot_map_merge. Install depending on the version (newer ROS versions may need to clone directly from the respective branches of m-explore).</li> <li><code>catkin_make</code></li> <li>If not using initial poses of robots, simply use <code>map_merge.launch</code> as is. In the initial file, <code>map_merge</code> has a namespace but by removing the namespace the merged map will be directl published to <code>/map</code>.</li> <li>If using initial poses of robots, you must add parameters within the robot namespace named <code>map_merge/init_pose_x</code>, <code>map_merge/init_pose_y</code>, <code>map_merge/init_pose_z</code>, and <code>map_merge/init_pose_yaw</code>.</li> </ol>"},{"location":"faq/multi-robot/multirobot-map-merge/#using-merged-map-for-cost-map","title":"Using merged map for cost map","text":"<p>The current map merge package does not publish a tf for the map. As such, one must create a TF frame for the map and connect it to the existing tree, making sure that the base_footprints of each robot can be reached from the map, before using things such as move_base.</p>"},{"location":"faq/multi-robot/multirobot-map-merge/#saving-multiplenamespaced-maps","title":"Saving multiple/namespaced maps","text":"<p>Maps can be saved directly through the command line, but multiple maps can also be saved at once by creating a launch file that runs the <code>map_saver</code> node. If running the <code>map_saver</code> node for multiple maps in one launch, each node will also have to be namespaced.</p> <p>When using <code>map_saver</code>, be sure to set the map parameter to the dynamic map of the robot (will look like <code>namespace/dynamic_map</code>, where <code>namespace</code> is the name of the robot\u2019s namespace), instead of just the at <code>namespace/map</code> topic.</p> <p>Example:</p> <pre><code>&lt;node name=\"map_saver\" pkg=\"map_server\" type=\"map_saver\" \n    args=\"-f $(find swarmbots)/maps/robot$(arg newrobots)\" output=\"screen\"&gt;\n        &lt;param name=\"map\" value=\"/robot$(arg newrobots)/dynamic_map\"/&gt;\n    &lt;/node&gt;\n</code></pre> <p>will set the map to be saved as the one specified in the parameter, meanwhile the argument <code>-f $(find swarmbots)/maps/robot$(arg newrobots)</code> will set the save destination for the map files as the <code>maps</code> folder, and the names of the files as <code>robot$(arg newrobots).pgm</code> and <code>robot$(arg newrobots).yaml</code>.</p> <p>To save a map merged by the map_merge package, do not namespace the map_merge launch file and instead have it publish directly to <code>/map</code>, and map saver can be used on <code>/map</code>.</p>"},{"location":"faq/multi-robot/multirobot-map-merge/#references","title":"References","text":"<ul> <li>Multirobot map merge package</li> </ul>"},{"location":"faq/multi-robot/setup-hints/","title":"Setup Troubleshooting","text":""},{"location":"faq/multi-robot/setup-hints/#hints-on-how-to-set-things-up","title":"Hints on how to set things up","text":"<ul> <li>Reflects the \"new\" multi robot setup</li> </ul>"},{"location":"faq/multi-robot/setup-hints/#bashrc-on-both-your-laptop-and-your-robot","title":".bashrc ON BOTH your laptop and your Robot","text":"<ul> <li>This shell script is run automatically whenever you open a new terminal or tab in a terminal</li> <li>This means that if you edit it and save it, it still does nothing until you open a new terminal</li> <li>If you want to apply it without opening a new terminal, do <code>source ~/.bashrc</code></li> <li>These lines should be on your laptop as well as your robot, the same way.</li> </ul> <pre><code>alias bu='roslaunch turtlebot3_bringup turtlebot3_robot.launch'\nexport ROS_MASTER_URI=http://roscore1.cs.brandeis.edu:11311\nexport ROS_NAMESPACE=roba\nexport ROS_IP=&lt;ip address of computer where this .bashrc is stored&gt;\nexport TB3_MODEL=burger\nexport TURTLEBOT3_MODEL=burger\n</code></pre> <ul> <li>ROS_HOSTNAME: the IP address of this computer.</li> <li>ROS_MASTER: the IP address of whereever ROSCORE is running. We have roscore always running on the little computer by the door. It happens to be called roscore1.cs.brandeis.edu</li> <li>ROS_NAMESPACE: Indicates a unique name for a specific robot. That way when we send a /cmd_vel it is turned into, e.g. /roba/cmd_vel. This way all the robots can coexist on one ROSCORE. It also means you don't have to run your own ROSCORE anymore.</li> </ul>"},{"location":"faq/multi-robot/setup-hints/#on-the-robot","title":"ON THE ROBOT","text":"<ul> <li>SSH into the robot</li> <li>Run the following command to launch ROS on the robot</li> </ul> <pre><code>roslaunch turtlebot3_bringup turtlebot3_robot.launch\n</code></pre>"},{"location":"faq/multi-robot/setup-hints/#test-the-configuration","title":"Test the configuration","text":"<ul> <li>Did you do bringup on the ROBOT (NOT ON YOUR COMPUTER!</li> <li>if not, do so!</li> <li>Did bringup work</li> <li>if not, check your environment variables</li> <li><code>printenv | grep ROS</code></li> <li>It should include all your settings and also env variables for the path to ROS</li> <li>Make sure you run <code>source ~/.bashrc</code></li> <li>Can you run rostopic list? <code>rostopic list</code></li> <li>If not, check your IP addresses</li> <li>Check whether the ROSCORE computer is running.</li> <li>Try pinging roscore1.cs.brandeis.edu</li> <li>Does it list /x/cmd_vel among them?</li> <li>If not, check your IP addresses. Also make sure you ran</li> <li>Can you teleop the robot? <code>roslaunch turtlebot3_teleop turtlebot3_teleop_key.launch</code></li> <li>if not, try a manual cmd_vel: rostopic pub /rob*/cmd_vel</li> </ul>"},{"location":"faq/multi-robot/setup-hints/#additional-intermediate-bash-tips","title":"Additional Intermediate Bash Tips","text":""},{"location":"faq/multi-robot/setup-hints/#aliases-live-better-type-lesstm","title":"Aliases: Live Better, Type Less(TM)","text":"<p>You may notice that the ros installation process added a few Aliases to your .bashrc file. These serve to make your life easier, and you can add more of your own custom aliases to make your life even easier. Here are a few recommended changes to make:</p>"},{"location":"faq/multi-robot/setup-hints/#new-editor-in-eb","title":"New Editor in eb","text":"<p>You'll notice eb uses nano by default. nano is fine, if you enjoy living in the dark ages. To make eb open a different text editor, just replace nano with whatever your favorite installed editor is - code (vscode), atom, vim, emacs, etc. Bonus tip: if you're not already using <code>eb</code> as a shortcut to edit the .bashrc, start using eb!</p>"},{"location":"faq/multi-robot/setup-hints/#networking-shortcuts","title":"Networking Shortcuts","text":"<p>Try using this chunk in your bashrc:</p> <pre><code>alias connect='ssh $ROBOT@$ROBOT.dyn.brandeis.edu'\nexport ROBOT=robc\nexport ROS_MASTER_URI=http://$ROBOT.dyn.brandeis.edu:11311\nexport IP=\"$(ifconfig | sed -En 's/127.0.0.1//;s/.*inet (addr:)?(([0-9]*\\.){3}[0-9]*).*/\\2/p')\"\nexport ROS_IP=$IP\n</code></pre> <p>NB the snippet of code above uses robc as the current robot, robc can be replaced with any valid robot name Here's what this chunk does: 1. The alias \"connect\" will ssh to your desired preconfigured robot (saves quite a bit of typing) 1. The variable ROBOT is used to reduce the redundancy of typing the robot name anythime you need to ssh or change your ROS_MASTER_URI 1. The last two lines will automatically obtain your IP address anytime you start a new terminal window Some additional things to note: 1. The connect alias grabs ROBOT when it is called, so it can be defined before ROBOT, but ROS_MASTER_URI grabs ROBOT when it is defined, so ROBOT must be defined first 1. The command <code>export ROBOT=&lt;name&gt;</code> can temporarily change the value of ROBOT - anything that accesses ROBOT will grab the temporary value (Think about what this means given the last point: connect wil ssh to the temporary robot, but ROS_MASTER_URI will not update)</p>"},{"location":"faq/multi-robot/setup-hints/#universally-useful-time-savers","title":"Universally Useful Time Savers","text":"<p>One of the most common problems is that when you kill a ROS script, the most recent cmd_vel latches and the robot will continue in that manner therefore, it is useful to have some sort of emergency stop procedures. 2 viable and common options: 1. create an alias for <code>rostopic pub</code> that publishes a blank twist to cmd_vel 1. create an alias that launches teleop - this take longer, but will immediately stop the robot as soon as it is running.</p>"},{"location":"faq/multi-robot/setup-hints/#the-possibilities-dont-stop-here","title":"The Possibilities Don't Stop Here","text":"<p>There's obviously still much more that could done to make a more efficient terminal experience. Don't be afraid to become a bash master. Here are some resources to help get started becoming more proficient in bash: Bash Cheatsheet</p>"},{"location":"faq/multi-robot/spawn_model_terminal/","title":"Spawn Object to Gazebo via Terminal ROS Service Call","text":"<p>By Frank Hu</p> <p>Have Gazebo Simulation started, and open a different terminal tab, and enter the following command to spawn a model (.urdf)</p> <pre><code>rosrun gazebo_ros spawn_model -file `rospack find MYROBOT_description`/urdf/MYROBOT.urdf -urdf -x 0 -y 0 -z 1 -model MYROBOT\n</code></pre> <p>This command can also been used for <code>.sdf</code> files by replacing flag <code>-urdf</code> with <code>-sdf</code> </p> <p>For <code>.xacro</code> files, <code>.xacro</code> first need to be converted to <code>.xml</code> with:</p> <pre><code>rosrun xacro xacro `rospack find rrbot_description`/urdf/MYROBOT.xacro &gt;&gt; `rospack find rrbot_description`/urdf/MYROBOT.xml\n</code></pre> <p>then</p> <pre><code>rosrun gazebo_ros spawn_model -file `rospack find rrbot_description`/urdf/MYROBOT.xml -urdf -y 1 -model rrbot1 -robot_namespace rrbot1\n</code></pre> <p>For more information, enter</p> <p><code>rosrun gazebo_ros spawn_model -h</code></p>"},{"location":"faq/multi-robot/spawn_multiple_robots/","title":"Spawning Multiple Robots","text":""},{"location":"faq/multi-robot/spawn_multiple_robots/#author-belle-scott","title":"Author: Belle Scott","text":"<p>In order to spawn multiple robots in a gazebo launch file (in this example, there are two robots, seeker and hider), you must define their x,y,z positions, as shown below. Also shown is setting the yaw, which is used to spin the robot. For example, if one robot was desired to face forward and the other backward, ones yaw would be 0 while the others would be 3.14. </p> <pre><code>    &lt;arg name=\"model\" default=\"burger\" doc=\"model type [burger, waffle, waffle_pi]\"/&gt;\n    &lt;arg name=\"seeker_pos_x\" default=\"0\"/&gt;\n    &lt;arg name=\"seeker_pos_y\" default=\" -0.5\"/&gt;\n    &lt;arg name=\"seeker_pos_z\" default=\" 0.0\"/&gt;\n    &lt;arg name=\"seeker_yaw\" default=\"0\"/&gt;\n    &lt;arg name=\"hider_pos_x\" default=\"0\"/&gt;\n    &lt;arg name=\"hider_pos_y\" default=\"0.5\"/&gt;\n    &lt;arg name=\"hider_pos_z\" default=\" 0.0\"/&gt;\n    &lt;arg name=\"hider_yaw\" default=\"0\" /&gt;\n</code></pre> <p>After setting their initial positions, you must use grouping for each robot. Here, you put the logic so the launch file knows which python files correspond to which robot. It is also very important to change the all the parameters to match your robots name (ex: $(arg seeker_pos_y)). Without these details, the launch file will not spawn your robots correctly. </p> <pre><code>    &lt;group ns=\"seeker\"&gt;\n        &lt;param name=\"robot_description\" command=\"$(find xacro)/xacro.py $(find turtlebot3_description)/urdf/turtlebot3_$(arg model).urdf.xacro\" /&gt;\n        &lt;node pkg=\"robot_state_publisher\" type=\"robot_state_publisher\" name=\"robot_state_publisher\" output=\"screen\"&gt;\n            &lt;param name=\"publish_frequency\" type=\"double\" value=\"50.0\" /&gt;\n            &lt;param name=\"tf_prefix\" value=\"seeker\" /&gt;\n        &lt;/node&gt;\n        &lt;node name=\"spawn_urdf\" pkg=\"gazebo_ros\" type=\"spawn_model\" args=\"-urdf -model seeker -x $(arg seeker_pos_x) -y $(arg seeker_pos_y) -z $(arg seeker_pos_z)          -Y $(arg seeker_yaw) -param robot_description\" /&gt;\n    &lt;/group&gt;\n    &lt;group ns=\"hider\"&gt;\n        &lt;param name=\"robot_description\" command=\"$(find xacro)/xacro.py $(find turtlebot3_description)/urdf/turtlebot3_$(arg model).urdf.xacro\" /&gt;\n        &lt;node pkg=\"robot_state_publisher\" type=\"robot_state_publisher\" name=\"robot_state_publisher\" output=\"screen\"&gt;\n            &lt;param name=\"publish_frequency\" type=\"double\" value=\"50.0\" /&gt;\n            &lt;param name=\"tf_prefix\" value=\"hider\" /&gt;\n        &lt;/node&gt;\n        &lt;node name=\"spawn_urdf\" pkg=\"gazebo_ros\" type=\"spawn_model\" args=\"-urdf -model hider -x $(arg hider_pos_x) -y $(arg hider_pos_y) -z $(arg hider_pos_z) -Y               $(arg hider_yaw) -param robot_description\" /&gt;\n    &lt;/group&gt;\n</code></pre>"},{"location":"faq/multi-robot/spawn_multiple_robots/#references","title":"References","text":"<ul> <li>This is from TheConstruct: Launch Multiple Robots in Gazebo</li> <li>And this is from our own Belle Scott: Hide and Seek</li> </ul>"},{"location":"faq/storage/copying-robot-sd-cards/","title":"How to copy robot OS from one sd card to another on Linux","text":"<p>Often in the lab the need arises to create several robots with identical operating systems and packages, however it is incredibly time consuming to flash the base operating system to a new sd card and download all the required packages to each new robot. </p> <p>This guide will explain how to create identical boot sd cards from one working operating system in order to scale the ever-growing fleet of terminator robots this lab hopes to one day create, so let's get started.</p>"},{"location":"faq/storage/copying-robot-sd-cards/#shrink-the-existing-sd-card-partition","title":"Shrink the existing SD card partition","text":"<p>Say you have a working operating system on a robot with an SD card of 128GB, but most of the space on that SD card is unused. If you created an OS image from this SD card it too would be 128GB which is large and time consuming to flash, not to mention it won't fit on smaller SD cards. You'll first need to shrink your existing card to the minimum size to fit the operating system.</p> <p>Plug the SD card into your linux computer and boot up the program GParted. Select the SD card from the menu in the top left. You'll want to unmount both of the partitions displayed by right clicking on them in the list menu, and you'll then see they key icon next to their names disappear.</p> <p></p> <p>You're going to resize the larger of the two partitions by selecting it from the list menu pressing the resize button in the top menu. Resize the partition to be slightly larger than the amount of used space. In my case here, I'm resizing the partion to be 13000MB with 12612MB used.</p> <p></p> <p>Then you're going to apply the resizing operation by clicking the green checkmark on the top menu bar, it should take a little bit of time to complete. </p> <p>Now you are ready to create your image file. Open your terminal and type the command <code>sudo fdisk -l</code>. It should display a list of storage partitions, so find the partition in the list </p> <p></p> <p>Now </p>"},{"location":"faq/storage/ssd-instructions/","title":"Creating a bootable SSD","text":"<p>This procedure is very finicky. We used it before we had our Rover cluster. Avoid doing this if you can!</p> <p>Important</p> <p>THis procedure is no longer used in the class. Students get an account on our Rover cluster so you don't need to figure out how to set up ROS on your own computer.</p>"},{"location":"faq/storage/ssd-instructions/#introduction","title":"Introduction","text":"<ul> <li>SSD Solid state disk. Either internal or external. In our case we are using external SSDs which connect to your computer with a USB connection.</li> <li>USB Stick Also known as a thumb drive, flash drive, memory stick, a memory key. Its a small form factor, dongle like device which also plugs into your computer.</li> <li>Bootable SSD and Bootable USB Stick have been formatted and created in such a way that the computer can boot from them and thereby leave your default internal disks untouched.</li> </ul> <p>Overview of the procedure We are going to create a bootable SSD with these instructions. You will:</p> <ul> <li>Download the right version of Ubuntu onto your computer</li> <li>Plug in your USB stick (which will be erased)</li> <li>\"Flash\" the USB stick which will make it bootable</li> <li>Boot using that USB stick</li> <li>This brings up the Ubuntu installer. You will use it to just \"try ubuntu\"</li> <li>You will now plug in the SSD and format it with the right partitions</li> <li>Next you will install Ubuntu 18.04 onto the SSD and reboot.</li> </ul>"},{"location":"faq/storage/ssd-instructions/#create-bootable-ubuntu-usb-stick","title":"Create Bootable Ubuntu USB stick","text":"<p>In order to install Ubuntu onto an SSD, we need to first create a bootable USB Flash Drive. This drive will be used to boot Ubuntu and install it onto the SSD. The process can be completed as follows:</p> <ol> <li>Plug the USB SSD drive into your computer</li> <li> <p>Download the appropriate Ubuntu 18.04 desktop image for your machine</p> <p>http://releases.ubuntu.com/18.04/ 3. Flash the Ubuntu image onto your USB stick.</p> </li> </ol> <p>How to flash Ubuntu with Mac: https://tutorials.ubuntu.com/tutorial/tutorial-create-a-usb-stick-on-macos#0</p> <p>How to flash Ubuntu with Windows: https://tutorials.ubuntu.com/tutorial/tutorial-create-a-usb-stick-on-windows#0</p> <p>How to flasg Ubuntu with Ubuntu: https://tutorials.ubuntu.com/tutorial/tutorial-create-a-usb-stick-on-ubuntu#2</p> <p>With Ubuntu flashed onto your flash drive, it can now be booted and installed onto your SSD. If your SSD has anything on it, be sure to format it before moving on to the next step.</p>"},{"location":"faq/storage/ssd-instructions/#2-boot-ubuntu","title":"2. Boot Ubuntu","text":"<p>Booting from a USB depends on your operating system. Instructions for Windows and Mac are below:</p>"},{"location":"faq/storage/ssd-instructions/#macos","title":"MacOS","text":"<p>Restart your computer. While it is booting up, hold down the alt/option button. A menu should appear, select the \"EFI Boot\" option. If the MacOS login screen appears, you will need to restart and try again.</p>"},{"location":"faq/storage/ssd-instructions/#windows","title":"Windows","text":"<p>Restart your computer. While it is booting up, press the boot menu key. This is often one of the F keys on most Windows machines, but sometimes ESC as well. With the boot menu button pressed, you will be shown the BIOS menu. From here you need to change the boot order so that your Ubuntu USB drive is prioritized above the drive where Windows is installed. Once this is done, reboot and Ubuntu should be booted.</p> <p>Continuing...</p> <p>With Ubuntu running on your machine, plug in your external SSD (while keeping the USB stick plugged in) and select \u201cTry ubuntu without installing\u201d to begin the installation process.</p> <p>NOTE: If you are unable to use your mouse and keyboard on the Ubuntu desktop, a USB keyboard and mouse is required from this point on.</p>"},{"location":"faq/storage/ssd-instructions/#3-install-ubuntu-on-external-ssd","title":"3. Install Ubuntu on External SSD","text":"<p>Now that you are on the Ubuntu desktop, it is time to install Ubuntu to the SSD. The following steps will take you through installing Ubutnu onto your SSD:</p>"},{"location":"faq/storage/ssd-instructions/#partitioning-your-ssd","title":"Partitioning your SSD","text":"<p>You will use a program called GParted to partition your SSD for installation. From the applications menu in Ubuntu (top left corner), open up GParted and in the top right corner, select your SSD.</p> <p>Once the drive is selected, remove all the current partitions by selecting them and pressing the delete / backspace key. With these all removed, select the green check in the top menu to confirm the changes.</p> <p>Now that all the original partitions have been removed, the SSD needs to be partitioned for Ubuntu. To create a new partition, select the add button in the top left corner. You will need to create 3 partitions with the following specifications:</p> <p>Partition 1:</p> <p>New Size = 8000 MB File System = linux-swap</p> <p>Partition 2:</p> <p>New Size = 20,000 MB File system = ext4</p> <p>Partition 3:</p> <p>New Size = 80,000 MB File System = ext4</p> <p>To complete the process, click the green checkmark from the top menu once again. With all the partitions created, your GParted should look like this:</p> <p></p>"},{"location":"faq/storage/ssd-instructions/#installing-ubuntu-onto-the-ssd","title":"Installing Ubuntu onto the SSD","text":"<p>With the SSD partitioned, we can now install Ubuntu. Click the Install Ubuntu 18.04 icon from the side bar to begin the process. You will be prompted to go through a series of menus. Choose \"minimal\" install.</p> <pre><code>BE SURE TO CHECK THE BOX TO INSTALL THIRD PARTY SOFTWARE. Without this, you will be unable to connect to WiFi.\n</code></pre> <p>To set up wifi, you need to choose: and the eduroam account robotics@brandeis.edu with our traditional password. See the image below for how to fill in the Wifi dialog box.</p> <p>You will eventually reach a screen which asks to you how to install, titled Installation Type.</p> <pre><code>DO NOT SELECT THE OPTION TO ERASE DISK AND INSTALL UBUNTU. THIS WILL ERASE YOUR CURRENT OS AND MEMORY. BE SURE TO SELECT \"SOMETHING ELSE\"\n</code></pre> <p>The \u201cSomething else\u201d option allows us to assign mount points to the previously partitioned SSD. In the list of partitions, find the SSD. It should be called something like SDx. Be really careful that you are picking the right one. You will see the 3 partitions that were made in GParted. To set the partitions up for Ubuntu, double click on and configure the partitions as follows:</p>"},{"location":"faq/storage/ssd-instructions/#sdx2","title":"sdx2","text":"<pre><code>Use as: ext4\nformat: checked\nMount point = /\n</code></pre>"},{"location":"faq/storage/ssd-instructions/#sdx3","title":"sdx3","text":"<pre><code>Use as: ext4\nformat: checked\nMount point = /home\n</code></pre> <p>The partitions are now ready and Ubuntu can be installed. Select the SSD from the menu below and click install. Ubuntu will now install, and once done you will need to restart your system and remove the thumb drive.</p>"},{"location":"faq/storage/ssd-instructions/#make-the-ssd-bootable","title":"Make The SSD Bootable","text":"<p>Now that Ubuntu has been installed, we need to make it bootable. To do this, we will follow a tutorial online. The tutorial begins at the section Create an ESP on the Ubuntu HDD, and can be found at the following link:</p> <p>https://www.dionysopoulos.me/portable-ubuntu-on-usb-hdd/</p> <p>NOTE: You should skip the <code>umount</code> command instruction</p> <p>To be sure that this process worked, the final command should return a message saying the installation completed with no errors.</p>"},{"location":"faq/storage/ssd-instructions/#5-install-ros","title":"5. Install ROS","text":"<p>The final step in the process is to install ROS. In order to install ROS, you will need access to the internet. If you are using Eduroam, the following settings will need to be used:</p> <p></p> <p>Once you are connected to the internet, follow these instructions to install ROS:</p> <p>http://emanual.robotis.com/docs/en/platform/turtlebot3/pc_setup/</p> <p>With this completed, your SSD is configured!</p>"},{"location":"faq/tb3/customize_tb3/","title":"Customizing Turtlebot3","text":""},{"location":"faq/tb3/customize_tb3/#pito-salas-april-2022","title":"Pito Salas / April 2022","text":""},{"location":"faq/tb3/customize_tb3/#objective","title":"Objective","text":"<p>When we customize a TB3 with bigger wheels or a larger wheelbase, it is necesary to update the software in the OpenCr board.</p> <ul> <li> <p>The steps required by OpenCR when changing wheel size or distance between wheels can be summarized as follows</p> </li> <li> <p>change WHEEL_RADIUS, WHEEL_SEPARATION and TURNING_RADIUS in turtlebot3_burger.h</p> </li> <li> <p>Change the circumference of the wheel in turtlebot3_core.ino. Here, the circumference can be calculated by 2Pi * WHEEl_RADIUS. The default value of the circumference of burger is 0.207.</p> </li> <li> <p>Install Arduino IDE on your PC and write the edited file (turtlebot3_core.ino). Please refer to the following e-Manual for this detail. https://emanual.robotis.com/docs/en/platform/turtlebot3/opencr_setup/#opencr-setup 3.3. At the end of OpenCR Setup, there are instructions for using the Arduino IDE, click to expand and see the details.</p> </li> </ul>"},{"location":"faq/tb3/using-opencv-with-tb3/","title":"Some tips for using OpenCV and Turtlebot3 Camera","text":"<p>Junhao Wang</p>"},{"location":"faq/tb3/using-opencv-with-tb3/#read-compressedimage-type","title":"Read CompressedImage type","text":"<p>When using Turtlebot in the lab, it only publishs the CompressedImage from '/raspicam_node/image/compressed'. Here's how to read CompressedImage and Raw if you need. <pre><code>from cv_bridge import CvBridge\ndef __init__(self):\n   self.bridge = CvBridge()\ndef image_callback(self, msg):\n        # get raw image\n        # image = self.bridge.imgmsg_to_cv2(msg)\n\n        # get compressed image\n        np_arr = np.frombuffer(msg.data, np.uint8)\n        img_np = cv2.imdecode(np_arr, cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(img_np, cv2.COLOR_BGR2HSV)\n</code></pre></p>"},{"location":"faq/tb3/using-opencv-with-tb3/#limit-frame-rate","title":"Limit frame rate","text":"<p>When the computation resource and the bandwidtn of robot are restricted, you need limit the frame rate.</p> <pre><code>def __init__(self):\n   self.counter = 1\ndef image_callback(self, msg):\n        # set frame rate 1/3\n        if self.counter % 3 != 0:\n            self.counter += 1\n            return\n        else:\n            self.counter = 1\n</code></pre>"},{"location":"faq/tb3/using-opencv-with-tb3/#republish-the-image","title":"Republish the image","text":"<p>Again, when you have multiple nodes need image from robot, you can republish it to save bandwidth.</p> <pre><code>#!/usr/bin/env python\n\nimport rospy\nfrom sensor_msgs.msg import CompressedImage\nfrom cv_bridge import CvBridge\n\ndef image_callback(msg):\n    # Convert the compressed image message to a cv2 image\n    bridge = CvBridge()\n    cv2_image = bridge.compressed_imgmsg_to_cv2(msg, desired_encoding='passthrough')\n\n    # Process the image (e.g., resize, blur, etc.)\n    processed_image = process_image(cv2_image)\n\n    # Convert the processed cv2 image back to a compressed image message\n    compressed_image_msg = bridge.cv2_to_compressed_imgmsg(processed_image)\n\n    # Publish the processed image on the new topic\n    processed_image_pub.publish(compressed_image_msg)\n\ndef process_image(cv2_image):\n    # Perform image processing (e.g., resize, blur, etc.) and return the processed image\n    # Example: resized_image = cv2.resize(cv2_image, (new_width, new_height))\n    return cv2_image\n\nif __name__ == '__main__':\n    rospy.init_node('image_processing_node')\n    rospy.Subscriber('/robot/camera_topic', CompressedImage, image_callback)\n    processed_image_pub = rospy.Publisher('/processed_image_topic', CompressedImage, queue_size=1)\n\n    rospy.spin()\n</code></pre>"},{"location":"faq/tf/tf_cheatsheet/","title":"TF Cheatsheet","text":""},{"location":"faq/tf/tf_cheatsheet/#introduction","title":"Introduction","text":"<p>This document gives coding tips for using tfs in ROS. It assumes that the reader has:</p> <ol> <li>studied the ROS Wiki tf2 tutorial; and</li> <li>understood the basic math behind transforms (e.g., what coordinate frames    are, what the translation from one frame to another is, etc.).</li> </ol>"},{"location":"faq/tf/tf_cheatsheet/#prelude","title":"Prelude","text":"<p>Suppose that we've executed:</p> <pre><code>import tf2_ros\n\ntf_buffer = tf2_ros.Buffer()\ntf_listener = tf2_ros.TransformListener(tf_buffer)\ntf_broadcaster = tf2_ros.TransformBroadcaster()\n</code></pre>"},{"location":"faq/tf/tf_cheatsheet/#tips","title":"Tips","text":"<p>To get the pose of an existing frame <code>A</code>:</p> <pre><code>A_tf = tf_buffer.lookup_transform('A', 'A', rospy.Time()).transform\nA_translation = A_tf.translation\nA_rotation = A_tf.rotation\n</code></pre> <p>To write a node that broadcasts a frame continuously: (i) wrap the broadcasting functionality in a <code>while</code> loop with the <code>try-except</code> structure below, and (ii) don't forget to update the timestamp of your frame.</p> <pre><code>while not rospy.is_shutdown():\n    try:\n        new_tfs = TransformStamped()\n        # ...\n        # Configure new_tfs here.\n        # ... \n        new_tfs.header.stamp = rospy.Time.now()        \n    except (\n        tf2_ros.LookupException,\n        tf2_ros.ExtrapolationException,\n        tf2_ros.ConnectivityException\n        ):\n        continue\n    rate.sleep()\n</code></pre> <p>To broadcast a new frame <code>C</code> as a child of <code>A</code> that: (i) has the same rotation as <code>A</code>, and (ii) shares its origin with frame <code>B</code> (that is part of the same tf tree as <code>A</code>):</p> <pre><code>C_tfs = TransformStamped()\nC_tfs.header.frame_id = 'A'\nC_tfs.child_frame_id = 'C'\n\nwhile not rospy.is_shutdown():\n    try:\n        A_tf = tf_buffer.lookup_transform('A', 'A', rospy.Time()).transform\n        A_to_B_tf = tf_buffer.lookup_transform('A', 'B', rospy.Time()).transform\n        C_tfs.transform.rotation = A_tf.rotation\n        C_tfs.transform.translation = A_to_B_tf.translation\n        C_tfs.header.stamp = rospy.Time.now()        \n    except (\n        tf2_ros.LookupException,\n        tf2_ros.ExtrapolationException,\n        tf2_ros.ConnectivityException\n        ):\n        continue\n    rate.sleep()\n</code></pre> <p>To get the distance between the origin of frame <code>A</code> and that of frame <code>B</code>:</p> <pre><code>import numpy as np\nA_to_B_transl = tf_buffer.lookup_transform(\n    'A',\n    'B',\n    rospy.Time()\n).transform.translation\n\ndist = np.linalg.norm(\n    np.array(\n        [\n            A_to_B_transl.x,\n            A_to_B_transl.y,\n            A_to_B_transl.z,\n        ]\n    )\n)\n</code></pre> <p>```</p>"},{"location":"faq/udev/teensy-and-udev/","title":"Teensy and Udev","text":""},{"location":"faq/udev/teensy-and-udev/#intro","title":"Intro","text":"<p>As mentioned, the udev subsystem in Ubuntu allows us to recognize a particular USB device no matter what port it is attached to. It is controled by files called udev rules which are all stored in /etc/udev/rules.d. They are all \"processed\" by the udev subsytem.</p>"},{"location":"faq/udev/teensy-and-udev/#checking-configuration","title":"Checking configuration","text":"<p>On the rasberry Pi, type the command <code>ls /dev/lino*</code>. It should list two devices, <code>linolidar</code> (the lidar) and <code>linobase</code> (the teensy). It they appear, you can ignore the rest of this document.</p>"},{"location":"faq/udev/teensy-and-udev/#fixing-the-udev-configuration","title":"Fixing the udev configuration","text":"<p>For all our robots based on LinoRobot, we use Teensy controller and they have some particular udev requirements. </p> <ol> <li>Use the official latest teensy udev rule from the vendor.</li> <li>Create a udev rule to map the teensy to the standard name, /dev/linobase. The standard name is then in turn used by the code to open the connection to the teensy, </li> </ol> <p>Copy both these files to the /etc/udev/rules.d directory.</p>"},{"location":"faq/udev/teensy-and-udev/#creating-the-linobase-udev-rule","title":"Creating the linobase udev rule","text":"<p>Don't use lino_udev.py!</p> <p>The Linorobot1 instructions have a little script called lino_udev.py.  We do not use it because it uses old apis that no longer work!</p> <p>The basic udev rule file contains lines something like this: <pre><code>KERNEL==\"ttyUSB?\", SUBSYSTEM==\"tty\", ATTRS{idVendor}==\"10c4\", ATTRS{idProduct}==\"ea60\", ATTRS{serial}==\"0001\", MODE=\"0666\" SYMLINK+=\"linolidar\"\nSUBSYSTEM==\"usb\", ATTRS{idVendor}==\"16c0\", ATTRS{idProduct}==\"0483\", ATTRS{serial}==\"15412350\", MODE=\"0666\" SYMLINK+=\"linobase\"\n</code></pre> The order does not matter. The first line detects the Lidar USB and links it to the name <code>linolidar</code>. The second line is suppsed to detect the teensy and link it to the name <code>linobase</code>. However depending on the model teensy we have (and they differ on our robots) the particulars will varry. The key particulars are idVendor, idProduct, and serial.</p> <p>To get that information we run a little script:</p>"},{"location":"faq/udev/teensy-and-udev/#script-to-print-out-the-detected-devices","title":"Script to print out the detected devices","text":"<p>First: <code>python -m pip install pyserial</code></p> <p>With that tool, now you can list the information about each device plugged into a USB with this little app:</p> <pre><code>from serial.tools import list_ports\nports = list_ports.comports()\nfor port in ports:\n    print(f\"Device: {port.device}, Name: {port.name}, Description: {port.description}, HWID: {port.hwid}\")\n</code></pre> <p>In the output to this script you will find the required magic numbers.</p>"},{"location":"faq/udev/teensy-and-udev/#updating-the-udev-rules","title":"Updating the udev rules","text":"<p>Once you edit the udev rule file, unplug the usb for the teensy from the Pi and plug it back in. The teensy should be detected.</p>"},{"location":"faq/udev/teensy-and-udev/#handy-tool-udevadm-udev-admin","title":"Handy tool: Udevadm - Udev admin","text":"<p>This has many commands. The ones I have used are:</p> <p><code>Udevadm monitor</code></p> <p><code>udevadm monitor --property</code></p>"},{"location":"faq/udev/teensy-and-udev/#linrobots-lino_udev-package","title":"Linrobot's lino_udev package","text":"<p>This package is problematic because it requires Python2. We will not be using it.</p>"},{"location":"faq/udev/teensy-and-udev/#teensy-properties","title":"Teensy Properties","text":"<p>Udev rules recognize the Teensy by a few properties that they can detect when the teensy is plugged in.</p>"},{"location":"faq/udev/teensy-and-udev/#teensy-pid-and-vid","title":"Teensy PID and VID","text":""},{"location":"faq/udev/teensy-and-udev/#product-id-or-pid-for-teensy","title":"Product ID or PID for Teensy","text":"<ul> <li>Teensy 1.0 uses PID 0x0487</li> <li>Teensy++ 1.0 uses PID 0x04AE</li> <li>Teensy 2.0 uses PID 0x04B2</li> <li>Teensy 3.0/3.1/3.2 use PID 0x04B3</li> <li>Teensy 3.5 uses PID 0x04B5</li> <li>Teensy 3.6 uses PID 0x04B6</li> <li>Teensy 4.0 uses PID 0x04B7</li> </ul>"},{"location":"faq/udev/teensy-and-udev/#vendor-id-or-vid-for-teensy","title":"Vendor ID or VID for Teensy","text":"<ol> <li>Teensy 1.0: Product ID 0483</li> <li>Teensy 2.0: Product ID 0483</li> <li>Teensy++ 1.0: Product ID 0483</li> <li>Teensy++ 2.0: Product ID 0483</li> <li>Teensy 3.0: Product ID 16C0</li> <li>Teensy 3.1: Product ID 16C0</li> <li>Teensy 3.2: Product ID 0483</li> <li>Teensy LC: Product ID 16C0</li> <li>Teensy 3.5: Product ID 16C0</li> <li>Teensy 3.6: Product ID 16C0</li> <li>Teensy 4.0: Product ID 16C0</li> <li>Teensy 4.1: Product ID 16C0</li> </ol>"},{"location":"faq/udev/udev-intro/","title":"Udev","text":"<p>The name udev is an umbrella for a collection of concepts and tools that are used to give various serial devices a name. For example for linorobot there are two serial devices that we rely on: the teensy, which is connected by a USB and the Lidar which is connected to another USB. </p> <p>It would be convenient if we could refer to them by name. As is the case with the Linorobot.org package, we refer to them as <code>/dev/linobase</code> and <code>/dev/linolidar</code></p> <p>The udev mechanism ensures that these two \"arbitrary\" USB connections get those names, *no matter which USB port you plug them into\".</p> <p>This is low level stuff and we would not care about it except when configuring a robot for the first time.</p>"},{"location":"faq/urdf/bouncy-objects/","title":"Creating Bouncy Objects in Gazebo","text":""},{"location":"faq/urdf/bouncy-objects/#august-soderberg-joe-pickens","title":"August Soderberg &amp; Joe Pickens","text":"<p>This guide will give you all the tools you need to create a bouncy surface in Gazebo whether you want to make a basketball bounce on the ground or to make a hammer rebound off of an anvil.</p> <p>First you will need to create the URDF file for your model, examine any of the URDF files given to you for help with this, I will put a generic example of a ball I created below.</p> <pre><code>&lt;robot name=\"ball\"&gt;\n    &lt;link name=\"base_link\"&gt;\n        &lt;visual&gt;\n            &lt;origin xyz=\"0 0 0\" rpy=\"0 0 0\"/&gt;\n            &lt;geometry&gt;\n                &lt;sphere radius=\".5\"/&gt;\n            &lt;/geometry&gt;\n        &lt;/visual&gt;\n\n        &lt;collision&gt;\n            &lt;origin xyz=\"0 0 0\" rpy=\"0 0 0\"/&gt;\n            &lt;geometry&gt;\n                &lt;sphere radius=\".5\"/&gt;\n            &lt;/geometry&gt;\n        &lt;/collision&gt;\n\n        &lt;inertial&gt;\n            &lt;origin xyz=\"0 0 0\" rpy=\"0 0 0\"/&gt;\n            &lt;mass value=\"1\"/&gt;\n            &lt;inertia ixx=\"8e-03\" ixy=\"-4-05\" ixz=\"1e-04\"\n                                 iyy=\"8e-03\" iyz=\"-4-06\"\n                                             izz=\"8e-03\" /&gt;\n        &lt;/inertial&gt;\n    &lt;/link&gt;\n    &lt;gazebo reference=\"base_link\"&gt;\n        &lt;mu1&gt;1&lt;/mu1&gt;\n        &lt;mu2&gt;1&lt;/mu2&gt;\n        &lt;kp&gt;500000&lt;/kp&gt;\n        &lt;kd&gt;0&lt;/kd&gt;\n        &lt;minDepth&gt;0&lt;/minDepth&gt;\n        &lt;maxVel&gt;1000&lt;/maxVel&gt;\n    &lt;/gazebo&gt;\n&lt;/robot&gt;\n</code></pre> <p>A few notes on the sample URDF we will be discussing above: the model is a 0.5m radius sphere, the inertia tensor matrix is arbitrary as far as we are concerned (a third-party program will generate an accurate tensor for your models if you designed them outside of Gazebo), and the friction coefficients \"mu1\" and \"mu2\" are also arbitrary for this demonstration.</p> <p>To create a good bounce we will be editing the values within the \"gazebo\" tags, specifically the \"kp\", \"kd\", \"minDepth\", and \"maxVel\".</p>"},{"location":"faq/urdf/bouncy-objects/#maxvel","title":"maxVel","text":"<p>When objects collide in Gazebo, the simulation imparts a force on both objects opposite to the normal vector of their surfaces in order to stop the objects from going inside of one another when we have said they should collide. The \"maxVel\" parameter specifies the velocity (in meters/second) at which you will allow Gazebo to move your object to simulate surfaces colliding, as you will see later, this value is unimportant as long as it is greater than the maximum velocity your object will be traveling.</p>"},{"location":"faq/urdf/bouncy-objects/#mindepth","title":"minDepth","text":"<p>Gazebo only imparts this corrective force on objects to stop them from merging together if the objects are touching. The \"minDepth\" value specifies how far (in meters) you will allow the objects to pass through each other before this corrective velocity is applied; again we can set this to 0 and forget about it.</p>"},{"location":"faq/urdf/bouncy-objects/#kd","title":"kd","text":"<p>The \"kd\" value is the most important for our work, this can be thought of as the coefficient of elasticity of collision. A value of 0 represents a perfectly elastic collision (no kinetic energy is lost during collision) and as the values get the collision becomes more inelastic (more kinetic energy is converted to other forms during collision). For a realistic bouncing ball, just start at 0 and work your way up to higher values until it returns to a height that feels appropriate for your application, a value of 100 is more like dropping a block of wood on the ground for reference.</p>"},{"location":"faq/urdf/bouncy-objects/#kp","title":"kp","text":"<p>The \"kp\" value can be thought of as the coefficient of deformation. A value of 500,000 is good for truly solid objects like metals; keep in mind that an object with a \"kd\" value of 0 and a \"kp\" value of 500,000 will still bounce all around but will be hard as metal. A \"kp\" of 500 is pretty ridiculously malleable, like a really underinflated yoga ball, for reference. Also, keep in mind that low \"kp\" values can often cause weird effects when objects are just sitting on surfaces so be careful with this and err on the side of closer to 500,000 when in doubt.</p> <p>There you go, now you know how to create objects in Gazebo that actually bounce!</p>"},{"location":"faq/urdf/bouncy-objects/#tips-when-debugging","title":"Tips When Debugging","text":"<p>Here are a bunch of random tips for debugging:</p> <p>When testing how much objects bounce, try it on the grey floor of Gazebo and not on custom created models since those models will also affect how your object bounces.</p> <p>Keep in mind all external forces, especially friction. If a robot is driving into a wall and you want it to bounce off, that means the wheels are able to maintain traction at the speed the robot is travelling so even if that speed is quickly reversed, your robot will simply stop at the wall as the wheels stop all bouncing movement.</p> <p>When editing other URDF files, they will often have a section at the top like:</p> <pre><code>&lt;xacro:include filename=\"$(find turtlebot3_description)/urdf/common_properties.xacro\"/&gt;\n&lt;xacro:include filename=\"$(find turtlebot3_description)/urdf/turtlebot3_waffle.gazebo.xacro\"/&gt;\n</code></pre> <p>Make sure these files aren't affecting the behavior of your model, they could be specifying values of which you were unaware.</p> <p>If your simulation is running at several times real time speed, you could miss the bouncing behavior of the ball, make sure simulation is running at a normal \"real-time-factor\".</p> <p>If any small part has a low \"maxVel\" value, it could change the behavior of the entire model.</p> <p>Good luck!</p>"},{"location":"faq/urdf/how-to-add-sdf-in-ros/","title":"How to add an SDF Model","text":""},{"location":"faq/urdf/how-to-add-sdf-in-ros/#how-to-add-sdf-model-in-ros","title":"How to add SDF model in ROS","text":"<p>Author: Shuo Shi</p>"},{"location":"faq/urdf/how-to-add-sdf-in-ros/#overview","title":"Overview","text":"<p>This tutorial describes the details of a SDF Model Object.</p> <p>SDF Models can range from simple shapes to complex robots. It refers to the  SDF tag, and is essentially a collection of links, joints, collision objects, visuals, and plugins. Generating a model file can be difficult depending on the complexity of the desired model. This page will offer some tips on how to build your models."},{"location":"faq/urdf/how-to-add-sdf-in-ros/#components-of-sdf-models","title":"Components of SDF Models","text":"<p>Links: A link contains the physical properties of one body of the model. This can be a wheel, or a link in a joint chain. Each link may contain many collision and visual elements. Try to reduce the number of links in your models in order to improve performance and stability. For example, a table model could consist of 5 links (4 for the legs and 1 for the top) connected via joints. However, this is overly complex, especially since the joints will never move. Instead, create the table with 1 link and 5 collision elements.</p> <p>Collision: A collision element encapsulates a geometry that is used for collision checking. This can be a simple shape (which is preferred), or a triangle mesh (which consumes greater resources). A link may contain many collision elements.</p> <p>Visual: A visual element is used to visualize parts of a link. A link may contain 0 or more visual elements.</p> <p>Inertial: The inertial element describes the dynamic properties of the link, such as mass and rotational inertia matrix.</p> <p>Sensor: A sensor collects data from the world for use in plugins. A link may contain 0 or more sensors.</p> <p>Light: A light element describes a light source attached to a link. A link may contain 0 or more lights.</p> <p>Joints: A joint connects two links. A parent and child relationship is established along with other parameters such as axis of rotation, and joint limits.</p> <p>Plugins: A plugin is a shared library created by a third party to control a model.</p>"},{"location":"faq/urdf/how-to-add-sdf-in-ros/#building-a-model","title":"Building a Model","text":""},{"location":"faq/urdf/how-to-add-sdf-in-ros/#step-1-collect-your-meshes","title":"Step 1: Collect your meshes","text":"<p>This step involves gathering all the necessary 3D mesh files that are required to build your model. Gazebo provides a set of simple shapes: box, sphere, and cylinder. If your model needs something more complex, then continue reading.</p> <p>Meshes come from a number of places. Google's 3D warehouse is a good repository of 3D models. Alternatively, you may already have the necessary files. Finally, you can make your own meshes using a 3D modeler such as Blender or Sketchup.</p> <p>Gazebo requires that mesh files be formatted as STL, Collada or OBJ, with Collada and OBJ being the preferred formats.</p>"},{"location":"faq/urdf/how-to-add-sdf-in-ros/#step-2-make-your-model-sdf-file","title":"Step 2: Make your model SDF file","text":"<p>Start by creating an extremely simple model file, or copy an existing model file. The key here is to start with something that you know works, or can debug very easily.</p> <p>Here is a very rudimentary minimum box model file with just a unit sized box shape as a collision geometry and the same unit box visual with unit inertias:</p> <p>Create the box.sdf model file</p> <p>gedit box.sdf Copy the following contents into box.sdf: <pre><code>&lt;?xml version='1.0'?&gt;\n&lt;sdf version=\"1.4\"&gt;\n  &lt;model name=\"my_model\"&gt;\n    &lt;pose&gt;0 0 0.5 0 0 0&lt;/pose&gt;\n    &lt;static&gt;true&lt;/static&gt;\n    &lt;link name=\"link\"&gt;\n      &lt;inertial&gt;\n        &lt;mass&gt;1.0&lt;/mass&gt;\n        &lt;inertia&gt; &lt;!-- inertias are tricky to compute --&gt;\n          &lt;!-- http://gazebosim.org/tutorials?tut=inertia&amp;cat=build_robot --&gt;\n          &lt;ixx&gt;0.083&lt;/ixx&gt;       &lt;!-- for a box: ixx = 0.083 * mass * (y*y + z*z) --&gt;\n          &lt;ixy&gt;0.0&lt;/ixy&gt;         &lt;!-- for a box: ixy = 0 --&gt;\n          &lt;ixz&gt;0.0&lt;/ixz&gt;         &lt;!-- for a box: ixz = 0 --&gt;\n          &lt;iyy&gt;0.083&lt;/iyy&gt;       &lt;!-- for a box: iyy = 0.083 * mass * (x*x + z*z) --&gt;\n          &lt;iyz&gt;0.0&lt;/iyz&gt;         &lt;!-- for a box: iyz = 0 --&gt;\n          &lt;izz&gt;0.083&lt;/izz&gt;       &lt;!-- for a box: izz = 0.083 * mass * (x*x + y*y) --&gt;\n        &lt;/inertia&gt;\n      &lt;/inertial&gt;\n      &lt;collision name=\"collision\"&gt;\n        &lt;geometry&gt;\n          &lt;box&gt;\n            &lt;size&gt;1 1 1&lt;/size&gt;\n          &lt;/box&gt;\n        &lt;/geometry&gt;\n      &lt;/collision&gt;\n      &lt;visual name=\"visual\"&gt;\n        &lt;geometry&gt;\n          &lt;box&gt;\n            &lt;size&gt;1 1 1&lt;/size&gt;\n          &lt;/box&gt;\n        &lt;/geometry&gt;\n      &lt;/visual&gt;\n    &lt;/link&gt;\n  &lt;/model&gt;\n&lt;/sdf&gt;\n</code></pre> Note that the origin of the Box-geometry is at the geometric center of the box, so in order to have the bottom of the box flush with the ground plane, an origin of 0 0 0.5 0 0 0 is added to raise the box above the ground plane.</p>"},{"location":"faq/urdf/how-to-add-sdf-in-ros/#step-3-add-to-the-model-sdf-file","title":"Step 3: Add to the model SDF file","text":"<p>With a working .sdf file, slowly start adding in more complexity.</p> <p>Under the &lt;geometry&gt; label, add your .stl model file <pre><code>&lt;geometry&gt;\n        &lt;mesh filename=\"package://package_name/model_path/model.stl\" scale=\"0.01 0.01 0.01\"/&gt;\n&lt;/geometry&gt;\n</code></pre></p> <p>The &lt;geometry&gt; label can be added below &lt;collision&gt; and &lt;visual&gt; label.</p>"},{"location":"faq/urdf/how-to-add-sdf-in-ros/#import-the-model-in-you-world-file","title":"Import the model in you .world file","text":"<p>In your .world file, import the sdf file using &lt; include &gt; tag <pre><code>&lt;include&gt;\n      &lt;uri&gt;model://my_model&lt;/uri&gt;\n&lt;/include&gt;\n</code></pre></p> <p>Then open termial to add the model path to Gazebo variable. <pre><code>export GAZEBO_MODEL_PATH=~/catkin_ws/src/package_name/model_path/\n</code></pre></p>"},{"location":"faq/urdf/how-to-add-texture-to-sdf/","title":"Adding Texture","text":"<p>by Tongkai Zhang, tongkaizhang@brandeis.edu</p> <p>This is a quick guide for adding textures(.png) to your model in gazebo.</p> <p></p>"},{"location":"faq/urdf/how-to-add-texture-to-sdf/#prerequisite","title":"Prerequisite","text":"<ul> <li>gazebo</li> <li>model in sdf format</li> </ul>"},{"location":"faq/urdf/how-to-add-texture-to-sdf/#texture-configuration","title":"Texture Configuration","text":"<p>After building your own model in the sdf format, you should have a structured model <code>cargo</code>directory same as the one below.</p> <p></p> <p>It's important that you have the directory exactly the same, since importing the texture into gazebo is based on finding parameters and texture images within that directory.</p> <ul> <li> <p>scripts: <code>cargo.material</code>, formatted file, defining the name, visual property for the material.</p> </li> <li> <p>textures: the image texture file</p> </li> <li> <p><code>model.config</code>:  meta data for the model</p> </li> <li> <p>```    </p> </li> </ul> <p> cargo_with_marker 1.0 model.sdf <p> Tongkai Zhang </p> <p>export GAZEBO_MODEL_PATH=~/catkin_ws/src/warehouse_worker/model/   \u00a0\u00a0\u00a0 Cargo   \u00a0     ```</p> <ul> <li><code>model.sdf</code>: model itself</li> </ul> <p>Basic Steps:</p> <ul> <li> <p>Get the texture image ready and put them under <code>/textures</code></p> </li> <li> <p>Define texture in <code>cargo.material</code>, note how texture image is included as well as how the name is set.</p> </li> </ul> <p><code>material Cargo/Diffuse</code> is the unique name for this material.</p> <pre><code>material Cargo/Diffuse\n{\n  receive_shadows off\n  technique\n  {\n    pass\n    {\n      texture_unit\n      {\n        texture cargo.jpg\n      }\n    }\n  }\n}\n\nmaterial Marker/Diffuse\n{\n  receive_shadows off\n  technique\n  {\n    pass\n    {\n      texture_unit\n      {\n        texture MarkerData_1.png\n      }\n    }\n  }\n}\n</code></pre> <ul> <li>Add this material in your sdf model. It should be enclosed by the <code>&lt;visual&gt;</code>tag. Note the <code>uri</code> sexport GAZEBO_MODEL_PATH=~/catkin_ws/src/warehouse_worker/model/hould be correctly set and the <code>name</code> of the material should be identical to the name defined in <code>cargo.material</code></li> </ul> <pre><code>&lt;visual&gt;\n    &lt;material&gt;\n              &lt;script&gt;\n                &lt;uri&gt;model://cargo/materials/scripts&lt;/uri&gt;\n                &lt;uri&gt;model://cargo/materials/textures&lt;/uri&gt;\n                &lt;name&gt;Marker/Diffuse&lt;/name&gt;\n              &lt;/script&gt;\n    &lt;/material&gt;\n&lt;/visual&gt;\n</code></pre> <ul> <li>Adding model to your world</li> </ul> <pre><code>  &lt;model name='cargo_with_marker'&gt;\n        &lt;pose&gt;2.5 0 0.15 0 0 -1.57&lt;/pose&gt;\n        &lt;include&gt;\n            &lt;uri&gt;model://cargo&lt;/uri&gt;\n        &lt;/include&gt;\n    &lt;/model&gt;\n</code></pre> <ul> <li>Set the <code>model</code>environment variable in terminal. When gazebo launches the world, it will search the  model <code>cargo</code> under its default <code>model</code> directory. If you want to include your models in your project folder, you should set the <code>GAZEBO_MODEL_PATH</code> variable in your terminal.</li> </ul> <p><code>export GAZEBO_MODEL_PATH=~/catkin_ws/src/warehouse_worker/model/</code></p> <p>Now gazebo also searches model under <code>/warehouse_worker/model/</code></p>"},{"location":"faq/urdf/sdf_to_urdf/","title":"SDF to URDF conversion","text":""},{"location":"faq/urdf/sdf_to_urdf/#authors-alexion-ramos-ilan-hascal","title":"Authors - Alexion Ramos &amp; Ilan Hascal","text":""},{"location":"faq/urdf/sdf_to_urdf/#creating-a-gazebo-model","title":"Creating a Gazebo Model","text":"<p>For some users it is easier to create a model through the given Gazebo simulator in the Model Editor than having to write a custom file. The problem that arises is using objects and specific services else where in a project can be difficult due to the extension names of the file. </p>"},{"location":"faq/urdf/sdf_to_urdf/#building-saving-a-model","title":"Building  Saving a model","text":"<p>When you want to build a model in Gazebo it is best that you do it in the Model Editor that you can get you with CTRl m</p> <p>Once the model is built and you save it to the right directory it is saved as an .sdf file </p>"},{"location":"faq/urdf/sdf_to_urdf/#why-is-sdf-not-useful-for-autonomous-pacman-and-how-it-conflicts-with-services","title":"Why is .sdf not useful for Autonomous Pacman and how it conflicts with Services?","text":"<p>For the sake of the autonomous pacman project, our goal was to implement collectibles that pacman could pick up to make him \"invincible\". Though ,the gazebo SpawnModel &amp; DeleteModel services expect a .urdf file in order to track the model down to Spawn or Delete.  Example of spawning a custom .urdf is below. </p> <pre><code>    spawn_model_client = rospy.ServiceProxy('/gazebo/spawn_sdf_model', SpawnModel)\n    spawn_model_client(\n    model_name='TESTCYLINDER',\n    model_xml=open('/my_ros_data/catkin_ws/src/ros-autonomous-pacman/models/TESTCYLINDER/TESTCYLINDER.urdf', 'r').read(), \n    initial_pose=Pose(),\n    reference_frame='world'\n    )\n</code></pre> <p>Documetnation for Spawn -  http://docs.ros.org/en/jade/api/gazebo_msgs/html/srv/SpawnModel.html Documentation for Delete - http://docs.ros.org/en/jade/api/gazebo_msgs/html/srv/DeleteModel.html</p>"},{"location":"faq/urdf/sdf_to_urdf/#solution","title":"Solution","text":"<p>After reasearching it was with great pleasure that we found an open source library that allows you to put input the name of the .sdf file and then converts it to .urdf as output to where you specify. It is a really straight forward process that can be done by using the following links below </p> <p>Github repo - https://github.com/andreasBihlmaier/pysdf Youtube video instruction - https://www.youtube.com/watch?v=8g5nMxhi_Pw</p>"},{"location":"images/","title":"Index","text":"<p>Images Folder of Reports</p> <p>This folder allows students to save images used in their Final Reports</p>"},{"location":"lab-robots/SUMMARY/","title":"SUMMARY","text":"<ul> <li>Lab Robots</li> <li>Current Robots<ul> <li>Turtlebot3<ul> <li>tb3/*</li> </ul> </li> <li>Platform<ul> <li>platform/*</li> </ul> </li> <li>Branbot<ul> <li>branbot/*</li> </ul> </li> </ul> </li> <li>Older Robots!!<ul> <li>MiniRover<ul> <li>minirover/*</li> </ul> </li> <li>Pupper<ul> <li>pupper/*</li> </ul> </li> <li>Linorobot<ul> <li>linorobot/*</li> </ul> </li> </ul> </li> </ul>"},{"location":"lab-robots/intro/","title":"Our Lab Robots","text":"<p>Over the years we've built up a varied collection of robots. Some are no longer in use and several types still are. This section contains a lot of the robot specific details. Today we use three main robot models:</p>"},{"location":"lab-robots/intro/#main-robots","title":"Main Robots","text":"<ol> <li>Turtlebot3</li> <li>Platform</li> <li>Branbot</li> </ol>"},{"location":"lab-robots/intro/#software-platform","title":"Software Platform","text":"<p>The Platform and Branbot (as well as some others) are based on the Linorobot platform. </p>"},{"location":"lab-robots/our-robots/","title":"Specifications","text":"<ul> <li>Obsolete. Now this is maintained in this google sheet: https://docs.google.com/spreadsheets/d/130Y508Y508Y508Y508Y508Y508Y508Y508Y508Y508Y/edit?gid=0</li> </ul> hostname maintainer password last updated username ros OS known problems IMU MAC/Lan MAC/Wifi platform1 pito default 2 Mar 2024 ubuntu noetic ubuntu 20.04 IMU9250 platform2 pito default 2 Mar 2024 ubuntu noetic ubuntu 20.04 IMU9250 platform3 pito default 2 Mar 2024 ubuntu noetic ubuntu 20.04 IMU9250 platform4 pito default 2 Mar 2024 ubuntu noetic ubuntu 20.04 IMU9250 mr1 pito default 2 Mar 3 2022 pi noetic ubuntu 20.04 none mr2 pito default 2 Jan 15 2022 pi noetic ubuntu 20.04 none rafael pito Default 1 Jan 15 2022 ubuntu noetic ubuntu 20.04 none robb adam default 1 Feb 16 2022 ubuntu noetic ubuntu 20.04 none robc adam default 1 Dec 10 2021 ubuntu noetic ubuntu 20.04 none roba adam default 1 Feb 2 2022 ubuntu noetic ubuntu 20.04 none donatello adam default 1 Feb 16 2022 ubuntu noetic ubuntu 20.04 none bullet1 pito default 1 Nov 11 2021 ubuntu noetic ubuntu 20.04 none IMU9250 cat1 pito default 2 Mar 3 2022 ubuntu noetic ubuntu 20.04 none alien pito default 2 Mar 25 2022 ubuntu noetic ubuntu 20.04 no lidar"},{"location":"lab-robots/our_ip_addresses/","title":"Lab Robot InformationXX","text":""},{"location":"lab-robots/our_ip_addresses/#robotics-lab-mac-addresses","title":"Robotics Lab Mac Addresses","text":"<ul> <li>One entry for each robot or roscore.</li> <li>All these devices live in the Robotics Lab in Gzang 006</li> </ul>"},{"location":"lab-robots/our_ip_addresses/#name-mac-address-ip-address-dns-name","title":"Name, mac address, IP address, dns name","text":"<ul> <li>Wifi Connected Rasberry Pi's</li> <li>b8:27:eb:e3:9b:a2 - mutant - mutant.dyn.brandeis.edu</li> <li>b8:27:eb:75:89:b1 - roba - roba.dyn.brandeis.edu</li> <li>b8:27:eb:01:e7:69 - robb - robb.dyn.brandeis.edu</li> <li>b8:27:eb:83:21:e4 - robc - robc.dyn.brandeis.edu</li> <li>b8:27:eb:8c:90:c5 - donatello - donatello.dyn.brandeis.edu</li> <li>b8:27:eb:a4:d5:ec - rafael - rafael.dyn.brandeis.edju</li> <li>74:40:bb:d5:ea:2f - alien - alien.dyn.brandeis.edu</li> <li>dc:a6:32:20:ac:16 - platform2 - platform1.dyn.brandeis.edu</li> <li> <p>e4:5f:01b7:ff - platform3 - platform3.dyn.brandeis.edu</p> </li> <li> <p>Wifi connected Linux Notebook</p> </li> <li>5c:ff:35:0f:ef:6d - roscore2</li> <li>Wired Linux Desktop</li> <li>44:37:e6:b7:4b:d1 - roscore1</li> </ul>"},{"location":"lab-robots/our_ip_addresses/#internal-dns-names","title":"Internal DNS names","text":"<ul> <li>Currently the names are formed as .dyn.brandeis.edu</li> <li>is literally the <code>hostname</code> of the particular robot</li> <li>You can check and set it with the <code>$ hostname</code> command</li> </ul>"},{"location":"lab-robots/robot-master-list/","title":"Robot Master List","text":""},{"location":"lab-robots/robot-master-list/#introduction","title":"Introduction","text":"<p>This list is superceded by the google sheets listing.</p>"},{"location":"lab-robots/robot-master-list/#list","title":"List","text":"<ol> <li>mutant<ul> <li>Teensy Version: N/A</li> <li>Raspi Version: 3B</li> <li>Raspi Memory: 868MiB</li> <li>Wifi MAC Address: b8:27:eb:e3:9b:a2</li> <li>Ethernet MAC Address: b8:27:eb:b6:ce:f7</li> <li>MicroSD Label: AAB</li> </ul> </li> <li>roba <ul> <li>Teensy Version: N/A</li> <li>Raspi Version: 3B</li> <li>Raspi Memory: 868MiB</li> <li>Wifi MAC Address: b8:27:eb:75:89:b1</li> <li>Ethernet MAC Address: b8:27:eb:20:dc:e4</li> <li>MicroSD Label: 20N</li> </ul> </li> <li>robb <ul> <li>Teensy Version: N/A</li> <li>Raspi Version: 3B</li> <li>Raspi Memory: 868MiB</li> <li>Wifi MAC Address: b8:27:eb:01:e7:69</li> <li>Ethernet MAC Address: b8:27:eb:54:b2:3c</li> <li>MicroSD Label: 16</li> </ul> </li> <li>robc <ul> <li>Teensy Version: N/A</li> <li>Raspi Version: 3B</li> <li>Raspi Memory: 868MiB</li> <li>Wifi MAC Address: b8:27:eb:83:21:e4</li> <li>Ethernet MAC Address: b8:27:eb:d6:74:b1 </li> <li>MicroSD Label: AAA </li> </ul> </li> <li>donatello <ul> <li>Teensy Version: N/A</li> <li>Raspi Version: 3B</li> <li>Raspi Memory: 868MiB</li> <li>Wifi MAC Address: b8:27:eb:8c:90:c5</li> <li>Ethernet MAC Address: b8:27:eb:d9:c5:90</li> <li>MicroSD Label: AAC</li> </ul> </li> <li>rafael <ul> <li>Teensy Version: N/A</li> <li>Raspi Version: 3B</li> <li>Raspi Memory: 868MiB</li> <li>Wifi MAC Address: b8:27:eb:a4:d5:ec</li> <li>Ethernet MAC Address: b8:27:eb:f1:80:b9</li> <li>MicroSD Label: AAD</li> </ul> </li> <li>alien<ul> <li>Teensy Version: N/A</li> <li>Raspi Version: 3B</li> <li>Raspi Memory: 868MiB</li> <li>Wifi MAC Address: b8:27:eb:c8:76:c3</li> <li>Ethernet MAC Address: b8:27:eb:9d:23:96</li> <li>MicroSD Label: T1 </li> </ul> </li> </ol>"},{"location":"lab-robots/branbot/assembly-tutorial/","title":"BranBot Assembly Tutorial","text":""},{"location":"lab-robots/branbot/assembly-tutorial/#introduction","title":"Introduction","text":"<p>This is a tutorial on how to assemble the BranBot from the BranBot List of Parts.</p> <p>'MakerLab' and 'Automation Lab' below refer to the organizations located in the Brandeis Library.</p> <p>WARNING: Always be careful with tools. Safety comes first!</p>"},{"location":"lab-robots/branbot/assembly-tutorial/#overview-of-instructions","title":"Overview of Instructions","text":"<ol> <li>Prepare Acrylic Frames</li> <li>Attach the LIDAR to the LAF</li> <li>Attach the LIDAR Board to the LAF</li> <li>Attach the Charlie Board to the LAF</li> <li>Attach the Raspberry Pi to the LAF</li> <li>Attach the IMU to the LAF</li> <li>Attach the LED Driver to the LAF</li> <li>Attach the Motor Brackets to the LAF</li> <li>Attach the Motor Assemblies to the Brackets</li> <li>Attach the Wheel Adapters to the Motor Assemblies</li> <li>Attach the Fixed Wheels to the Wheel Adapters</li> <li>Attach the Caster Wheels to the LAF</li> <li>Attach the Camera to the Camera Mount</li> <li>Attach the Camera Mount to the LAF</li> <li>Attach the Battery Brackets to the LAF</li> <li>Attach the LED Holders to the LAF</li> <li>Wiring<ol> <li>Connect the Raspberry Pi and the Teensy</li> <li>Connect the LED Board with the Raspberry Pi</li> <li>Connect the Raspberry Pi and the LIDAR Board to the Battery</li> <li>Connect the Battery to the Charlie Board Screw Terminal</li> <li>Connect the LED Driver to the IMU </li> <li>Connect the IMU to the Charlie Board</li> <li>Connect the Motor Assembly Wires to the Charlie Board</li> <li>Connect the LEDs to the LED Driver</li> <li>Connect the Camera to the Raspberry Pi</li> </ol> </li> <li>Attach the Standoffs to the LAF</li> <li>Attach the Upper Acrylic Frame to the Standoffs</li> </ol>"},{"location":"lab-robots/branbot/assembly-tutorial/#prepare-acrylic-frames","title":"Prepare Acrylic Frames","text":"<ol> <li>Parts: Lower Acrylic Frame, Upper Acrylic Frame</li> <li>Tools: Curved Nose Plier </li> <li>Steps:<ol> <li>It's best to do this step in the Automation Lab, where a curved nose    plier is readily available.</li> <li>Take a curved nose plier and punch through any incompletely drilled holes    in the Lower and Upper Acrylic Frames.</li> </ol> </li> </ol>"},{"location":"lab-robots/branbot/assembly-tutorial/#attach-the-lidar-to-the-laf","title":"Attach the LIDAR to the LAF","text":"<ol> <li>Note: 'LAF' stands for 'Lower Acrylic Frame'</li> <li>Parts: LIDAR, 4 M2.5x6 screws</li> <li>Tools: Screwdriver</li> <li>Steps:<ol> <li>Align the LIDAR to the correct holes on the LAF.</li> <li>Attach the LIDAR to the LAF with the 4 M2.5x6 bolts</li> </ol> </li> </ol>"},{"location":"lab-robots/branbot/assembly-tutorial/#attach-the-lidar-board-to-the-laf","title":"Attach the LIDAR Board to the LAF","text":"<ol> <li>Parts: LIDAR Board, 4 nylon M2 screws, 4 nylon M2 standoffs, 4 nylon M2 nuts</li> <li>Tools: Screwdriver</li> <li>Steps:<ol> <li>Fix the 4 nylon standoffs to the appropriate holes in the LAF with the 4    nylon M2 screws.</li> <li>Insert the LIDAR Board onto the nylon standoffs via its 4 holes.</li> <li>Fix the LIDAR Board onto the nylon standoffs via the 4 nylon M2 nuts.</li> </ol> </li> </ol>"},{"location":"lab-robots/branbot/assembly-tutorial/#attach-the-charlie-board-to-the-laf","title":"Attach the Charlie Board to the LAF","text":"<ol> <li>Parts: Charlie Board, 4 nylon M2 screws, 4 nylon M2 standoffs, 4 nylon M2    nuts</li> <li>Tools: Screwdriver</li> <li>Steps:<ol> <li>Observe that the Charlie Board should be attached to the underside of the    LAF as in the picture below.</li> <li>Attach the Charlie Board to the LAF using the nylon screws, standoffs,    and nuts (cf. the LIDAR board to LAF attachment instructions).</li> </ol> </li> <li>Notes:<ol> <li>The Charlie Board should already have the Motor Driver, the Teensy, the    Screw terminal, and the I2C right-angle connector soldered onto it. The    terminal and the connector should have been directly soldered on. On the    other hand, the Motor Driver and the Teensy should have male pins    soldered onto them, which should have been inserted into female plugs    that have themselves been soldered to the Charlie Board.</li> <li>If the above is not the case, ask the Automation Lab for assistance.</li> </ol> </li> </ol>"},{"location":"lab-robots/branbot/assembly-tutorial/#attach-the-raspberry-pi-to-the-laf","title":"Attach the Raspberry Pi to the LAF","text":"<ol> <li>Parts: Raspberry Pi, 4 nylon M2 screws, 4 nylon M2 standoffs, 4 nylon M2 nuts</li> <li>Tools: Screwdriver</li> <li>Steps:<ol> <li>Find the appropriate holes in the LAF for the Raspberry Pi.</li> <li>Attach the Pi to the LAF using the nylon screws, standoffs, and nuts (cf.    the LIDAR board to LAF attachment instructions).</li> </ol> </li> </ol>"},{"location":"lab-robots/branbot/assembly-tutorial/#attach-the-imu-to-the-laf","title":"Attach the IMU to the LAF","text":"<ol> <li>Parts: IMU, 4 nylon M2 screws, 4 nylon M2 standoffs, 4 nylon M2 nuts</li> <li>Tools: Screwdriver</li> <li>Steps:<ol> <li>Find the appropriate holes in the LAF for the IMU.</li> <li>Attach the IMU to the LAF using the nylon screws, standoffs, and nuts (cf.    the LIDAR board to LAF attachment instructions).</li> </ol> </li> </ol>"},{"location":"lab-robots/branbot/assembly-tutorial/#attach-the-led-driver-to-the-laf","title":"Attach the LED Driver to the LAF","text":"<ol> <li>Parts: LED Driver, 4 nylon M2 screws, 4 nylon M2 standoffs, 4 nylon M2 nuts</li> <li>Tools: Screwdriver</li> <li>Steps:<ol> <li>Find the appropriate holes in the LAF for the LED Driver.</li> <li>Attach the LED Driver to the LAF using the nylon screws, standoffs, and    nuts (cf.  the LIDAR board to LAF attachment instructions).</li> </ol> </li> <li>Notes:<ol> <li>The LED Driver is the wrong side up in the picture below. So are the     male pins that have been attached to it. This does not affect the     functionality of the Driver, but attaching it the right side up is     preferable.</li> <li>The LED driver should already have male pins soldered onto it. If it    doesn't, ask the Automation Lab for assistance.</li> </ol> </li> </ol>"},{"location":"lab-robots/branbot/assembly-tutorial/#attach-the-motor-brackets-to-the-laf","title":"Attach the Motor Brackets to the LAF","text":"<ol> <li>Parts: 2 Motor Brackets, 8 M3x10 bolts, 8 M3 nuts</li> <li>Tools: Screwdriver, Plier</li> <li>Steps:<ol> <li>Align a bracket's four holes that form a square with appropriate holes    in the LAF. </li> <li>Fix the bracket to the Frame with bolts and nuts. It helps to hold a nut    in place with a plier while screwing a bolt into it.</li> <li>Repeat for the other side.</li> </ol> </li> </ol>"},{"location":"lab-robots/branbot/assembly-tutorial/#attach-the-motor-assemblies-to-the-brackets","title":"Attach the Motor Assemblies to the Brackets","text":"<ol> <li>Parts: Motor Brackets, 2 Motor Assemblies, 12 M3x6 bolts</li> <li>Tools: Screwdriver</li> <li>Steps:<ol> <li>Ensure that the shaft of a Motor Assembly is positioned such that it    occupies the lower portion of the large hole in a Motor Bracket. </li> <li>Align the holes in the Motor Assembly with those in the Motor Bracket.</li> <li>Fix the Motor Assembly with the Motor Bracket using 6 M3x6 bolts.</li> <li>Repeat for the other side.</li> </ol> </li> <li>Note: The length of the bolts should be no longer than 6mm. Otherwise, they    might interfere with the functioning of the gear box in the Motor Assembly.</li> </ol>"},{"location":"lab-robots/branbot/assembly-tutorial/#attach-the-wheel-adapters-to-the-motor-assemblies","title":"Attach the Wheel Adapters to the Motor Assemblies","text":"<ol> <li>Parts: 2 Wheel Adapters (thick parts, see picture below), 4 Stud bolts (these    come with the Pololu wheel adapters, and are small headless bolts) </li> <li>Tools: Screwdriver</li> <li>Steps:<ol> <li>Find the flat edge of the shaft of a Motor Assembly.</li> <li>Insert a wheel adapter onto the shaft and rotate it such that the two    holes on the adapter's side align with the mentioned flat edge. Ensure    that:</li> <li>the shaft does not stick out through the adapter, but that the end of       the shaft is flush with the adapter (cf. the picture below). </li> <li>The thinner end of the thick wheel adapter faces outward (cf. the       picture below).</li> <li>Insert two stud bolts into the two holes to fix the adapter to the shaft.</li> <li>Repeat for the other side.</li> </ol> </li> </ol>"},{"location":"lab-robots/branbot/assembly-tutorial/#attach-the-fixed-wheels-to-the-wheel-adapters","title":"Attach the Fixed Wheels to the Wheel Adapters","text":"<ol> <li>Parts: 2 Wheel Adapters (thin parts, see picture below), 6 M3x20 bolts</li> <li>Tools: Screwdriver</li> <li>Steps:<ol> <li>Insert a Fixed Wheel into the part of the Wheel Adapter that is attached    to the Motor Assembly.</li> <li>Insert the thin part of the Wheel Adapter into the central hole of the    wheel, ensuring that its three holes align with that in the thick part of    the wheel adpater.</li> <li>Fix the thin part of the Wheel Adapter to its thick counterpart by    inserting three M3x20 bolts into the three holes.</li> <li>Repeat for the other side.</li> </ol> </li> <li>Note: The length of the bolts should be no longer than 20mm. Otherwise, they    might interfere with the rotation of the Motor Assembly's shaft. </li> </ol>"},{"location":"lab-robots/branbot/assembly-tutorial/#attach-the-caster-wheels-to-the-laf","title":"Attach the Caster Wheels to the LAF","text":"<ol> <li>Parts: Caster Wheel Offsets, Caster Wheels, 8 M6x20 bolts, 8 M6 nuts</li> <li>Tools: Screwdriver, Pliers</li> <li>Steps:<ol> <li>Use the pliers to remove the breaks from a Caster Wheel.</li> <li>Align the holes of a Caster Wheel Offset and those of a Caster    Wheel to an appropriate set of holes in the LAF.</li> <li>Use hands to loosely fix the Caster Wheel Offset, the Caster Wheel, and    the LAF via the M6 nuts and bolts.</li> <li>Use the plier to hold the nuts in place and the screwdriver to fasten the    bolts into the nuts.</li> <li>Repeat for the other side.</li> </ol> </li> <li>Notes:<ol> <li>The Caster Wheel Offsets are manufactured by MakerLab. </li> </ol> </li> </ol>"},{"location":"lab-robots/branbot/assembly-tutorial/#attach-the-camera-to-the-camera-mount","title":"Attach the Camera to the Camera Mount","text":"<ol> <li>Parts: 2 M2x4 screws, Camera, Camera Mount</li> <li>Tools: Screwdriver</li> <li>Steps:<ol> <li>Fasten the Camera to the Camera Mount with the screws.</li> </ol> </li> <li>Note: Camera Mounts are printed by the MakerLab.</li> </ol>"},{"location":"lab-robots/branbot/assembly-tutorial/#attach-the-camera-mount-to-the-laf","title":"Attach the Camera Mount to the LAF","text":"<ol> <li>Parts: 2 M3x16 bolts, 2 M3 nylon lock nuts, Camera Mount</li> <li>Tools: Pliers Screwdriver</li> <li>Steps:<ol> <li>Fasten the Camera Mount to the LAF using the bolts and nylon nuts. Hold    the nylon nut in place with a plier when fastening the bolt. </li> </ol> </li> </ol>"},{"location":"lab-robots/branbot/assembly-tutorial/#attach-the-battery-brackets-to-the-laf","title":"Attach the Battery Brackets to the LAF","text":"<ol> <li>Parts: 2 Battery Brackets with heat-set nuts, 2 foam inserts, 4 M3x10 bolts</li> <li>Tools: Screwdriver</li> <li>Steps:<ol> <li>Observe the orientation of the battery relative to the brackets in the    pictures below.</li> <li>Insert the battery into the brackets, using the foam inserts to secure it    in place. </li> <li>Attach the battery to the LAF using the 4 M3x10 bolts</li> </ol> </li> <li>Notes:<ol> <li>The battery brackets should already have heat-set nuts set into them. If    this isn't the case, ask for assistance from the MakerLab.</li> <li>The MakerLab is also responsible for providing the foam inserts.</li> </ol> </li> </ol>"},{"location":"lab-robots/branbot/assembly-tutorial/#attach-the-led-holders-to-the-laf","title":"Attach the LED Holders to the LAF","text":"<ol> <li>Parts: 4 M3x10 screws, 4 LED Holders</li> <li>Tools: Screwdriver</li> <li>Steps:<ol> <li>Insert the LED bulbs/wires into the LED holders.</li> <li>Fix the LED holders to the appropriate holes in the LAF with the screws.</li> </ol> </li> <li>Notes:<ol> <li>The LED Holders should have heat-set nuts melted into them. If they do    not, ask for assistance from the Automation Lab.</li> <li>The LED wires should have bulbs on one end and female plugs on the other.    If the female plugs are missing, ask for assistance from the Automation    Lab.</li> </ol> </li> </ol>"},{"location":"lab-robots/branbot/assembly-tutorial/#wiring","title":"Wiring","text":""},{"location":"lab-robots/branbot/assembly-tutorial/#connect-the-raspberry-pi-and-the-teensy","title":"Connect the Raspberry Pi and the Teensy","text":"<ol> <li>Part: USB 2.0 to Micro USB Right-Angle Connector.</li> </ol>"},{"location":"lab-robots/branbot/assembly-tutorial/#connect-the-led-board-with-the-raspberry-pi","title":"Connect the LED Board with the Raspberry Pi","text":"<ol> <li>Part: USB 2.0 to Micro USB Cable.</li> </ol>"},{"location":"lab-robots/branbot/assembly-tutorial/#connect-the-raspberry-pi-and-the-lidar-board-to-the-battery","title":"Connect the Raspberry Pi and the LIDAR Board to the Battery","text":"<ol> <li>Part: USB 2.0 to Micro USB and USB-C Splitter Charging Cable </li> </ol>"},{"location":"lab-robots/branbot/assembly-tutorial/#connect-the-battery-to-the-charlie-board-screw-terminal","title":"Connect the Battery to the Charlie Board Screw Terminal","text":"<ol> <li>Part: Battery Power Cable</li> </ol>"},{"location":"lab-robots/branbot/assembly-tutorial/#connect-the-led-driver-to-the-imu","title":"Connect the LED Driver to the IMU","text":"<ol> <li>Part: I2C Cable</li> <li>Note: The LED Driver is the wrong side up in the picture below. So are the    male pins that have been attached to it. This does not affect the    functionality of the Driver, but attaching it the right side up is    preferable.</li> </ol>"},{"location":"lab-robots/branbot/assembly-tutorial/#connect-the-imu-to-the-charlie-board","title":"Connect the IMU to the Charlie Board","text":"<ol> <li>Part: I2C Cable</li> </ol>"},{"location":"lab-robots/branbot/assembly-tutorial/#connect-the-motor-assembly-wires-to-the-charlie-board","title":"Connect the Motor Assembly Wires to the Charlie Board","text":""},{"location":"lab-robots/branbot/assembly-tutorial/#connect-the-leds-to-the-led-driver","title":"Connect the LEDs to the LED Driver","text":""},{"location":"lab-robots/branbot/assembly-tutorial/#connect-the-camera-to-the-raspberry-pi","title":"Connect the Camera to the Raspberry Pi","text":"<ol> <li>Part: Raspberry Pi Camera Ribbon Cable</li> <li>Note: Observe how the ribbon cable twists as it goes from the pi to the    camera.</li> </ol>"},{"location":"lab-robots/branbot/assembly-tutorial/#attach-the-standoffs-to-the-laf","title":"Attach the Standoffs to the LAF","text":"<ol> <li>Parts: 6 75mm standoffs, 6 M3x10 screws</li> <li>Tools: Screwdriver</li> <li>Steps:<ol> <li>Attach the standoffs to the appropriate holes on the LAF with the screws.</li> </ol> </li> </ol>"},{"location":"lab-robots/branbot/assembly-tutorial/#attach-the-upper-acrylic-frame-to-the-standoffs","title":"Attach the Upper Acrylic Frame to the Standoffs","text":"<ol> <li>Parts: 6 M3x10 screws</li> <li>Tools: Screwdriver</li> <li>Steps:<ol> <li>Attach the Upper Acrylic Frame to the Standoffs with the M3x10 screws.</li> </ol> </li> </ol>"},{"location":"lab-robots/branbot/home/","title":"Branbot","text":"<p>The Branbot evolved from the Platform robot. It is refined, standardized and beautified. The team who built it consisted of Tim Hebert, Charlie Squires, James, Daniel and Tri.</p>"},{"location":"lab-robots/branbot/home/#detailed-construction","title":"Detailed Construction","text":"<p>Check this out: Assembly Tutorial</p> <p></p>"},{"location":"lab-robots/linorobot/home/","title":"Linorobot","text":""},{"location":"lab-robots/linorobot/home/#what-it-is","title":"What it is","text":"<p>Linorobot is a software package for building our own robots. We have used their software and instructions to construct our own robots and configure the software for them. The above link is a major resource for you. In this section we will explain things that are specific to Brandeis but we won't repeat it.</p>"},{"location":"lab-robots/linorobot/howto/","title":"How To","text":""},{"location":"lab-robots/linorobot/howto/#rebuild-and-upload-teensy-software","title":"Rebuild and upload teensy software","text":"<p><pre><code>$ roscd linorobot\n$ cd teensy/firmware\n$ pio run --target upload -e &lt;platform&gt;\n</code></pre> Where currently platform is either teensy31 (Plarform2) or teensy41 (Platform1)</p>"},{"location":"lab-robots/linorobot/platform_hardware/","title":"Platform Hardware","text":"<ul> <li> <p>Consistent across Platform 1, 2 and 3?</p> </li> <li> <p>Raspberry Pi 4b</p> <ul> <li>Arduino Teensy 3.1</li> <li>MPU 9250 IMU</li> <li>YDLidar X4</li> </ul> </li> </ul>"},{"location":"lab-robots/linorobot/stack/","title":"Intro to LinoRobot","text":""},{"location":"lab-robots/linorobot/stack/#intro","title":"Intro","text":"<p>Examine Linorobot  again. You will see very detailed instructions for building a robot, both the hardware and the software. In our world, bullet, platform, cat are all fully compliant Linorobot robots. </p>"},{"location":"lab-robots/linorobot/stack/#base-hardware-stack","title":"Base hardware stack","text":"<ul> <li>For the SBC we use either a Rasperry Pi 3B+ or a Raspberry Pi 4<ul> <li>Lidar is connected via USB to the Raspberry Pi</li> <li>The Microcontroller is connected via USB to the Raspberry Pi</li> </ul> </li> <li>For the microcontroller we use either a Teensy 3.2 or a Teensy 4.x (check this)<ul> <li>The motor controller is connected to the Teensy via</li> <li>The IMU is connected to the Teensy via I2C bus</li> </ul> </li> </ul>"},{"location":"lab-robots/linorobot/stack/#sbc-software","title":"SBC Software","text":"<p>The SBC is running Ubuntu 20.04 and ROS 1.0. It is a standard install which we get from the Linorobot installation. Of course our own ROS code and scripts are then added to it. Certain standard Linorobot Nodes are launched.</p>"},{"location":"lab-robots/linorobot/stack/#standard-linorobot-nodes","title":"Standard Linorobot Nodes","text":""},{"location":"lab-robots/linorobot/stack/#microcontroller-software","title":"Microcontroller Software","text":"<p>The Teensy code is provided by Linorobot. We have tweaked it in small ways. See How To for information on rebuilding it and installing the software. This software has the following jobs:</p> <ol> <li>Read the encoders to determine the apparent speed and direction of the robot</li> <li>Subscribe to cmd_vel to determine the desired speed and direction</li> <li>Use a PID controller to drive the motors to meet the desired speed and direction</li> <li>Publish the actual speed and direction as computed by the encoders as <code>ODOM_RAW</code></li> <li>Read the IMU data (via the I2C bus) and publish it as <code>IMU_RAW</code></li> <li>Read other I2C sensors and actuators (coming soon!)</li> </ol>"},{"location":"lab-robots/linorobot/troubleshooting/","title":"Troubleshooting","text":""},{"location":"lab-robots/linorobot/troubleshooting/#linobaseh-pins","title":"linobase.h Pins","text":"<p>The most common problem with a new build is that the pin numbers are incorrect. You can find the hardware configuration in <code>linorobot/teensy/firmware/lib/config/lino_base.h</code>. In that same directory you will find other versions of that file for our different models. If you are qualifying a new model, then you should add a copy there.</p> <p>There are numberous variables in this file. The key ones for now are: <pre><code>#define MOTOR1_IN_A 20\n#define MOTOR1_IN_B 1\n#define MOTOR1_PWM 22\n\n#define MOTOR2_IN_A 6\n#define MOTOR2_IN_B 8\n#define MOTOR2_ PWM 5\n</code></pre></p> <p>and</p> <pre><code>#define MOTOR1_ENCODER_A 14\n#define MOTOR1_ENCODER_B 15\n\n#define MOTOR2_ENCODER_A 12\n#define MOTOR2_ENCODER_B 11\n</code></pre> <p>MOTOR1 is front left, and MOTOR2 is front right. There are several misconfigurations possible, basically all the permutations of left/right and forward/backward, on both the motors and encoders. </p>"},{"location":"lab-robots/linorobot/troubleshooting/#symptoms","title":"Symptoms","text":"<p>If all wheels are spinning but the robot spins in circles, goes backward, is unresponsive to <code>cmd_vel</code> commands or in general acts crazy, your first hypothesis should be that one or more of the pins above are incorrect or swapped.</p> <p>If one of the wheels doesn't spin at all then it's probably an electrical connection to the motor. If both wheels don't spin then it's probably a power/battery issue.</p>"},{"location":"lab-robots/linorobot/troubleshooting/#erratic-travel-on-a-newly-built-robot","title":"Erratic Travel on a newly built robot","text":""},{"location":"lab-robots/linorobot/troubleshooting/#check-encoders","title":"Check Encoders","text":"<p>To check the encoders, disconnect the power from the motors, leaving just the encoders. Then when you roslaunch linorobot minimal.launch it will print four numbers over and over again which are the readings of the potential for encoders (two in front and two in the back.) </p> <p>As I had only two wheels I just got two non-zero numbers. Put the robot on the floor and push it gently forward. Those two numbers will change. They should both go up approximately as fast, even though they are not the same number. In my case, they went in opposite directions. Note the one that is going down. The left wheel is encoder1 and the right wheel is encoder2. In the setup file I switched the pins for the corresponding motor and that solved it.</p> <p>(Note: This is very obvious but I got it wrong: The caster is on the BACK of the robot not the front. You need to know what is the front of the robot to know which wheel is left or right.)</p>"},{"location":"lab-robots/linorobot/troubleshooting/#check-motors","title":"Check Motors","text":"<p>For me, this didn't quite solve it. So the next thing to check was the motor itself. I changed the Arduino code (.ino) so that instead of sending the calculated numbers to the motors, I sent 100 to the left motor and 500 to the right motor. This is so that I could tell if the left motor turned slower than the right. If not, I had to switch those pins. Next I had to tell that both motors turned so that the effect was forward motion of the robot. That was incorrect for me too. That too is fixed by switching PIN numbers.</p>"},{"location":"lab-robots/linorobot/troubleshooting/#check-pid","title":"Check PID","text":"<p>Next came the PID settings. The instructions are good there in terms of getting monitoring the result of the pid calculations but not as far as what numbers are right. There are an infinite number of references on the web on tuning pid and they are all vague and different. </p> <p>Here again I made a small change to the Arduino code. I had it print out the error between the desired and actual rate of the left and right wheels. If things are working like they should that error starts at zero and when you give the robot a command it temporarily goes away from zero and then returns nicely to zero. I don' know yet what \"technique\" I used, nor whether I have the right parameters yet. But at least the robot is behaving better.</p>"},{"location":"lab-robots/linorobot/validating/","title":"Validating","text":"<p>When a new robot is built, there are a number of steps needed to see that it is properly configured.</p>"},{"location":"lab-robots/linorobot/validating/#encoders","title":"Encoders","text":"<p>Encoders need to be connected correctly. The left and right can be swapped, and also they may be backwards. These issues lead to very crazy and incomprehensible behavir, so it's best to check them first if your new robot is acting weird.</p>"},{"location":"lab-robots/linorobot/validating/#motors","title":"Motors","text":"<p>Motors also need to be connected correctly. They can be left-right swapped or forward-reverse swapped. It is worth again to test them separately. The kind of incomprehenible behavor from this kind of problem is totally different from the one when the encoders are backwards or swaps.</p>"},{"location":"lab-robots/linorobot/validating/#front-and-back","title":"Front and back","text":"<p>It is very important that you know what the front of the robot is. Otherwise nothing makes sense. Our robots have the big wheels in front and the casters in back.</p>"},{"location":"lab-robots/linorobot/validating/#imu","title":"IMU","text":"<p>The IMU is connceted via an I2C Quick Connector to the Teensy. We have seen problems when the IMU doesn't work that were caused by the placement or length of the wire, so keep an eye out for that case.</p>"},{"location":"lab-robots/minirover/home/","title":"MiniRover","text":"<p>MiniRover is our modification of the GoPiGo3 robot. It comes with software, a VPN and all kinds of fancy things which is why I decided to give it a special name. This page is under construction as I troubleshoot and make notes about things. I expect feedback from everyone where the instructions don't meet reality!</p>"},{"location":"lab-robots/minirover/mrbuild/","title":"mrbuild.md","text":""},{"location":"lab-robots/minirover/mrbuild/#gopigo3-basic-kit","title":"GoPiGo3 Basic Kit","text":"<p>We recommend that you start with a GoPiGo3 Basic Kit. Build it following the instructions. There is a lot written and a lot to google about it. Here are some useful links:</p> <ul> <li>Dexter Forum - where you can get many questions answered</li> <li>Build your GoPiGo3 - where you find step by step instructions.</li> <li>MiniRover Purchase Program - instructions for students taking Cosi119a at Brandeis University to order a miniRover.</li> </ul> <p>In addition to what you received with the Basic Kit you will need the following to get through the instructions.</p> <ul> <li>A Raspberry Pi 3+</li> <li>A battery pack (with Y-cable and charger)</li> <li>A MicroSD card</li> <li>A Lidar</li> <li>A very short USB cable</li> </ul> <p>The Battery pack is different from what the instructions talk about. The Y-Cable is used to connect the barrel connector of the battery pack with the corresponding battery connector on the \"Red Board\" which in turn will power the Raspberry Pi.</p> <p>The robot will come with a microSD card with the dexter software. There is some very very simple calibration that you need to do.</p>"},{"location":"lab-robots/minirover/mrbuild/#battery-pack","title":"Battery Pack","text":"<ul> <li>The battery pack is specific in terms of voltage and capacity. Don't substitute it for another one.</li> <li>Note that the battery pack needs to be in the \"on\" position in order to have the charger do anything. And obviously it has to be in \"ON\" for the robot to work.</li> <li>It also comes with a charger and a y-cable. The y-cable is about 10\" long and has three barrel connectors on it in a y-configuration.</li> <li>We use only two of the 3 ends of the Y-connector. One goes into the barrel connector on the red board, and one goes into the barrel connector on the battery pack.</li> <li>We then connect the very short usb cable with one end in the battery pack and the other end in the \"power\" connection of the Lidar board. The Lidar board is tiny about 1.5\" square. On the one side you have the funny Lidar wires and on the other side you have two micro usb connectors. If you look very carefully one is marked data and one is marked power. The one marked data is connected with a short usb cable to one of the usb connectors of the pi. The one marked power is connected with another short usb cable to the battery pack.</li> </ul>"},{"location":"lab-robots/minirover/mrbuild/#camera","title":"Camera","text":"<p>You will have bought the camera separately, but the instructions talk about how to mount it to the robot. Make sure it is centered in all three dimensions. A good spot is on the front of the top plexiglass part. It can be do without any extra parts, but Dexter has this bracket which works nicely too.</p>"},{"location":"lab-robots/minirover/mrbuild/#lidar","title":"Lidar","text":"<p>The Lidar needs to be mounted on the top plexiglass part. You will need to drill two holes. Make sure that the Lidar is exactly centered and pointed forward. As far as the drilling: I kept it to a minimum and drilled only two. I think it doesn't matter much. Mostly that it be straight and centered. Note that the narrower part of the Lidar is the front. You want it centered and facing forward (which on the other side of the ball caster.) I drilled holes for the two posts at the back of the Lidar (the wider part.) I don't see that it matters much though, that was what was easiest for me.</p> <p> </p>"},{"location":"lab-robots/minirover/mrsetup/","title":"Setup of MiniRover","text":"<p>This section will help you get your robot set up to go. Note that if you received this from the Autonomous Robotics Lab then part of these steps may already be done.</p> <p>See the important information about turning the robot on and off here: Using the Robot.</p>"},{"location":"lab-robots/minirover/mrsetup/#microsd-card-if-needed","title":"MicroSD Card (if needed)","text":"<p>If your robot was set up by us, then you should skip this step!</p> <p>You are going to load software onto the microSD Card - which will wipe out what you loaded onto it according the the instructions before. We provide you a disk image on which to store it. It changes from time to time. This link is to a google folder that will contain the versions. You should use the latest one in that folder. I recommend you use the app \"Balena Etcher\" to copy the image onto your microSD card.</p> <p>To create a new MicroSD from the old one, see Backup Raspi Card on MacOS</p>"},{"location":"lab-robots/minirover/mrsetup/#connecting-to-the-network","title":"Connecting to the network","text":"<p>Now we are facing a dilemma. We need to get the robot on your network. There are several ways of doing this. Below are two specific scenarios that we support:</p> <ol> <li>A usb keyboard, usb mouse and a HDMI display and eduroam wifi access</li> <li>OR A network cable and non-eduroam wifi access</li> <li>A MiniRover v2 and non-eduroam wifi access. It should be labeled with a name.</li> </ol>"},{"location":"lab-robots/minirover/mrsetup/#scenario-1","title":"Scenario 1","text":"<ol> <li> <p>We will tell you what the name of your robot is. Plug it in below wherever you see <code>gopigo3</code></p> </li> <li> <p>Locate a wired network connection (on your router for example) and use a network cable to connect your robot to the network</p> </li> <li> <p>Now turn the power on (see Using MiniRoverfor instructions. It should boot up into linux. But you won't know this because there's no keyboard or screen!</p> </li> <li> <p>Using your own computer or development environment that is attached to the network check that you see the robot. This includes a linux computer where you program, or a browser web development environment. Broadly speaking, it's the computer \"other than\" to raspberry pi on the robot (we will refer to this as the remote computer from now on).</p> </li> </ol> <pre><code>ping gopigo3.local\n</code></pre> <p>Once this works you know that you have access to the robot from your remote computer. Make note of the robot' ip address. It will likely look like 192.168.1.xx but not necessarily.</p> <ol> <li>Now use <code>ssh</code> (secure shell) to get to the robot from the remote:</li> </ol> <pre><code>ssh pi@gopigo3.local\n</code></pre> <p>It will ask you for the password for account <code>pi</code>. It is <code>raspberry</code>. Once you get in you are 1/2 way there!</p> <ol> <li>Now we want to get your robot onto your local wifi network. You need to know the wifi network's name and password. On the robot command line type:</li> </ol> <pre><code>sudo nmcli d wifi connect &lt;SSID&gt; password &lt;password&gt;\n</code></pre> <p>Where \\&lt;SSID&gt; is the network's name. You can surround it in quotes if there are spaces or other funny characters.  is the password. <ol> <li> <p>Next shutdown the robot nicely (see above), disconnect the network cable, and start the robot up nicely again (see above.)</p> </li> <li> <p>Once it's back, follow the same steps to <code>ssh pi@gopigo3.local</code> and enter the password <code>raspberry</code> and you should have a wireless enabled robot.</p> </li> </ol>"},{"location":"lab-robots/minirover/mrsetup/#scenario-2","title":"Scenario 2","text":"<ol> <li> <p>Connect your mouse, keyboard and screen to the Raspberry pi. You will find several free USB ports and an HDMI port. Look closely they are all there.</p> </li> <li> <p>Boot up the Raspberry Pi and wait until it is running and you see the desktop</p> </li> <li>Locate the network settings dialog box by clicking the network icon on the top right</li> <li>Add eduroam as a network, and fill it in as follows:</li> </ol> <p></p> <ol> <li>Finally shutdown the robot, unplug the keyboard, mouse and monitor and reboot</li> <li>Once it's back <code>ssh pi@gopigo3.local</code> and enter the password <code>raspberry</code> and you should have a wireless enabled robot</li> </ol>"},{"location":"lab-robots/minirover/mrsetup/#scenario-3-incomplete-and-incorrect","title":"Scenario 3 incomplete and incorrect","text":"<p>You've received a fully set up and tested Minrover v2, and you know the name. The following steps are written as if the robot is called <code>mr1</code>. Scenario 3 Robots will have the account ubuntu with the password ubuntu (so different from scenario 1 and 2)</p> <ol> <li>Now turn the power on (see Using MiniRoverfor instructions. It should boot up into linux. But you won't know this because there's no keyboard or screen!</li> <li>MiniRover will come up as a Wifi access point. The SSID is  mr1XXXX where XXXX is part of the MAC address. The wifi password is robotseverywhere.s Connect to it via your computer's wifi.</li> </ol>"},{"location":"lab-robots/minirover/mrsetup/#troubleshooting-ssh-gopigo3local","title":"Troubleshooting ssh gopigo3.local","text":"<p>Under certain circumstances gopigo3.local will be found. If so you need to find out the ip address of your robot when it is on wifi (not wired). If then this should work:</p> <pre><code>ssh pi@&lt;ip address&gt;\n</code></pre>"},{"location":"lab-robots/minirover/mrsetup/#updating-the-hostname-of-your-robot","title":"Updating the hostname of your Robot","text":"<p>Rename the <code>hostname</code> of your robot. It comes to you called \"gopigo3\" but they are all called that and this can cause confusion. Let's say you want to call your robot <code>pitosalas</code>. Oddly you have to change it in two places. Here's how\"</p> <p><pre><code># In the shell of the robot:\nsudo hostname pitosalas\n\n# In the hostname file:\nsudo nano /etc/hostname\n</code></pre> Now the robot is called <code>pitosalas</code> and at least it will be different from other miniRovers.</p>"},{"location":"lab-robots/minirover/mrsetup/#eduroam","title":"Eduroam","text":"<p>On eduroam it is often automatically known as gopigo3.dyn.brandeis.edu. So after rebooting it check if this works. It's not guaranteed but often it works just fine.</p>"},{"location":"lab-robots/minirover/mrsetup/#vpn","title":"VPN","text":"<p>You can run everything on the robot itself but it doesn't have a screen. You could plug an external screen, keyboard and mouse and have a complete ROS enabled computer (the robot) at your finger-tips. But it would be real slow. So instead we are going to add the robot to a \"VPN\" - virtual private network which will allow you to work with it remotely.</p> <ol> <li>Prepare the VPM configuration by:</li> </ol> <pre><code>sudo apt-get remove -y tailscale\nsudo rm -rf /var/lib/tailscale/tailscaled.state\n\n# Reboot right after!\nsudo reboot\n</code></pre> <ol> <li>Setup VPN</li> </ol> <pre><code># Get the tskey from Pito\ncd ~/rosutils\nchmod +x pi_connect.sh\n\n# Run the script with the tailscale authkey\nsudo ./pi_connect.sh &lt;tskey-123abc456&gt;\n\n# On successful connect, you should see this\n# Connected. IP address: 100.xx.xxx.xxx\n</code></pre> <ol> <li><code>myvpnip</code> should now return that same IP address.</li> </ol>"},{"location":"lab-robots/minirover/mrsetup/#updates-for-your-robot","title":"Updates for your Robot","text":"<p>We have installed some Brandeis specific software which should be updated:</p> <pre><code>cd ~/rosutils\ngit pull\ncp ~/rosutils/bashrc_template.bash ~/.bashrc\ncd ~/catkin_ws/src/gpg_bran4\ngit pull\n</code></pre> <p>And then edit the new ~/.bashrc according to the instructions in the file.</p>"},{"location":"lab-robots/minirover/mrsetup/#updates-to-your-cloud-desktop","title":"Updates to your Cloud Desktop","text":"<ol> <li>Check that you have ~/rosutils directory on your cloud desktop. If not:</li> </ol> <pre><code>cd\ngit clone https://github.com/campusrover/rosutils.git\ncp rosutils/bashrc_template.bash .bashrc\n</code></pre> <p>Edit .bashrc according to the instructions in it</p>"},{"location":"lab-robots/minirover/mrsetup/#this-will-continue-to-be-updated","title":"This will continue to be updated","text":""},{"location":"lab-robots/minirover/mrtroubleshooting/","title":"Troubleshooting","text":"<p>Author: Pito Salas</p>"},{"location":"lab-robots/minirover/mrtroubleshooting/#gemeral","title":"Gemeral","text":"<ul> <li>An excellent and very detailed troubleshooting guide for the Raspberry Pi</li> </ul>"},{"location":"lab-robots/minirover/mrtroubleshooting/#led-on-the-red-board","title":"LED on the Red Board","text":"<ul> <li>Blinking green is the \u201cstarting up\u201d indication.</li> <li>Solid green is the \u201cready\u201d indication.*</li> <li>Solid yellow is the \u201clow battery\u201d caution indication.</li> <li>Solid red is the \u201clow battery\u201d warning.</li> <li>Blinking red is the \u201cshutting down\u201d indication.</li> <li>Blinking purple is (AFAIK) the \u201cI don\u2019t know what is going on\u201d indication, but I could be wrong.</li> </ul> <p>But, note that all of these indications are only valid when using a Dexter Industries / Modular Robotics operating system. (i.e. Raspbian for Robots, GoPiGo OS, or Dexter OS). It will continue to blink green when using any other O/S. The Minirover configuration is Ubuntu! Also see: https://www.dexterindustries.com/GoPiGo/get-started-with-the-gopigo3-raspberry-pi-robot/2-connect-to-the-gopigo-3/</p>"},{"location":"lab-robots/minirover/mrtroubleshooting/#python-default","title":"Python default","text":"<p>To Change the default python on ubuntu (assuming you want it to be python3.4)</p> <p><code>sudo update-alternatives --install /usr/bin/python python /usr/bin/python3.4 1</code></p>"},{"location":"lab-robots/minirover/mrtroubleshooting/#check-whether-spi-is-working-on-the-pi","title":"Check whether spi is working on the pi","text":"<p>```pythons = </p> <p>import spidev spi = spidev.SpiDev() spi.open(0,1) Traceback (most recent call last):   File \"\", line 1, in  PermissionError: [Errno 13] Permission denied ```"},{"location":"lab-robots/minirover/mruse/","title":"mruse.md","text":""},{"location":"lab-robots/minirover/mruse/#turning-it-on","title":"Turning it on","text":"<p>It is important that you follow a rigid procedure when turning the robot on and off.</p> <p>Assuming it is totally off:</p> <ol> <li>Make sure the battery pack is correctly connected to the robot.</li> <li>Switch on the battery pack</li> <li>Click the micro button on the red board</li> </ol> <p></p> <ol> <li>Look at the Raspberry Pi (not the \"red board\") You will see tiny red and green LEDs blinking. Wait until the green one has settled down to slow flickering.</li> </ol>"},{"location":"lab-robots/minirover/mruse/#turning-it-off","title":"Turning it off","text":"<ol> <li>From your ssh command line on the remote computer, type <code>sudo shutdown now</code></li> <li>Once the Raspberry Pi has stopped blinking you can turn off the power switch on the battery pack.</li> </ol>"},{"location":"lab-robots/minirover/mruse/#charging-the-battery","title":"Charging the Battery","text":"<p>The Battery pack came with a charger. It has a light on which is red while charging and green when fully charged. Note that the battery will not charge when its switch is set to off.</p>"},{"location":"lab-robots/minirover/mruse/#rset-command","title":"rset command","text":"<p>We've implemented a very simple tool to set up the IP addresses correctly. It will be changing as I figure out how to make it better. So for now...</p> <ol> <li>If you have a cloud desktop and want to run simulations without a actual miniRover\" <code>rset cloud</code></li> <li>If you have a cloud desktop add a real robot, run <code>rset robot</code> on your cloud desktop and <code>rset pi</code> on the actual miniRover (over ssh)</li> <li>If you have a local docker based desktop, run <code>rset docker</code> there.</li> </ol> <p>rset by itself displays the current status</p> <p>(I know a combination is missing and plan a revision of this)</p>"},{"location":"lab-robots/minirover/mruse/#starting-the-minirover-ros-applications","title":"Starting the MiniRover ROS applications","text":"<p>Note that all we run on the MiniRover itself are roscore, and the nodes needed for the motors, lidar and camera. Everything else runs on your \"remote\". The following commands are to be done on the MiniRover from \\~/catkin_ws</p> <pre><code># launch the motor controller and lidar\nroslaunch minirover mr_bringup.launch\n\n# launch camera (optional if you need the camera)\nroslaunch gpg_bran raspicam.launch\n</code></pre>"},{"location":"lab-robots/minirover/mruse/#web-desktop-tools","title":"Web Desktop tools","text":"<p>Note that this includes all flavors, cloud based, local docker based, and gpu based browser desktops. If you just want to use the simulators on their own and are not using an actual miniRover, then: <code>rset cloud</code> is enough. At that point you can run your ROS programs.</p> <p>There are numerous scripts, programs and launch files that are preinstalled on your ROS wegpgb desktop. I will document only some of them here but you can look around and find more that are interesting. All of them from the book are here. I have not renamed any of them for that reason.</p> <pre><code># Fun prr examples\nrosrun prrexamples red_light_green_light.py\nrosrun prrexamples wander.py\n\n# Visualize robot with rviz\nroslaunch gopigo3_navigation gopigo3_rviz.launch \n\n# Or with gazebo (not as useful)\nroslaunch gopigo3_navigation gopigo3_gazebo.launch \n\n# Run different simulated stages to experiment with roslaunch \nroslaunch turtlebot3_gazebo turtlebot3_stage_1.launch\nroslaunch turtlebot3_gazebo turtlebot3_stage_2.launch\nroslaunch turtlebot3_gazebo turtlebot3_stage_3.launch\nroslaunch turtlebot3_gazebo turtlebot3_stage_4.launch\n\n# Control the robot\nroslaunch turtlebot3_teleop turtlebot3_teleop_key.launch\n\n# Begin building the map on real robot\nroslaunch gopigo3_navigation gopigo3_slam.launch\n\n# Begin localization with AMCL\nroslaunch gopigo3_navigation amcl.launch\n</code></pre>"},{"location":"lab-robots/minirover/mruse/#rset-command_1","title":"Rset Command","text":"<p>In order to facilitate working in all the combinations of environments we have these commands:</p> <ul> <li>rset pi - declare that this is the raspberry pi</li> <li>rset cloud - declare that this is a cloud desktop working with sim</li> <li>rset robot - declare that this is a cloud desktop working with a real robot</li> <li>rset - display settings</li> </ul>"},{"location":"lab-robots/minirover/mruse/#aliases","title":"Aliases","text":"<p>There are a bunch of handy aliases:</p> <ul> <li><code>myip</code> - this computer's regular local ip</li> <li><code>myvpnip</code> - this computer's vpn ip if it has one</li> <li><code>stopnow</code> - immediately stop the robot</li> <li><code>teleop</code> - run a simple teleop tool</li> <li><code>ru</code> - connect to rosutils</li> <li><code>cm</code> - catkin_make</li> <li><code>cw</code> - cd \\~/catkin_ws</li> <li><code>cs</code> - cd \\~/catkin_ws/src</li> </ul>"},{"location":"lab-robots/minirover/mruse/#accounts-and-passwords","title":"Accounts and passwords","text":"<ul> <li>miniRover</li> <li>hostname <code>gopigo3</code></li> <li>default account <code>pi</code></li> <li>default password raspberry</li> <li>Cloud or Docker Desktop</li> <li>default password <code>dev@ros</code></li> <li>desktop: url: http://vnc..ros.campusrover.org</li> <li>vscode: http://code..ros.campusrover.org</li> </ul>"},{"location":"lab-robots/minirover/mruse/#key-environment-variables","title":"Key Environment Variables","text":"<p><code>ROS_MASTER_URI=http:/100.94.206.80:11311</code> (example!) should always be set to the computer where roscore is running. If you are using a physical robot, then roscore runs on the robot itself. If you are in the web deskop and working just with simulation then roscore would run there.</p>"},{"location":"lab-robots/minirover/mruse/#robot","title":"Robot","text":"<p>ROS_MASTER_URI = robot's own ip address ROS_IP = robot's own ip address</p>"},{"location":"lab-robots/minirover/mruse/#remote-computer","title":"Remote Computer","text":"<p>ROS_MASTER_URI = robots ip address ROS_IP = remote computer's own IP address</p> <p>These IP addresses are on different networks and cannot access each other. So instead we've created what is called a \"virtual private network\" that connects them together. Both your robot and your cloud desktop have an alternate ip address which they can both see.</p>"},{"location":"lab-robots/minirover/mruse/#ip-addresseses","title":"IP Addresseses","text":"<ul> <li><code>myip</code> returns your local ip address</li> <li><code>myvpnip</code> returns your vpn ip address (if you have one)</li> </ul>"},{"location":"lab-robots/platform/home/","title":"The Platform Line of Robots","text":"<p>Note</p> <p>This is a note. It just a feature of markdown in our CMS. </p>"},{"location":"lab-robots/pupper/boundary-generation/","title":"Boundary Generation","text":""},{"location":"lab-robots/pupper/boundary-generation/#environment","title":"Environment","text":"<ul> <li>Boundary generation occurs in the <code>environment</code> class within <code>src/boundary_detection.py</code> </li> <li>The class works by setting an ego agent (the pupper), adding each detected fiducial as an obstacle, and if a fiducial with <code>id = 0</code> was detected then a goal object is added to the class as well</li> <li>Obstacle locations are updated if a fiducial with an existing <code>id</code> is detected and obstacle locations can be cleared with <code>clear_obstacles</code></li> <li>Then a list of boundaries corresponding to each obstacle is created using the Minkowski Sum formula</li> </ul>"},{"location":"lab-robots/pupper/boundary-generation/#configuration-space","title":"Configuration Space","text":"<ul> <li>Configuration space is essentially a series of points that represent infeasible locations and rotations for a robot in an environment</li> <li>We can use this along with discretization to generate a graph of all the feasible locations for the pupper robot in an environment with obstacles, assuming we have the models for the agent and the obstacles</li> <li>Below are two example configuration spaces shown in red for an ego agent in green for the obstacle in blue, notice how the configuration spaces changes when the rotation of the agent changes</li> </ul>"},{"location":"lab-robots/pupper/controls/","title":"Controls","text":"<p>The <code>controller.py</code> provides an API to control the robot by emulating a PS4 controller, similar to this. Main difference is some keybinds are different, but it can also be called from code.</p>"},{"location":"lab-robots/pupper/controls/#the-udp-message","title":"The UDP message","text":"<p>This is a breakdown of the dictionary's fields that is sent through UDPComms to the robot.</p> Field Value Comments lx [-1,1] Walk left or right ly [-1,1] Walk front or backwards rx [-1,1] Turn left or right (yaw) ry [-1,1] Tilt up or down (pitch) L2 0 Nothing R2 0 Nothing R1 1 or 0 Toggle trot/rest L1 1 or 0 Toggle activation/deactivation dpadx [-1,1] Move body up or down dpady [-1,1] Roll left/right (roll) square 0 Nothing circle 0 Nothing triangle 0 Nothing x 0 Nothing message_rate 20 Rate of which messages are sent <p>Note: values <code>[-1,1]</code> means any values between -1 and 1 and values <code>1 or 0</code> are a toggle. This means that the first time <code>1</code> is sent, it will cause the value on the pupper to change. A <code>0</code> needs to be sent before another <code>1</code> will cause a toggle.</p> <p>It is a good idea to use some sort of smoothing for <code>lx</code>, <code>ly</code> and <code>rx</code> to avoid abrupt stops.</p>"},{"location":"lab-robots/pupper/controls/#keybinds","title":"Keybinds","text":"<p>If running the controller manually, these are the controls:</p> Keybind Function space Toggle trot/rest v Toggle activation w/s Move forward/back a/d Turn left/right q/e Move left/right x Stop any sort of movement"},{"location":"lab-robots/pupper/controls/#control-sequence","title":"Control sequence","text":"<p>Robot must first be activated, this will also trigger calibration if the pupper software was run with the <code>--zero</code> flag. Then it must be in trotting mode and only then can it be controlled with other functions.</p>"},{"location":"lab-robots/pupper/fiducial-detection/","title":"Fiducial Detection","text":""},{"location":"lab-robots/pupper/fiducial-detection/#raspicam","title":"Raspicam","text":"<ul> <li>We use a RaspberryPi v1 camera whose full documentation can be found at Raspicam Docs, this also includes hardware and software specs </li> <li>Additional information on how to install Raspicam hardware can be found at Raspicam Installation</li> </ul>"},{"location":"lab-robots/pupper/fiducial-detection/#apriltags","title":"AprilTags","text":"<ul> <li>We use the <code>pupil_apriltags</code> package to detect fiducials in the Raspicam image so full documentation for the AprilTags can be found at Pupil AprilTags</li> <li>The package works by taking the camera parameters, fiducial size and family, and additional fiducial detection parameters and creating a <code>detector</code> class which contains a <code>detect</code> method that inputs a camera frame and outputs a list of detected fiducials</li> <li>In order to print new apriltags you have to follow the instructions at AprilTag Generation and Premade AprilTags</li> </ul>"},{"location":"lab-robots/pupper/fiducial-detection/#parameters-and-tuning","title":"Parameters and Tuning","text":"<ul> <li>The <code>params.yaml</code> file contains all of the fiducial vision parameters</li> <li>The camera parameters which are used in the <code>params.yaml</code> file were found online in the raspicam section</li> <li>The size of the fiducial in meters can be adjusted by printing out new fiducials of a larger size </li> </ul>"},{"location":"lab-robots/pupper/fiducial-detection/#transformation-math","title":"Transformation Math","text":"<ul> <li>The <code>src/transforms.py</code> and <code>src/geometry.py</code> contain the methods used for transforming the fiducial detection results into easier to work with translations and rotations </li> <li>Also, parameters in <code>params.yaml</code> are used to slightly adjust the detection results after transformation</li> </ul>"},{"location":"lab-robots/pupper/hardware/","title":"Hardware","text":"<p>The robot was donated by the Hands-On Robotics Initiative. Due to time constraints and some delays, a prebuilt robot was delivered instead of a parts kits. The build instructions can be found on this Google Doc. It also includes other relevant instructions that will be referenced later, such as motor calibration, software installation and run instructions.</p>"},{"location":"lab-robots/pupper/hardware/#hardware-overview","title":"Hardware Overview","text":""},{"location":"lab-robots/pupper/hardware/#default-hardware-robot","title":"Default hardware (robot)","text":"<p>The robot consists of 12 C610 motor controllers, 12 M2006 motors, a Teensy board, cables and the chassis.</p>"},{"location":"lab-robots/pupper/hardware/#additional-hardware","title":"Additional hardware","text":"<ul> <li>Raspberry Pi</li> <li>Raspberry Pi Camera</li> <li>6S Lipo Battery 22.2V</li> <li>USB Battery Pack for Raspberry Pi</li> <li>Keyboard (for emergency stop)</li> </ul>"},{"location":"lab-robots/pupper/hardware/#battery-charging-settings","title":"Battery Charging Settings","text":"<p>The battery charger in the lab supports charging the 6S Lipo Battery. Settings can be found below:</p> <pre><code>Chemistry:    LiPo\nVoltage:      22 Volts\nCapacity:     1300mAh\nCharge Rate:  5C\nSeries Count: 6S\n</code></pre> <p>The batteries took around 1 hour to charge and last around 2-3 hours per charge.</p> <p>The USB Battery Pack used was a generic one that provided enough voltage for the Pi. These are usually battery packs that support fast charge technology for smart phones. An appropriate cable is needed (e.g: USB 3.0 to Micro-USB or otherwise)</p>"},{"location":"lab-robots/pupper/hardware/#calibration-tips","title":"Calibration Tips","text":"<ul> <li>Motor calibration can be found in the doc linked earlier. The robot's casing may have to be opened to access some of the motor controllers.</li> <li>Instructions for leg calibration can be found here. Best results were achieved by supporting the robot with a box and adding paddings (made of paper towels and tape) between the legs and the box to get desired motor angles.</li> </ul>"},{"location":"lab-robots/pupper/home/","title":"Pupper main","text":""},{"location":"lab-robots/pupper/introduction/","title":"Introduction","text":""},{"location":"lab-robots/pupper/introduction/#context","title":"Context","text":"<p>The Pupper project uses the Stanford Pupper 2.0 which does not currently have public documentation but information on the original Pupper project can be found at Stanford Pupper. More detailed documentation on the original Pupper can be found at Pupper Docs. This project explores topics related to fiducial detection, boundary generation, motion planning, controls, and hardware.</p> <p></p>"},{"location":"lab-robots/pupper/planning/","title":"Planning","text":""},{"location":"lab-robots/pupper/planning/#discretization","title":"Discretization","text":"<ul> <li>The discretization of the environment is done in <code>/src/path_finder.py</code> in the <code>PathFinder</code> class </li> <li>The parameters for discretization are in the <code>params.yaml</code> file and affect the inflation of obstacles in the graph, step size of the robot (distance from one vertex to it's immediate neighbors), the length and width of the graph to generate, as well as if the graph search should explore nodes diagonally</li> <li>The discretization happens in the <code>explore</code> method which uses two serial doubly nested lambda expressions to create a matrix of <code>Point</code> objects which gets converted into a matrix of <code>Node</code> objects containing <code>point</code>, <code>distance</code>, <code>previous</code>, and <code>explored</code> which are the required fields to perform a graph search on this data </li> </ul>"},{"location":"lab-robots/pupper/planning/#graph-search","title":"Graph Search","text":"<ul> <li>The graph search happens in the <code>PathFinder</code> class in the <code>solve</code> method which performs a Dijkstra shortest path search from the node corresponding to the agent location to the node nearest to the goal location</li> <li>The algorithm operates the same as a standard shortest path search but has several optimizations built in to account for the limited hardware of the RaspberryPi</li> </ul>"},{"location":"lab-robots/pupper/planning/#path-profiling","title":"Path Profiling","text":"<ul> <li>Path profiling is the process of converting the path, a list of <code>Point</code> objects to a series of distances, and headings for the controller to follow</li> <li>The math for it is in <code>src/geometry.py</code> which finds the angle and distance between two points</li> </ul>"},{"location":"lab-robots/pupper/planning/#potential-improvements","title":"Potential Improvements","text":"<p>Many potential improvements exist to boos the performance, accuracy, and resolution of the planning module. Some ideas are:  * Use dynamic programming to eliminate redundant <code>in_range_of_boundary_quick</code> checks for the same node * Implement a gradient based approach to converting configuration space into edge weights * Use a better shortest path search algorithm or a sampling-based approach so discretization is not necessary </p>"},{"location":"lab-robots/pupper/software-overview/","title":"Software Overview","text":"<p>The code can be found at Pupper GitHub Repository</p>"},{"location":"lab-robots/pupper/software-overview/#directory","title":"Directory","text":"<pre><code>\u251c\u2500\u2500\u2500models\n\u2502   \u2514\u2500\u2500\u2500agent.py\n\u2502   \u2514\u2500\u2500\u2500goal.py\n\u2502   \u2514\u2500\u2500\u2500obstacle.py\n\u251c\u2500\u2500\u2500plots\n\u2502   \u2514\u2500\u2500\u2500map.png\n\u251c\u2500\u2500\u2500src\n\u2502   \u2514\u2500\u2500\u2500boundary_detection.py\n\u2502   \u2514\u2500\u2500\u2500controller.py\n\u2502   \u2514\u2500\u2500\u2500fiducial_vision.py\n\u2502   \u2514\u2500\u2500\u2500geometry.py\n\u2502   \u2514\u2500\u2500\u2500main.py\n\u2502   \u2514\u2500\u2500\u2500node.py\n\u2502   \u2514\u2500\u2500\u2500path_finder.py\n\u2502   \u2514\u2500\u2500\u2500boundary_profiler.py\n\u2502   \u2514\u2500\u2500\u2500transforms.py\n\u2502   \u2514\u2500\u2500\u2500path_test.py\n\u2502   \u2514\u2500\u2500\u2500viz.py\n\u251c\u2500\u2500\u2500params.yaml\n</code></pre>"},{"location":"lab-robots/pupper/software-overview/#components-overview","title":"Components Overview","text":""},{"location":"lab-robots/pupper/software-overview/#models","title":"Models","text":"<ul> <li>The models can be found at <code>/models/</code></li> <li>The obstacle, agent, and goal models are of type Shape which can be found within  <code>src/boundary_detection.py</code></li> <li>The parameters of the models can be found within <code>params.yaml</code> </li> <li>The goal model is a shape containing only a single point </li> <li>The obstacle and agent models are defined by 4 corner points which are then interpolated to create a series of points defining the boundary of the model</li> <li>Each model also has a center as well which would be it's relative location to the Pupper </li> </ul>"},{"location":"lab-robots/pupper/software-overview/#computer-vision","title":"Computer Vision","text":"<ul> <li>The computer vision module can be found at <code>/src/fiducial_vision.py</code></li> <li>The fiducial and computer vision package requires numerous parameters to work correctly, these include fiducial tag size, lens size, and center pixel of the image</li> <li>The module itself contains the <code>Vision</code> class which  contains a method <code>capture_continuous</code> which returns a generator which yields the results of the fiducial detection module on frames of the RaspberryPi camera</li> </ul>"},{"location":"lab-robots/pupper/software-overview/#boundary-generation-and-navigation","title":"Boundary Generation and Navigation","text":"<ul> <li>The modules relevant to boundary generation and planning are <code>src/boundary_detection.py</code>, <code>src/path_finder.py</code>, <code>/src/path_profiler.py</code>, and <code>path_test.py</code></li> <li>Boundary generation works by taking in the detected positions and rotations of fiducials and then creating <code>obstacle</code> classes to represent each fiducial. Then each <code>obstacle.points</code> of type <code>Point[]</code> are transformed to it's corresponding fiducials location by the <code>src/transform.py</code> class. Then The Pupper robot model <code>models/agent.py</code> which is at <code>(0, 0)</code> and each <code>obstacle</code> are used to calculate the configuration space for robot using the Minkowski Sum</li> <li>Also, the <code>models/goal.py</code>class is used to represent the goal of the Pupper which corresponds to the fiducial with <code>id = 0</code> </li> <li>Each point in the resulting configuration space is used to generate a graph of the area where vertices of the graph close to the points in the configuration space are removed so that when a shortest path search is performed the resulting path only includes valid positions from the robot's current position to the goal </li> <li>Finally, the path is interpolated and converted into an array of distances to travel and at what angle it should travel at, which is then converted into command interfaces commands based on the velocity of the robot </li> </ul>"},{"location":"lab-robots/pupper/software-overview/#visualization","title":"Visualization","text":"<ul> <li>The visualization class in <code>src/viz.py</code> uses <code>matplotlib</code> to generate a plot of the agent model, obstacles, obstacle boundaries, position of the goal, graph nodes, and the path of the robot</li> </ul>"},{"location":"lab-robots/pupper/software-overview/#main","title":"Main","text":"<ul> <li>The program can be run simply by navigating to the root of the repository and then running <code>python3 main.py</code> </li> </ul>"},{"location":"lab-robots/pupper/software-setup/","title":"Software Setup","text":""},{"location":"lab-robots/pupper/software-setup/#pupper-base-software-setup","title":"Pupper Base Software Setup","text":"<p>Instructions for the base software setup can be found in the doc. See notes section for project-specific instructions.</p>"},{"location":"lab-robots/pupper/software-setup/#notes-on-the-setup-doc","title":"Notes on the setup doc","text":"<ul> <li>3.c: <code>en1</code> might not exist on the Pi. To find out which network interface to use run <code>sudo ifconfig</code> and look at the first interface it lists. For this project, it was <code>eth0</code>.</li> <li>4.d.ii: Although it says <code>Mac/linux</code>, this is not always the case. If <code>ls /dev | grep tty.usbmodem</code> shows no results try <code>ls /dev | grep ttyACM</code>. For this project, it was <code>ttyACM0</code>.</li> </ul>"},{"location":"lab-robots/pupper/software-setup/#running-base-software","title":"Running base software","text":"<p>Follow instructions here. The keyboard program mentioned can be found here.</p>"},{"location":"lab-robots/pupper/software-setup/#setting-up-this-project","title":"Setting up this project","text":"<p>Install the dependencies using pip:</p>"},{"location":"lab-robots/pupper/software-setup/#dependencies","title":"Dependencies","text":"<ul> <li><code>picamera</code></li> <li><code>pupil_apriltags</code></li> <li><code>tqdm</code></li> <li><code>scipy</code></li> <li><code>UDPComms</code></li> <li><code>pyyaml</code></li> <li><code>opencv</code> or <code>cv2</code></li> <li><code>argparse</code></li> <li><code>pickle</code></li> <li><code>matplotlib</code></li> </ul>"},{"location":"lab-robots/pupper/software-setup/#running","title":"Running","text":"<p>The full software can be run using <code>python main.py</code>, or the controller can be run separately using <code>python3 src/controller.py</code>.</p>"},{"location":"lab-robots/pupper/testing/","title":"Testing","text":""},{"location":"lab-robots/pupper/testing/#setup","title":"Setup","text":"<ul> <li>We tested with 16 printed <code>0.03 meter</code> fiducials and a floor sign to hold the goal fiducial</li> </ul>"},{"location":"lab-robots/pupper/testing/#requirements","title":"Requirements","text":"<ul> <li>A Pupper 2.0 robot flashed with the low level Teensey software for the motor controllers</li> <li>A successful calibration and correctly configured motor IDs </li> <li>The RaspberryPi and RaspberryPi camera running with the latest software </li> </ul>"},{"location":"lab-robots/pupper/testing/#running","title":"Running","text":"<ul> <li>Running the test is simple, once you have the calibrated pupper robot operating you can then run <code>python3 /src/main.py</code> </li> </ul>"},{"location":"lab-robots/pupper/testing/#unit-testing","title":"Unit Testing","text":"<ul> <li>A test for the boundary generation, discretization, and graph search can be run through <code>python3 /src/path_test.py</code></li> <li>This opens a <code>pickled Environment</code> object containing obstacle, agent, and goal data from a testing session and then performs a graph search using the <code>PathFinder</code> class </li> </ul>"},{"location":"lab-robots/tb3/Turtlebot3s/","title":"Setting up a new Operating System on Turtlebot3","text":"<p>Download Image: ROS Noetic Ubuntu 20.04</p>"},{"location":"lab-robots/tb3/Turtlebot3s/#instructions","title":"Instructions:","text":"<ol> <li> <p>Flash the image into a microsd card using an imager (dd for linux, or RPiImager).</p> </li> <li> <p>Connect the Turtlebot to a monitor and keyboard, insert the microsd card and turn on the bot.</p> </li> <li> <p>Log into the turtlebot with the username: ubuntu and the password: ROSlab134</p> </li> <li> <p>You will want to remove the tailscale information using the following commands:     <pre>\n    sudo apt-get remove tailscale\n    sudo rm /var/lib/tailscale/tailscaled.state\n    sudo nano /etc/hostname \n    # change the hostname in this file from \"roba\" to your robot's name\n    sudo reboot now\n    </pre></p> </li> <li> <p>Once the turtlebot is rebooted, change the hostname and reinstall tailscale:     <pre>\n    sudo apt-get install tailscale\n    sudo tailscale up --authkey=ask-pito-for-code\n    </pre></p> </li> <li>Now update the OpenCR board with the following commands:     <pre>\n    export OPENCR_PORT=/dev/ttyACM0\n    export OPENCR_MODEL=burger_noetic # or waffle_noetic if you have a waffle tb3\n    rm -rf ./opencr_update.tar.bz2\n    wget https://github.com/ROBOTIS-GIT/OpenCR-Binaries/raw/master/turtlebot3/ROS1/latest/opencr_update.tar.bz2 \n    tar -xvf opencr_update.tar.bz2 \n    cd ./opencr_update\n    ./update.sh $OPENCR_PORT $OPENCR_MODEL.opencr\n    </pre></li> <li>Once the OpenCR board is updated, shut down your bot and turn it back on and you are done with the setup.</li> </ol>"},{"location":"lab-robots/tb3/home/","title":"Turtlebot3","text":"<p>Note</p> <p>Coming soon. </p>"},{"location":"networking/SUMMARY/","title":"Networking Home","text":"<p>Here we collect under one roof everything we know about setting up and operating the network in the map, with robots, and other general tips.</p>"},{"location":"networking/dns/","title":"DNS Problems","text":"<p>Sometimes, you can connect to your robot over ssh, so you know that the wifi is working correctly. And yet, git push and other commands are failing. This might be a problem DNS lookups.</p>"},{"location":"networking/dns/#checking","title":"Checking","text":"<p>There are many ways to check. One that checks specifically DNS is: <pre><code>nslookup google.com\n</code></pre> If this command hangs and times out with an error, this is good evidence that you have a problem with the DNS service.</p> <p>Next check if the DNS service is working on your robot: <pre><code>systemd-resolve --status\n</code></pre> If that gives an error, then for some reason your DNS service has stopped. Try to restart it like this:</p>"},{"location":"networking/dns/#restarting-the-service","title":"Restarting the service","text":"<pre><code>sudo systemctl restart systemd-resolved\n</code></pre>"},{"location":"networking/dns/#verifying","title":"Verifying","text":"<p>If things are working you will see output like: </p> <p><pre><code>nslookup google.com # (1)\nServer:         127.0.0.53\nAddress:        127.0.0.53#53\n\nNon-authoritative answer:\nName:   google.com\nAddress: 142.250.80.78\nName:   google.com\nAddress: 2607:f8b0:4006:80c::200e\n</code></pre> 1: foobar</p>"},{"location":"networking/hostname/","title":"Hostnames","text":"<p>All computers (<code>hosts</code>) on the network have (or can have) a hostname. This is nothing more that what you might think of as the name of the computer. In the case of our robots, each robot has a hostname, e.g. <code>platform3</code>.</p>"},{"location":"networking/hostname/#setting-hostnames","title":"Setting hostnames","text":"<p>To change the hostname on an Ubuntu system, you need to modify it in several places. Here are the steps to change the hostname permanently:</p> <ol> <li> <p>Edit the <code>/etc/hostname</code> file:    <pre><code>sudo nano /etc/hostname\n</code></pre>    Replace the current hostname with the new one you want.</p> </li> <li> <p>Update the <code>/etc/hosts</code> file:    <pre><code>sudo nano /etc/hosts\n</code></pre>    Find the line with the old hostname and update it to the new hostname.</p> </li> <li> <p>Use the <code>hostnamectl</code> command to set the new hostname:    <pre><code>sudo hostnamectl set-hostname new-hostname\n</code></pre>    Replace <code>new-hostname</code> with your desired hostname.</p> </li> <li> <p>To prevent cloud services from resetting the hostname, you may need to modify cloud-init configuration:    <pre><code>sudo nano /etc/cloud/cloud.cfg\n</code></pre>    Find the line <code>preserve_hostname: false</code> and change it to <code>preserve_hostname: true</code>.</p> </li> <li> <p>Reboot your system to apply all changes:    <pre><code>sudo reboot\n</code></pre></p> </li> </ol>"},{"location":"networking/private-networking/","title":"Tailscale VPN","text":"<pre><code>  100.89.2.122 cluster-1 \\       / robot-1 100.89.2.122\n                          \\     /\n  100.99.32.12 cluster-2 - - - - - robot-2 100.99.31.234\n                           /    \\\n  100.88.77.234 cluster-3 /      \\ robot-3 100.86.232.111\n</code></pre>"},{"location":"networking/private-networking/#introduction","title":"Introduction","text":"<p>All our robots and cluster accounts have a common VPN enabled. In fact any other computers, like your laptop, can be added to the VPN. The effect of this is that they all will be on the same IP name space, that is, they can all communicate with each other via the IP. In fact the effect of enabling the VPN (Tailscale) is that you will see an exctra \"virtual\" network adapter when you type for example <code>ip addr</code>. That network adapter is created by tailscale.</p>"},{"location":"networking/private-networking/#tools","title":"Tools","text":""},{"location":"networking/private-networking/#tson","title":"tson","text":"<p>To get a list of all the devices on our tailscale VPN you can use the <code>tson</code> alias.</p> <pre><code>$ tson\ntest3 - 100.73.71.98\nezimmerman - 100.83.124.19\nhello.ts.net - 100.101.102.103\nrpsalas - 100.73.134.92\notproject.taila146c.ts.net - 100.117.130.99\nsuperset.taila146c.ts.net - 100.127.74.101\nleejs8128 - 100.112.15.61\nplat2.taila49c0.ts.net - 100.100.240.69\n</code></pre>"},{"location":"networking/private-networking/#tsoff","title":"tsoff","text":"<p>To get a list of all the devices sometimes are on our tailscale VPN but are not right now you can use the <code>tsoff</code> alias.</p>"},{"location":"networking/private-networking/#ip-addr","title":"ip addr","text":"<p>To see all the IP addresses associated with this computer, The output will include one entry for each network adapter. The ones that are currently connected to the network will have an ip address. </p> <pre><code>$ ip addr\n1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host\n       valid_lft forever preferred_lft forever\n2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP group default qlen 1000\n    link/ether dc:a6:32:20:ac:14 brd ff:ff:ff:ff:ff:ff\n    inet 192.168.5.51/22 brd 192.168.7.255 scope global dynamic eth0\n       valid_lft 14124sec preferred_lft 14124sec\n    inet6 2600:4040:52f4:5c00:dea6:32ff:fe20:ac14/64 scope global dynamic mngtmpaddr noprefixroute\n       valid_lft 4561sec preferred_lft 4561sec\n    inet6 fdb7:7ac:6f0f:1:dea6:32ff:fe20:ac14/64 scope global dynamic mngtmpaddr noprefixroute\n       valid_lft 2591726sec preferred_lft 604526sec\n    inet6 fe80::dea6:32ff:fe20:ac14/64 scope link\n       valid_lft forever preferred_lft forever\n3: wlan0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc fq_codel state DOWN group default qlen 1000\n    link/ether dc:a6:32:20:ac:16 brd ff:ff:ff:ff:ff:ff\n4: tailscale0: &lt;POINTOPOINT,MULTICAST,NOARP,UP,LOWER_UP&gt; mtu 1280 qdisc fq_codel state UNKNOWN group default qlen 500\n    link/none\n    inet 100.100.240.69/32 scope global tailscale0\n       valid_lft forever preferred_lft forever\n    inet6 fd7a:115c:a1e0::7264:f045/128 scope global\n       valid_lft forever preferred_lft forever\n    inet6 fe80::a3c7:d2ff:ae5:c63b/64 scope link stable-privacy\n       valid_lft forever preferred_lft forever\n</code></pre>"},{"location":"networking/private-networking/#setup","title":"Setup","text":"<ol> <li>Make sure ~/rosutils is up to date by performing a git pull</li> <li>Get a tailscale key from Pito if needed</li> <li>`sudo ~/rosutils/pi_connect.sh [put ts key here]</li> <li>On successful connection: <code>Connected. IP address: 100.xx.xxx.xxx</code></li> </ol>"},{"location":"networking/ssh-guide/","title":"Note","text":"<p>All our lab robots, as of now, have SSH set up. This is for new robots.</p>"},{"location":"networking/ssh-guide/#setting-up-ssh-on-a-new-robot","title":"setting up ssh on a new robot","text":"<p>By default, ubuntu MATE comes with ssh installed but disabled. So there are a few steps to setup ssh to your raspberry pi.</p> <ol> <li>ensure that <code>openssh-server</code> is installed with <code>sudo apt install openssh-server</code></li> <li>check ssh status with <code>sudo systemctl status ssh</code></li> <li>if it is <code>inactive</code>, you can start it with <code>sudo systemctl start ssh</code>, and/or automatically start ssh on boot with <code>sudo systemctl enable ssh</code></li> <li>go to settings -&gt; login window and set automatic login to your robot's username.</li> </ol> <p>For more information, here is a useful site that explains it more</p>"},{"location":"networking/tailscale/","title":"Tailscale VPN","text":"<p>We use a product called tailscale to create a vpn that allows our robots (which are on wifi) to be connected to our Rover cluster (which are connected to the network.) </p>"},{"location":"networking/tailscale/#problem-two-robots-have-the-same-ip-or-have-the-same-or-wrong-name","title":"Problem: Two Robots have the same IP or have the same or wrong name","text":"<p>When this occurs you need to fully uninstall Tailscale from one of the robots. This is the sequence:</p> <pre><code>sudo systemctl stop tailscaled\nsudo rm /var/lib/tailscale/tailscaled.state\nsudo systemctl start tailscaled\ntailscale up\n</code></pre>"},{"location":"networking/ubuntubrandeis/","title":"Ubuntu Brandeis Tips","text":""},{"location":"networking/ubuntubrandeis/#setting-up-eduroam","title":"Setting Up Eduroam","text":""},{"location":"networking/NetworkManager/full-nm-uninstall/","title":"Full NetworkManager Uninstall","text":"<p>To completely uninstall NetworkManager and switch to using Netplan instead, follow these steps:</p> <ol> <li>Stop and disable NetworkManager:</li> </ol> <pre><code>sudo systemctl stop NetworkManager\nsudo systemctl disable NetworkManager\n</code></pre> <ol> <li>Uninstall NetworkManager and related packages:</li> </ol> <pre><code>sudo apt purge network-manager network-manager-gnome network-manager-pptp network-manager-openvpn\n</code></pre> <ol> <li>Remove any leftover configuration files:</li> </ol> <pre><code>sudo rm -rf /etc/NetworkManager\n</code></pre> <ol> <li>Update the package list:</li> </ol> <pre><code>sudo apt update\n</code></pre> <ol> <li>Install netplan if it's not already installed:</li> </ol> <pre><code>sudo apt install netplan.io\n</code></pre> <ol> <li>Create a new Netplan configuration file:</li> </ol> <pre><code>sudo nano /etc/netplan/01-netcfg.yaml\n</code></pre> <ol> <li>Add your network configuration to the file. THis is the one we use\u00ab\u00ab\u00ab</li> </ol> <p>```yaml</p> <p>network:     ethernets:         eth0:             dhcp4: true             optional: true     version: 2     wifis:         wlan0:             access-points:                 eduroam:                     auth:                         identity: \"robotics@brandeis.edu\"                         key-management: \"eap\"                         method: \"peap\"                         password: \"bapor-kibra\"                         phase2-auth: \"MSCHAPV2\"                 \"one boston\":                     password: \"ffabcd4444\"             dhcp4: true             optional: true</p> <p>```</p> <ol> <li> <p>Apply the netplan configuration:    <pre><code>sudo netplan apply\n</code></pre></p> </li> <li> <p>Reboot your system to ensure all changes take effect:</p> </li> </ol> <pre><code>sudo reboot\n</code></pre> <p>After these steps, NetworkManager will be completely uninstalled and your system will be using netplan for network configuration.</p>"},{"location":"networking/NetworkManager/home/","title":"Don't Use NetworkManager","text":""},{"location":"networking/NetworkManager/home/#networkmanager-and-networkd","title":"NetworkManager and networkd","text":"<p>For historical reasons there are two different network management subsistems on Ubuntu. If you try to use both at the same time, bad things happen. </p>"},{"location":"networking/NetworkManager/home/#dont-use-networkmanager","title":"Don't Use NetworkManager","text":"<p>Because of that we don't use NetworkManager and nmcli on any of our robots. Don't install it or set it up. Some of our older robots may still be running NetworkManager. In that case leave it alone and let a Lab technician sort it out.</p>"},{"location":"networking/NetworkManager/home/#how-to-uninstall-networkmanager","title":"How to Uninstall NetworkManager","text":"<p>For instructions on how to uninstall NetworkManager, please refer to our NetworkManager Uninstallation Guide. This guide provides step-by-step instructions for safely removing NetworkManager from your system without disrupting your network connectivity.</p>"},{"location":"networking/NetworkManager/linux_terminal_eduroam_setup/","title":"Configuring Wifi for Eduroam","text":""},{"location":"networking/NetworkManager/linux_terminal_eduroam_setup/#setting-up-an-eduroam-connection-via-command-line-interface-on-linux","title":"Setting up an Eduroam connection via Command Line Interface on Linux","text":"<p>Often in robotics, you will find that it is necessary to configure wifi through a terminal interface rather than a GUI. This is often due to the fact that many on-board operating systems for robots lack a desktop environment, and the only way to interface with them is through a terminal.</p> <p>The task of connecting to a wireless network can be especially difficult when configuring Eduroam which requires a more involved setup process. The following method for connecting to Eduroam has been tested on a barebones version of Ubuntu 20.04.</p>"},{"location":"networking/NetworkManager/linux_terminal_eduroam_setup/#connection-to-eduroam","title":"Connection to Eduroam","text":"<p>Run the following command to get the names of your wireless devices.</p> <ul> <li><code>ip link show</code></li> </ul> <p>Running this command will list all of your networking devices. You will want to note the name of your wireless networking device, for this tutorial I will assume the wireless device's name will be <code>wlan0</code> as it is named on the Raspberry Pi 3b+, however you will want to substitute this for the name of your wireless device if your's differs.</p> <p>Next you will run the following commands to connect your machine to eduroam.</p> Setting up eduroam<pre><code>sudo nmcli con add type wifi con-name \"eduroam\" ifname wlan0 ssid \"eduroam\" wifi-sec.key-mgmt wpa-eap 802-1x.identity \"exampleemail@brandeis.edu\" 802-1x.password \"examplepassword123\" 802-1x.system-ca-certs yes 802-1x.eap \"peap\" 802-1x.phase2-auth mschapv2\nnmcli connection up eduroam --ask\n</code></pre> <p>You may then be prompted to enter in the wifi username and password, however the fields should already be filled in and you will just need to press enter.</p>"},{"location":"networking/NetworkManager/linux_terminal_eduroam_setup/#troubleshooting","title":"Troubleshooting","text":"<ol> <li> <p>If you are trying to connect a robot and it is failing, sometimes it's good to try it on the wired network. It should just work. Brandeis networking requires that the MAC address of special devices (like raspberry Pis) be recorded. This is only for the wired network. To do this use this link: : https://www.brandeis.edu/its/services/network-connectivity/netreg.html . It is extremely slow so be patient. When you click the netreg button it is very very slow (minutes) but eventually it allows you to add a mac adress to the list.</p> </li> <li> <p>Sometimes despite checking everything, the wireless (eduroam) network refuses to connect. One important detail that has caused problems is the system time on the Rasberry Pi. It has to be correct or close to it. Apparently that's part of the authentication. I think the command is <code>date -s \"19 APR 2012 11:14:00\"</code> and that sets the UTC time.\u00df</p> </li> </ol>"},{"location":"networking/NetworkManager/linux_terminal_eduroam_setup/#new-attempt","title":"New Attempt","text":"<p>```bash title=\"Adding eduroam in steps\" sudo nmcli conn add type wifi con-name eduroam ssid eduroam sudo nmcli conn mod eduroam ifname wlan0 sudo nmcli conn mod eduroam ipv4.method auto 802-1x.eap peap 802-1x.phase2-auth mschapv2 802-1x.identity \"robotics@brandeis.edu\" sudo nmcli conn mod eduroam 802-1x.password \"x\"</p>"},{"location":"networking/NetworkManager/nmcli/","title":"Network Manager and NMCLI","text":"<p>Important</p> <p>We don't use nmcli on the Brandeis Robots. This is informational only Here is how to uninstall NetworkManager:</p>"},{"location":"networking/NetworkManager/nmcli/#using-nmcli","title":"Using NMCLI","text":"<p><code>nmcli</code> is a command line tool used for networking on Linux machines. It is typically installed by default on Ubuntu Linux, so you should not need to connect your machine through   a wired connection and install it before using it. However, if <code>nmcli</code> is not installed, follow the installation instructions.</p>"},{"location":"networking/NetworkManager/nmcli/#installation-if-needed","title":"Installation (If needed)","text":"<p>Connect your machine through a wired connection (e.g. To a router/wall connection through an ethernet port) and the following commands to install <code>nmcli</code> and configure it to start upon bootup of your machine.</p> <p>!!! Note:       If it doesn't work even with a connection, and you are on campus, send an email to help@brandeis.edu noting the mac address of the <code>eth0</code> device (using <code>ip link show</code>) and the fact that you are trying to connect. Ask them if they see any activity at that particular mac address.</p> Install and run network manager<pre><code>sudo apt-get update\nsudo apt install network-manager\nsystemctl start NetworkManager.service\nsystemctl enable NetworkManager.service\n</code></pre> Make sure networkd is disabled<pre><code>sudo systemctl stop systemd-networkd.service\nsudo systemctl disable systemd-networkd.service\n# more needed, have to look them up.\n</code></pre> <p>Note</p> <p>I am not an expert in ubuntu networking! There are two different and sort of compatible/incompatible network management stacks, known sometimesa as NeworkManager and networkd. I have found that they fight with each other. I try to make sure that networkd is totally turned off (which is suprisingly difficult!) Problems arise because there are two or three network management schemes on Ubuntu. There's <code>networkd</code> and there's <code>network-manager</code>. And they interact in very obscure ways. My current model (which remains to be proven) is to try to disable fully networkd and use only network-manager.</p>"},{"location":"networking/NetworkManager/nmcli/#cheat-sheet","title":"Cheat Sheet","text":"nmcli cheat sheet<pre><code>nmcli                                                           # is the cli for network-manager.\nnmcli general                                                   # Summary status\nnmcli device wifi connect \"ssid\" password \"password\"            # Set up a simple wifi connection\nnmcli -t -f active,ssid dev wifi                                # to find out what SSID I am connected over wifi\nsudo nmcli dev wifi                                             # list all wifi SSIDs visible\nnmcli connection show                                           # to show all connections that nmcli knows about\nnmcli connection show &lt;connection-name&gt;`                        # View specific details of a connection, including its autoconnect priority\nnmcli conn del &lt;UUID&gt;                                           # Delete a connection\nnmcli conn mod &lt;current-name&gt; connection.id &lt;new-name&gt;          # Changing the name of a connection:\nnmcli -f NAME,UUID,TYPE,DEVICE,STATE con show                   # Showing a list of connections with specific properties\nnmcli connection modify &lt;connection-name&gt; \\\n        connection.autoconnect yes                              # Change autoconnect property\nnmcli connection modify &lt;connection-name&gt; \\\n        connection.autoconnect-priority &lt;priority-value&gt;        # Set priority of a connection\n        # &lt;connection-name&gt;: The name of the connection profile you want to modify.\n        # &lt;priority-value&gt;: An integer value representing the priority. Higher values have higher priority.\nnmtui                                                           # for a textui to nmcli (networkmanager)\n</code></pre>"},{"location":"obsolete/ros-melodic/","title":"ROS Melodic","text":""},{"location":"obsolete/ros-melodic/#gen-4-going-to-ubuntu-1804","title":"Gen 4 - Going to Ubuntu 18.04","text":"<ul> <li>We are planning to move students' and teachers' computers to Ubuntu 18.04</li> <li>It has a nicer to use Desktop and is also the currently supported Ubuntu</li> <li>It supports ROS Melodic (and ROS Melodic does not support the old Ubuntu)</li> </ul>"},{"location":"obsolete/ros-melodic/#installation-notes","title":"Installation notes","text":"<ul> <li>Follow Ubuntu's own installation instructions to install a clean copy of Ubuntu</li> <li>As Robotis is still on Ubuntu 16.04 and ROS Kinetic we have to amend their instructions a little</li> <li>Our baseline is http://emanual.robotis.com/docs/en/platform/turtlebot3/pc_setup/</li> </ul>"},{"location":"obsolete/ros-melodic/#commands","title":"Commands","text":""},{"location":"obsolete/ros-melodic/#core-install-of-melodic","title":"Core install of Melodic","text":"<pre><code>sudo apt-get update\nsudo apt-get upgrade\nwget https://raw.githubusercontent.com/ROBOTIS-GIT/robotis_tools/master/install_ros_melodic.sh &amp;&amp; chmod 755 ./install_ros_melodic.sh &amp;&amp; bash ./install_ros_melodic.sh\n</code></pre>"},{"location":"obsolete/ros-melodic/#further-install-of-needed-melodic-packages","title":"Further install of needed melodic packages","text":"<pre><code>sudo apt-get install ros-melodic-joy ros-melodic-teleop-twist-joy ros-melodic-teleop-twist-keyboard ros-melodic-laser-proc ros-melodic-rgbd-launch ros-melodic-depthimage-to-laserscan ros-melodic-rosserial-arduino ros-melodic-rosserial-python ros-melodic-rosserial-server ros-melodic-rosserial-client ros-melodic-rosserial-msgs ros-melodic-amcl ros-melodic-map-server ros-melodic-move-base ros-melodic-urdf ros-melodic-xacro ros-melodic-compressed-image-transport ros-melodic-rqt-image-view ros-melodic-gmapping ros-melodic-navigation ros-melodic-interactive-markers ros-melodic-turtle-tf2 ros-melodic-tf2-tools ros-melodic-tf\n</code></pre>"},{"location":"obsolete/ros-melodic/#further-install-of-ros-packages","title":"Further install of ROS Packages","text":"<pre><code>cd ~/catkin_ws/src/\n\n# Robotis\ngit clone https://github.com/ROBOTIS-GIT/turtlebot3_msgs.git\ngit clone https://github.com/ROBOTIS-GIT/turtlebot3.git\ngit clone https://github.com/ROBOTIS-GIT/turtlebot3_simulations.git\n\n# Cosi119a PRR Samples\ngit clone https://github.com/campusrover/prrexamples.git\nexit\n</code></pre>"},{"location":"obsolete/ros-melodic/#important-open-a-new-shell","title":"Important: OPEN A NEW SHELL","text":"<pre><code>cd ~/catkin_ws &amp;&amp; catkin_make\n</code></pre>"},{"location":"packages/SUMMARY/","title":"SUMMARY","text":"<ul> <li>Behavior Trees</li> <li>Speech Recognition:<ul> <li>Speech Recognition</li> </ul> </li> </ul>"},{"location":"packages/home/","title":"Packages","text":"<p>Here you will find ROS and other packages we developed as part of the course. Before we add the software to this collection, it should be well documeted and reasonably usable by a future student.</p>"},{"location":"packages/speech-recognition_report/","title":"Speech Recognition","text":""},{"location":"packages/speech-recognition_report/#github","title":"Github","text":""},{"location":"packages/speech-recognition_report/#veronika-belkina-vbelkinabrandeisedu","title":"Veronika Belkina (vbelkina@brandeis.edu)","text":""},{"location":"packages/speech-recognition_report/#spring-2023","title":"Spring 2023","text":""},{"location":"packages/speech-recognition_report/#description","title":"Description","text":"<p>This project aims to utilize Python's speech recognition library to listen to user commands through a microphone. The recognized speech is then sent to another node where it is translated into dynamic messages using json, which are sent to a robot via roslibpy. The robot will execute the incoming commands based on the received messages. This program can be run locally on the robot, if you wish. It can also be run remotely from your computer. </p>"},{"location":"packages/speech-recognition_report/#links","title":"Links","text":"<ul> <li>Bluetooth</li> <li>Rosbridge</li> </ul>"},{"location":"packages/speech-recognition_report/#installation","title":"Installation","text":"<p>These instructions assume that you have ROS NOETIC installed. This has not been tested on any other distro. To install them, first git clone this package into your catkin_ws and then run: </p> <pre><code>git clone https://github.com/vbelkina/whisper_4.git\npip install -r requirements.txt\nsudo apt install ros-noetic-rosbridge-server\n</code></pre> <p>core:</p> <ul> <li>listen.py<ul> <li>Using Python's speech-to-text package, you can listen to commands from the user through a microphone and then send the message to the designated topic (/whisper/command).</li> </ul> </li> <li>execute.py<ul> <li>By listening to the specified topic (/whisper/command), incoming commands can be executed. This process involves the use of roslibpy and json, which allow for the dynamic publishing of messages to the robot.</li> </ul> </li> <li>commands.json<ul> <li>a json file with the format shown below where <pre><code>\"command_1\": {\n    \"command_2\": { \n        \"receiver\": \"/cmd_vel\",\n        \"type\": \"geometry_msgs/Twist\",\n        \"msg\" : {\n            \"linear\": {\n                \"x\": 0.2,\n                \"y\": 0.0,\n                \"z\": 0.0\n            },\n            \"angular\": {\n                \"x\": 0.0,\n                \"y\": 0.0,\n                \"z\": 0.0\n            }\n        }\n    }\n- *command_1* and *command_2* are the commands for certain actions for the robot to execute- for example, in \"go back\", command_1 = \"go\" and command_2 = \"back\"\n- *receiver* is the Topic to be published to \n- *type* is the Message type for the topic\n- *msg* is the message to be sent, formatted in the same structure as the message is seen in ROS documention.\n</code></pre> </li> </ul> </li> </ul> <p>misc: </p> <ul> <li>find_mic.py<ul> <li>determine the device index of the microphone you want to use and see if there are any errors. </li> </ul> </li> </ul>"},{"location":"packages/speech-recognition_report/#run","title":"Run","text":"<p>To run this project in one terminal (could be on robot or remote computer, but robot works fine)</p> <pre><code>roslaunch whisper_4 command.launch device_index:=0\n</code></pre> <p>Or to run the files in separate terminals (same comment)</p> <pre><code>roslaunch rosbridge_server rosbridge_websocket.launch\nrosrun whisper_4 execute.py\nrosrun whisper_4 listen.py\n</code></pre>"},{"location":"packages/speech-recognition_report/#connecting-to-a-microphone","title":"Connecting to a microphone","text":"<p>Any microphone should work, even the built in microphone on your laptop. If you are running it on a linux machine, then it shouldn't be a problem as you can access the device indexes and see what they are using the find_mic.py file which should list all available microphone devices.  I have not tested what happens on a Mac or Windows, however, my guess is that if you leave the device index to be 0, then it should choose the default microphone and speaker. </p>"},{"location":"packages/speech-recognition_report/#known-errors","title":"Known Errors","text":"<ul> <li> <p><code>malloc(): mismatching next-&gt;prev_size (unsorted)</code> </p> <ul> <li>This error can occur when you select a device index for your microphone that is recognized as a microphone, but is not functioning properly. If you attempt to record sound using this microphone, you will encounter this error. I'm not sure how to catch this because it occurs after the microphone is already connected and has something to do with memory allocation. </li> </ul> </li> <li> <p>small pause between publishing of messages</p> <ul> <li>This 'error' occurs when publishing the same message over and over and there will be a slight pause between each message so the robot will pause for a second. I'm unable to fix this in a way that I am happy with at the moment. </li> </ul> </li> <li> <p><code>ALSA library warnings</code> </p> <ul> <li>I added some error supressors, however, it still shows up sometimes after initializing the program and then it shouldn't show up again. It doesn't affect the functionality but it looks a little ugly. </li> </ul> </li> </ul>"},{"location":"packages/speech-recognition_report/#story","title":"Story","text":""},{"location":"packages/speech-recognition_report/#speech-recognition-option-exploration","title":"Speech recognition option exploration","text":"<p>When starting this project, I explored several options for speech recoginition. The goal was to have something that can run locally on the robot so it had to be lightweight. It also had to be accurate and fast. Initially, I looked at Pocketsphinx based speech recognition which was recommended in several different places as a lightweight speech recognition program that worked well. However, after some exploration, I struggled to get accurate results from pocketsphinx, even after a local dictionary. </p> <p>After doing some more research, I found that python has its own speech recognition module which let's you have access to Google Speech to Text, OpenAI's Whisper, and so on. These worked much better. I ended up deciding on Google's Speech to Text as I found it to be the best and fastest at interpreting data and returning results.  </p>"},{"location":"packages/speech-recognition_report/#bluetooth-struggles","title":"Bluetooth struggles","text":""},{"location":"packages/speech-recognition_report/#_1","title":"Speech Recognition Report","text":"<p>Initially, I wanted to use a bluetooth headset to send commands. However, using bluetooth on linux and raspberry pi is not so simple. There was a lot of struggle to get a headset to connect. For example, different bluetooth headphones with microphones might have different profiles. Originally, I was trying to use the PulseAudio library which was not very simple to use with bluetooth and I played around a lot with changing a variety of settings. I ended up finding a set of instructions and switched to using PipeWire which is a server for handling audio, video streams, and hardware on Linux. This seemed to work better. In the end though, I switched to a USB headset which was much simpler to use as the headset is interpreted as hardware instead of a bluetooth device. </p>"},{"location":"packages/speech-recognition_report/#rosbridge-and-roslibpy","title":"Rosbridge and roslibpy","text":"<p>Throughout the project, I wanted to have a dynamic way to send messages to the robot. It took a while to find a way that worked and it came in the form of using the rosbridge-server and roslibpy. The Rosbridge server created a websocket connection which allows you to pass JSON messages from the websocket to the rosbridge library which then converts the JSON to ROS calls. Roslibpy lets Python interact with ROS without having to have ROS installed. It uses WebSockets to connect to rosbridge 2.0 and provides publishing, subscribing, and other essential ROS functionalities. </p>"},{"location":"packages/speech-recognition_report/#hopes-for-the-future","title":"Hopes for the future","text":"<p>This version of the project is a base with minimal commands and room for improvement. Some ideas to further improve this project is to add more interesting commands. For example, it would be interesting if a command such as \"solve this maze\" was said and that could start a node or send a series of tasks that would connect with algorithms that would then solve a maze. </p> <p>Another idea is to add parameters at the end of commands. For example, when the command \"go forward\" is given, it would good to have some numerical value to specify the speed at which the robot would go. Perhaps, \"go forward speed 20\" would indicate to move forward at 0.2 m/s or \"go foward distance 5\" would indicate to move foward for 5 meters, with some default values set if these specifiers are not mentioned. </p>"},{"location":"packages/behavior-trees/SUMMARY/","title":"SUMMARY","text":"<ul> <li>Summary</li> <li>Nodes</li> <li>*</li> </ul>"},{"location":"packages/behavior-trees/basic_movement/","title":"Basic Movement","text":"<p>The 3 nodes classified as \"basic movement\" nodes are the fundamental building blocks needed in order to make a robotics behavior tree application.</p>"},{"location":"packages/behavior-trees/basic_movement/#linearangularstatic","title":"LinearAngularStatic","text":"<p>When this node is activated, it will publish a cmd_vel message including the linear and angular velocities specified during its construction:</p> <p>Example: <pre><code>{\n    \"name\":\"my_node\",\n    \"type\":\"LinearAngularStatic\",\n    \"lin_vel\":0.5,\n    \"ang_vel\":1\n}\n</code></pre></p>"},{"location":"packages/behavior-trees/basic_movement/#stop","title":"Stop","text":"<p>A special case of LinearAngularStatic; upon activation the Stop node will publish a cmd_vel message with all velocity values set to 0.</p> <p>Example: <pre><code>{\n    \"name\":\"my_node\",\n    \"type\":\"Stop\"\n}\n</code></pre></p>"},{"location":"packages/behavior-trees/basic_movement/#linearangulardynamic","title":"LinearAngularDynamic","text":"<p>This node is similar to LinearAngularStatic how,ever instead of providing velocities during construction, you provide names of blackboard topics which correspond to the linear and angular velocities that the node will publish in cmd_vel when activated. This allows for velocities to be updated by update nodes and then have one LinearAngularDynamic node providing dynamic movements.</p> <p>Example: <pre><code>{\n    \"name\":\"my_node\",\n    \"type\":\"LinearAngularDynamic\",\n    \"linear_var_name\":\"my_linear_vel\",\n    \"angular_var_name\":\"my_angular_vel\",\n    \"blackboard\":{\n        \"my_angular_vel\":null,\n        \"my_linear_vel\":null\n    }\n}\n</code></pre></p>"},{"location":"packages/behavior-trees/nodes/","title":"Nodes","text":"<p>A behavior tree is a tree-shaped data structure consisting of nodes, each of which contain a logical method which executes code. The behavor tree is evaluated recursively starting  at the root node. Each node has the abilility to execute code which will either run a script, or execute all of its children. Each node will also return one of 3 outputs to its  parent: \"success\", \"failure\", or \"running\". There are two main types of nodes: the control-flow (parent nodes) nodes and the leaf nodes.</p>"},{"location":"packages/behavior-trees/nodes/#control-flow-nodes","title":"Control-Flow Nodes","text":"<ul> <li>Selector</li> <li>The Selector executes its children sequentially from left to right. </li> <li>If one of its children returns either \"success\" or \"running\", it will halt execution of its children and it will return the result of the child it stopped on.</li> <li>If all of its children return \"failure\", the Selector will also return \"failure\".</li> <li>Sequencer</li> <li>The Sequencer executes its children sequentially from left to right.</li> <li>The Sequencer will not halt execution of its children unless one of them returns \"failure\" or \"running\", in which case it will also return \"failure\" or \"running\".</li> <li>If all children return \"success\" the Sequencer will return \"success\"</li> <li>Multitasker</li> <li>The Multitasker runs all of its children concurrently, each in a separate thread.</li> <li>The Multitasker will return \"success\" only if all of it's children return \"success\".</li> <li>If any of its children return \"running\" but none return \"failure\", the Multitasker will return \"running\".</li> <li>If any of its children return \"failure\", the Multitasker will return \"failure\".</li> </ul>"},{"location":"packages/behavior-trees/nodes/#leaf-nodes","title":"Leaf Nodes","text":"<ul> <li>Action Nodes</li> <li>Action nodes send ROS topic messages from the behavior tree to the robot.</li> <li>Often the type of message sent from an Action node is a cmd_vel message which encodes movement instructions for the robot.</li> <li>Update Nodes</li> <li>Update nodes are designated for updating data in the blackboard.</li> <li>Often times the types of data updates performed by Update nodes include preprocessing or processing of message data from the robot.</li> <li>Conditional Nodes</li> <li>Conditional nodes will return either \"success\" or \"failure\", corresponding to the boolean values \"true\" and \"false\" respectively</li> <li>Conditional nodes will access data in the blackboard and return one of the two values listed above based on if a particular condition is met within the data.</li> </ul>"},{"location":"packages/behavior-trees/overview/","title":"Behavior Tree Framework for Lab Robots","text":"<p>Github repo for mr_bt: https://github.com/campusrover/mr_bt</p> <p>The goal behind the MRBT project was to create a efficient, modular, and user-friendly solution for programming complex behaviors into the Turtlebot3 robot stack. Our solution came in the form of a popular method in the videogaming industry for programming behaviors into NPCs (Non-Playable Characters in videogames; think enemies in HALO or Bioshock). With the use of behavior trees, we are now able to program complex behaviors using a simple tree definition in a JSON file, and we can partition a behavior into multiple different subtrees which can then be used elsewhere or standalone.</p>"},{"location":"packages/behavior-trees/overview/#installation","title":"Installation","text":"<ul> <li>Requires ROS Noetic, Ubuntu 20.04, and a catkin workspace</li> <li><code>cd</code> into your <code>catkin_ws/src</code> directory</li> <li>Run <code>git clone https://github.com/campusrover/mr_bt.git</code></li> <li>Run <code>cd ../ &amp;&amp; catkin_make</code></li> </ul>"},{"location":"packages/behavior-trees/overview/#usage","title":"Usage","text":"<ul> <li>To run one of the example behavior trees you must use a Turtlebot3 robot connected to ROS.</li> <li>Run the example <code>roslaunch mr_bt btree.launch tree:=move_to_position</code></li> <li>If you want to run any example in the folder <code>mr_bt/src/tree_jsons/</code>, pass in the name of the example folder containing a <code>root.json</code> as the <code>tree</code> argument for the launch script.</li> </ul>"},{"location":"packages/behavior-trees/overview/#key-components-of-the-behavior-tree","title":"Key Components of the Behavior Tree","text":"<p>Any complex behavior tree can be broken down into a few key components which highlight the overall logical structure of how they operate.</p>"},{"location":"packages/behavior-trees/overview/#blackboard","title":"Blackboard","text":"<p>The Blackboard is the main storage component for data accessible to nodes in the behavior tree. It is stored as a Python dictionary and its reference is passed down to each node in the behavior tree from the root node. The effect of this is that each node in the tree is able to change and share data through the Blackboard. Additionally, data that is sent from the sensors on the robot, including camera data, lidar data, etc, is accessable from each node in the tree.</p>"},{"location":"packages/behavior-trees/overview/#nodes","title":"Nodes","text":"<p>A behavior tree is a tree-shaped data structure consisting of nodes, each of which contain a logical method which executes code. The behavor tree is evaluated recursively starting  at the root node. Each node has the abilility to execute code which will either run a script, or execute all of its children. Each node will also return one of 3 outputs to its  parent: \"success\", \"failure\", or \"running\". There are two main types of nodes: the control-flow (parent nodes) nodes and the leaf nodes.</p>"},{"location":"packages/behavior-trees/overview/#control-flow-nodes","title":"Control-Flow Nodes","text":"<ul> <li>Selector</li> <li>The Selector executes its children sequentially from left to right. </li> <li>If one of its children returns either \"success\" or \"running\", it will halt execution of its children and it will return the result of the child it stopped on.</li> <li>If all of its children return \"failure\", the Selector will also return \"failure\".</li> <li>Sequencer</li> <li>The Sequencer executes its children sequentially from left to right.</li> <li>The Sequencer will not halt execution of its children unless one of them returns \"failure\" or \"running\", in which case it will also return \"failure\" or \"running\".</li> <li>If all children return \"success\" the Sequencer will return \"success\"</li> <li>Multitasker</li> <li>The Multitasker runs all of its children concurrently, each in a separate thread.</li> <li>The Multitasker will return \"success\" only if all of it's children return \"success\".</li> <li>If any of its children return \"running\" but none return \"failure\", the Multitasker will return \"running\".</li> <li>If any of its children return \"failure\", the Multitasker will return \"failure\".</li> </ul>"},{"location":"packages/behavior-trees/overview/#leaf-nodes","title":"Leaf Nodes","text":"<ul> <li>Action Nodes</li> <li>Action nodes send ROS topic messages from the behavior tree to the robot.</li> <li>Often the type of message sent from an Action node is a cmd_vel message which encodes movement instructions for the robot.</li> <li>Update Nodes</li> <li>Update nodes are designated for updating data in the blackboard.</li> <li>Often times the types of data updates performed by Update nodes include preprocessing or processing of message data from the robot.</li> <li>Conditional Nodes</li> <li>Conditional nodes will return either \"success\" or \"failure\", corresponding to the boolean values \"true\" and \"false\" respectively</li> <li>Conditional nodes will access data in the blackboard and return one of the two values listed above based on if a particular condition is met within the data.</li> </ul>"},{"location":"packages/behavior-trees/visualization/","title":"Visualization of the Behavior Tree","text":"<p>Each tree that you build will be able to be visualized as a ROS Image topic under the topic name <code>/btree</code></p> <p>Once your behavior tree is executed with <code>roslaunch mr_bt btree.launch tree:=my_tree</code> the <code>/btree</code> topic will begin to be published and updated according to the state of your tree in real time.</p> <p>The nodes which have not been run will appear white, those which have been run and returned <code>\"failure\"</code> will appear red, those which have been run and returned <code>\"success\"</code> will appear green, and those which returned <code>\"running\"</code> will appear yellow.</p> <p>Since the image is updated in real time, you will be able to get feedback on which state your tree is in at any given moment, and also debug any issues.</p>"},{"location":"packages/behavior-trees/visualization/#using-rviz-to-visualize-the-behavior-tree","title":"Using RVIZ to visualize the Behavior Tree","text":"<p>You can run <code>rviz</code> to open up the program and add the <code>/btree</code> topic as an image to visualize the tree.</p> <p></p>"},{"location":"packages/behavior-trees/visualization/#logging-the-blackboard","title":"Logging the Blackboard","text":"<p>You can log the values in the blackboard by using the <code>log</code> argument when running the mr_bt launch file: <code>roslaunch mr_bt btree.launch tree:=my_tree log:=true</code></p> <p>The blackboard will then be printed in the terminal where you run the launch file in the ROS log.</p> <p>Because some blackboard variables will be two large to print, not all of the variables will show up in the output. Strings, floats, ints, and booleans will be printed, as well as the first 5 elements of any lists or arrays. If the full value of a variable is not printed, its data type will be printed instead.</p>"},{"location":"packages/behavior-trees/build/","title":"Building a Behavior Tree in JSON","text":"<p>The mr_bt project allows users to define behavior trees as JSON files. Each node in the behavior tree is a JSON dictionary object, for example:</p> <pre><code>{\n    \"name\":\"goal_pid\",\n    \"type\":\"Multitasker\",\n    \"children\":[\n\n        {\n            \"name\":\"Calculate angular velocity necessary to face towards the position\",\n            \"type\":\"PathCorrection\",\n            \"correction_var_name\":\"angular_goal_pid\",\n            \"goal_value_var_name\":\"goal_rotation\",\n            \"current_value_var_name\":\"rotation\",\n            \"max_vel\":1.5\n          },\n          {\n            \"name\":\"Calculate linear velocity to get to position\",\n            \"type\":\"PathCorrection\",\n            \"correction_var_name\":\"linear_goal_pid\",\n            \"goal_value_var_name\":\"dist\",\n            \"current_value_var_name\":\"dummy_current_value\",\n            \"max_vel\":0.1,\n            \"offset\":0\n          }\n    ],\n    \"blackboard\":{\n        \"angular_goal_pid\":null,\n        \"linear_goal_pid\":null,\n        \"dummy_current_value\":0,\n        \"dist\":null\n    },\n    \"blackbox\":true\n}\n</code></pre>"},{"location":"packages/behavior-trees/build/#folder-structure","title":"Folder structure","text":"<p>When building a new project create a new folder with the name of your project in <code>mr_bt/src/tree_jsons</code>.</p> <p>Each behavior tree project must have a root JSON file named <code>root.json</code>. This file will define the root node of your behavior tree and must be unique in your project folder. For example if you want to make a new project named <code>my_new_bt_project</code>, your folder structure should look something like this: <pre><code>mr_bt/src/tree_jsons/\n  my_new_bt_project/\n    root.json\n    action1.json\n    action2.json\n</code></pre></p>"},{"location":"packages/behavior-trees/build/defining_blackboard/","title":"Blackboard","text":"<p>The blackboard is the main data storage component of the behavior tree in mr_bt which contains ROS messages, and other variables vital to the function of your program. The variables in the blackboard will be defined in your tree JSON or throughout multiple tree JSONs in your project.</p> <p>The blackboard is, at its core, just a referece to a python dictionary that gets passed down through each node recursively in your tree. All nodes called in your tree have access to the same python dictionary so they can share information.</p>"},{"location":"packages/behavior-trees/build/defining_blackboard/#populating-variables-in-the-blackboard","title":"Populating variables in the Blackboard","text":"<p>Let's start off with defining a blackboard that has two input variables: <code>max_speed</code> and <code>goal_position</code>. You want to define these two variables yourself at the root of your tree, so your tree JSON <code>root.json</code> should look something like this: <pre><code>{\n    \"name\":\"move_to_position\",\n    \"type\":\"Selector\",\n    \"children\":[\n        ...\n    ],\n    \"blackboard\":{\n        \"max_speed\":0.5,\n        \"goal_position\":[1, 1]\n    }\n}\n</code></pre> You can see that the blackboard is defined within the node with the keyword <code>\"blackboard\"</code> and the value being another JSON dictionary containing the key, value pairs of the blackboard variables.</p>"},{"location":"packages/behavior-trees/build/defining_blackboard/#initializing-variables-in-the-blackboard","title":"Initializing variables in the blackboard","text":"<p>Let's say you want a few variables in your blackboard to keep track of where your robot is, and whether it has reached a certain position. You want your variables to be called <code>current_position</code> and <code>reached_position</code>.</p> <p>The various nodes within your tree will update these variables with the correct information when the tree is executed, however you don't want to populate them yourself. In this case, you can simply define the blackboard as so: <pre><code>{\n    ...\n    \"blackboard\":{\n        \"current_position\":null,\n        \"reached_position\":null\n    }\n}\n</code></pre> Since your other nodes will populate these values upon execution of the tree, you can initialize them as <code>null</code>.</p>"},{"location":"packages/behavior-trees/build/defining_blackboard/#ros-messages-in-the-blackboard","title":"ROS messages in the Blackboard","text":"<p>You'll likely need to subscribe to ROS messages coming from either your robot, or other ros programs in your behavior tree. In order to program your behavior tree to subscribe to these messages, you must define them in the blackboard as a special case.</p> <p>For example, let's say you need want to use the <code>GetPosition</code> node to populate the <code>current_position</code> variable from the previous example. The <code>GetPosition</code> node uses an <code>Odometry</code> message to get the current positionof the robot, so your blackboard should look like this: <pre><code>{\n    \"name\":\"Get my robot's position\",\n    \"type\":\"GetPosition\",\n    \"position_var_name\":\"current_position\",\n    \"blackboard\":{\n        \"current_position\":null,\n        \"/odom\":\"Odometry\"\n    }\n}\n</code></pre> You'll notice that in the special case of subscribing to a ROS topic in the blackboard, the name of the variable is preceded with a <code>/</code>, and the value is the name of the ROS message type. This will be the case for any ROS topic subscription in your behavior tree.</p>"},{"location":"packages/behavior-trees/build/defining_blackboard/#defining-the-blackboard-in-multiple-jsons","title":"Defining the blackboard in multiple JSONs","text":"<p>Likely you will split your project up into multiple different files, and it will be difficult to consolidate all of the required blackboard variables and ROS topic subscriptions into one blackboard definition. Luckily, you don't need to do that.</p> <p>You can define different sections of the blackboard in different JSON files, and all of the different blackboard definitions will be combined into one upon compilation of your tree.</p> <p>For example, if you have two files in your tree each with a different blackboard definition such as:</p> <p><pre><code>{\n    ...\n    \"blackboard\":{\n        \"current_position\":null,\n        \"/odom\":\"Odometry\"\n    }\n}\n</code></pre> And then <pre><code>{\n    ...\n    \"blackboard\":{\n        \"max_speed\":0.5,\n        \"goal_position\":[1, 1]\n    }\n}\n</code></pre> The final functional blackboard will be </p> <pre><code>\"blackboard\":{\n    \"current_position\":null,\n    \"/odom\":\"Odometry\",\n    \"max_speed\":0.5,\n    \"goal_position\":[1, 1]\n}\n</code></pre>"},{"location":"packages/behavior-trees/build/defining_nodes/","title":"Working with Leaf Nodes in Tree JSON","text":"<p>The usage of the included nodes can be generalized to a few rules.</p> <ul> <li> <p>All of the included nodes exist within python files inside the folder <code>mr_bt/src/nodes/</code>. The files themselves are using the snake naming convention, for example: <code>my_bt_node</code>. The classes inside each file uses the CapWords naming convention conversion of the file name, for example: <code>class MyBtNode:</code>. When calling the node in a JSON tree, use reference the <code>\"type\"</code> with the CapWords name of the node class, i.e. <pre><code>{\n    \"name\":\"node name\",\n    \"type\": \"MyBtNode\",\n    ...\n}\n</code></pre></p> </li> <li> <p>The arguments passed into the node definition in the tree JSON should exactly match the names of the arguments defined in the python class <code>__init__</code> function, for example if the class definition looks like this:  <pre><code>class MyBtNode:\n    def __init__(self, arg1: str, arg2: int, arg3: float):\n        ...\n</code></pre></p> </li> <li>Your tree JSON should look like this:  <pre><code>{\n    \"name\":\"node name\",\n    \"type\": \"MyBtNode\",\n    \"arg1\": \"hello world\",\n    \"arg2\": 4,\n    \"arg3\": 10.8\n}\n</code></pre></li> </ul>"},{"location":"packages/behavior-trees/build/defining_nodes/#working-with-parent-nodes-in-tree-json","title":"Working with Parent Nodes in Tree JSON","text":"<p>The usage of parent nodes follows the same rules as the usage of the leaf nodes, however all parent nodes require their children to be defined in the tree JSON as well. The children are defined as a list of nodes within the <code>\"children\"</code> agument of the parent node. Here is an example parent node with two children:  <pre><code>{\n    \"name\":\"reached_goal\",\n    \"type\":\"Sequencer\",\n    \"children\":[\n        {\n            \"name\":\"reached_position\",\n            \"type\":\"ReachedPosition\",\n            \"goal_pos_var_name\":\"goal_pos\",\n            \"error\":0.05\n        },\n        {\n            \"name\":\"stop\",\n            \"type\":\"Stop\"\n        }\n    ]\n}\n</code></pre></p>"},{"location":"packages/behavior-trees/build/defining_references/","title":"Working with Tree References","text":"<p>Due to the recursively defined nature of the behavior tree, the JSON definitions can get messy and unreadable for larger trees. The solution for splitting up different parts of your tree is using a reference to another JSON within your workspace instead of an entire node or tree definition. </p> <p>Anywhere you could include a node or tree definition, you could instead reference another JSON using the <code>\"ref\"</code> keyword. The root directory of the tree parser is set to <code>mr_bt/src/tree_jsons/</code>, so your reference provided must be relative to that root directory. </p> <p>For example, let's say that you are creating a new behavior tree in the folder <code>mr_bt/src/tree_jsons/my_new_tree/</code> and your folder contains the following: <pre><code>/mr_bt/src/tree_jsons/my_new_tree/\n    root.json\n    move.json\n    calculate_dist.json\n</code></pre> You want to include <code>move.json</code> and <code>calculate_dist.json</code> as children for a parent node in <code>root.json</code>. This is what that would look like <pre><code>{\n    \"name\":\"my_new_tree_root\",\n    \"type\":\"Sequencer\",\n    \"children\":[\n        {\n            \"ref\":\"my_new_tree/calculate_dist.json\"\n        },\n        {\n            \"ref\":\"my_new_tree/move.json\"\n        }\n    ]\n}\n</code></pre> You can also make references to other behavior tree JSONs as long as they are in the directory <code>mr_bt/src/tree_jsons</code> by providing their path within that directory.</p>"},{"location":"packages/behavior-trees/custom_nodes/","title":"Writing custom Behavior Tree nodes","text":"<p>You may find yourself wanting to extend the functionality provided by the default nodes in this package by adding your own parent, action, conditional, or update nodes.</p>"},{"location":"packages/behavior-trees/custom_nodes/#directory-structure","title":"Directory structure","text":"<p>In order to create your own custom nodes, the nodes can exist anywhere in the directory <code>mr_bt/src/nodes/</code> as a <code>.py</code> file.</p>"},{"location":"packages/behavior-trees/custom_nodes/#naming","title":"Naming","text":"<p>All behavior tree nodes are created as a python class in a python file. The name of your <code>.py</code> file should correspond with the name of your class within the file. </p> <p>For example, if you want to create a node called <code>MyNewNode</code>, the file that it exists in should be called <code>my_new_node.py</code> and the class definition should be <code>class MyNewNode(SomeNodeType):</code>.</p>"},{"location":"packages/behavior-trees/custom_nodes/#required-function","title":"Required function","text":"<p>Each different type of node will have a required \"tick\" function, but the naming is different depending on the type of node you are creating. Regardless of what the \"main\"  of function is called, it always returns one of three string type values: <code>\"success\"</code>, <code>\"failure\"</code>, or <code>\"running\"</code>.</p> <p>For further details on how to create custom nodes of each type, see the following sections.</p>"},{"location":"packages/behavior-trees/custom_nodes/custom_action/","title":"Creating a custom action node","text":"<p>To create a custom action node, you must define your node class by inheriting from the superclass that defines a generic action node.</p> <p>You also must include the <code>execute</code> function as the \"tick\" function that performs the actions of the node.</p> <p>Here is an example of a custom action node:</p> <pre><code>from ...nodes.action import Action\n\nclass MyActionNode(Action):\n    def __init__(self):\n        super().__init__()\n\n    def execute(self, blackboard: dict) -&gt; str:\n        ...\n</code></pre> <p>This type of node will return one of the following string values: <code>\"success\"</code>, <code>\"failure\"</code>, or <code>\"running\"</code></p>"},{"location":"packages/behavior-trees/custom_nodes/custom_conditional/","title":"Creating a custom Conditional node","text":"<p>To create a custom conditional node, you must define your node class by inheriting from the superclass that defines a generic conditional node.</p> <p>You also must include the <code>condition</code> function as the \"tick\" function that returns a <code>bool</code> type depending on the state of the relavent blackboard</p> <p>Here is an example of a custom conditional node:</p> <pre><code>from ...nodes.conditional import Conditional\n\nclass MyConditionalNode(Conditional):\n    def __init__(self):\n        super().__init__()\n\n    def condition(self, blackboard: dict) -&gt; bool:\n        ...\n</code></pre> <p>This type of node will return a boolean value.</p>"},{"location":"packages/behavior-trees/custom_nodes/custom_update/","title":"Creating a custom Update node","text":"<p>To create a custom update node, you must define your node class by inheriting from the superclass that defines a generic update node.</p> <p>You also must include the <code>update_blackboard</code> function as the \"tick\" function that updates the blackboard with relevant information.</p> <p>Here is an example of a custom update node:</p> <pre><code>from ...nodes.update import Update\n\nclass MyUpdateNode(Update):\n    def __init__(self):\n        super().__init__()\n\n    def update_blackboard(self, blackboard: dict) -&gt; str:\n        ...\n</code></pre> <p>This type of node will return one of the following string values: <code>\"success\"</code>, <code>\"failure\"</code>, or <code>\"running\"</code></p>"},{"location":"packages/behavior-trees/included_nodes/action_nodes/","title":"Action Nodes","text":""},{"location":"packages/behavior-trees/included_nodes/action_nodes/#basic-movement","title":"Basic Movement","text":"<ul> <li> <p>LinearAngularStatic     <pre><code>Publishes a Twist message on the topic /cmd_vel with a linear and angular velocity defined in the node JSON\n\nParams:\n    lin_vel (float): Linear x velocity for Twist msg\n    ang_vel (float): Angular z velocity for Twist msg\n</code></pre></p> </li> <li> <p>LinearAngularDynamic     <pre><code>Publishes a twist message on the topic /cmd_vel with a linear and angular velocity provided by variables in the blackboard\n\nParams:\n    linear_var_name (string): Name of the blackboard variable holding the linear x velocity to add to Twist msg\n    angular_var_name (string): Name of the blackboard variable holding the angular z velocity to add to Twist msg\n</code></pre></p> </li> <li>Stop     <pre><code>Publishes a Twist msg on /cmd_vel with all velocity values set to 0\n</code></pre></li> </ul>"},{"location":"packages/behavior-trees/included_nodes/conditional_nodes/","title":"Conditional Nodes","text":""},{"location":"packages/behavior-trees/included_nodes/conditional_nodes/#basic-conditionals","title":"Basic Conditionals","text":"<ul> <li>BoolVar     <pre><code>Returns \"success\" if boolean variable in blackboard is true, otherwise returns \"failure\".\n\nParams:\n    var_name (string): name of boolean variable in blackboard\n</code></pre></li> <li>BoolVarNot     <pre><code>Returns \"failure\" if boolean variable in blackboard is true, otherwise returns \"success\".\n\nParams:\n    var_name (string): name of boolean variable in blackboard\n</code></pre></li> </ul>"},{"location":"packages/behavior-trees/included_nodes/conditional_nodes/#odometry-conditionals","title":"Odometry Conditionals","text":"<ul> <li>ReachedPosition     <pre><code>Returns \"success\" if the position of the robot given by the /odom topic is close within error to the provided goal position variable\n\nParams:\n    goal_pos_var_name (string): Name of the blackboard variable holding an array of the x,y goal coordinates.\n    error (float): Tolerated error for reaching the goal position\n</code></pre></li> </ul>"},{"location":"packages/behavior-trees/included_nodes/conditional_nodes/#laserscan-conditionals","title":"LaserScan Conditionals","text":"<ul> <li>ClearAhead</li> </ul>"},{"location":"packages/behavior-trees/included_nodes/included_nodes/","title":"Included nodes","text":""},{"location":"packages/behavior-trees/included_nodes/included_nodes/#documentation-for-included-nodes","title":"Documentation for included nodes","text":""},{"location":"packages/behavior-trees/included_nodes/included_nodes/#action-nodes","title":"Action Nodes","text":"Basic Movement  The 3 nodes classified as \"basic movement\" nodes are the fundamental building blocks needed in order to make a robotics behavior tree application.   LinearAngularStatic  When this node is activated, it will publish a cmd_vel message including the linear and angular velocities specified during its construction:  *Example:* <pre><code>{\n    \"name\":\"my_node\",\n    \"type\":\"LinearAngularStatic\",\n    \"lin_vel\":0.5,\n    \"ang_vel\":1\n}\n</code></pre> Stop  A special case of LinearAngularStatic; upon activation the Stop node will publish a cmd_vel message with all velocity values set to 0.  *Example:* <pre><code>{\n    \"name\":\"my_node\",\n    \"type\":\"Stop\"\n}\n</code></pre> LinearAngularDynamic  This node is similar to LinearAngularStatic how,ever instead of providing velocities during construction, you provide names of blackboard topics which correspond to the linear and angular velocities that the node will publish in cmd_vel when activated. This allows for velocities to be updated by update nodes and then have one LinearAngularDynamic node providing dynamic movements.  *Example:* <pre><code>{\n    \"name\":\"my_node\",\n    \"type\":\"LinearAngularDynamic\",\n    \"linear_var_name\":\"my_linear_vel\",\n    \"angular_var_name\":\"my_angular_vel\",\n    \"blackboard\":{\n        \"my_angular_vel\":null,\n        \"my_linear_vel\":null\n    }\n}\n</code></pre>"},{"location":"packages/behavior-trees/included_nodes/included_nodes/#conditional-nodes","title":"Conditional Nodes","text":"Basic Conditionals BoolVar BoolVarNot Odometry Conditionals Laserscan Conditionals"},{"location":"packages/behavior-trees/included_nodes/included_nodes/#update-nodes","title":"Update Nodes","text":"Basic Updates Movement Control Updates Odometry Updates Laserscan Updates"},{"location":"packages/behavior-trees/nodes/","title":"Nodes","text":"<p>A behavior tree is a tree-shaped data structure consisting of nodes, each of which contain a logical method which executes code. The behavor tree is evaluated recursively starting  at the root node. Each node has the abilility to execute code which will either run a script, or execute all of its children. Each node will also return one of 3 outputs to its  parent: \"success\", \"failure\", or \"running\". There are two main types of nodes: the control-flow (parent nodes) nodes and the leaf nodes.</p>"},{"location":"packages/behavior-trees/nodes/leaf_nodes/","title":"Leaf nodes","text":""},{"location":"packages/behavior-trees/nodes/leaf_nodes/#leaf-nodes","title":"Leaf Nodes","text":"<ul> <li>Action Nodes</li> <li>Action nodes send ROS topic messages from the behavior tree to the robot.</li> <li>Often the type of message sent from an Action node is a cmd_vel message which encodes movement instructions for the robot.</li> <li>Update Nodes</li> <li>Update nodes are designated for updating data in the blackboard.</li> <li>Often times the types of data updates performed by Update nodes include preprocessing or processing of message data from the robot.</li> <li>Conditional Nodes</li> <li>Conditional nodes will return either \"success\" or \"failure\", corresponding to the boolean values \"true\" and \"false\" respectively</li> <li>Conditional nodes will access data in the blackboard and return one of the two values listed above based on if a particular condition is met within the data.</li> </ul>"},{"location":"packages/behavior-trees/nodes/parent_nodes/","title":"Parent nodes","text":""},{"location":"packages/behavior-trees/nodes/parent_nodes/#control-flow-nodes","title":"Control-Flow Nodes","text":"<ul> <li>Selector</li> <li>The Selector executes its children sequentially from left to right. </li> <li>If one of its children returns either \"success\" or \"running\", it will halt execution of its children and it will return the result of the child it stopped on.</li> <li>If all of its children return \"failure\", the Selector will also return \"failure\".</li> <li>Sequencer</li> <li>The Sequencer executes its children sequentially from left to right.</li> <li>The Sequencer will not halt execution of its children unless one of them returns \"failure\" or \"running\", in which case it will also return \"failure\" or \"running\".</li> <li>If all children return \"success\" the Sequencer will return \"success\"</li> <li>Multitasker</li> <li>The Multitasker runs all of its children concurrently, each in a separate thread.</li> <li>The Multitasker will return \"success\" only if all of it's children return \"success\".</li> <li>If any of its children return \"running\" but none return \"failure\", the Multitasker will return \"running\".</li> <li>If any of its children return \"failure\", the Multitasker will return \"failure\".</li> </ul>"},{"location":"reports/","title":"Final Project Reports","text":"<p>The following pages are each a project report by a student in Autonomous Robotics at Brandeis University. Here's how to add your content:</p> <ol> <li>Git clone this repository to your computer: </li> <li>In the directory structure, find the folder called reports. In the 2024 folder, create an .md for your report. </li> <li>Write your project report using markdown. You can include images as well as links. There are many tutorials on Markdown but you most likely already know it.</li> <li>Use the template below as a starting point</li> <li>Git add, commit and push your changes</li> <li>If you did it right then they will appear in the Labnotebook</li> </ol> <p></p>"},{"location":"reports/#template-for-your-report","title":"Template for your report","text":"<pre><code>---\ntitle: Campus Rover Project\nauthor: Pito Salas\ndate: Dec 5 2024\n---\n## Introduction\n1. Problem statement, including original objectives\n1. Relevant literature\n1. Team members (names and emails)\n## What was created (**biggest section**)\n1. Technical descriptions, illustrations\n1. Discussion of interesting algorithms, modules, techniques\n1. Guide on how to use the code written\n1. Clear description and tables of source files, nodes, messages, actions and so on\n## Story of the project. \n1. How it unfolded, how the team worked together\n1. Your own assessment\n1. problems that were solved, pivots that had to be taken\n</code></pre>"},{"location":"reports/2019/past-gen-letters/","title":"Past Generation Letters","text":""},{"location":"reports/2019/past-gen-letters/#letters-from-past-gens","title":"Letters From Past Gens","text":""},{"location":"reports/2019/past-gen-letters/#to-the-students-of-gen-4","title":"To the students of Gen 4","text":"<p>We started our iteration with the the code from Gen 2. This consisted of several nodes which were compatible with the Turtlebot2. We immediately eliminated some nodes which did not exist on the Mutant (such as package_handler), as we did not have such a sensor. Other nodes required reworking, such as pickup_detector, as the Turtlebot2 had sensors to see whether it was on the ground or not but the Mutant did not have this functionality built in. In addition, during our time in the class, namespacing was introduced, so multiple robots could run on the same roscore. This introduced some difficulties, specifically with certain nodes that were not being namespaced correctly.</p> <p>It seems that many of the problems we were having resulted from the Turtlebot2 running off of a lot of proprietary software, while the Mutant (based off of a Turtlebot3) is much simpler. Using ROS, we did run into many hiccups and there was a significant amount of troubleshooting involved. A lot of information about the troubleshooting can be found in the lab notebook, on the Mutant page. As ROS is very complicated, without a doubt any future generation will also be faced with a significant amount of troubleshooting. We found the rqt_graph tool to be one of the most helpful -- it allows the user to see which nodes are subscribing/publishing, etc. Much of the time problems stem from this, and being able to see this visually can be very helpful.</p> <p>We participated in the development of the campus rover similar to Gen 2, where we would brainstorm projects and Pito would lead the adoption by each group of a given project. While everyone was (for the most part) able to complete their projects, we left integration of the different parts (computer vision, hand gestures, navigation) until the last day, which created some difficulties. We highly recommend teams to work on integration throughout the process so that integration is as smooth as possible for the demo at the end of the semester!</p> <p>In future generations, we recommend further improving the robustness of features that were implemented, such as hand gestures and CV. In addition, part of Gen 3\u2019s goal was to allow the project to run on any Turtlebot. While the code should be able to run on any Turtlebot3, a speaker of some sort would need to be attached for the talk services. These talk services allow the robot to update the user with its progress throughout its use. With these new features, we tried to give direction to further development of the project. We think that these features will guide future generations of the class in development of the campus rover, ultimately leading to a campus rover that will actually be able to deliver packages on campus.</p> <p>Best of luck,</p> <p>Gen 3</p>"},{"location":"reports/2019/past-gen-letters/#eli-cohn-5172019","title":"Eli Cohn, 5/17/2019","text":""},{"location":"reports/2019/past-gen-letters/#to-the-students-of-gen-3","title":"To the students of Gen 3","text":"<p>We, the Gen 2 team, hope in this letter (and in the ancillary documentation contained in the <code>labnotebook</code> repo) to pass on the experience and knowledge we have gathered over the past semester to the Gen 3 team and beyond. We hope that future teams will not only build upon our accomplishments, but also that they will learn from and avoid our mistakes.</p> <p>We began Campus Rover as a first-come-first-serve brainstorming of projects and possible features of the final Campus Rover product, where students would create teams (1-3 student sized teams for 6 total students) and pick from the list of projects for that current iteration (typically the week from the current class to the next week's meeting). Often, a project would take longer than a single iteration, in which case a team would usually remain on the project for the next iteration.</p> <p>While this system allowed us to initially take on virtually any project we liked, and gave us a great deal of control over the Campus Rover's final functionality, the protocol gave us expected headaches as we approached the final demo date. By the third or fourth to last iteration (3-4 weeks before the final demo), we were forced as a team to change gears towards combining and debugging the individual projects from our previous iterations. This involved the implementation of states (<code>states.py</code> and <code>all_states.py</code>), controller nodes (<code>rover_controller.py</code>), and other nodes solely for structurally organizational purposes. While these nodes perform satisfactorily for the extent of our Campus Rover (demo and features-wise), they could have put in much more work if implemented chronologically.</p> <p>In future generations of the Campus Rover, we recommend starting where we finished. With a robust, scaleable node framework, implementing new features would not only be faster and easier, but the nodes will also be able to be implemented with customized standardized guidelines that will gurantee consistency throughout the architecture. For example, with the introduction of states, we had hoped to better organize the robot by its current objective (i.e. navigating, teleoping, etc). However, since many of our existing functionalities were already built, we ran into problems refactoring them to reflect and operate under states.</p> <p>Implementing high level node control proved its worth again when developing the web application. In order for the application to provide rich feedback including the robot's position, navigation goal, etc, it must be able to access this information. With some exhaustive high-level state and node handling, this information would already be available throughout the architecture, and easily integrated into the web app (see <code>Flask &amp; ROS.md</code>).</p> <p>For an ideal Gen 3 implementation of Campus Rover, we think that full syncronization and managing of nodes is imperative. Most (if not all) robot functionality should come second to a standardized and organized framework. Consider sorting messages coming from sensors, the web app, and other nodes, so that any subscribing node can easily select and sort through incoming messages. Also, though our implementation was useful to us in the end, a deeper and more easily managed state mechanism would greatly help organization of the robot's features and tasks.</p> <p>Good luck!</p> <p>Gen 2</p>"},{"location":"reports/2019/past-gen-letters/#ben-alberxt-121618","title":"Ben Alberxt 12/16/18","text":""},{"location":"reports/2019/robot-arm/","title":"robot-arm.md","text":""},{"location":"reports/2019/robot-arm/#final-project-arm-interfacing","title":"Final Project Arm Interfacing","text":"<p>Jacob Smith COSI 119a Fall 2019, Brandeis University</p> <p>Github Repository: [Arm Interface Project](\\&lt;https://github.com/campusrover/Robot-Arm-Interface)</p> <p>Introduction and Relevant literature</p> <p>\u200b The goal of this project was to implement a robotic arm to the campus rover. The scope of the project was planned to go from the high level details of a ROS package and nodes to the low level servo commands to control the motor and everything in between. The final goal of the project was to use the arm in a campus rover application.</p> <p>\u200b Adding an arm not only increases the possible behaviors of the campus rover, but created a framework to integrate any electronic component. In addition, it allowed the testing of the concept of interfacing by seeing if a user not familiar with the arm can control it with ROS.</p> <p>\u200b The author already had a background in Arduino Programming. The main source of information for this project were online tutorials and electronics forums and consultation with Charlie Squires (see Appendix A). For the evaluation of the arm's performance, I two publications on evaluation of robot structures and measuring user experience where consulted [1,2].</p> <p>\u200b The requirements of the project where to interface the ROS based campus rover with a four servo robotic arm and an ultrasonic distance sensor. The servos can each move half a rotation at any speed using the VarSpeedServo library [8], and the ultrasonic sensor returns the distance in inches when requested. Both components are interacted using the Arduino microprocessor attached to the arm. That microprocessor is attached to the Rasberry Pi that is the main computer of the campus rover. The goal of the project was to allow these components to communicate with each other and to integrate into ROS, which is explained next.</p> <p>Technical descriptions, illustrations</p> <p>\u200b The final project is a comprehensive electronics interacting system that allows the arm, manipulator, and distance sensor to be controlled using ROS framework using abstractions. The interfacing scheme is represented below, and could easily be expanded for other electronics components. The main concept is to separate the command of a component from the low-level instruction, similar to how a user can publish<code>cmd_vel</code> and control a dynamixel motor. The figure below represents the existing commands that can be sent to the arm, utilizing both inputs and outputs.</p> Goal ROS Command Arduino Command Inputs other than Name Outputs Comment Get Distance Published to Arm Response topic Automatic Distance Type Output Sensor Set Arm Coordinates ARM x y base power set_arm(100, 200, 90, 0 servoSpeed); 4 coordinates, speed Type 4 Coordinates Set Manipulator MANIP setting manipulator(true =open/false =code); open or close Type Binary Manipulator <p>\u200b This report will now explain the components of the project in a top-down order, as a potential user of the arm would view it. These components consist of: the startup scripts, ROS Publisher, ROS Subscriber, Arduino Program, and the hardware being interfaced with.</p> <p>\u200b The startup scripts allow the appropriate ROS nodes to start and allow the arm to be controlled by a ROS Publisher. These are important because they provide information on how to use the arm to someone unfamiliar with Serial and ssh connection issues.</p> <p>\u200b The ROS Publisher publishes to the <code>armcommand</code> topic. The command is formatted in terms of a component tag, along with integers for required parameters. For example, <code>MANIP 1</code> closes the hand or <code>ARM 0 330 0 20 20</code> extends the arm at low power. The python node below waves the arm when a face is shown below, and shows how simply the arm can be controlled in ROS.</p> <p></p> <p>\u200b The Arm Interface Node is run locally on the Rasberry Pi on the robot because it needs to communicate to the Arduino. This node establishes the Serial connection to the Arduino on the arm ( returning an error message of the arm is unplugged and attempting to reconnect). Then, the node subscribes to the armcommand topic. When a command is published to that topic, the callback function of the subscriber sends the command to the Arduino and publishes the feedback from the Arduino to the ArmResponse Topic. The graph of ROS nodes demonstrates the chain of arm interface and response nodes.</p> <p></p> <p>\u200b Finally, the Arduino Program running on the Arm itself sets up the program to run the motors and distance sensor, establish the serial connection, and parses data coming over the serial connection. This is accomplished by reading the tag and waiting for the correct number of integer arguments. For example, if \"ARM\" is received, the program loads the next four arguments received into an array and makes a function call to move the arm position. The program to control the manipulator is shown below, note how it hides the actual degree measurements on whether the servo is open or closed and returns an error message if the improper command is specified.</p> <p></p> <p>\u200b Then, the Arduino commands the motor to move to the specified angle and returns the distance sensor reading (The heartbeat doubles as the distance reading). The arm and associated reverse kinematics program was provided by Charlie Squires (with the author rearranging the microprocessor and battery connection so the arm could be removed from its original mounting and added to the robot).</p> <p></p> <p>\u200b Next, a survey of interesting problems and techniques is presented. One persistent and surprising problem with the project was the USB connection between the raspberry Pi and the Arduino. Firstly, the USB needs to be unplugged for the arm to be connected properly, and the arm doesn't check which USB port it's plugged in to. These problems could be solved with a voltage regulator and a modification to the arm interface node to check a list of possible usb ports.</p> <p>\u200b An interesting challenge when integrating the arm into the facial recognition project (see below) was when the arm command topic was being published many times a minute, faster than the arm could actually move. This problem doesn't occur with <code>cmd_vel</code> because the motor moves continuously, while the arm's actions are discrete. We solved the problem with a node which only requests to move the arm every few seconds.</p> <p>\u200b In terms of the Arduino program, it uses the simplest form of Charlie Squire's kinematics program, and that could be abstracted into an Arduino library for code clarity. Currently all commands do not stop the execution of the Arduino program, meaning that the program doesn't wait for the arm to finish moving. This allows the components to be redirected on the fly.</p> <p>\u200b Finally, one must consider how the project can be extended to other electronics components, such as a second arm or a package delivery system. In future work, sensors and a ROS action could be implemented to allow for more complicated movements, such as an action to grab a package. That would be implemented by finding the location of a package, and perhaps using a camera to calculate its location. That location could then be passed into coordinates and published to the arm command topic.</p> <p>Story of the project</p> <p>\u200b I completed this project individually, but benefited from the support of Charlie Squires, both from his original work on the arm and his advice throughout the project. He remounted and fixed the arm, added the ultrasonic sensor, and was a helpful consultant. Late in the project, I also was able to work with Luis and Chris. Chris and I worked quickly on a proof of concept showing how the behavior trees can be used in connection with the arm, and with Luis we where able to prepare our facial recognition-arm demonstration (See videos Appendix A).</p> <p>\u200b Next, the workflow of the project will be discussed, taking a bottom-up approach. First, I wrote an Arduino program to print output over the USB cable, and a python program to read the output from the raspberry pi. Then in version two, I had the Raspberry Pi write to the Arduino, and when the Arduino received a character, it would open and close the manipulator. Next, I improved the Arduino program to open or close the gripper based on whether an 'o' or a 'c' character was sent by the raspberry pi, which also printed output from the Arduino. This is an example of two way communication between the Arduino and the Raspberry pi. Then, I added the ability to move the rest of the servos in the arm.</p> <p>\u200b With the basic infrastructure in place to control, I converted the python program on the Raspberry Pi to the ROS arm interface subscriber and added more error handling. I then wrote a simple ROS node to publish commands to the subscriber. In other words, in the first part of the project I allowed the arm to be controlled from the raspberry Pi, and then I used ROS to allow the arm to be controlled with any ROS node.</p> <p>\u200b The record of the stages of networking the components of this project will prove useful to the student who wishes to learn about the low level details of interfacing (Arduino Library Example V1-V4 and Raspi V1-V4 in 1). Finally, I made the interface general by creating the tagging system of component and arguments (MANIP 0), which can be easily extended to other components, such as the ultrasonic sensor which reports the distance to the ground.</p> <p>\u200b A major goal of the project was to see how useful it is to people not familiar with Arduino who want to contribute to Campus Rover. To that end, I verified the bash scripts with a TA for the class, who was able to get the arm running using my instructions. In addition, Luis was able to control the arm just by publishing the the arm command topic, and not going to a lower level. This integration with the facial recognition project also allowed me to make the program easier to use and port into another project.</p> <p>Problems that were solved, pivots that had to be taken</p> <p>\u200b The main problem of this project occurred when a piece of hardware didn't perform the way I expected it to. It was easy to get sidetracked with the details of exactly how a servo works compared to the higher level architectural details.</p> <p>\u200b One pivot during this project was the realization that the servos have no ability to report their angle to the user. I investigated the Servo library I am using, VarSpeedSevo to determine the <code>isMoving</code> function could at least return whether the motor was moving, but my testing showed that it couldn't detect this [9]. Plotting the number of times the servo moved by different degrees shows that there is no correlation between where the servo is jammed and what the isMoving function returns. The lack of feedback data made implementing a ROS action less feasible, because an action should be able to know when it is finished. This is an example of the kind of investigation that the next user of the arm interfacing project should hopefully be able to avoid.</p> <p></p> <p>\u200b Another problem was the mounting of the arm, as the campus rover was not designed to have a place for it. I initially mounted it above the lidar, but we ended up mounting it on top of the robot (see videos in Appendix A). Hopefully the next iteration of campus rover will include a mount for the arm that is stable and doesn't interfere with the lidar.</p> <p>Assessment</p> <p>\u200b Overall, I believe the project was a success. The robotic arm is now mounted on the campus rover and can be controlled with ROS commands. In addition, the project presents an on overview of related problems and should be a strong foundation for other hardware additions to the campus rover robot.</p> <p>\u200b The next step of the project would be to add rotation sensors to the arm and write a ROS action to control them, but that should be feasible for the next user now that the main layout is in place. Finally, my main intellectual challenge was thinking architecturally, as this is the first time I had to create a generic system that could be used after me.</p> <p>\u200b In conclusion, I wish to thank the COSI 119a TAs, Professor Salas, and Mr. Squires for their support of this project, and I hope future students find my work useful.</p>"},{"location":"reports/2019/robot-arm/#appendix-a-links-and-sources","title":"Appendix A: Links and Sources","text":"<p>Sources</p> <p>[1] Arm Interface Project</p> <p>[2] Facial Recognition and and Arm Interfacing</p> <p>[3] Behavior Trees and Arm Interfacing</p> <p>[4] Vemula, Bhanoday. Evaluation of Robot Structures. 2015 https://www.diva-portal.org/smash/get/diva2:859697/FULLTEXT02.pdf Accessed November 19 2019 [5] Tullis Thomas and Albert, William Chapter 5 of Measuring the User Experience, 2nd Edition 2013</p> <p>[6] My Original Project Proposal</p> <p>[7] My Revised Project proposal</p> <p>[9] Var Speed Servo Library</p> <p>[8] (Please see github repository README files for sources)</p> <p>Videos</p> <p>Main Project Video</p> <p></p> <p>Behavior Tree Integration Video</p> <p></p> <p>Facial Recognition Video</p> <p></p>"},{"location":"reports/2020/FiducialSLAM/","title":"Fiducial SLAM","text":""},{"location":"reports/2020/FiducialSLAM/#author-addison-pitha-additions-by-pito-salas","title":"Author: Addison Pitha (additions by Pito Salas)","text":""},{"location":"reports/2020/FiducialSLAM/#link","title":"Link","text":"<p>Source Code For This Project</p>"},{"location":"reports/2020/FiducialSLAM/#introduction","title":"Introduction","text":"<p>This project began as an effort to improve the localization capabilities of campus rover as an improvement to lidar based localization. Motivating its conception were the following issues: Upon startup, in order to localize the robot, the user must input a relatively accurate initial pose estimate. After moving around, if the initial estimate is good, the estimate should converge to the correct pose. </p> <p>When this happens, this form of localization is excellent, however sometimes it does not converge, or begins to diverge, especially if unexpected forces act upon the robot, such as getting caught on obstacles, or being picked up. Furthermore, in the absence of identifiable geometry of terrain features, localization may converge to the wrong pose, as discussed in Joint 3D Laser and Visual Fiducial Marker based SLAM for a Micro Aerial Vehicle. While the article may discuss aerial vehicles, the issue is clearly common to SLAM. </p> <p>One solution incolves fiducials, which are square grids of large black and white pixels, with a bounding box of black pixels that can be easily identified in images. The pattern of pixels on the fiducial encodes data, and from the geometry of the bounding box of pixels within an image we can calculate its pose with respect to the camera. So, by fixing unique fiducials at different places within the robot's operating environment, fixed locations can be used to feed the robot accurate pose estimates without the user needing to do so manually. </p> <p>Fiducial SLAM is not a clearly defined algorithm. Not much is written on it specifically, but it is a sub-problem of SLAM, on which a plethora of papers have been written. In TagSLAM: Robust SLAM with Fiducial Markers the authors discuss how Fiducial SLAM can be seen as a variation of Landmark SLAM. This broader algorithm extracts recognizable features from the environment and saves them as landmarks, which is difficult with changes in lighting, viewing landmarks from different angles, and choosing and maintaining the set of landmarks. </p> <p>If successful, using landmarks should be allow the robot to quickly identify its pose in an environment. Fiducial SLAM provides a clear improvement in a controlled setting: fiducials can be recognized from a wide range of angles, and given that they are black and white, are hopefully more robust to lighting differences, with their main downside being that they must be manually placed around the area. Therefore, the goal of this project was to use fiducial markers to generate a map of known landmarks, then use them for localization around the lab area. It was unknown how accurate these estimates would be. If they were better than lidar data could achieve, fiducials would be used as the only pose estimation source, if not, they could be used simply as a correction mechanism.</p>"},{"location":"reports/2020/FiducialSLAM/#what-was-created","title":"What was created","text":"<p>This project uses the ROS package fiducials, which includes the packages aruco_detect and fiducial_slam. Additionally, something must publish the map_frame, for which I use the package turtlebot3/turtlebot3_navigation, which allows amcl to publish to map_frame, and also provide the utility of visualizing the localization and provide navigation goals.</p> <p>I used the raspberry pi camera on the robots, recording 1280x960 footage at 10 fps. Setting the framerate too fast made the image transport slow, but running it slow may have introduced extra blur into the images. Ideally the camera is fixed to look directly upwards, but many of the lab's robots have adjustable camera angles. The transform I wrote assumes the robot in use has a fixed angle camera, which was at the time on Donatello.</p> <p>To set up the environment for fiducial mapping, fiducials should be placed on the ceiling of the space. There is no special requirement to the numbering of the fiducials, but I used the dictionary of 5x5 fiducials, DICT_5X5_1000, which can be configured in the launch file. They do not need to be at regular intervals, but there should not be large areas without any fiducials in them. The robot should be always have multiple fiducials in view, unless it is pressed up against a wall, in which case this is obviously not practical. This is because having multiple fiducials in view at once provides a very direct way to observe the relative position of the fiducials, which fiducial_slam makes use of. Special attention should be made to make sure the fiducials are flat. If, for example, one is attached on top of a bump in the ceiling, it will change the geometry of the fiducial, and decrease the precision with which its pose, especially rotation, can be estimated by aruco_detect.</p> <p>Before using the code, an accurate map generated from lidar data or direct measurements should be constructed. In our case we could use a floor plan of the lab, or use a map generated through SLAM. It does not matter, as long as it can be used to localize in with lidar. Once the map is saved, we can begin constructing a fiducial map. First, amcl should be running, to publish to map_frame. This allows us to transform between the robot and the map. We must also know the transform between the robot and the camera, which can be done with a static transform I wrote, rpicam_tf.py. aruco_detect provides the location of fiducials in the camera's frame. Then we run fiducial_slam, which can now determine fiducial's locations relative to the map_frame. We must pay special attention to the accuracy of amcl's localization while constructing the fiducial map. Although the purpose of this project is to prevent the need to worry about localization diverging, at this stage we need to make sure the localization is accurate, at least while the first few fiducials are located. The entire area should be explored, perhaps with teleop as I did.</p>"},{"location":"reports/2020/FiducialSLAM/#discussion-of-the-fiducial_slam-package","title":"Discussion of the fiducial_slam package","text":"<p>The fiducial_slam package was clearly the most important part of this project, but after using it, I realize it was also the biggest problem with the project, so I am dedicating this section to discussing the drawbacks to using it and how to correct those, should anyone want to try to use fiducial SLAM in the future.</p> <p>The package is very poorly documented. Its ROS wiki page contains very little information on how it works, and how to run it, even if one follows the Github link provided. The information on how to run the packages at a very basic level is incomplete, since it does not specify that something must publish to map_frame for the packages to function. </p> <p>The error messages provided are not very helpful. For example, when fiducial_slam and aruco_detect are run without amcl publishing to move base, they will both appear to work fine and show no errors, even though the rviz visualization will not show a camera feed. There is another error I received which I still have not solved. Upon launching the fiducial_slam package, it began crashing, and in the error message there was only a path to what I assume to be the source of the error, which was the camera_data argument, as well as a C++ error which related to creating an already existent file, with no information about which file that is. However, when I checked the camera_data argument, and even changed the file so I did not provide that parameter, the same error specifying a different path showed up. Note that upon initially trying to use the package, I did not specify the camera_data argument and this error did not appear, which suggests that parameter was not actually the problem. Issues like these are incredibly difficult to actually diagnose, making the use of the package much more challenging than it should be.</p> <p>Code documentation is poor, and even now I am unsure which variety of SLAM the code uses. Compare to the previously mentioned paper on TagSLAM, which clearly indicates the algorithm uses a GTSAM nonlinear optimizer for graph-based slam. TagSLAM is a potential alternative to fiducial_slam, although as I have not tested its code, I can only analyze its documentation. I think TagSLAM would be much easier to use than fiducial_slam, or perhaps some other package I have not looked at. It may also be possible for a large team (at least three people) to implement the algorithm themselves.</p>"},{"location":"reports/2020/FiducialSLAM/#story-of-the-project","title":"Story of the project","text":"<p>The original learning goal of this project was for me to better understand how fiducial detection works, and how SLAM works. Instead, my time was mostly spent discovering how the fiducial_slam package works. My first major issue was when trying to follow the tutorial posted on the ROS wiki page. I wasn't given any error messages in the terminal window running fiducial_slam, but noticed that rviz was not receiving camera data, which I assumed indicated that the issue was with the camera. </p> <p>After much toil and confusion, it turned out that for some reason, the data was not passed along unless something published to move_base. I did not originally try to run the packages with amcl running because I wrongly assumed that the package would work partially if not all parameters were provided, or at least it would give error messages if they were necessary. After solving that problem, I encountered an issue with the camera transforms, which I quickly figured out was that there was no transform between base_link and the camera. I solved this with a static transform publisher, which I later corrected when fiducial_slam allowed me to visualize where the robot thought observed fiducials were. Once I was mapping out fiducial locations, I believed that I could start applying the algorithm to solve other problems. However, overnight, my code stopped working in a way I'm still confused about. I do not remember making any modifications to the code, and am not sure if any TAs updates the campus rover code in that time in a way that could make my code break. After trying to run on multiple robots, the error code I kept getting was the fiducial_slam crashed, and that there was some file the code attempted to create, but already existed. Even after specifying new map files (which didn't yet exist) and tweaking parameters, I could not diagnose where the error was coming from, and my code didn't work after that.</p> <p>In hindsight, I see that my problem was choosing the fiducial_slam package. I chose it because it worked with ArTag markers, which the lab was already using, and I assumed that would make it simpler to integrate it with any existing code, and I could receive help from TAs in recognizing fiducials with aruco_detect. Unfortunately that did not make up for the fact that the fiducial_slam package wasn't very useful. So one thing I learned from the project was to pick packages carefully, paying special attention to documentation. Perhaps a package works well, but a user might never know if they can't figure out what the errors that keep popping up are.</p>"},{"location":"reports/2020/behavior-trees/","title":"Behavior Trees (in progress)","text":""},{"location":"reports/2020/behavior-trees/#author-chris-tam","title":"Author: Chris Tam","text":""},{"location":"reports/2020/behavior-trees/#introduction","title":"Introduction","text":"<p>The campus rover behavior tree project was conceived of as a potential avenue of exploration for campus rover\u2019s planning framework. Behavior trees are a relatively new development in robotics, having been adapted from non-player character AI in the video game industry around 2012. They offer an alternative to finite state machine control that is both straightforward and highly extensible, utilizing a parent-child hierarchy of actions and control nodes to perform complex actions. The base control nodes - sequence, selector, and parallel - are used to dictate the overarching logic and runtime order of action nodes and other behavior subtrees. Because of their modular nature, behavior trees can be both combined and procedurally generated, even used as evolving species for genetic algorithms, as documented by Collendachise, Ogren, and others. Numerous graphical user interfaces make behavior trees yet more accessible to the practicing technician, evidenced by the many behavior tree assets available to game designers now as well as more recent attempts such as CoSTAR used for robotics. My aim for this project, then, was to implement these trees to be fully integrable with Python and ROS, discover the most user-friendly way to interact with them for future generations of campus rover, and integrate my library with the current campus rover codebase by communicating with move_base and sequencing navigation while, optimistically, replacing its finite state machine structure with predefined trees.</p>"},{"location":"reports/2020/behavior-trees/#what-was-created","title":"What was created","text":"<p>For the most part, I was successful in accomplishing these goals, and the final result can be found at https://github.com/chris8736/robot_behavior_trees. This repository contains a working behavior tree codebase written entirely in Python with ROS integration, including all main control nodes, some decorator nodes, and simple processes to create new action and condition nodes. Secondly, I was able to implement a unique command parser to interpret command strings to autonomously generate behavior trees designed to run. This command parser was later used to read a sequence of plain English statements from a text file, providing a means for anyone to construct a specific behavior tree with no knowledge of code. Thirdly, I was able to merge my project seamlessly with the robot arm interface project and the campus rover navigation stack to easily perform sequenced and parallel actions.</p> <p>The codebase is inspired by the organization of the C++ ROS-Behavior-Trees package, found here: https://github.com/miccol/ROS-Behavior-Tree. Though powerful, I found this package somewhat unsupportive of new Python nodes, requiring obscure changes to catkin\u2019s CMakeLists text file after every new action node or tree declaration. Despite this, I found the package\u2019s overall organization neat and its philosophy of implementing behavior tree action nodes as ROS action servers sensible, so I attempted to adopt them. Rather than have singular action nodes, I implement auxillary client nodes that call pre-instantiated action servers that may in the future accept unique goals and provide feedback messages. For now, these action servers simply execute methods that may publish ROS topics, such as moving forward for two seconds, twisting 90 degrees clockwise, or just printing to console. This modified structure is shown below, in a program that moves forward and backward if battery is sufficient:</p> <p></p> <p>Control nodes - e.g. sequence, parallel, and selector nodes - are normal Python classes not represented by ROS nodes. I connect these classes and client nodes via parent-child relationships, such that success is implicitly propagated up the tree, and resets naturally propagate down. With these node files alone, behavior trees can be explicitly defined and run in a separate Python file through manually setting children to parent nodes.</p> <p>My command parsing script simply reads in strings from console or lines in a text file and interprets them as instructions for constructing a novel behavior tree. It utilizes the shunting-yard algorithm along with a dictionary of keywords to parse complex and potentially infinitely long commands, giving the illusion of natural language processing. Shunting-yard splits keywords recognized in the input string into a stack of values and a stack of operators, in this case allowing logic keywords such as \u201cand\u201d to access its surrounding phrases and set them as children of a control node. In addition to storing keys of phrases, the dictionary also holds references to important classes required to build the tree, such as condition nodes and action servers. The below figure shows example execution for the command \u201cmove forward then twist right and print hello\u201d (note: one action represents both the client and server nodes):</p> <p></p> <p>On its own, this approach has its limitations, namely that all the trees it creates will be strictly binary and right-heavy. I was able to solve the latter problem while implementing file parsing through the keyword \u201cis\u201d, allowing keywords to reference entire subtrees through commands such as:</p> <p></p> <p>More complex behaviors can thus be built from the bottom up, allowing for multiple levels of abstraction.    Conciseness and good documentation allow this codebase to be highly modular. By inheriting from general Action and Condition node classes, making an external process an action or condition requires only a few extra lines of code, while incorporating it into the command parser requires only a dictionary assignment. This ease of use allowed me to very quickly implement action nodes referencing move_base to navigate campus rover towards landmarks, as well as actions publishing to the robot arm interface to make it perform sequenced gestures.</p>"},{"location":"reports/2020/behavior-trees/#story-of-the-project","title":"Story of the project","text":"<p>I began this project by searching online for behavior tree libraries I could use to further my understanding of and expedite my work. Within this week of searching, I settled on ROS-Behavior-Trees, utilized by Collendachise in the above literature. Though I succeeded at running the example code and writing up new Python action nodes, I had great difficulty attaching those new nodes to the library\u2019s control nodes, written entirely in C++. After a particularly inefficient week, I decided to switch gears and move on to my own implementation, referencing the C++ library\u2019s architecture and the literature\u2019s pseudocode to set up my main nodes. Though successful, I found writing up a whole python file to represent a behavior tree while having to remember which parent-child relationships I had already set exceedingly inconvienient.</p> <p>As the third generation of campus rover was not yet stable by this time, I decided the best use of my remaining time was to make the use of my code more intuitive, leading me to the command parser. I had used the shunting-yard algorithm in the past to parse numerical expressions, and was curious to see if it would work for lexical ones as well. Though it was challenging to hook up each of my servers to a unique key phrase while importing across directories, I eventually made something workable without any major roadblocks.</p> <p>When I was finally able to get campus rover running on my machine, I had only a few weeks left to understand its code structure, map which topics I was to publish to, and rewrite existing action functionality with parameterized arguments if I wanted to do a meaningful integration. Though I worked hard to identify where navigation requests were sent and found success in having getting a robot find its way to the lab door, AMCL proved to be unreliable over many trials, and I struggled to figure out how to write new action goals to make even a surface level integration work with all landmarks. With the hope of making behavior trees easier to understand and access for future developers, I instead used the remaining time to refine my command parser, allowing it to read from file with all the functionality I mention above.</p> <p>All things considered, I feel I have satisfied nearly all the initial goals I could have realistically hoped to accomplish. I made my own behavior tree library from scratch with no previous knowledge of the topic, and I find my codebase both very easy and enjoyable to work with. Though I was unable to work with campus rover for the majority of the final project session, I proved that integrating my codebase with outside projects through ROS could be done incredibly quickly. Finally, I was able to make progress in what I believe to be an unexplored problem - interpreting human commands to generate plans robots can use in the future.</p>"},{"location":"reports/2020/behavior-trees/#references","title":"References","text":"<p>https://ieeexplore.ieee.org/abstract/document/6907656 </p> <p>https://ieeexplore.ieee.org/abstract/document/7790863</p> <p>https://ieeexplore.ieee.org/abstract/document/8319483</p> <p>https://ieeexplore.ieee.org/abstract/document/7989070 </p>"},{"location":"reports/2020/cvmaze/","title":"Computer Vision Maze Solver","text":""},{"location":"reports/2020/cvmaze/#author-zekai-wang-zhidong-liu","title":"Author: Zekai Wang &amp; Zhidong Liu","text":""},{"location":"reports/2020/cvmaze/#introduction","title":"Introduction","text":"<p>Solving a maze is an interesting sub-task for developing real-world path schedule solutions on robots. Typically, developers prefer to use depth sensors such as lidar or depth camera to walk around in a maze, as there a lots of obstacle walls that the robot need to follow and avoid crashing. Depth sensors have their advantage to directly provide the distance data, which naturally supports the robot to navigate against the obstacles.  </p> <p>However, we have come up with another idea to solve the maze using the computer vision approach. By reading articles of previous experts working on this area, we know that the RGB image cameras (CMOS or CCD) could also be used to get the depth information when they have multiple frames captured at different points of view at the same time, as long as we have the knowledge of the relative position where the frames are taken. This is basically the intuition of how human eyes work and developers have made a lot of attempts on this approach.  </p> <p>Though it could calculate the depth data by using cameras no less than 2, it is not as accurate as directly using the depth sensors to measure the environment. Also, the depth calculation and 3D reconstruction requires a lot of computational resource with multiple frames to be processed at the same time. Our hardware resource might not be able to support a real-time response for this computation. So we decide not to use multiple cameras or calculate the depth, but to extract features from a single image that allows the robot to recognize the obstacles.  </p> <p>The feature extraction from a single frame also requires several image processing steps. The features we chose to track are the lines in the image frames, as when the robot is in a maze boundary lines will could be detected as because walls and the floor could have different colors. We find the lines detected from the frames, which slopes are in certain ranges, are exactly the boundaries between the floor and walls. As a result, we tried serval approaches to extract these lines as the most important feature in the navigation and worked on optimization to let the robot perform more reliable.  </p> <p>We also implemented several other algorithms to support the robot get out of the maze, including a pid controller. They will be introduced later.</p>"},{"location":"reports/2020/cvmaze/#relevant-literature","title":"Relevant literature","text":"<p>To detect the proper lines stably, we used some traditional algorithms in the world of computer vision to process the frames step by step, including:  </p> <ol> <li> <p>Gaussian blur to smooth the image, providing more chance to detect the correct lines. https://web.njit.edu/~akansu/PAPERS/Haddad-AkansuFastGaussianBinomialFiltersIEEE-TSP-March1991.pdf https://en.wikipedia.org/wiki/Gaussian_blur  </p> </li> <li> <p>converting image from RGB color space to HSV color space https://en.wikipedia.org/wiki/HSL_and_HSV  </p> </li> <li> <p>convert the hsv image to gray and then binary image  </p> </li> <li> <p>do Canny edge detection on the binary image http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.420.3300&amp;rep=rep1&amp;type=pdf https://en.wikipedia.org/wiki/Canny_edge_detector</p> </li> <li> <p>do Hough line detection on the edges result http://www.ai.sri.com/pubs/files/tn036-duda71.pdf https://en.wikipedia.org/wiki/Hough_transform</p> </li> <li> <p>apply an slope filter on the lines to reliably find the boundary of the walls and the floor</p> </li> </ol> <p>We also implemented a pid controller for navigation. Here are some information about the pid controller: https://en.wikipedia.org/wiki/PID_controller</p>"},{"location":"reports/2020/cvmaze/#technical-descriptions-illustrations","title":"Technical descriptions, illustrations","text":"<p>The general idea has already been described in the above sections. In this section we will talk about the detail structure and implementation of the code.  </p> <p>Some of the algorithms we are using to detect the lines have implementations in the OpenCV library. For the Gaussian, HSV conversion, gray and binary conversion, Canny edge detection and Hough line detection, OpenCV provides packaged functions to easily call and use. At beginning of the project, we just simply tried these functions around in the gazebo simulation environment, with a single script running to try to detect the edge from the scene, which are the boundaries between the floor and the walls.  </p> <p>When the lines could be stably detected, we tested and adjusted in the real world. After it works properly, we tidy the code and make it a node called the line detector server. It subscribe the image from the robot's camera and always keeping to process the frames into the lines. It publishes the result as a topic where other nodes can subscribe to know about the information of the environment such as whether there is a wall or not. (As in some cases even there is a line at side, there could be no wall, we also check several pixels at the outer side of the line to determine whether there is truly a wall or not)</p> <p>e.g. left line with left wall (white brick indicates a wall): </p> <p>e.g. left line with no left wall (black brick indicates a floor): </p> <p>Then, we considered about the maze configuration and determined that a maze can have 7 different kinds of possible turns, that are:  </p> <ol> <li>crossroad</li> <li>T-road where the robot comes from the bottom of the 'T'</li> <li>T-road where the robot comes from the left</li> <li>T-road where the robot comes from the right</li> <li>left turn</li> <li>right turn</li> <li>dead end</li> </ol> <p>We then decided that it is necessary to hard code the solution of these cases and make it an action node to let the robot turn properly as soon as one of such situation is detected. The reason of using the hard-coded solution in a turn is because when the robot walks close to the wall, the light condition will change so sharply that it could not detect the edges correctly. So we let the robot to recognize the type of the turn accurately before it enter and then move forward and/or turn for some pre-defined distances and degrees to pass this turn.  </p> <p>In order to solve the problem that the pi camera's view of angle is too small, we let robot to stop before enter a turn and turn in place to look left and right for some extra angles so that it will have broad enough view to determine the situation correctly.  </p> <p>e.g. left line detected correctly </p> <p>e.g. fail to detect the left line, not enough view </p> <p>e.g. turn a bit to fix the problem </p> <p>e.g. no right line, turn and no detection, correct </p> <p>This node is called corner handle action server. When the main node detects that there is a wall (line) in the front which is so near that the robot is about to enter a turn, it will send request to this corner handler to determine the case and drive over the turn.</p> <p>In order to finally get out the maze, the robot also need a navigation algorithm, which is a part of the corner handler. We chose an easy approach which is the left-wall-follower. The robot will just simply turn left whenever it is possible, or go forward as the secondary choice, or turn right as the third choice, or turn 180 degrees backwards as the last choice. In this logic, the above 7 turn cases can be simplified into these 4 cases. For example, when the robot find there is a way to the left, it does not need to consider the right or front anymore and just simply turn left. When the actions (forward and turning) has been determined, the corner handle action server will send request to another two action servers to actually execute the forward and turning behaviors.</p> <p>So the general idea is: when the robot detects a turn in the maze by analyzing the lines, its main node will call an action to handle the case and wait until the robot has passed that turn. In other time, the robot will use the pid algorithm to try to keep at the middle of the left and right walls in the maze, going forward until reaches the next turn. We have wrote the pid controller into a service node that the main node will send its request to calculate the robot's angular twist when the robot is not handling turns,  the error is calculated by the different of intercept of left line on x = 0 and right line on x = w (w is the width of the frame image).</p> <p>In conclusion, we have this node relationship diagram </p> <p>Then the workflow of our whole picture is pretty straightforward. The below flowchart describes it: </p>"},{"location":"reports/2020/cvmaze/#discussion-of-interesting-algorithms-modules-techniques","title":"Discussion of interesting algorithms, modules, techniques","text":"<p>In this section, we briefly talk about some details of the algorithms we use.</p> <p>The Process of Canny edge detection algorithm can be broken down to 5 different steps:</p> <ol> <li> <p>Apply Gaussian filter to smooth the image in order to remove the noise</p> </li> <li> <p>Find the intensity gradients of the image</p> </li> <li> <p>Apply non-maximum suppression to get rid of spurious response to edge detection</p> </li> <li> <p>Apply double threshold to determine potential edges</p> </li> <li> <p>Track edge by hysteresis: Finalize the detection of edges by suppressing all the other edges that are weak and not connected to strong edges.</p> </li> </ol> <p>The simplest case of Hough transform is detecting straight lines, which is what we use. In general, the straight line y = mx + b can be represented as a point (b, m) in the parameter space. However, vertical lines pose a problem. They would give rise to unbounded values of the slope parameter m. Thus, for computational reasons, Duda and Hart proposed the use of the Hesse normal form</p> <pre><code>r = x cos \u2061 \u03b8 + y sin \u2061 \u03b8\n</code></pre> <p>where r is the distance from the origin to the closest point on the straight line, and \u03b8 is the angle between the x axis and the line connecting the origin with that closest point.</p> <p></p> <p>It is therefore possible to associate with each line of the image a pair ( r , \u03b8 ).The ( r , \u03b8 ) plane is sometimes referred to as Hough space for the set of straight lines in two dimensions. This representation makes the Hough transform conceptually very close to the two-dimensional Radon transform. (They can be seen as different ways of looking at the same transform.)</p> <p>Given a single point in the plane, then the set of all straight lines going through that point corresponds to a sinusoidal curve in the (r,\u03b8) plane, which is unique to that point. A set of two or more points that form a straight line will produce sinusoids which cross at the (r,\u03b8) for that line. Thus, the problem of detecting collinear points can be converted to the problem of finding concurrent curves.</p> <p>After use the Hough line algorithm to find a set of straight lines in the frame. we use a slope filter to find a single line which slope is in a particular range and is also closest to a particular reference value. For the front line, we always use the one at most bottom of the frame, which means it is the closest wall.</p> <p>Pid controller stands for a proportional, integral and derivative controller, which out put is them sum of these three terms, with each of them times a constant factor. </p> <p>In our pid implementation, the raw input of the error is the term that will times the proportional factor. The integral term is calculated by the sum of the content of a error history deque, which keeps the latest 10 error history values and sum their product with the time gap between each other (graphically the area of a trapezoid). The derivative term is calculated by the difference of the current error and the last error, divided by the difference of their time gap.</p>"},{"location":"reports/2020/cvmaze/#story-of-the-project","title":"Story of the project.","text":"<p>As above mentioned, our project is strongly relied on the accuracy of edge detection. At the beginning, we only apply Gaussian filter to remove the noise in the image then just apply double threshold to determine edges. However, it does not work well. Then we try to convert the denoised  image to a hsv image and convert this hsv image into a binary image before applying canny function. Unfortunately, it does not work every time. Sometimes because of the light and shadow issue it cannot detect the desired edges. Finally, we changed our binary image generation method by considering 3 features, hue, saturation and value, rather than only one feature hue, which turns out works much more robust.</p> <p>The algorithm to make robot walk straight without hitting the wall is also necessary. At the beginning we applied the traditional wall following algorithm, single side wall following. It only worked on some simple occasions and easily went wrong. What making this single side following algorithm worse is taking turns. Because we use the camera as our only sensor and when taking turns it might lose wall due to the perspective change. Therefore, it is hard to extend the single-side-wall-following algorithm to an all situation handle algorithm. Then we come up with an idea to detect both sides wall. To maintain the robot drives at the middle of road, we got both sides boundaries between ground and wall and maintain those 2 lines intersecting the edges of image at the same height. Another benefit from this algorithm is that the more lines we detected, the more information we get, then the more accurate reflection of surroundings. </p>"},{"location":"reports/2020/cvmaze/#github-link","title":"GitHub Link","text":"<p>cv maze</p>"},{"location":"reports/2020/cvmaze/#demo-video-links","title":"Demo video Links","text":"<p>Third view: https://drive.google.com/file/d/1lwV-g8Pd23ECZXnbm_3yocLtlppVsSnT/view?usp=sharing First view on screen: https://drive.google.com/file/d/1TGCNDxP1frjd9xORnvTLTU84wuU0UyWo/view?usp=sharing  </p>"},{"location":"reports/2020/dangersigns/","title":"Danger Signs","text":""},{"location":"reports/2020/dangersigns/#author-matthew-millendorf-jesse-yang","title":"Author: Matthew Millendorf &amp; Jesse Yang","text":""},{"location":"reports/2020/dangersigns/#section-1-introduction","title":"Section 1 - Introduction:","text":"<p>Object avoidance is of the upmost importance for autonomous robotics applications. From autonomous vehicles to distribution center robots, the ability to avoid collisions is essentially to any successful robotics system. Our inspiration for this project came from the idea that the current Campus Rover significantly lacks semantic scene understanding. One type of scenes of particular concern are those that pose immediate danger to the robot. A possible scenarios is the campus rover is delivering a package and is moving along the hallway. The floor is wet and the robot is unable to understand the meaning of the yellow wet floor sign. The robot\u2019s navigation system avoids the wet floor sign, but the radius of the water spill is greater than the width of the wet floor sign, and the robot\u2019s wheels hit the water puddle. There is a staircase adjacent to this spill and when the robot\u2019s wheels make contact with water, it spirals sideways, crashing down the stairs, breaking several of its components and the contents of the package. Our aim in this project is to provide a way for robots to avoid these scenarios through recognition of the trademark wet floor signs. The following report will go as follows. In section II, we will dive into relevant literature concerning our problem. Following that, we will detail our process and the difficulties we faced. Finally, in Section IV, we will provide a reflection of how our project went.</p> <p> </p>"},{"location":"reports/2020/dangersigns/#section-2-relevant-literature","title":"Section 2 - Relevant Literature:","text":"<p>The use of deep convolutional neural networks has enjoyed unprecedented success over the past decade in applications everywhere from facial recognition to autonomous navigation in vehicles. Thus, it seemed appropriate for our project that such techniques would be optimal. We decided our best methodology for allowing a robot to \u2018understand\u2019 a wet floor sign was to train a deep convolutional neural network on images of a yellow wet floor sign. Upon further research of this problem statement and methodology, we discovered a research paper from ETH Zurich that seemed to best fit our needs. The paper, written in 2018, details how ETH Zurich\u2019s Autonomous Formula One Racing Team computes the 3D pose of the traffic cones lining the track. Although we only have about six weeks to complete our project, we ambitiously set out to replicate their results. We would take their methodology and apply it to our wet floor sign problem. Their methodology consisted of three main phases. Phase 1 would be to perform object detection on a frame inputted from the racing car\u2019s camera and output the 2D location of the traffic cone. We specify 2D because it is important to note that detection of obstacles in an image frame consists not of two coordinates, but of three. In this first phase, the researchers from ETH Zurich were only concerned with the computation of the location of bounding boxes around the cone within the frame. The output of this model would provide a probability that the detected bounding box contains a cone and what the x- and y-coordinates of the bounding box were in the image frame. The neural network architecture employed by the researchers was the YoloV3 architecture. Additionally, the researches trained this network on 2,000 frames labelled with the bounding box coordinates around the cones. In the second phase of the paper, a neural network was trained to regress the location of key points of the cones. For instance, the input to this network was the cropped bounding box region of where a cone was in the frame. This cropped image was then labelled with \u2018key points\u2019, specific locations on the cone that the second neural network would be trained. From there, once we have at least four points on the cone, knowing the geometry of the cone and calibration of the camera, the perspective n-points can be used to find the 3D pose of the cone.</p> <p> </p>"},{"location":"reports/2020/dangersigns/#section-3-our-process","title":"Section 3 - Our Process:","text":"<p>Using the research paper as our guidelines, we set out to replicate their results. First came building the dataset that would be used for training both of our neural networks. We needed a yellow wet floor sign so we took one from a bathroom in Rabb. Over the course of two weeks, we teleoperated a Turtlebot3 around various indoor environments, driving it into the yellow wet floor sign, recording the camera data. From there, we labelled over 2,000 frames, placing bounding boxes around the yellow wet floor sign. This was by far the most tedious process we faced. From there, we needed to find a way to train a neural network to generalize on new frames. Upon further research, we came across a GitHub repository that was an implementation of the Yolov3 architecture in Keras and Python. There was quite a bit of configuration but we eventually were able to clone the repository onto our local machines and pass our data through it. However, the passing of data through our network required a significant amount of computation, and hence, we encountered our first roadblock. Needing a computer with greater computational resources, we turned to the Google Cloud Platform. We created a new account and were given a trial $300 in cloud credits. We created a virtual machine that we added two P100 GPUs. Graphic processing units are processors specifically made for rendering graphics. Although we were not rendering graphics, GPUs are used to processing tensors and thus, would perform the tensor and matrix computations at each node of the neural network with greater speed than standard central processing units. Once we trained a model to recognize the yellow wet floor signs, we turned our attention over to the infrastructure side of our project. This decision to focus on incremental steps proved to be our best decision yet as this was the aspect of our project with the most amount of bottlenecks. When trying to create our program, we encountered several errors with using Keras in the ROS environment. Notorious to debug, there were several errors within the compilation and execution of the training of the neural network that were impossible to solve after a couple weeks of figuring it out. We pivoted once again, starting from scratch and training a new implementation of the Yolov3 architecture. However, this repository was written in Pytorch, an API for deep learning that was more compatible with the ROS environment.</p> <p> </p> <p>Using Torch instead of Keras, we moved towards repositories and restarted training with our old dataset. In this particular pivot, we approximately ~500 frames from the old 2,000 frame dataset was relabeled in Pascal VOC format instead of YOLO weights format. This decision was made to make training easier on Jesse's GPU. To replicate our training of YOLO weights on a custom data set, we followed this process below:</p> <p>Firstly, decide upon an annotation style for the dataset. We initially had annotations in YOLO formats and then in Pascal VOC formats. These are illustrated below.</p>"},{"location":"reports/2020/dangersigns/#example-pascal-voc-annotation","title":"Example Pascal VOC annotation","text":"<p>Each frame should have an accompanied annotation file. Example ('atrium.jpg' and 'atrium.xml'). Pascal VOC is in XML format, and should include the path, height, width, and channels for the image. In addition, there should be the class of the image in the name section, as well as any bounding boxes (including their respective x and y).</p> <pre><code>  &lt;annotation&gt;\n    &lt;folder&gt;signs_images&lt;/folder&gt;\n    &lt;filename&gt;atriuma2.jpg&lt;/filename&gt;\n    &lt;path&gt;/Users/jesseyang/Desktop/signs_images/atriuma2.jpg&lt;/path&gt;\n    &lt;source&gt;\n        &lt;database&gt;Unknown&lt;/database&gt;\n    &lt;/source&gt;\n    &lt;size&gt;\n        &lt;width&gt;416&lt;/width&gt;\n        &lt;height&gt;416&lt;/height&gt;\n        &lt;depth&gt;3&lt;/depth&gt;\n    &lt;/size&gt;\n    &lt;segmented&gt;0&lt;/segmented&gt;\n    &lt;object&gt;\n        &lt;name&gt;sign&lt;/name&gt;\n        &lt;pose&gt;Unspecified&lt;/pose&gt;\n        &lt;truncated&gt;0&lt;/truncated&gt;\n        &lt;difficult&gt;0&lt;/difficult&gt;\n        &lt;bndbox&gt;\n            &lt;xmin&gt;232&lt;/xmin&gt;\n            &lt;ymin&gt;218&lt;/ymin&gt;\n            &lt;xmax&gt;281&lt;/xmax&gt;\n            &lt;ymax&gt;290&lt;/ymax&gt;\n        &lt;/bndbox&gt;\n    &lt;/object&gt;\n  &lt;/annotation&gt;\n</code></pre>"},{"location":"reports/2020/dangersigns/#example-yolo-annotation","title":"Example YOLO annotation","text":"<p>Similarly, each frame should have an accompanying text file. For example ('atrium.jpg' and 'atrium.txt'). These should also be in the same directory with the same name.</p> <p>In this txt file, each line of the file should represent an object which an object number, and its coordinates in the image. As seen below in the form of:</p> <p>line: object-class x y width height</p> <p>The object class should be an integer that is represented on the names file that the training script shall read from. (0 - Dog, 1 - Cat, etc).</p> <p>The x, y, width, and height, should be float values that are to the width and height of the image (these are floats between 0.0 and 1.0)</p> <pre><code>1 0.716797 0.395833 0.216406 0.147222\n0 0.687109 0.379167 0.255469 0.158333\n1 0.420312 0.395833 0.140625 0.166667\n</code></pre>"},{"location":"reports/2020/dangersigns/#training-configurations","title":"Training Configurations","text":"<p>As we were retraining weights from PJReddie's site. It is here that one can find some of the pretrained weights for different YOLO implementations and their performance in mAP and FLOPS on the COCO dataset. The COCO dataset is a popular dataset for training in object detection and is called the Common Objects in Context dataset that includes over a quarter of a million images.</p> <p>For our initial training, we used the YOLOv3-416 configuration and weights for 416 by 416 sized images, as well as the YOLOv3 tiny configuration and weights in the event of latency issues. Modifications were made in the yolov3.cfg file that we used for training for our single class inference.</p> <p>Since we are doing single class detection our data and names file looks as so:</p> <pre><code>classes=1\ntrain=data/signs/train.txt\nvalid=data/signs/val.txt\nnames=config/coco.names\nbackup=backup/\n</code></pre> <pre><code>sign\n</code></pre> <p>Fairly sparse as one can see, but easily modifiable in the event of training multiple class objects.</p> <p>Actual training was done using the train.py script in our github repo. We played around with a couple hyper-parameters, but stuck with 25 epochs training in batch sizes of 16 on the 500 Pascal VOC labeled frames. Even with only 20 epochs, training on Jesse's slow GPU took approximately 20 hours.</p> <p>Unfortunately, valuable data on our loss, recall, and precision were lost as laptop used to train unexpectedly died overnight during the training process. Lack of foresight resulted in not being able to collect these metrics during the training as they were to be conglomerated after every epoch finished. Luckily, the weights of 19 epochs were recovered in checkpoints however their performance had to be manually tested.</p>"},{"location":"reports/2020/dangersigns/#prediction","title":"Prediction","text":"<p>Inference via the turtlebot3s are done via a ROS node on a laptop that subscribes to the turtlebot's rpi camera, does some minor image augmentations and then saves the image to a 'views' directory. From this directory, a YOLO runner program is checking for recently modified files, then subsequently performs inference, draws bounding box predictions on said files and then writes to a text file with results as well as the modified frame.</p> <p>The rational behind reading and writing files instead of predicting on ROS nodes from the camera is a result of struggles throughout the semester to successfully integrate Tensorflow graphs onto the ROS system. We ran into numerous session problems with Keras implementations, and ultimately decided on moving nearly all of the processing from the turtlebot3 into the accompanying laptop. This allows us to (nearly) get inference real time given frames from the turtlebot3.</p> <p> </p> <p>An alternate way of doing inference, but in this case doing object detection with items with the COCO dataset rather than our custom dataset can be run via our yolo_runner.py node directly on ROS. This is an approach that does not utilize read/write and instead prints predictions and bounding boxes to the screen. However, with this approach drawing to the screen was cut due to how it lagged the process when attempting to perform inference on the turtlebot3's frames.</p>"},{"location":"reports/2020/dangersigns/#section-4-reflection","title":"Section 4 - Reflection","text":"<p>This project was a strong lesson in the risks of attempting a large project in a short time span that combines two complex fields within computer science (Robotics and Machine Learning) whilst learning much of it on the go. It was quite humbling to be very ambitious with our project goals and fall short of many of the \"exciting\" parts of the project. However, it was also a pleasure to have the freedom to work on and learn continuously while attempting to deploy models and demos to some fairly cutting edge problems. Overall, this made the experience a learning experience that resulted in not only many hours of head-banging and lost progress but also a deeper appreciation for the application of computer vision in robotics and the challenges of those who helped pave the way for our own small success.</p> <p>The two largest challenges in the project was data collection for our custom dataset and integration with ROS. That is, creating and annotating a custom dataset took longer than expected and we ran into trouble with training where our images were perhaps not distinct enough/did not cover enough environments for our weights to make sufficient progress. For integration, we ran into issues between python2 and python3, problems with integrating Keras with ROS, and other integration issues that greatly slowed the pace of the project. This made us realize that in the future, not to discount the work required for integration across platforms and frameworks.</p> <p>Link to github repo</p> <p>References: * PJReddie for Darknet Weights * Erik Lindernoren's PyTorch YOLOv3 * Qqwweee's Keras YOLOv3 * Cfotache PyTorch Object Detection</p>"},{"location":"reports/2020/pathplanning/","title":"Path Planning Scenarios","text":""},{"location":"reports/2020/pathplanning/#github-link-httpsgithubcomcampusroverpathfinding","title":"Github Link: https://github.com/campusrover/PathFinding","text":""},{"location":"reports/2020/pathplanning/#author-daniel-suh","title":"Author: Daniel Suh","text":""},{"location":"reports/2020/pathplanning/#robot-path-finding-using-a-star-algorithm-wave-front-algorithm-and-reinforcement-learning","title":"Robot Path Finding using A Star Algorithm, Wave front Algorithm, and Reinforcement Learning","text":"<p>The current society considers robots as a crucial element. It is because they have substituted human beings concerning the daily functions, which could be dangerous or essential. Therefore, the designation of an effective technique of navigation for robots that can achieve mobility as well as making sure that they are secure is the crucial considerations related to autonomous robotics. The report, therefore, addresses the pathfinding in robots using A-Star Algorithm, Wavefront Algorithm, as well as using Reinforcement Learning. The primary aim of path planning in robotics is to establish safe paths for the robots that can achieve mobility. The preparation of routes in robotics should achieve optimality. These algorithms target the development of strategies that are intended to provide solutions to problems like the ability of a robot to detect the distance between objects that causes obstructions, techniques of evading the obstacles, as well as how to achieve mobility between two elements of blockage in the shortest possible path. Moreover, the project is meant to help in coming up with the shortest route, if possible, that the Robot can use to reach the destination.</p> <p>When carrying out the project, there were a lot of resources, even though some were not directly addressing the issues, and I was able to get numerous information that helped in connecting the points and getting the right idea. Apart from the readily available internet sources, I as well-referred to several related papers, Professor Sutton's book, and Professor David Silver's lecture.</p>"},{"location":"reports/2020/pathplanning/#what-was-created","title":"What was created?","text":"<p>A two-dimensional environment where the robot training was to be undertaken was created. The atmosphere was made in such a way that for the Robot to finds its path from location A to B, there are several obstacles, destructions, and walls that would hinder it from getting to the destination. The result would be to identify how the Robot would meander through the barriers to reach the desired position. A robot sensor was used to help the robot to perceive the environment. To determine the best way of path determination, the following criteria were utilized, A star Algorithm and Wavefront Algorithm and Reinforcement Learning.</p> <p></p>"},{"location":"reports/2020/pathplanning/#a-star-algorithm","title":"A Star Algorithm","text":"<p>The technique is a crucial way of traversal of graphs and pathfinding. It has brains, unlike other traversal strategies and algorithms. The method can be efficiently adopted into ensuring that the shortest path is efficiently established. It is regarded as one of the most successful search algorithms, which are the best to be used to find the most concise way in the graphs and notes. The Algorithm uses heuristics to find the solution paths as well as the information concerning the path cost to come up with the best and shortest route. In this project, the use of the A-star Algorithm would be necessary for that it would help in the determination of the shortest path that can be used by the Robot to reach the destination. In working places or any other environment, we always look for the shortest routes which we can use to reach the target. The Robot was designed using this Algorithm, which helps in the productive interaction with the environment to find out the best paths that can be used to reach the destination. An example of an equation that shows this form algorithm is f(n) = g(n) + h(n) in which f(n) is the lowest cost in the node, g(n) is the exact cost of the path and h(n) the heuristic estimated cost from a node.</p> <p>Path MAP </p>"},{"location":"reports/2020/pathplanning/#wave-front-algorithm","title":"Wave front Algorithm","text":"<p>To make it more efficient and more tactical, the wavefront algorithm was utilized to help in determining the correct path that can be used by the Robot to reach the destination effectively. This type of Algorithm is regarded as a cell decomposition path method. When the workplace paths are divided into equal segments, getting the right paths can always be an issue; however, using this Algorithm will help to come up with the correct way that can easily use by the Robot in reaching the destination. The method uses the breadth-first search based on the position in which the target is. The nodes in this method are increasingly assigned values from the target note. At sometimes, the modified wavefront algorithm would be suggested in future research to come up with the best estimate of the path that is needed to be followed. The improved version of the system as well uses the notes; nevertheless, it provides for the most accurate results. In this project, for the remote censored Robot to find the correct path to reach the target faster, the wavefront algorithm was utilized.</p> <p>In the allocation of the nodes, the following equation holds</p> <p>Map (i,j) = {min (neighborhood (i j)) + 4 [empty diagonal cell]/ min (neighborhood (i j)) + 3 other empty  cell obstacle cell]</p> <p>The equation in shows the allocation of the nodes with reference to the coordinates (i,j). The algorithm thus provides a solution if it exists incompleteness and provides the best solution.</p> <p>Path MAP </p>"},{"location":"reports/2020/pathplanning/#deep-learning-reinforcement-in-path-findings","title":"Deep Learning Reinforcement in Path Findings","text":"<p>To ensure that the Robot is adequately trained on the environment to determine the best path that can quickly and faster lead to the target, a deep reinforcement algorithm will be necessary. The reinforcement algorithm will, therefore, be the component that will majorly control the Robot in the environment. For instance, the Algorithm will be planted on the Robot to help in avoiding the obstructions such that when the Robot is about to hit an obstacle, it will quickly sense the obstacle and change the direction but leading to the target. Moreover, deep learning with help the Robot to get used to the environment of operation. It is said that the Robot is as well able to master the path through the deep learning inclusions. Still and moving obstacles can only be determined and evaded by the Robot through the use of deep learning, which can help the Robot to recognize the objects. In general, one way to make the Robot to find the path of operating effectively and to work in the environment effectively is to involve deep learning in this operation. To improve the artificial intelligence of the system, a deep learning algorithm will be of importance. </p> <p>\ud835\udc44\u02c6(\ud835\udc60, \ud835\udc4e) = (1 \u2212 \ud835\udefc\ud835\udc61). \ud835\udc44\u02c6(\ud835\udc60, \ud835\udc4e) + \ud835\udefc\ud835\udc61(\ud835\udc5f + \ud835\udefe max \ud835\udc4e \u2032 \ud835\udc44\u02c6(\ud835\udc60 \u2032 , \ud835\udc4e\u2032 ))</p> <p></p> <p></p> <p>Figure portrays the single target fortification learning system. Where the specialist, spoke to by an oval, is in the state (s) at time step (t). The operator plays out the activity-dependent on the Q-esteem. The earth gets this activity, and accordingly, it restores the following states' to the operator and a comparing reward (r) for making a move (a). </p> <p>Path MAP </p>"},{"location":"reports/2020/pathplanning/#training-result-difference-between-learning-rate","title":"Training Result Difference between Learning Rate","text":"<p>Learning Rate = 0.0001, Batch Size = 200 </p>"},{"location":"reports/2020/pathplanning/#best-model","title":"Best Model","text":"<p>Learning Rate = 0.0002, Batch Size = 200  </p> <p>Learning Rate = 0.0003, Batch Size = 200 </p> <p>Learning Rate = 0.0004, Batch Size = 200 </p>"},{"location":"reports/2020/pathplanning/#double-q-learning","title":"Double q learning","text":"<p>In most cases, in a system which is a bit complex, the deep reinforcement learning may not be the best option but, one can decide to go by the deep double q learning which is embedded by Q-matrix which is being advanced to help in finding the paths in very complicated places. Moreover, when the Robot is to be operated in a large environment, it may take time for the deep reinforcement learning to help in getting the correct codes; thus, the use of double Q learning should be an option. It is in this manner recommended that in the next projects, there can be the use of this system to get more harmonized results.</p> <p>\ud835\udc49 \ud835\udf0b (\ud835\udc60) = \ud835\udc38{ \u2211\ufe01\u221e \ud835\udc58=0 \ud835\udefe \ud835\udc58 \ud835\udc5f\ud835\udc61+\ud835\udc58+1|\ud835\udc60\ud835\udc61 = \ud835\udc60}</p> <p>As opposed to Q learning, the Double Q learning equation is vectorial functions.</p> <p></p> <p>The figure portrays the multi-objective reinforcements support learning structure. Like the single goal RL system, in MORL structure, the operator is spoken to by an oval and is in the state (s) at time step (t). The specialist makes a move dependent on the accessible Q-values. The earth gets this activity, and accordingly, it restores the following state s' to the specialist.</p>"},{"location":"reports/2020/pathplanning/#process-problem","title":"Process &amp; Problem","text":"<p>In the early days, there were many concerns about how to proceed with this project. I wanted to study how to find something new rather than simply applying an existing path algorithm to the robot. I also considered a model called GAN, but decided to implement reinforcement learning, realizing that it was impossible to generate as many path images as to train GAN. The biggest problem with implementing reinforcement learning was computing power. Reinforcement learning caused the computer's memory to soar to 100 percent and crash. I tried and ran several options, but to no avail, I decided to reinstall Ubuntu. After the reinstallation, I don't know why, but it's no longer to a computer crash due to a sharp rise in memory. Currently, even with a lot of repetitive learning, it does not exceed 10% of memory.</p> <p>Another problem with reinforcement learning was the difficulty in adjusting hyperparameters. Since reinforcement learning is a model that is highly influenced by hyperparameters, it was necessary to make fine adjustments to maximize performance. Initially, the code did not include grid search, but added a grid search function to compare hyperparameters. It took about six hours to get around 500 epochs, so I didn't experiment with a huge variety of hyperparameters. However, after three consecutive weeks of training, I found the best performing hyperparameters.</p> <p>The third problem with reinforcement learning was that the results of learning were not very satisfactory. According to various papers, existing Deep Q Learning can be overestimate because same Q is used. Therefore, although it has not been fully applied to the code yet, I will apply Double Q Learning and Dueling Network in the near future.</p>"},{"location":"reports/2020/pathplanning/#conclusion","title":"Conclusion","text":"<p>In conclusion, it gets imperative to note the issue of path findings in the intelligence machine has been long in existence. Nevertheless, through these methods, the correct paths can be gotten, which is short but can help the Robot in reaching the target.</p>"},{"location":"reports/2020/pathplanning/#references","title":"References","text":"<p>Mnih, Volodymyr, et al. \"Human-level control through deep reinforcement learning.\"\u00a0Nature\u00a0518.7540 (2015): 529.</p> <p>Mnih, Volodymyr, et al. \"Playing atari with deep reinforcement learning.\"\u00a0arXiv preprint arXiv:1312.5602\u00a0(2013). https://arxiv.org/abs/1312.5602</p> <p>Nuin, Yue Leire Erro, et al. \"ROS2Learn: a reinforcement learning framework for ROS 2.\"\u00a0arXiv preprint arXiv:1903.06282\u00a0(2019). https://arxiv.org/abs/1903.06282</p> <p>Van Hasselt, Hado, Arthur Guez, and David Silver. \"Deep reinforcement learning with double q-learning.\"\u00a0Thirtieth AAAI conference on artificial intelligence. 2016. https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/viewPaper/12389</p>"},{"location":"reports/2020/reinforcement-learning-racer/","title":"Reinforcement Learning Racer","text":""},{"location":"reports/2020/reinforcement-learning-racer/#author-luis-andino-ben-ballintyn","title":"Author: Luis Andino &amp; Ben Ballintyn","text":"<p>### Introduction</p> <p>In this project, we proposed to use deep learning methods to accomplish two tasks. 1. To navigate the robot throughout a racecourse using LIDAR data only and 2. To recognize certain signs (stop, go) and act accordingly. For the first objective, we implemented 2 deep reinforcement learning algorithms (Deep Deterministic Policy Gradient and Soft Actor-Critic) to try and learn to map LIDAR sensor data to appropriate actions (linear and angular velocities). We intended to train these algorithms on multiple simulated race tracks in the Gazebo simulation environment and if possible deploy the trained network on a real-world robot. For the second objective, we adapted existing computer vision models to recognize hazard signs like stop, yield, go, and caution. The last of our original objectives was to implement a facial recognition algorithm to work with the TurtleBot 3 architecture and hardware constraints.</p>"},{"location":"reports/2020/reinforcement-learning-racer/#relevant-literature","title":"Relevant literature","text":"<p>Deep reinforcement learning is a family of techniques that utilizes deep neural networks to approximate either a state-action value function (mapping state-action pairs to values) or a state directly to action probabilities (known as policy-gradient methods). In this project, we implemented and tested two Deep RL algorithms. The first is the DDPG agent first proposed by DeepMind [^1]. To help in understanding and implementing this algorithm we also found a very helpful blog post [2] and associated PyTorch implementation of the DDPG agent. The second algorithm implemented was the Soft Actor-Critic agent proposed by Haarnoja et al. from the University of California Berkeley [3]. Again, a blog post was very helpful in understanding the agent and provided a PyTorch implementation [4].</p> <p>The computer vision components, there was a mixture of GitHub repositories, Open-CV tutorials and a blog post posted on analyticsvidhya, written by Aman Goel. The analyticsvidhya blog post helped me to understand the facial_recognition library in python and how to properly implement it[5]. The GitHub repository that was used as a reference for the proof of concept implementation that was done on Mac OS was written by Adam Geitgey, who is the author of the facial recognition python library[6]. His Github repository along with his README helped me understand how the algorithm works and how to latter modify it within an ROS environment.</p> <p>There was an assortment of ROS wiki articles used to troubleshoot and understand the errors that were occurring with OpenCV reading the data that was coming from the TurtleBot 3. The wiki articles lead me to the cv_bridege ROS package that can convert a ROS compressed image topic to an OpenCV format[7].</p>"},{"location":"reports/2020/reinforcement-learning-racer/#technical-descriptions-illustrations","title":"Technical descriptions, illustrations","text":"<p>For the driving component of this project, the first step was to create a racetrack to simulate the robot on. To do this, I used Gazebo\u2019s building editor to create a racecourse out of straight wall pieces (there are no default curved pieces in Gazebo). Creating the course (seen in Figure 1) involved saving a .sdf file from gazebo inside the turtlebot3_gazebo/models folder, a .worlds file in turtlebot3_gazebo/world, and a launch file in turtlebot3_gazebo/launch. The initial goal was to train the agent to drive counter-clockwise around the track as fast as possible. To do this, I implemented 5 types of nodes to carry out the different parts of the canonical RL loop (Figure 2) and manually chose 31 points around the track as \u201ccheckpoints\u201d to monitor the robot\u2019s progress within a training episode. How close the agent is to the next checkpoint is monitored from timestep to timestep and is used to compute the rewards used to train the agent.</p> <p></p> <p>This tracking of the robot\u2019s ground truth position in the track is done by a node called GazeboListener.py which subscribes to Gazebo\u2019s  /gazebo/model_states topic. This node also requested reward values from a service called RewardServer.py every 100ms and published them to a topic called /current_reward. These rewards are the critical instructive component of the canonical RL loop as they instruct the agent as to which actions are good or bad in which situations. For this project, the \u201cstate\u201d was the average of the past 5 LIDAR scans. This averaging was done in order to smooth out the relatively noisy LIDAR readings. The LIDAR smoothing was implemented by a node called scanAverager.py which subscribed to /scan and published to /averaged_scan. </p> <p></p> <p>To choose the actions, I made minor alterations to the PyTorch implementations of the DDPG or SAC agents given by [2,4]. This involved wrapping PyTorch neural network code in class files myRLclasses/models.py and myRLclasses/SACmodels.py. I then wrapped the logic for simulating these agents in the Gazebo track in a ROS node called RLMaster_multistart.py which handled action selection, sending cmd_vel commands, storing experience in the agent\u2019s replay buffer, and resetting the robot\u2019s position when a collision with a wall was detected (end of an episode). These collisions actually proved difficult to detect given the minimum range of the robot\u2019s LIDAR (which is larger than the distance to the edge of the robot). Therefore I said there was a collision if 1) the minimum distance in the LIDAR scan was &lt; .024M, 2) the actual velocity of the robot differed significantly (above a threshold) then the velocity sent to the robot (which in practice happened a lot), and 3) the actual velocity of the robot was less than .05M/s. This combination of criteria, in general, provided relatively robust collision detection and was implemented in a node called CollisionDetector.py.</p> <p>Having implemented the necessary nodes, I then began training the robot to drive. My initial approach was to have every training episode start at the beginning of the track (having the episode ends when the robot crashed into a wall). However, I found that this caused the robot to overfit this first corner and be unable to learn to make the appropriate turn after this corner. Therefore, I switched to a strategy where the robot would start at a randomly chosen checkpoint for 10 consecutive episodes and then a different randomly chosen checkpoint would be used for the next 10 episodes and so on. This allowed the robot to gain experience with the whole track even if it struggled on particular parts of it. Every timestep, the current \u201cstate\u201d (/averaged_scan) was fed into the \u201cactor\u201d network of the DDPG agent which would emit a 2D action (linear velocity and angular velocity). I would then clip the action values to be in the allowable range (-.22 - .22 for linear velocity and -.5 to .5 for angular velocity). Then, noise generated by an Ornstein-Uhlenbeck process would be added to these clipped actions. This added noise provides the exploration for the DDPG model. For the SAC agent, no noise is added to the actions as the SAC agent explicitly is designed to learn a stochastic policy (as opposed to the deep deterministic policy gradient). This noisy action would then be sent as cmd_vel commands to the robot. The subsequent timestep, a new state (scan) would be observed and the resulting reward from the prior timestep\u2019s action would be computed. Then the prior scan, the chosen action, the subsequent scan, and the resulting reward would be pushed as an \u201cexperience tuple\u201d into the DDPG agent\u2019s memory replay buffer which is implemented as a deque. This replay buffer (stored in myRLclasses/utils.py) holds up to 500,000 such tuples to provide a large body of data for training. At the end of each timestep, a single batch update to all of the agent\u2019s networks would be carried out, usually with a batch_size of 1024 experience tuples. Additionally, every 50 episodes I would do 1000 network updates and then save the agent as a .pkl file.</p> <p>Finally, I wrote a node to allow a human to control the robot using teleop controls and save the experience in the agent\u2019s replay buffer. This was in the hope that giving the agent experience generated by a human would accelerate the learning process. This was implemented by a node called StoreHumanData.py. </p> <p></p> <p>The facial recognition aspect of the RL Racer project took different python techniques and converted and optimized it to work with ROS. There is no launch file for this program. To run this program, you must first have the TurtleBot 3 and have brought up the camera software and have your facial_ recognition as the current directory in terminal. The command to run the code is as follows: $rosrun facial_recognition find_face.py</p> <p>To do this we had to fundamentally rethink the program structure and methods for it to work. The first thing we had to contend with was the file structure while running the program through ROS, which you can see in figure 4 and figure 5. This was just a little quirk while working with ROS as compared to Python. While running the program you have to be directly in the Facial_recognition folder, anywhere else would result in a \u201cfile not found\u201d error while trying to access the images. The Alternative to having to be in the folder for this to work, was to have all the images in the same \u201cscripts\u201d folder, which made for messy \u201cScripts\u201d and made it confusing while adding new photos.</p> <p> </p> <p>The steps for adding a photo are as simple as adding to the \u201cface_pcitures\u201d directory as shown figure 6. The image should have a clear view of the face you want the algorithm to recognize. The image does not have to be a giant picture of just a face.  After you add an image to the file path you have to package you must call \u201cload_image_file\u201d, which will load the image. The next step is to encode the loaded image and find the face in the image. To do this you must all the \u201cface_encodings\u201d method in the facial_recognition package and pass the image you have loaded as an argument.  After the images are added and encoded you add the facial encoding to a list and add the names to a list of names of type String of the people in the list in order. Since Luis is the first face that was added to the prior list, then it will be the first name in the list of names. This is the implementation of the node version of facial recognition that constantly publishes the names of faces that it finds, but there is also a service implementation.</p> <p> </p> <p>The major difference between the service implementation is the segmentation of the image callback and image processing as you can see in figure 7.  We keep track of the frame in a global variable called frame that can be later accessed. We also need the name of the person to be passed and we only process the next 20 frames and if we don\u2019t see the person requested, we will send no. This also is reinforced with a suggested control request implementation.</p> <p> </p> <p>The color recognition and control node came from a desire to define hazards. There was also a need to still follow other rules while training the neural net and to keep other factors in mind. This uses the basics of OpenCV to create color masks that you can put a specific RGB value you want to pick up. To run the program, you need to run python files in two separate windows. The first command is $ rosrun facial_ recognition racecar_signal.py The second command is: $ rosrun faical_recognition red_light_green_light.py. The racecar_signal node does all the image processing while the red_light_green_light node is a state machine that determines what to send over cmd_vel.</p> <p>To add color ranges that you want to pick up you have to add the RGB ranges as NumPy arrays as seen in figure 8. After you manually add the RGB range or individual value you want to detect, you must create a find colors object based on the ranges. In this case, the ROS video image comes from the HSV variable. That was set whole converting the ROS Image to an OpenCV format.</p> <p> </p>"},{"location":"reports/2020/reinforcement-learning-racer/#discussion-of-interesting-algorithms-modules-techniques","title":"Discussion of interesting algorithms, modules, techniques","text":"<p>The two deep RL algorithms tried were the Deep Deterministic Policy Gradient (DDPG) and Soft Actor-Critic (SAC) agents. The major difference between these methods is that the DDPG is deterministic, that is it outputs a single action every timestep, whereas the SAC agent outputs the parameters of a probability distribution from which an action is drawn. In addition, the DDPG agent is trained simply to maximize reward, whereas the SAC agent also tries to maximize the \u201centropy\u201d of its policy. Simply, it balances maximizing reward with a predefined amount of exploration. For the first few weeks of the project, I exclusively tried training the DDPG agent by playing around with different hyperparameters such as noise magnitude and batch size as well as tuning the collision detection. I found that the agent was able to make significant progress and at one point completed two full laps of the track. However, after leaving the agent to train overnight, I returned to find that its performance had almost completely deteriorated and was mostly moving backward, clearly a terrible strategy. After doing a little more reading on the DDPG I found that it is an inherently unstable algorithm in that it is not guaranteed to continually improve. This prompted me to look for an alternative deep RL algorithm that was not so brittle. This search led me to the SAC algorithm which was specifically said to be more robust and sample efficient. I again implemented this agent with help from a blog post and started tuning parameters and found that it took longer (almost 3500 episodes) to show any significant learning however it seems to be maintaining its learning better. I am unsure what the problem with my current training method is for the DDPG agent and suspect the problem may just be due to the brittleness of the algorithm. One possible change that could help both agents would be to average over the LIDAR readings (within one scan) to reduce the dimensionality of the input (currently 360 inputs) to the networks which should help reduce learning complexity at the cost of lower spatial resolution. Finally, an interesting behavior produced by both robots was that during straight parts of the course, they learned a strategy of alternating between full left and full right turn instead of no turn at all. It will be interesting to see if this strategy gets worked out over time or is an artifact of the simulation environment </p> <p>The facial_recognition package comes with a pre-built model that has a  99.38% accuracy rate and built-in methods for facial encoding and facial comparison[8]. The algorithm takes the facial encodings from loaded images that can be manually added a given script and uses landmarks to encode the pictures face. Using OpenCV we take the frames from a given camera, the algorithm finds the faces in the frame, encodes them, then compares the captured data facial encodings. One of the main challenges that I faced while trying to optimize the algorithm further resulted in failure because I could not figure out how to increase the processing time of facial recognition. This was mostly to do with my computer, but there was also a concern of wifi image loss or just slow wifi. Since ROS compressed image is just a published topic published, which relies on wifi. I got around this by manually dropping frames. I tried ROS each way at first limiting the queue and adding sleep functions, but it causes some problems when trying to implement this with other nodes. These methods would just throughout the frames it would still wait within a queue and still have to be processed. To get around these limitations I hardcoded a counter to only take 1 every 6 frames that come in and process it. It is not ideal but it works. The service implementation does something similar as I only want to a total of 24 processed frames, but still following the 1 out of 6 frame method. In total even though I am being sent (6*24) =144, I am only processing 24 frames, but this only occurs when the desired face is not in the frame.</p> <p>The sign detection of RL racer was supposed to feature real signs detection, but We ran into a problem finding models of American sign or even when we did we could not get it to run. This is why we went the color route and because we could not train a model in time. The control nodes and the color detection nodes all work, but we were never able to implement it into the RL racer because it took longer than expected to train racer. The theory of color detection is that we can mimic signs by using specific RGB values that we want the robot to pick up. So, Green would be Go, Red would be stopped, and Yellow would be a cushion. Obviously, this is to mimic traffic signals.</p>"},{"location":"reports/2020/reinforcement-learning-racer/#story-of-the-project","title":"Story of the project.","text":"<p>This project was very interesting and a lot of fun to work on, even though we had to pivot a few times in order to have a finished product. However, we still learned a lot and worked well together always talking, even though we had two distinct parts we wanted to get done in order to come together. Unfortunately, we could not bring everything together that we wanted we still accomplished a lot.</p> <p>We were disappointed to learn just how unstable the DDPG agent was after initial success but are hopeful that the now operational SAC agent will be able to accomplish the task with more training.</p> <p>The problem that my facial recognition aspect has is that you have to be within 3 feet for an algorithm to reliably tell who you are or if there is a face on screen. I tried a few things with resizing the image or even try to send uncompressed image topics, but it did not work. An interesting solution that we tried was if we can tell that there was a face there, but now who it was we could take the bounding box of the unknown face, cut it out because we knew where it was on the image, then zoom in. This approach did not work because the image was too low quality. </p> <p>We both did a good job of accomplishing what we wanted to, even though we had to pivot a few times. We believe that the code we implemented could be used in further projects. The deep RL code could be used to learn to drive around pre-gazeboed maps with some adaptation and the general method could be applied anywhere continuous control is needed. The facial recognition algorithm can be easily added to the code base of Campus Rover for package delivery authentication and any other facial recognition applications. </p>"},{"location":"reports/2020/reinforcement-learning-racer/#github-link","title":"GitHub Link","text":"<p>Reinforcement Learning Racer on the Campus Rover Git Hub: </p> <p>Reinforcement Learning Racer: </p> <p></p>"},{"location":"reports/2020/reinforcement-learning-racer/#bibliography","title":"Bibliography","text":"<p>[1]: Lillicrap et al., 2015. Continuous control with deep reinforcement learning https://arxiv.org/pdf/1509.02971.pdf</p> <p>[2]: Chris Yoon. Deep Deterministic Policy Gradients Explained. https://towardsdatascience.com/deep-deterministic-policy-gradients-explained-2d94655a9b7b</p> <p>[3]: Haarnoja et al., 2018. Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor. https://arxiv.org/abs/1801.01290 </p> <p>[4]: Vaishak V. Kumar. Soft Actor-Critic Demystified. https://towardsdatascience.com/soft-actor-critic-demystified-b8427df61665</p> <p>[5]: Blog, Guest. \u201cA Simple Introduction to Facial Recognition (with Python Codes).\u201d Analytics Vidhya, May 7, 2019. https://www.analyticsvidhya.com/blog/2018/08/a-simple-introduction-to-facial-recognition-with-python-codes/.</p> <p>[6]: Geitgey, Adam, Facial Recognition, GitHub Repository https://github.com/ageitgey/face_recognition </p> <p>[7]: \u201cWiki.\u201d ros.org. Accessed December 13, 2019. http://wiki.ros.org/cv_bridge/Tutorials/UsingCvBridgeToConvertBetweenROSImagesAndOpenCVImages.</p> <p>[8]: PyPI. (2019). face_recognition. [online] Available at: https://pypi.org/project/face_recognition/ [Accessed 16 Dec. 2019].</p>"},{"location":"reports/2020/stalkerbot/","title":"Stalkerbot","text":""},{"location":"reports/2020/stalkerbot/#cosi-119a-autonomous-robotics","title":"Cosi 119a - Autonomous Robotics","text":""},{"location":"reports/2020/stalkerbot/#team-stalkerbot","title":"Team: Stalkerbot","text":""},{"location":"reports/2020/stalkerbot/#team-members-danbing-chen-maxwell-hunsinger","title":"Team members: Danbing Chen, Maxwell Hunsinger","text":""},{"location":"reports/2020/stalkerbot/#github-repository-httpsgithubcomcampusroverstalkerbot","title":"Github Repository: https://github.com/campusrover/stalkerbot","text":""},{"location":"reports/2020/stalkerbot/#final-report","title":"Final Report","text":""},{"location":"reports/2020/stalkerbot/#introduction","title":"Introduction","text":""},{"location":"reports/2020/stalkerbot/#background","title":"Background","text":"<p>In the recent years, the pace of development of autonomous robotics has quickened, due to the increase in demand for robots which perform actions in the absence of human control and minimal human surveillance. The robots are through the years more capable of performing an ever-various range of tasks, and one such task, which has become more prevalence in the capabilities of modern robots, is the ability to follow a person. Robots which are capable of following people are widely deployed and used for caretaking and assistance purposes.</p> <p>Although the premise of a person-follow algorithm is simple, in practice the algorithm must consider several complexities and challenges, some of which depend on the general purpose of such said robots, and some of which are due to the inherent complication of the practice of robotics.</p>"},{"location":"reports/2020/stalkerbot/#overview","title":"Overview","text":"<p>Stalkerbot is an algorithm which detects and follows the targeted individual until the robot has reached a certain distance within the target, which then it will stop. Although the algorithm has the capability to detect and avoid obstacles, it does not operate well in a spatially crowded environment where the room for robotic maneuver is limited. The algorithm works well only in an environment with enough light exposure; hence it will not function properly or at all in the darkness.</p> <p>One of the most basic and perhaps fundamental challenge of such algorithm is the recognition of the targeted person. The ability of the algorithm to quickly and accurately identify and focus on the target is crucial, as it is closely linked to most of the other parts of the algorithm, which may include path finding, obstacle avoidance, motion detect, etc.</p>"},{"location":"reports/2020/stalkerbot/#algorithm-design","title":"Algorithm Design","text":""},{"location":"reports/2020/stalkerbot/#external-libraries-and-dependencies","title":"External Libraries and Dependencies","text":"<p>Although there are sophisticated methods of recognizing the potential target, including 3D Point Clouds and convoluted computer vision algorithms, our project will only be using a rather simple library in rospy called the Aruco Detect, which is a fiducial marker detection library which can detect and identify specific markers using a camera and return the pose and transformation of the fiducial. We chose to use the library because it is simple to implement and does not involve sophisticated and heavy computation such as machine learning, with, of course, the downside of the algorithm\u2019s having dependency on fiducial marker as well as the library.</p> <p>We will be using SLAM for local mapping of the robot, which takes in lidar data and generate the map of its immediate surroundings. We will also be using ros_navigation package, or more specifically, the move_base module in the package to facilitate the path finding algorithm of Stalkerbot. It is important to note that these two packages make up most of the computational consumption of the algorithm, as the rest of the generic scripts written in Python are standard, light-weight ROS nodes.</p>"},{"location":"reports/2020/stalkerbot/#hardware-specification","title":"Hardware Specification","text":"<p>Sensors are the foundation of every robotics application, they are to bridge the gap between the digital world and the real world, they give perception to robots and allow them to respond to the changes in the real world, therefore it is not surprising that the other defining characteristics of a path finding algorithm is its sensors.</p> <p>The sensors which we use for our algorithm are the lidar and the raspberry-pi camera. Camera will be used to detect the target via the aforementioned Aruco Detect library. We will support the detection algorithm with sensor data from the lidar which is equipped on the Turtlebot. These data will be used to map the surroundings of the robot in real time and play an important part in the path planning algorithm, which will enable the robot to detect and avoid obstacles on the path to the target.</p> <p>On the Turtlebot3, the camera will be installed in the front at an inclination of about 35 degrees upwards and the lidar is installed at around 10cm above the ground level. These premises are important factors in the consideration of the design of the algorithm, and they often ease or pose limitations on the design space of the algorithm. In addition, the Turtlebot runs on Raspberry-pi 3 board. The board is adequate for running simple programming scripts; however, they are not fit to run computationally heavy script such as Aruco Detect or SLAM, therefore, these programs have to be run remotely and the data will have to be communicated to the robots wirelessly.</p>"},{"location":"reports/2020/stalkerbot/#algorithm","title":"Algorithm","text":"<p>The very first design choice which we must consider is this: Where does the robot go? The question might sound simple, or even silly, because the goal of the algorithm is to follow a person, so the target of the robot must be the person which it follows. However, it is important to remember that the robot does not reach the target, instead it follows the target up to a certain distance behind the target, therefore a second likely choice for which where the robot should aim to move to is the point which is at said distance behind the target.</p> <p>The two different approaches translate to two very different behaviors of the robots, for example, a direct following approach reacts minimally to the target\u2019s sudden change in rotation, whereas the indirect following approach reacts more drastically to it. It should be said that under different situations one might be better than the other. If the target simply rotates but does not turn, for example, then by using the direct approach the robot would stand still, however, if the target intends to rotate, then the indirect approach helps more adequately to better prepare for the change of course preemptively than the direct approach.</p> <p>We have chosen the direct approach in the development of Stalkerbot, because even though the indirect approach is often the more advantageous approach in many situations, it has a severe drawback that is associated with how the camera is mounted on the robot. For example, when the target turns, the robot must move side-ways, and since its wheels are not omnidirectional, but bidirectional, the robot must turn to move to the appropriate spot, and in term lose sight of the target in the process, because the robot can only detect what is in front of it, since the camera is mounted directly ahead of the robot, and no additional camera is mounted on the robot. The continuous update of the state of the target is crucial in the algorithm, and the lack of the information on the target is unjustifiable, hence the indirect approach is not considered.</p> <p>Fiducial markers can be identified by their unique barcode-like patterns, and each is assigned an identification number. We only want to identify specific fiducial markers. The reason which we want to do this is this: there are other projects, including the campus_rover_3, who are using the same library as we do, and we do not wish to confuse the robots with the fiducials from other projects. All the accepted fiducials are stored as their identification number in an array in the configuration file. The node fiducial_filter filters out unwanted fiducial markers and republishes those that match against the fiducial markers whose identification number is in the configuration file.</p> <p>It is important to ensure that the robot follows the target in a smooth manner. However, the detection capability with the camera which is mounted on the robot is not perfect. Although Aruco Detect can detect the fiducial markers, it is awfully inconsistent, and the detection rate drops significantly with distance, with the marker becoming almost impossible to detect at any distance farther than 2.5 meters. At normal range, it is expected that the algorithm will miss certain frames. To ensure the smoothness of the robot\u2019s motor movement, a buffer time is introduced to the follow algorithm to take the momentary loss of information on the state of the target into account. If the robot is unable to detect any fiducial marker within this buffer duration, it will keep moving as if the target is still at the place when it is last detected, however when the robot is still unable to detect any markers after that grace period, the robot will stop. We have found that 200 milliseconds to be an appropriate time, with which the robot will run smoothly even if the detection algorithm does not.</p> <p>These algorithms are contained in the advanced_follow node, which is also where all of the motion algorithms lie. This node is responsible for how the robot reacts to the change in the sensor data, therefore, this node can be seen as the core of the person-following algorithm. The details of this node will be explained further later in this section.</p> <p>In order to compare the time elapsed since the robot has last recognized a fiducial marker and the current time, we have created a ROS node called fiducial_interval, which measures the time since the last fiducial marker is detected. Note that when the algorithm first starts, and no fiducial is detected, the node will publish nothing. Only when the first fiducial marker is detected, then the node will be activated and start to publish the duration.</p> <p>Aruco Detect publishes the information on the fiducial markers which the robot detects, however, to extract and filter the important data (pose, mainly), and to filter the unwanted data (timestamp, id, fiducial size, error rate, etc), we have created a custom node called location_pub, which subscribes to fiducial_filter and publishes only the important information which will be useful for the Stalkerbot algorithm.</p> <p>One of the biggest components of Stalkerbot is its communication with the ros_tf package. The package is a powerful library that aligns the locations and orientations of different objects that have coordinates on different coordinate frames. In Stalkerbot, we feed information of the relationship between two frames into ros_tf and we also extract those data from it. We require this because it is otherwise difficult to understand and calculate the pose of an object from, for example, the robot, when the data is gathered by the camera, which uses a different coordinate system, or frame, as the robot.</p> <p>The node location_pub_tf publishes to ros_tf the pose of an object as perceived by another object. In the node, two different relationships are transmitted to ros_tf. Firstly, the node broadcasts a static pose of the camera from the robot. The pose is static because the camera is mounted on the robot and the pose between them will not change during the execution and runtime of Stalkerbot. Because it is a static broadcast, the pose is sent to ros_tf once, at the beginning of the algorithm, via a special broadcast object that specifies this broadcast is static. Furthermore, the node will, at a certain frequency, listen to the topic /stalkerbot/fiducial/transform and broadcast the pose of the target with respective to the frame of the camera to ros_tf. Note that, just like the node fiducial_interval, nothing will be broadcasted before the detection of the first fiducial marker.</p> <p>Like mentioned before, not only do we broadcast the coordinates to ros_tf, we also request them from it as well. In fact, the only place where we request for any coordinates between two objects is in the advanced_follow node. The advanced follow node uses, additionally, the move_base module from ros_nav, however, to send a goal to the module requires the algorithm to have the coordinate frame between the frame map and target. Therefore, at a certain frequency, the node requests the pose between map and target from ros_tf. When the robot reaches the destination and the target is not in motion for a certain number of seconds, the follow algorithm will shut itself down. The detection of motion is contained in the motion_detect node. This is an algorithm just as complicated as the follow algorithm. It listens to the ros_tf transform between the robot and the target and calculate the distance from the pose of the current frame to the last known frame. However, most of the logic issued in this node is to enable the algorithm to recognize motion and the lack of motion in the presence of sensory noise. It uses average mean and a fluctuating distance threshold level to determine whether the object is moving at any given moment.</p> <p>The node also implements a lingering algorithm which has a counter which counts down when the rest of the algorithm returns false and reverts the response to positive. When the counter reaches zero, it is no longer able to revert any negative response. The counter is reset when a fiducial marker is detected. This is again a method to combat the inconsistency of Aruco Detect, you can think of the counter as the minimal number of times the algorithm will return true. However, during the development stage, this method is not used anymore. The counter is specified to 0 according to the configuration file. Similar to fiducial_filter, it also has its corresponding interval node, motion_interval, which returns the time since the last time when the target is in motion.</p> <p>Lastly, Stalkerbot also implements a special node called sigint_catcher, in which the node only activates when the program is terminated via KeyboardInterrupt. The only purpose of this node is to send an empty cmd_vel to roscore, thereby stopping the robot.</p>"},{"location":"reports/2020/stalkerbot/#story-of-the-project","title":"Story of the Project","text":"<p>The inspiration for this project is a crowdfunding project that was initiated by a company called \"Cowarobot\", which aims to create the world's first auto-following suitcase/luggage. This project is a scaled-down version of this project. Initially, we had great ambitions for the project; we had plans to use facial recognition and machine learning to recognize the target which the robot follows, however, it is soon clear to us that given our expertise, the time which we have, and the scale of the problem, we have to scale down the ambition of the project as well. At the end of the project proposal period, we have made a very conservative model for our project \u2013 A robot follows a person wearing the fiducial marker in an open, obstacle-free environment, and we are glad to say that we have achieved this very basic goal and some more.</p> <p>Over the course of the few weeks which we spent working on Stalkerbot, there arose numerous challenges, most of them were the result of our lack of deep understanding of ROS and robotic hardware. The two biggest challenge were the following: The inconsistency of the camera and Aruco Detect was our first big obstacle. We followed the lab instruction on how to enable the camera on the robot, however, for our project, a custom exposure mode is needed to improve the performance of the camera, and it took us some time to figure out how the camera should exactly be correctly configurated. However, over the course of the entire development of Stalkerbot, Aruco Detect sometimes cannot pick up on the fiducial markers. Sometimes, it is unable to detect any markers altogether at certain specific camera settings.</p> <p>The second challenge is the ros_tf library, and in general how orientation and pose function in the 3D world. At the beginning of the algorithm development, we did not have to deal with poses and orientations, but when we had to deal with them, we realized the poses and orientations we thought were aligned were all over the place. It turned out, that the axes used by Aruco Detect and ros_tf were different. The x axis in ros_tf is the y axis in Aruco Detect, for example. In addition, communicating with ros_tf requires a good understanding of the package as well as spatial orientation. There are many pitfalls, such as mixing the target frame and the child frame, mixing up the map frame and the odom frame.</p> <p>Over the course of the development, Danbing worked on the basic follow algorithm, fiducial_filter, fiducial_interval, motion_detect, detect_interval, and sigint_catcher. Max worked on location_pub, move_base_follow and the integration of move_base_follow to make advanced_follow.</p>"},{"location":"reports/2020/gesture-recognition/","title":"Hand Gesture Recognition","text":"<p>This semester I mainly worked on the hand gesture recognition feature. Honestly I spent most of the time learning new concepts and techniques because of my ignorance in this area. I tried several approaches (most of them failed) and I will briefly review them in terms of their robustness and reliability.</p>"},{"location":"reports/2020/gesture-recognition/color/","title":"Color Extraction","text":"<p>As said before, we need a reliable way to extract hands. The first approach I tried was to extract hands by colors: we can wear gloves with striking colors (green, blue and etc) and desired hand regions can be obtained by doing color filtering. This approach sometimes worked fine but easily got distracted by subtle changes in lighting at most of the time. In a word, it was not robust enough. After speaking to Prof. Hong, I realized this approached had been tried by hundreds of people decades ago and will never work. Code can be found here.</p> <p></p>"},{"location":"reports/2020/gesture-recognition/demo/","title":"Hand Gesture Recognition","text":""},{"location":"reports/2020/gesture-recognition/demo/#dependencies-overview","title":"Dependencies Overview","text":"<ul> <li>OpenCV</li> <li>ROS Kinetic (Python 2.7 is required)</li> <li>A Camera connected to your device</li> </ul>"},{"location":"reports/2020/gesture-recognition/demo/#how-to-run","title":"How to Run","text":"<ol> <li> <p>Copy this package into your workspace and run <code>catkin_make</code>.</p> </li> <li> <p>Simply Run <code>roslaunch gesture_teleop teleop.launch</code>. A window showing real time video from your laptop webcam will be activated. Place your hand into the region of interest (the green box) and your robot will take actions based on the number of fingers you show.</p> </li> <li>Two fingers: Drive forward</li> <li>Three fingers: Turn left</li> <li>Four fingers: Turn right</li> <li>Other: Stop</li> </ol> <p></p> <p></p> <p></p> <p></p>"},{"location":"reports/2020/gesture-recognition/demo/#brief-explaination-towards-the-package","title":"Brief Explaination towards the package","text":"<p>This package contains two nodes.</p> <ol> <li> <p><code>detect.py</code>: Recognize the number of fingers from webcam and publish a topic of type <code>String</code> stating the number of fingers. I won't get into details of the hand-gesture recognition algorithm. Basically, it extracts the hand in the region of insteret by background substraction and compute features to recognize the number of fingers.</p> </li> <li> <p><code>teleop.py</code>: Subscribe to <code>detect.py</code> and take actions based on the number of fingers seen.</p> </li> </ol> <pre><code>while not rospy.is_shutdown():\n   twist = Twist()\n   if fingers == '2':\n      twist.linear.x = 0.6\n      rospy.loginfo(\"driving forward\")\n   elif fingers == '3':\n      twist.angular.z = 1\n      rospy.loginfo(\"turning left\")\n   elif fingers =='4':\n      twist.angular.z = 1\n      rospy.loginfo(\"turning right\")\n   else:\n      rospy.loginfo(\"stoped\")\n      cmd_vel_pub.publish(twist)\n   rate.sleep()\n</code></pre>"},{"location":"reports/2020/gesture-recognition/demo/#later-plan","title":"Later Plan","text":"<ol> <li>Using Kinect on mutant instead of local webcam.</li> <li>Furthermore, use depth camera to extract hand to get better quality images</li> <li>Incorporate Skeleton tracking into this package to better localize hands (I am using region of insterests to localize hands, which is a bit dumb).</li> </ol>"},{"location":"reports/2020/gesture-recognition/gestures/","title":"Gestures","text":""},{"location":"reports/2020/gesture-recognition/gestures/#author-tirtho-aungon-and-jade-garisch","title":"Author: Tirtho Aungon and Jade Garisch","text":""},{"location":"reports/2020/gesture-recognition/gestures/#part-1-kinect-aka-libfreenect2","title":"Part 1: Kinect aka libfreenect2","text":""},{"location":"reports/2020/gesture-recognition/gestures/#intro","title":"Intro","text":"<p>In Kinect.md, the previous generations dicussed the prospects and limitations of using a Kinect camera. We attempted to use the new Kinect camera v2, which was released in 2014. </p> <p> </p> <p>Thus, we used the libfreenect2 package to download all the appropiate files to get the raw image output on our Windows. The following link includes instructions on how to install it all properly onto a Linux OS. </p> <p>https://github.com/OpenKinect/libfreenect2</p>"},{"location":"reports/2020/gesture-recognition/gestures/#issues","title":"Issues","text":"<p>We ran into a lot of issues whilst trying to install the drivers, and it took about two weeks to even get the libfreenect2 drivers to work. The driver is able to support RGB image transfer, IR and depth image transfer, and registration of RGB and depth images. Here were some essential steps in debugging, and recommendations if you have the ideal hardware set up: </p> <ul> <li>Even though it says optional, I say download OpenCL, under the \"Other\" option to correspond to Ubuntu 18.04+ </li> <li>If your PC has a Nvidia GPU, even better, I think that's the main reason I got libfreenect to work on my laptop as I had a GPU that was powerful enough to support depth processing (which was one of the main issues) </li> <li>Be sure to install CUDA for your Nvidia GPU </li> <li>Install OpenNI2 if possible </li> <li>Make sure you build in the right location </li> </ul> <p>Please look through this for common errors: </p> <p>https://github.com/OpenKinect/libfreenect2/wiki/Troubleshooting </p> <p>Although we got libfreenect2 to work and got the classifier model to locally work, we were unable to connect the two together. What this meant is that although we could use already saved PNGs that we found via a Kaggle database (that our pre-trained model used) and have the ML model process those gestures, we could not get the live, raw input of depth images from the kinect camera. We kept running into errors, especially an import error that could not read the freenect module. I think it is a solvable bug if there was time to explore it, so I also believe it should continued to be looked at. </p> <p>However, also fair warning that it is difficult to mount on the campus rover, so I would just be aware of all the drawbacks with the kinect before choosing that as the primary hardware. </p> <p>### Database  </p> <p>https://www.kaggle.com/gti-upm/leapgestrecog/data </p>"},{"location":"reports/2020/gesture-recognition/gestures/#machine-learning-model","title":"Machine Learning model","text":"<p>https://github.com/filipefborba/HandRecognition/blob/master/project3/project3.ipynb</p> <ul> <li>What this model predicts: Predicted Thumb Down Predicted Palm (H), Predicted L, Predicted Fist (H), Predicted Fist (V), Predicted Thumbs up, Predicted Index, Predicted OK, Predicted Palm (V), Predicted C</li> </ul>"},{"location":"reports/2020/gesture-recognition/gestures/#part-2-leap-motion-alternative-approach-semi-successful-one","title":"Part 2: Leap Motion: Alternative approach (semi-successful one)","text":""},{"location":"reports/2020/gesture-recognition/gestures/#intro_1","title":"Intro","text":"<p>As a very last minute and spontaneous approach, we decided to use a Leap Motion device. Leap Motion uses an Orion SDK, two IR camerad and three infared LEDs. This is able to generate a roughly hemispherical area where the motions are tracked.</p> <p></p> <p></p> <p>It has a smaller observation area dn higher resolution of the device that differentiates the product from using a Kinect (which is more of whole body tracking in a large space). This localized apparatus makes it easier to just look for a hand and track those movements. </p> <p>The set up is relatively simple and just involved downloading for the appropriate OS. In this case, Linux (x86 for a 32 bit Ubuntu system).  </p> <p>## Steps to downloading Leap Motion and getting it started:</p> <ol> <li>download the SDK from https://www.leapmotion.com/setup/linux; you can extract this package and you will find two DEB files that can be installed on Ubuntu.</li> <li> <p>Open Terminal on the extracted location and install the DEB file using the following command (for 64-bit PCs):</p> <p>$ sudo dpkg -install Leap-*-x64.deb </p> <p>If you are installing it on a 32-bit PC, you can use the following command:</p> <p>sudo dpkg -install Leap-*-x86.deb</p> </li> <li> <p>plug in leap motion and type dmesg in terminal to see if it is detected</p> </li> <li> <p>clone ros drivers:   </p> <p>$ git clone https://github.com/ros-drivers/leap_motion</p> </li> <li> <p>edit .bashrc:</p> <p>export LEAP_SDK=$LEAP_SDK:$HOME/LeapSDK</p> <p>export PYTHONPATH=$PYTHONPATH:$HOME/LeapSDK/lib:$HOME/LeapSDK/lib/x64 6. save bashrc and restart terminal then run:</p> <p>sudo cp $LeapSDK/lib/x86/libLeap.so /usr/local/lib</p> <p>sudo ldconfig</p> <p>catkin_make install --pkg leap_motion</p> </li> <li> <p>to test run:</p> <p>sudo leapd</p> <p>roslaunch leap_motion sensor_sender.launch</p> <p>rostopic list</p> </li> </ol> <p>Once having Leap Motion installed, we were able to simulate it on RViz. We decided to program our own motion controls based on angular and linear parameters (looking at directional and normal vectors that leap motion senses): </p> <p></p> <p>This is what the Leap Motion sees (the raw info):</p> <p></p> <p></p> <p>In the second image above, the x y and z parameters indicate where the leap motion detects a hand (pictured in the first photo)</p> <p>This is how the hand gestures looked relative to the robot's motion: </p>"},{"location":"reports/2020/gesture-recognition/gestures/#stationary","title":"Stationary","text":""},{"location":"reports/2020/gesture-recognition/gestures/#forward","title":"Forward","text":""},{"location":"reports/2020/gesture-recognition/gestures/#backward","title":"Backward","text":""},{"location":"reports/2020/gesture-recognition/gestures/#left-rotation","title":"Left Rotation","text":""},{"location":"reports/2020/gesture-recognition/gestures/#right-rotation","title":"Right Rotation","text":""},{"location":"reports/2020/gesture-recognition/gestures/#conclusion","title":"Conclusion","text":"<p>So, we got the Leap Motion to successfully work and are able to have the robot follow our two designated motion. We could have done many more if we had discovered this solution earlier. One important thing to note is that at this moment we are not able to mount the Leap Motion onto the physical robot as LeapMotion is not supported by the Raspberry Pi (amd64). If we are able to obtain an Atomic Pi, this project should be able to be furthered explored. Leap Motion is a very powerful and accurate piece of technology that was much easier to work with than the Kinect, but I advise still exploring both options. </p>"},{"location":"reports/2020/gesture-recognition/kinect/","title":"Attempts to using Kinect libraries","text":"<p>This still seems the best approach (if it ever worked). The Kinect camera was developed, at the first place,for human body gesture recognition and I could not think of any reason not using it but to reinvent wheels. However, since the library interfacing Kinect and ROS never worked and contains little documentation ( meaning it is very difficult to debug), I spent many sleepless nights and still wasn't able to get it working.However, for the next generation, I will still strongly recommend you give it a try.</p> <p>ROS BY EXAMPLE V1 INDIGO: Chapter 10.9 contains detailed instructions on how to get the package openni_tracker working but make sure you install required drivers listed in chapter 10.3.1 before you start.</p>"},{"location":"reports/2020/gesture-recognition/kinectv2/","title":"Kinect: libfreenect2","text":""},{"location":"reports/2020/gesture-recognition/kinectv2/#intro","title":"Intro","text":"<p>In Kinect.md, the previous generations dicussed the prospects and limitations of using a Kinect camera. We attempted to use the new Kinect camera v2, which was released in 2014. </p> <p> </p> <p>Thus, we used the libfreenect2 package to download all the appropiate files to get the raw image output on our Windows. The following link includes instructions on how to install it all properly onto a Linux OS. </p> <p>https://github.com/OpenKinect/libfreenect2</p>"},{"location":"reports/2020/gesture-recognition/kinectv2/#issues","title":"Issues","text":"<p>We ran into a lot of issues whilst trying to install the drivers, and it took about two weeks to even get the libfreenect2 drivers to work. The driver is able to support RGB image transfer, IR and depth image transfer, and registration of RGB and depth images. Here were some essential steps in debugging, and recommendations if you have the ideal hardware set up: </p> <ul> <li>Even though it says optional, I say download OpenCL, under the \"Other\" option to correspond to Ubuntu 18.04+ </li> <li>If your PC has a Nvidia GPU, even better, I think that's the main reason I got libfreenect to work on my laptop as I had a GPU that was powerful enough to support depth processing (which was one of the main issues) </li> <li>Be sure to install CUDA for your Nvidia GPU </li> <li>Install OpenNI2 if possible </li> <li>Make sure you build in the right location </li> </ul> <p>Please look through this for common errors: </p> <p>https://github.com/OpenKinect/libfreenect2/wiki/Troubleshooting </p> <p>Although we got libfreenect2 to work and got the classifier model to locally work, we were unable to connect the two together. What this meant is that although we could use already saved PNGs that we found via a Kaggle database (that our pre-trained model used) and have the ML model process those gestures, we could not get the live, raw input of depth images from the kinect camera. We kept running into errors, especially an import error that could not read the freenect module. I think it is a solvable bug if there was time to explore it, so I also believe it should continued to be looked at. </p> <p>However, also fair warning that it is difficult to mount on the campus rover, so I would just be aware of all the drawbacks with the kinect before choosing that as the primary hardware. </p> <p>### Database  </p> <p>https://www.kaggle.com/gti-upm/leapgestrecog/data </p>"},{"location":"reports/2020/gesture-recognition/kinectv2/#machine-learning-model","title":"Machine Learning model","text":"<p>https://github.com/filipefborba/HandRecognition/blob/master/project3/project3.ipynb</p> <ul> <li>What this model predicts: Predicted Thumb Down Predicted Palm (H), Predicted L, Predicted Fist (H), Predicted Fist (V), Predicted Thumbs up, Predicted Index, Predicted OK, Predicted Palm (V), Predicted C</li> </ul>"},{"location":"reports/2020/gesture-recognition/kinectv2/#github-repo","title":"GITHUB REPO","text":"<p>https://github.com/campusrover/gesture_recognition</p>"},{"location":"reports/2020/gesture-recognition/leap-motion/","title":"Leap Motion: Alternative approach (successful one)","text":""},{"location":"reports/2020/gesture-recognition/leap-motion/#intro","title":"Intro","text":"<p>As a very last minute and spontaneous approach, we decided to use a Leap Motion device. Leap Motion uses an Orion SDK, two IR camerad and three infared LEDs. This is able to generate a roughly hemispherical area where the motions are tracked.</p> <p></p> <p></p> <p>It has a smaller observation area dn higher resolution of the device that differentiates the product from using a Kinect (which is more of whole body tracking in a large space). This localized apparatus makes it easier to just look for a hand and track those movements.</p> <p>The set up is relatively simple and just involved downloading for the appropriate OS. In this case, Linux (x86 for a 32 bit Ubuntu system).  </p>"},{"location":"reports/2020/gesture-recognition/leap-motion/#steps-to-downloading-leap-motion-and-getting-it-started","title":"Steps to downloading Leap Motion and getting it started","text":""},{"location":"reports/2020/gesture-recognition/leap-motion/#link-if-needed","title":"Link if needed","text":"<ul> <li> <p>here</p> </li> <li> <p>download the SDK from this link; you can extract this package and you will find two DEB files that can be installed on Ubuntu.</p> </li> <li> <p>Open Terminal on the extracted location and install the DEB file using the following command (for 64-bit PCs):</p> <pre><code>    sudo dpkg -install Leap-*-x64.deb\n</code></pre> <p>If you are installing it on a 32-bit PC, you can use the following command:</p> <pre><code>    sudo dpkg -install Leap-*-x86.deb\n</code></pre> </li> <li> <p>plug in leap motion and type dmesg in terminal to see if it is detected</p> </li> <li> <p>clone ros drivers:</p> <pre><code>    git clone https://github.com/ros-drivers/leap_motion\n</code></pre> </li> <li> <p>edit .bashrc:</p> <pre><code>    export LEAP_SDK=$LEAP_SDK:$HOME/LeapSDK\n    export PYTHONPATH=$PYTHONPATH:$HOME/LeapSDK/lib:$HOME/LeapSDK/lib/x64\n</code></pre> </li> <li> <p>save bashrc and restart terminal then run:</p> <pre><code>    sudo cp $LeapSDK/lib/x86/libLeap.so /usr/local/lib\n    sudo ldconfig\n    catkin_make install --pkg leap_motion\n</code></pre> </li> <li> <p>to test run:</p> <pre><code>    sudo leapd\n    roslaunch leap_motion sensor_sender.launch\n    rostopic list\n</code></pre> </li> </ul> <p>Once having Leap Motion installed, we were able to simulate it on RViz. We decided to program our own motion controls based on angular and linear parameters (looking at directional and normal vectors that leap motion senses):</p> <p></p> <p>This is what the Leap Motion sees (the raw info):</p> <p></p> <p></p> <p>In the second image above, the x y and z parameters indicate where the leap motion detects a hand (pictured in the first photo)</p> <p>This is how the hand gestures looked relative to the robot's motion:</p>"},{"location":"reports/2020/gesture-recognition/leap-motion/#stationary","title":"Stationary","text":""},{"location":"reports/2020/gesture-recognition/leap-motion/#forward","title":"Forward","text":""},{"location":"reports/2020/gesture-recognition/leap-motion/#backward","title":"Backward","text":""},{"location":"reports/2020/gesture-recognition/leap-motion/#left-rotation","title":"Left Rotation","text":""},{"location":"reports/2020/gesture-recognition/leap-motion/#right-rotation","title":"Right Rotation","text":""},{"location":"reports/2020/gesture-recognition/leap-motion/#rqt-graph","title":"RQT Graph","text":""},{"location":"reports/2020/gesture-recognition/leap-motion/#conclusion","title":"Conclusion","text":"<p>So, we got the Leap Motion to successfully work and are able to have the robot follow our two designated motion. We could have done many more if we had discovered this solution earlier. One important thing to note is that at this moment we are not able to mount the Leap Motion onto the physical robot as LeapMotion is not supported by the Raspberry Pi (amd64). If we are able to obtain an Atomic Pi, this project should be able to be furthered explored. Leap Motion is a very powerful and accurate piece of technology that was much easier to work with than the Kinect, but I advise still exploring both options.</p>"},{"location":"reports/2020/gesture-recognition/leap_motion/","title":"Leap Motion: Alternative approach (semi-successful one)","text":""},{"location":"reports/2020/gesture-recognition/leap_motion/#intro","title":"Intro","text":"<p>As a very last minute and spontaneous approach, we decided to use a Leap Motion device. Leap Motion uses an Orion SDK, two IR camerad and three infared LEDs. This is able to generate a roughly hemispherical area where the motions are tracked.</p> <p></p> <p></p> <p>It has a smaller observation area dn higher resolution of the device that differentiates the product from using a Kinect (which is more of whole body tracking in a large space). This localized apparatus makes it easier to just look for a hand and track those movements. </p> <p>The set up is relatively simple and just involved downloading for the appropriate OS. In this case, Linux (x86 for a 32 bit Ubuntu system).  </p> <p>## Steps to downloading Leap Motion and getting it started:</p> <ol> <li>download the SDK from https://www.leapmotion.com/setup/linux; you can extract this package and you will find two DEB files that can be installed on Ubuntu.</li> <li> <p>Open Terminal on the extracted location and install the DEB file using the following command (for 64-bit PCs):</p> <p>$ sudo dpkg -install Leap-*-x64.deb </p> <p>If you are installing it on a 32-bit PC, you can use the following command:</p> <p>sudo dpkg -install Leap-*-x86.deb</p> </li> <li> <p>plug in leap motion and type dmesg in terminal to see if it is detected</p> </li> <li> <p>clone ros drivers:   </p> <p>$ git clone https://github.com/ros-drivers/leap_motion</p> </li> <li> <p>edit .bashrc:</p> <p>export LEAP_SDK=$LEAP_SDK:$HOME/LeapSDK</p> <p>export PYTHONPATH=$PYTHONPATH:$HOME/LeapSDK/lib:$HOME/LeapSDK/lib/x64 6. save bashrc and restart terminal then run:</p> <p>sudo cp $LeapSDK/lib/x86/libLeap.so /usr/local/lib</p> <p>sudo ldconfig</p> <p>catkin_make install --pkg leap_motion</p> </li> <li> <p>to test run:</p> <p>sudo leapd</p> <p>roslaunch leap_motion sensor_sender.launch</p> <p>rostopic list</p> </li> </ol> <p>Once having Leap Motion installed, we were able to simulate it on RViz. We decided to program our own motion controls based on angular and linear parameters (looking at directional and normal vectors that leap motion senses): </p> <p></p> <p>This is what the Leap Motion sees (the raw info):</p> <p></p> <p></p> <p>In the second image above, the x y and z parameters indicate where the leap motion detects a hand (pictured in the first photo)</p> <p>This is how the hand gestures looked relative to the robot's motion: </p>"},{"location":"reports/2020/gesture-recognition/leap_motion/#stationary","title":"Stationary","text":""},{"location":"reports/2020/gesture-recognition/leap_motion/#forward","title":"Forward","text":""},{"location":"reports/2020/gesture-recognition/leap_motion/#backward","title":"Backward","text":""},{"location":"reports/2020/gesture-recognition/leap_motion/#left-rotation","title":"Left Rotation","text":""},{"location":"reports/2020/gesture-recognition/leap_motion/#right-rotation","title":"Right Rotation","text":""},{"location":"reports/2020/gesture-recognition/leap_motion/#conclusion","title":"Conclusion","text":"<p>So, we got the Leap Motion to successfully work and are able to have the robot follow our two designated motion. We could have done many more if we had discovered this solution earlier. One important thing to note is that at this moment we are not able to mount the Leap Motion onto the physical robot as LeapMotion is not supported by the Raspberry Pi (amd64). If we are able to obtain an Atomic Pi, this project should be able to be furthered explored. Leap Motion is a very powerful and accurate piece of technology that was much easier to work with than the Kinect, but I advise still exploring both options. </p>"},{"location":"reports/2020/gesture-recognition/leap_motion/#git-hub-repo","title":"GIT HUB REPO","text":"<p>https://github.com/campusrover/gesture_recognition</p>"},{"location":"reports/2020/gesture-recognition/local-camera/","title":"Explicit Features Approach (counting fingers)","text":"<p>s### Hand Gesture Recognition After reaching the dead-end in the previous approach and being inspired by several successful projects (on Github and other personal tech blogs), I implemented an explict-feature-driven hand recognition algorithm. It relies on background extraction to \"extract\" hands (giving gray scale images), and based on which compute features to recognize the number of fingers. It worked pretty well if the camera and the background are ABSOLUTELY stationary but it isn't the case in our project: as the camera is mounted on the robot and the robot keeps moving (meaning the background keeps changing). Code can be founded here</p>"},{"location":"reports/2020/gesture-recognition/local-camera/#references","title":"References","text":"<p>Opencv python hand gesture recognition</p> <p>Hand Tracking And Gesture Detection</p>"},{"location":"reports/2020/gesture-recognition/local-camera/#dependencies-overview","title":"Dependencies Overview","text":"<ul> <li>OpenCV</li> <li>ROS Kinetic (Python 2.7 is required)</li> <li>A Camera connected to your device</li> </ul>"},{"location":"reports/2020/gesture-recognition/local-camera/#how-to-run","title":"How to Run","text":"<ol> <li>Copy this package into your workspace and run <code>catkin_make</code>.</li> <li>Simply Run <code>roslaunch gesture_teleop teleop.launch</code>. A window showing real time video from your laptop webcam will be activated. Place your hand into the region of interest (the green box) and your robot will take actions based on the number of fingers you show. 1. Two fingers: Drive forward 2. Three fingers: Turn left 3. Four fingers: Turn right 4. Other: Stop</li> </ol>"},{"location":"reports/2020/gesture-recognition/local-camera/#brief-explaination-towards-the-package","title":"Brief Explaination towards the package","text":"<p>This package contains two nodes.</p> <ol> <li><code>detect.py</code>: Recognize the number of fingers from webcam and publish a topic of type <code>String</code> stating the number of fingers. I won't get into details of the hand-gesture recognition algorithm. Basically, it extracts the hand in the region of insteret by background substraction and compute features to recognize the number of fingers.</li> <li><code>teleop.py</code>: Subscribe to <code>detect.py</code> and take actions based on the number of fingers seen.</li> </ol> <pre><code>while not rospy.is_shutdown():\n twist = Twist()\n if fingers == '2':\n     twist.linear.x = 0.6\n     rospy.loginfo(\"driving forward\")\n elif fingers == '3':\n     twist.angular.z = 1\n     rospy.loginfo(\"turning left\")\n elif fingers =='4':\n     twist.angular.z = 1\n     rospy.loginfo(\"turning right\")\n else:\n     rospy.loginfo(\"stoped\")\n cmd_vel_pub.publish(twist)\n rate.sleep()\n</code></pre>"},{"location":"reports/2020/gesture-recognition/local-camera/#later-plan","title":"Later Plan","text":"<ol> <li>Using Kinect on mutant instead of local webcam.</li> <li>Furthermore, use depth camera to extract hand to get better quality images</li> <li>Incorporate Skeleton tracking into this package to better localize hands (I am using region of insterests to localize hands, which is a bit dumb).</li> </ol>"},{"location":"reports/2020/gesture-recognition/ssd/","title":"SSD model (deep learning)","text":""},{"location":"reports/2020/gesture-recognition/ssd/#deep-learning","title":"Deep learning","text":"<p>The final approach I took was deep learning. I used a pre-trained SSD (Single Shot multibox Detector) model to recongnize hands. The model was trained on the Egohands Dataset, which contains 4800 hand-labeled JPEG files (720x1280px). Code can be found here</p> <p>Raw images from the robot\u2019s camera are sent to another local device where the SSD model can be applied to recognize hands within those images. A filtering method was also applied to only recognize hands that are close enough to the camera.</p> <p>After processing raw images from the robot, a message (hands recognized or not) will be sent to the State Manager. The robot will take corresponding actions based on messages received.</p>"},{"location":"reports/2020/gesture-recognition/ssd/#references","title":"References","text":"<p>Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy,Scott E. Reed, Cheng-Yang Fu, and Alexander C. Berg. SSD: single shot multibox detector. CoRR, abs/1512.02325, 2015. Victor Dibia, Real-time Hand-Detection using Neural Networks (SSD) on Tensorflow, (2017), GitHub repository, https://github.com/victordibia/handtracking</p>"},{"location":"reports/2022/NASCAR-style-turtlebot-racing/","title":"NASCAR TurtleBot Racing Series","text":"<p>Spring 2022 Brendon Lu, Joshua Liu</p>"},{"location":"reports/2022/NASCAR-style-turtlebot-racing/#introduction","title":"Introduction","text":""},{"location":"reports/2022/NASCAR-style-turtlebot-racing/#problem-statement-including-original-objectives","title":"Problem statement, including original objectives","text":"<p>As old as the field of robotics itself, robot competitions serve as a means of testing and assessing the hardware, software, in-between relationships, and efficiencies of robots. Our group was inspired by previous robot-racing related projects that navigated a robot through a track in an optimal time, as well as the challenge of coordinating multiple robots. Subsequently, our project aims to simulate a NASCAR-style race in order to determine and test the competitive racing capabilities of TurtleBots. Our project\u2019s original objectives were to limit-test the movement capabilities of TurtleBots, implement a system that would allow for the collaboration of multiple robots, and present it in an entertaining and interactive manner. We managed to accomplish most of our goals, having encountered several problems along the way, which are described below. As of the writing of this report, the current iteration of our project resembles a NASCAR time-trial qualifier, in which a TurtleBot moves at varying speed levels depending on user input through a gui controller, which publishes speed data to our \u2018driver\u2019 node. For our driver node, we\u2019ve adapted the prrexamples OpenCV algorithm in order to \u2018drive\u2019 a TurtleBot along a track, explained below, which works in conjunction with a basic PID controller in order to safely navigate the track.</p>"},{"location":"reports/2022/NASCAR-style-turtlebot-racing/#relevant-literature","title":"Relevant literature","text":"<p>Resizing an image w/ openCV.  openCV resizing interpolation.</p>"},{"location":"reports/2022/NASCAR-style-turtlebot-racing/#what-was-created-biggest-section","title":"What was created (biggest section)","text":""},{"location":"reports/2022/NASCAR-style-turtlebot-racing/#technical-descriptions-illustrations","title":"Technical descriptions, illustrations","text":"<p>Below is the general setup of each robot\u2019s \u201cnode tree.\u201d These are all running on one roscore; the names of these nodes are different per robot. Green nodes are the ones belonging to the project, whereas blue nodes represent nodes initialized by the Turtlebot. The interconnecting lines are topics depicting publisher/subscriber relationships between different nodes. The \u201cdriver\u201d in the robot is the Line Follower node. This is what considers the many variables in the world (the track, pit wall (controller), car status, and more) and turns that into on-track movement. To support this functionality, there are some nodes and functions which help the driver.</p> <p></p>"},{"location":"reports/2022/NASCAR-style-turtlebot-racing/#image-processor","title":"Image processor","text":"<p>This is a function inside the line follower node which we felt was better to inline. Its workings are explained in exhausting detail in the next section. The main bit is that it produces an image with a dot on it. If the dot is on the left, the driver knows the center of the track is on the left.</p>"},{"location":"reports/2022/NASCAR-style-turtlebot-racing/#lap-counter","title":"Lap Counter","text":"<p>This node is another subscriber to the camera. It notifies a car whenever it has crossed the finish line. This is accomplished, again, with the masking function of OpenCV. It is very simple but obviously necessary. This also required us to put a bit of tape with a distinct color (green in our case) to use as a finish line.</p>"},{"location":"reports/2022/NASCAR-style-turtlebot-racing/#overtake","title":"Overtake","text":"<p>For a robot to pass another on track, it has to drive around it. The first and final design we are going to use for this involves LiDAR. A robot knows when one is close enough to pass when it sees the scanner of another robot close enough in front of it. It then knows it can go back when it sees the passed robot far enough behind it, and also sees that the track to its side is clear.</p>"},{"location":"reports/2022/NASCAR-style-turtlebot-racing/#gui","title":"GUI","text":"<p>The GUI serves as the driver\u2019s controller, allowing the driver to accelerate or decelerate the car, while also showing information about current speed and current heat levels. In combination with the GUI, we also provide the driver with a window showing the Turtlebot\u2019s view from the raspicam, in order to simulate an actual driving experience.</p>"},{"location":"reports/2022/NASCAR-style-turtlebot-racing/#the-track","title":"The track","text":"<p>Track construction was made specifically with the game design in mind. The track is made of two circles of tape: blue and tan in an oval-sh configuration. In the qualifying race, the cars drive on the tan line. In the actual race, the cars primarily drive on the blue line, which is faster. Cars will switch to the outer tan track to pass a slower one in the blue line, however. The intent behind some of the track\u2019s odd angles is to create certain areas of the track where passing can be easier or harder. This rewards strategy, timing, and knowledge of the game\u2019s passing mechanics. In qualifying, the tan track has turns that, under the right conditions, can cause the robot to lose track of the line. This is used as a feature in the game, as the players must balance engine power output with staying on track. If a careless player sends the car into the corner too quickly, it will skid off of the track, leaving the car without a qualifying time.</p> <p></p>"},{"location":"reports/2022/NASCAR-style-turtlebot-racing/#discussion-of-interesting-algorithms-modules-techniques","title":"Discussion of interesting algorithms, modules, techniques","text":""},{"location":"reports/2022/NASCAR-style-turtlebot-racing/#line-follower-algorithm","title":"Line Follower Algorithm","text":"<p>The core of our robot navigation relies on prrexamples\u2019 OpenCV line follower algorithm, which analyzes a compressed image file to detect specific HSV color values, then computes a centroid representing the midpoint of the detected color values. The algorithm first subscribes to the robot\u2019s raspberry pi camera, accepting compressed images over raw images in order to save processing power, then passes the image through the openCV\u2019s bridge function, converting the ROS image to a CV2 image manipulatable by other package functions. In order to further save on process power, we resize the image using cv2.resize(), which changes the resolution of the image, reducing the number of pixels needing to be passed through the masking function. Regarding the resize() function, we recommend using cv2.INTER_AREA in order to shrink the image, and cv2.LINEAR_AREA or cv2.CUBIC_AREA in order to enlarge the image. Note that while using the cv2.CUBIC_AREA interpolation method will result in a higher quality image, it is also computationally more complex. For our project\u2019s purposes, we used the cv2.INTER_AREA interpolation method. The algorithm then accepts a lower and upper bound of HSV color values to filter out of the CV2 image, called \u2018masking\u2019. The image then has all of its pixels removed except for the ones which fall within the specified range of color values. Then, a dot is drawn on the \u201ccentroid\u201d of the remaining mass of pixels, and the robot attempts to steer towards it. To make the centroid more accurate and useful, the top portion of the image is stripped away before the centroid is drawn. This also means that if the color of surroundings happen to fall within the specified range of color values, the robot won\u2019t take it into account when calculating the centroid (which is desirable).</p>"},{"location":"reports/2022/NASCAR-style-turtlebot-racing/#modularity-technique","title":"Modularity Technique","text":"<p>Modular programming was a critical part of our program to determine which parts of our driver program were computationally intensive. The original line follower node processed the images inside the camera callback function. This arrangement causes the computer to be processed tens of times per second, which is obviously bad. The compression also would process the full size image. We tried remedying this by shrinking the already compressed image down to just 5% of its original size before doing the masking work. Unfortunately, both of these did not appear to have an effect on the problem.</p>"},{"location":"reports/2022/NASCAR-style-turtlebot-racing/#interacting-with-ros-using-a-tkinter-gui","title":"Interacting with ros using a tkinter gui","text":"<p>We were able to construct a tkinter gui that interacts with other nodes. We accomplished this by initializing the gui as its own node, then using rostopics to pass user input data between the gui and other nodes.</p>"},{"location":"reports/2022/NASCAR-style-turtlebot-racing/#execution","title":"Execution","text":"<p>Executing our project is twofold; first, run roslaunch nascar nascar.launch. Make sure the parameters of the launch file correspond to the name of the TurtleBot. Subsequently, open a separate terminal window and cd into the project folder, then run python tkinter_gui.py.</p>"},{"location":"reports/2022/NASCAR-style-turtlebot-racing/#story-of-the-project","title":"Story of the project.","text":"<p>Our original project was an entirely different concept, and was meant to primarily focus on creating a system that allowed for multi-robot (2-4+) collaboration. We originally intended to create a \u201csynchronized robot dancing\u201d software, which was imagined as a set of 2, 3 or 4 robots performing a series of movements and patterns of varying complexity. Due to the project lacking an end goal and purpose, we decided to shift our focus onto our idea of a NASCAR-style racing game involving two robots. Rather than focusing on designing a system and finding a way to apply it, we instead began with a clear end goal/visualization in mind and worked our way towards it.</p> <p>While working on our subsequent NASCAR-style multi-robot racing game, two major problems stuck out: one, finding suitable color ranges for the masking algorithm; two, computational complexity when using multiple robots. Regarding the first, we found that our mask color ranges needed to be changed depending heavily on lighting conditions, which changed based on time of day, camera exposure, and other hard-to-control factors. We resolved this by moving our track to a controlled environment, then calibrated our color range by driving the TurtleBot around the track. Whenever the TurtleBot stopped, we passed whatever image the raspicam currently saw through an image editor to determine HSV ranges, then multiplied the values to correspond with openCV\u2019s HSV values (whereas GIMP HSV values range from H: 0-360, S: 0-100, and V: 0-100, openCV\u2019s range from H: 0-180, S: 0-255, and V: 0-255). The cause of our second problem remains unknown, but after some experimentation we\u2019ve narrowed down the cause to relating to computational power and complexity. When running our driver on multiple robots, we noticed conflicting cmd_vels being published resulting from slowdowns in our image processing function, realizing that adding additional robots would exponentially increase the computational load of our algorithm. We attempted to resolve this by running the collection of nodes associated with each robot on different computers, with both sets of nodes associated with the same roscore.</p> <p>Overall, our group felt that we accomplished some of the goals we originally set for ourselves. Among the original objectives, which were to limit-test the movement capabilities of TurtleBots, implement a system that would allow for the collaboration of multiple robots, and present it in an entertaining and interactive manner, we felt we successfully achieved the first and last goal; in essence, our project was originally conceived as a means of entertainment. However, we were also satisfied with how we approached the second goal, as throughout the process we attempted many ways to construct a working multi-robot collaboration system; in the context of our project, we feel that our overtake algorithm is a suitable contribution to roboticists who would consider racing multiple robots.</p>"},{"location":"reports/2022/RoboTag/","title":"Work on progress","text":""},{"location":"reports/2022/RoboTag/#project-report-for-project-sample","title":"Project Report for Project Sample","text":"<ul> <li>Team: Chris Choi (mchoi@brandeis.edu), Lucian Fairbrother (lfairbrother@brandeis.edu), Eyal Cohen(eyalcohen@brandeis.edu)</li> <li>Date: 3rd May 2022</li> <li>Github repo: https://github.com/campusrover/RoboTag `</li> </ul>"},{"location":"reports/2022/RoboTag/#introduction","title":"Introduction","text":"<p>We wanted to create a dynamic project involving multiple robots. Since this project would be presented to people with a wide range of robotics knowledge, we wanted to create an intuitive project. We decided to recreate the game of tag using robots as most spectators know the rules, and so that each robot affects the other\u2019s behavior.</p> <p>The robots have one of two roles, cop and robber. The robbers flee from cops while avoiding obstacles, as the cops are in hot pursuit of them. When a cop is within a certain distance, and the cop catches the robber, their roles switch, and the new robber gets a 10-second head start to run away. If a spectating human feels left out of the fun, they can press a button to take control of their robot and chase the robber as the cop, or chase the cop as the robber.</p>"},{"location":"reports/2022/RoboTag/#problem-statement-including-original-objectives","title":"Problem Statement including original objectives","text":""},{"location":"reports/2022/RoboTag/#what-was-created","title":"What was created","text":"<p>Currently implemented two robots that can alternate between cop and robber, and user-controlled cop and robber. </p>"},{"location":"reports/2022/RoboTag/#technical-description-illustrations","title":"Technical Description, illustrations","text":""},{"location":"reports/2022/RoboTag/#discussion-of-interesting-algorithms-modules-techniques","title":"Discussion of interesting algorithms, modules, techniques","text":"<p>COP ALGORITHYM-     The cop algorithym was difficult to implement. The question of how to orient a cop towards moving coordinates was difficult for us to wrap our heads around. We first had to understand the pose variables. The pose orientation variable ranges from -3.14 to 3.14 and represents the angles a robot could be in, in radians. We eventually figured out a good compass algorithym, we used an if statement that calculated whether turning left or right was closer to the goal angle and then executed it. We had it go forward if the actual angle was within .2 radians of the goal angle</p> <p>UDP-SOCKETS-     We used UDP-sockets to send info accross our roscores. Because we had two roscores and we needed the robots to communicate their locations to each other we had them send their locations constantly over UDP sockets. We made a sender and receiver for each robot. The sender would subscribe to the AMCL_pose and then send out the message over the socket. The receiver would receive the message decode it, put it into a float64 list and publish it to the robot. </p> <p>STATE SWITCH-     State switching in our game is hugely important, if the robots aren't localized properly and one thinks a tag has happened while the other doesn't they will get caught as the same state. To deal with this we used AMCL to increase the localization and decrease any error. We also set up the tag such that the robber would stop for ten seconds after it became the cop and not be able to tag the new robber during that period. There were a few reasons we did this. Firstly because we wanted the new robber to have a chance to get away before it would get tagged again. Otherwise the two robots could get into an infinite loop of state switching. We also set the distance for the robber tag to be further than the cop to recognize a tag. The robber recognizes a tag at .35 and the cop recognizes it at .3 the reason for this is because the robber stops after recognizing the tag and the cop will keep going until it recognizes the tag. This makes it very unlikely for only one robot to recognize a tag which would result in them getting stuck in the same state.</p>"},{"location":"reports/2022/RoboTag/#guide-on-how-to-use-the-code-written","title":"Guide on how to use the code written","text":"<p>Every robot needs its own computer to run.  1. On each computer clone the repository 2. Go into allinone.py and change one initialized state to robber, such that you have a cop and robber 3. go into tf_sender.py and change the ip address to the ip address of the other computer 4. go into receiver.py and change the ip address to your ip address 5. go into your vnc and run roslaunch robotag robo.launch</p>"},{"location":"reports/2022/RoboTag/#clear-description-and-tables-of-source-files-nodes-messages-actions-and-so-on","title":"Clear description and tables of source files, nodes, messages, actions and so on","text":"<p>Robo.launch- The main launch file for our project</p> <p>NODES</p> <p>Allinone.py - main program node</p> <p>tf_sender.py - the socket sender node</p> <p>receiver.py - the socket receiver node</p> <p>OTHER FILES</p> <p>Map.yaml</p> <p>Map.pgm</p> <p>AMCL</p>"},{"location":"reports/2022/RoboTag/#problems-that-were-solved-pivots-that-had-to-be-taken","title":"problems that were solved, pivots that had to be taken","text":"<p>After the ideation period of Robotag, we faced a lot of difficulties and had to make many pivots throughout the project. We spent lots of time on not only development but also designing new methods to overcome obstacles. We had multiple sleepless nights in the lab, and hours-long Zoom meetings to discuss how we design our project. In these talks, we made decisions to move from one stage to another, start over from scratch for modularity, and strategize navigational methods such as move_base, TF, or our ultimate homebrewed algorithms. </p> <p>First stage: Broadcast/Listener Pair Using TF. We tried to let robots directly communicate with each other by using TF's broadcast/listener method. We initially turned to TF as it is a simple way to implement following between robots. However, this solution was overly-predictable and uninteresting. Solely relying on TF would limit our ability to avoid obstacles in crowded environments.</p> <p>Second stage: Increase modularity We decided to start over with a new control system with an emphasis on modularity.  The new control system called \u2018Control Tower\u2019 is a node run on the computer that keeps a record of all robot\u2019s roles and locations, then, orders where each robot has to go. Also, each robot ran the same code, that operates according to given stages. With this system, we would be able to switch roles freely and keep codes simple. </p> <p>Third stage: move_base Although the Control Tower could properly listen to each robot, publishing commands to each robot was a grueling experience. For optimized maneuvering around a map with obstacles, we decided to use move_base. We successfully implemented this feature on one robot. Given coordinates, a robot could easily utilize move_base to reach that location. We were planning for this to be a cop\u2019s method of tracking down the robber. We SLAMed a map of the second floor of our lab because it was mostly enclosed and has defining features allowing easier localizing. Using this map and AMCL, we could use move_base and move the robot wherever we wanted to. However, when it came time to run move_base on several robots, each robot\u2019s AMCL cost map and move_base algorithm confused the opposing bot. Although in theory, this solution was the most favorable\u2013due to its obstacle avoidance and accurate navigation\u2013in practice, we were unable to figure out how to get move_base to work with multiple robots and needed to give up on AMCL and move_base sooner than we did.</p> <p>Fourth stage: Using Odom After move_base and AMCL failed us, we used the most primitive method of localization, Odometry. We started each robot from the same exact position and watched as one robot would go roughly in the direction of the other. After a minute, each robot\u2019s idea of the coordinate plane was completely different and it was clear that we could not move forward with using odometry as our method of localization. Due to the limitation of each robot needing to run the program from the same location, and its glaring inaccuracies, we looked elsewhere for our localization needs.</p> <p>Fifth stage: Better Localization We missed the accuracy of AMCL. AMCL has fantastic localization capabilities combining both lidar and Odom and allows us to place a robot in any location on the map and know its present location, but we didn\u2019t have time to wrestle with conflicting cost maps again, so we elected to run each robot on a separate computer. This would also allow future users to download our code and join the game with their robots as well! Once we had each robot running on its own computer, we created a strategy for robots to communicate between a UDP Socket, such that each one can receive messages about the others\u2019 coordinates. This was an impromptu topic across multiple roscores!</p> <p>From then on, our localization of each robot was fantastic and allowed us to focus on improving the cops\u2019 tracking and the robber\u2019s running away algorithm while effectively swapping their states according to their coordinate locations in their respective AMCLs.</p>"},{"location":"reports/2022/litter_picker/","title":"The Litter Picker Robot","text":"<ul> <li>Team: Peter Zhao (zhaoy17@brandeis.edu) and Veronika Belkina (vbelkina@brandeis.edu)</li> <li>Date: May 2022</li> <li>Github repo: https://github.com/campusrover/litter-picker</li> </ul>"},{"location":"reports/2022/litter_picker/#introduction","title":"Introduction","text":"<p>The litter picker was born from a wish to help the environment, one piece of trash at a time. We decided on creating a robot that would wander around a specified area and look for trash using a depth camera and computer vision. Once it had located a piece of trash, it would move towards it and then push it towards a designated collection site. </p>"},{"location":"reports/2022/litter_picker/#problem-statement-including-original-objectives","title":"Problem Statement including original objectives","text":"<ul> <li>Be able to localize and move to different waypoints within a given area</li> <li>Capable of recognizing object to determine whether the object is trash</li> <li>Predict the trash's location using depth camera</li> <li>Collect the trash and bring to a collection site</li> </ul>"},{"location":"reports/2022/litter_picker/#relevant-literature","title":"Relevant literature","text":"<ul> <li>Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi: You Only Look Once: Unified, Real-Time Object Detection</li> <li>M. Bjelonic: YOLO ROS: Real-Time Object Detection for ROS</li> <li>Eitan Marder-Eppstein: ROS Navigation Stack</li> </ul>"},{"location":"reports/2022/litter_picker/#what-was-created","title":"What was created","text":""},{"location":"reports/2022/litter_picker/#technical-description-illustrations","title":"Technical Description, illustrations","text":"<p>In the summary of Nodes, Modules, and External Packages we show the corresponding tables with the names and descriptions of each file. Afterwards, we go into a more detailed description of some of the more important files within the program. </p>"},{"location":"reports/2022/litter_picker/#summary-of-nodes-modules-and-external-packages","title":"Summary of Nodes, Modules, and External Packages","text":"<p>We created a variety of nodes, modules, and used some external packages while trying to determine the best architecture for the litter picker. The following tables (Table 1, 2, 3) show descriptions of the various files that are separated into their respective categories. Table 4 shows the parameters we changed so that the robot moves more slowly and smoothly while navigating using move_base.  </p> <p>Table 1: Description of the ROS Nodes  | Node | Purpose | | ------ | ------ | | master.py | Coordinates the various tasks based on the current task's state | | vision.py | Processes and publishes computer vision related messages. It subscribes to the bounding box topic, the depth camera, and rgb camera and publishes a Trash custom message that will be used by the Task classes to determine what to do. |</p> <p>Table 2: Description of our python modules used by the above nodes | Modules | Purpose | | ------ | ------ | | task.py | Contains the interface which will be implemented by the other classes. | | navigation.py | Contains the navigation class, which uses the waypoints state to navigate using the AMCL navigation package and updates the waypoints. | | rotation.py | Includes the rotation class that publishes cmd_vel for the robot to rotate and look for trash. | | trash_localizer.py | Has the trash localizer class, which publishes cmd_vel to the robot depending on information received from the vision node. | | state.py | Holds the current state of the robot that is being passed around between the classes.| | utils.py | Provides some helper functions for the rest of the modules. | | topics.py | A single place to store all the subscribed topics name as constants| | trash_class | Contain the yolo classes that are considered trash|</p> <p>Table 3: External packages | Package | Purpose| | ----- | ----- | | darknet_ros | Uses computer vision to produce bounding boxes around the trash. | | navigation stack | Navigation stack for navigation and obstacle avoidance.  | | usb_cam | astra rgb camera driver |</p> <p>Table 4: Parameters changed within the navigation stack file: dwa_local_planner_params_waffle.yaml | Parameter | Value | | ------ | ------ | | max_vel_theta |  0.5 | | min_vel_theta | 0.2 | | yaw_goal_tolerance | 0.5 |</p>"},{"location":"reports/2022/litter_picker/#trash-custom-message","title":"Trash Custom Message","text":"<p>To simplify what information is being published and subscribed to, we created a custom message called Trash which includes three variables. A boolean called has_trash which indicated whether or not there is trash present. A float called err_to_center which returns a number that indicates how far from the center to the bounding_box center is. And a boolean called close_enough which will be true if we believe that the trash is at a reasonable distance to be collected, otherwise it will be false. These are used in the Rotation and Trash_localizer classes to publish the appropriate cmd_vel commands. It is under the message topic litter_picker/vision.  </p> <pre><code>bool has_trash\nfloat32 err_to_center\nbool close_enough\n</code></pre>"},{"location":"reports/2022/litter_picker/#visionpy","title":"vision.py","text":"<p>The vision node has several methods and callback functions to process the messages from the various topics it subscribes to and then publish that information as a Trash custom message. </p>"},{"location":"reports/2022/litter_picker/#darknet_ros","title":"darknet_ros","text":"<p>Image 1: Display of the darknet_ros putting a bounded box around the identified object</p> <p>darknet_ros is the computer vision package that looks for the types of trash specified in the trash_class.py. We want to be able to find things like bottles, cups, forks, and spoons that are lying around. This package integrates darknet with ros for easier use. We then subscribe to the <code>darknet_ros/bounding_boxes</code> topic that it publishes to and use that to to determine where the trash is. </p>"},{"location":"reports/2022/litter_picker/#task-interface-and-its-children","title":"Task interface and its children","text":"<p>Image 2: A diagram to show the setup of the Task interface and its children classes</p> <p>The Task interface contains its constructor and two other methods, start() and next(). When a Task instance is initiated, it will take in a state in the form of the LitterPickerState class which contains a list of waypoints and the current waypoint index. The start() method is where the task is performed and the next() method is where the next Task is set. We created three Task children classes called Navigation, Rotation, and TrashLocalizer which implemented the Task interface. </p> <p>The Navigation class communicates with the navigation stack. In start(), it chooses the current waypoint and sends it to move_base using the move base action server. If the navigation stack returns that it has successfully finished, the has_succeed variable of the Navigation class will be set to true. If the navigation stack returns that it has failed, has_succeed will be set to false. Then when the next() method is called and if has_succeed is true,  it will update the waypoint index to the next one and return a Rotation class instance with the updated state. If has_succeed is false, it will return an instance of the Navigation class and attempt to go to the same waypoint again. </p> <p>The Rotation class publishes a cmd_vel command for a certain amount of time so that the robot will rotate slowly. During this rotation, the camera and darknet_ros are constantly looking for trash. The Rotation class subscribes to the Trash custom message. In the start() method, it looks at the has_trash variable to check if there is any trash present while it is rotating. If there is, then it will stop. In the next() method, it sets has_trash is true, then it will set the next task instance to be the TrashLocalizer task. Otherwise, it will set it to a Navigation task. </p> <p>The TrashLocalizer class also subscribes to the Trash custom message. In the start() method, it publishes cmd_vel commands to move towards the trash and collect it in the plow. It uses the boolean close_enough to determine how far it has to go before before stopping. Then it sees if it sees that there is still a bounding box, it will use the trap_trash() method to move forward for a specified amount of time. </p>"},{"location":"reports/2022/litter_picker/#masterpy","title":"master.py","text":"<p>The master node has a class called LitterPicker which coordinates the Tasks as the robot executes them. There is a constructor, an execute() method, and a main method within it. It also includes the LitterPickerState class which contains a list of all the waypoints and an index for the current waypoint. The constructor takes in a Task and a LitterPickerState and two variables are initialized based on those two: current_task and state. </p> <p>In the execute() method, there are two lines, as you can see below. The first line starts the execution of the current_task and the second line calls the next function of the current_task so that it can decide which Task should be executed next.  <pre><code>def execute(self):\n        self.current_task.start()\n        self.current_task = self.current_task.next()\n</code></pre></p> <p>The main function initializes the node, creates an instance of the LitterPickerState using the waypoints file, creates an instance of the LitterPicker class using that state and a NavigationTask class. Then in the while not rospy.is_shutdown loop, it runs the execute() function. </p>"},{"location":"reports/2022/litter_picker/#guide-on-how-to-use-the-code-written","title":"Guide on how to use the code written","text":"<p>If you wish to change the map file to better suit your location, then you can edit the map file (in ) inside the litter_picker.launch, as well as change the initial position of the robot.  <pre><code>&lt;include file=\"$(find litter_picker)/launch/navigation.launch\"&gt;\n      &lt;arg name=\"initial_pose_x\" default=\"-1.1083\"/&gt;\n      &lt;arg name=\"initial_pose_y\" default=\"-1.6427\"/&gt;\n      &lt;arg name=\"initial_pose_a\" default=\"-0.6\"/&gt;\n      &lt;arg name=\"map_file\" default=\"$(find litter_picker)/maps/new_map.yaml\"/&gt;\n&lt;/include&gt;\n</code></pre> <p>You can also edit the new_map_waypoints.txt file in the waypoints folder to match the coordinates that you would like for the robot to patrol around that should be in the following format: </p> <pre><code>-4.211, 1.945, 0 \n-4.385, 4.937, 0\n-4.38, 4.937, 0\n-7.215, 1.342, 0\n</code></pre> <p>When you ssh onto the robot, you will need to run <code>bringup</code>.</p> <p>You will also need to run <code>rosrun usb_cam usb_cam_node</code> onboard the robot as well to initialize the depth camera's rgb stream. </p> <p>Then enter <code>roslaunch litter-picker litter_picker.launch</code>. The program should launch with a few windows: rviz and YOLO camera view.  </p>"},{"location":"reports/2022/litter_picker/#story-of-the-project","title":"Story of the project","text":"<p>There were many challenges that we faced with this project, but let\u2019s start from the beginning. At the start, we had tried to divide the work more so that we can create functionality more efficiently. Peter worked on finding a computer vision package that could create and creating the corresponding logic within the rotation node as well as creating the master node and implementing action lib when we had been using it. Veronika worked on creating the map and integrating the navigation stack into the program, installing the depth camera, and using it to localize the trash. Once we created most of the logic and were trying to get things to work, we became masters of pair programming and debugging together and for the rest of the project, we mainly worked together and constantly bounced ideas off each other to try and solve problems.  </p>"},{"location":"reports/2022/litter_picker/#master-node-and-navigation-stack","title":"master node and navigation stack","text":"<p>In the beginning, we had started with a simple state system using the topic publisher and subscriber way of passing information between nodes, however, that quickly became very complicated and we thought we needed a better way to keep track of the current state. We chose to use the actionlib server method which would send goals to nodes and then wait for results. Depending on the results, the master node would send a goal to another node. We also created a map of the area that we wanted to use for our project. Then we incorporated the AMCL navigation system into our state system. We decided that moving from waypoint to waypoint would be the best way to obtain the benefits of AMCL such as path planning and obstacle avoidance. This was a system that we kept throughout the many changes that we underwent throughout the project. </p> <p>However, when we tested it with the ROS navigation stack, things did not work as expected. The navigation would go out of control as if something was interfering with it. After some testing, we concluded that the most likely cause of this was the actionlib servers, however we were not completely sure why this was happening. Perhaps the waiting for results by the servers was causing some blocking effect to the navigation stack. </p> <p>We ended up completely changing the architecture of our program. Instead of making each task as a separate node, we made them into python classes where the master node can initiate different actions. The reason for this change is that we felt that the tasks does not have to be running in parallel, so a more linear flow would be more appropriate and resource-efficient. This change greatly simplified our logic and also worked much better with the navigation stack. We also edited some parameters which has been previously described within the navigation stack that allowed for the robot to move more smoothly and slowly. </p>"},{"location":"reports/2022/litter_picker/#computer-vision","title":"computer vision","text":"<p>Initially, we had wanted to train our computer vision model using the TACO dataset. However, there was not enough data and it took too long to train so we ended up choosing the pretrained yolo model that has some object classes such as bottle, cup, fork, and spoon that could be identified as trash. The YOLO algorithm and integrated with ros through the darknet_ros package as mentioned above. We had to make some changes to the darknet_ros files so that it could take in CompressedImage messages. </p>"},{"location":"reports/2022/litter_picker/#depth-camera","title":"depth camera","text":"<p>Close to the beginning of the project, we had set up the Astra depth camera. The story of the depth camera is long and arduous. We had begun the trash_localizer node by attempting to calculate a waypoint that the navigation stack could move to by determining the distance to the identified object and then calculating the angle. From there, we used some basic trigonometry to calculate the amount that needed to be traveled in the x and y directions. However, we found that the angle determined from the depth camera did not correspond with reality and therefore the waypoint calculated was not reliable. The next method we tried was to use the rotation node and stop the rotation when the bounding box was in the center and then use a pid controller to move towards the trash using the distance returned from the depth camera. There were several problems with that such as the bounding box being lost, but in general, this method seemed to work. Since we did most of this testing in Gazebo due to the LiDAR on Alien being broken, we didn't realize that we actually had an even larger problem at hand... </p> <p>When we started to test the depth camera on the actual robot in a more in depth manner, we realized that it was not returning reliable data with the method that we had chosen. We started to explore other options related to the depth stream but found that we did not have enough time to fully research those functionalities and integrate it in our program. This is when we decided pivot once again and not use the depth part of the depth camera. Instead we used the position of the bounded box in the image to determine whether the object was close enough and then move forward. </p>"},{"location":"reports/2022/litter_picker/#plow-system","title":"plow system","text":"<p>There was a robotic arm in the original plan, however, due to the physical limitations of both the arm, the robot, and time constraints, we ended up deciding to use a snow plow system instead. We had several ideas of having a movable one, but they were too small for the objects that we wanted to collect so we created a stationary one out of cardboard and attached it to the front of the robot. It works quite well with holding the trash within its embrace. </p> <p> Image 3: Alien with the Astra depth camera on top and the plow setup below posing for a photo</p>"},{"location":"reports/2022/litter_picker/#personal-assessment","title":"personal assessment","text":"<p>This project was much more challenging than we had anticipated. Training our own data and localizing an object based on an image and bounded box turned out to be very difficult. Despite the many problems we encountered along the way, we never gave up and persisted to present something that we could be proud of. We learned many lessons and skills along the way both from the project and from each other. One lesson we learned deeply is that a simulator is very different from the real world. Though for a portion of the time, we could only test simulator due to a missing LiDAR on our robot, it still showed the importance of real world testing to us. We also learned the importance of reasoning through the project structure and logic before actually writing it. Through the many frustrations of debugging, we experienced exciting moments of when something worked properly for the first time and then continued to work properly the second and third times. </p> <p>In conclusion, we were good teammates who consistently showed up to meetings, communicated with each other, spent many hours working side by side, and did our best to complete the work that we had assigned for ourselves. We are proud of what we have learned and accomplished even if it does not exactly match our original idea for this project. </p>"},{"location":"reports/2022/mini_scouter/","title":"mini_scouter.md","text":""},{"location":"reports/2022/mini_scouter/#final-deliverable","title":"Final Deliverable","text":""},{"location":"reports/2022/mini_scouter/#miniscout-project","title":"MiniScout Project","text":"<p>Team: Nazari Tuyo (nazarituyo@brandeis.edu) and Helen Lin (helenlin@brandeis.edu)</p> <p>COSI 119a Fall 2022, Brandeis University</p> <p>Date: May 4, 2022</p>"},{"location":"reports/2022/mini_scouter/#github-repo-mini-scouter","title":"Github repo: Mini Scouter","text":""},{"location":"reports/2022/mini_scouter/#introduction","title":"Introduction","text":""},{"location":"reports/2022/mini_scouter/#problem-statement-including-original-objectives","title":"Problem Statement (including original objectives)","text":"<p>If humans can\u2019t enter an area because of unforeseen danger, what could be used instead? We created MiniScouter to combat this problem. The goal of our project was to create a robot that can be used to navigate or \u201cscout\u201d out spaces, with directions coming from the Leap Gesture Controller or voice commands supported by Alexa. The turtlebot robot takes in commands through hand gestures or voice, and interprets them. Once interpreted, the robot preforms the action requested.****</p>"},{"location":"reports/2022/mini_scouter/#relevant-literature","title":"Relevant Literature","text":"<p>We referred to several documentations for the software used in this project:</p> <p>Boto3 Python Documentation</p> <p>Boto3 Simple Queue Service Documentation</p> <p>Leap Motion Python SDK Documentation</p> <p>Alexa Skill Developer Design Guide</p> <p>Python Lambda Documentation</p> <p>ROS Documentation</p>"},{"location":"reports/2022/mini_scouter/#what-was-created","title":"What was created","text":"<p>We designed and created a voice and gesture controlled tele-operated robot. Voice control utilizes Alexa and Lambda integration, while gesture control is supported with the Leap Motion Controller. Communication between Alexa, the controller, and the robot is all supported by AWS Simple Queue Service integration as well as boto3.</p>"},{"location":"reports/2022/mini_scouter/#technical-description-illustrations","title":"Technical Description, illustrations","text":""},{"location":"reports/2022/mini_scouter/#leap-motion","title":"Leap Motion","text":"<p>The Leap Motion Controller</p> <p></p> <p>The Leap Motion Controller is an optical hand tracking module that captures hand movements and motions. It can track hands and fingers with a 3D interactive zone and identify up to 27 different components in a hand. The controller can be used for desktop based applications (as this project does) or in virtual reality (VR) applications.</p> <p>The controller use an infrared light based stereoscopic camera. It illuminates the space near it and captures the user\u2019s hands and fingers. The controller then uses a tracking algorithm to estimate the position and orientation of the hand and fingers. The range of detection is about 150\u00b0 by 120\u00b0 wide and has a preferred depth of between 10cm and 60cm, but can go up to about 80cm maximum, with the accuracy dropping as the distance from the controller increases.</p> <p></p> <p>The Leap Motion Controller maps the position and orientation of the hand and fingers onto a virtual skeletal model. The user can access data on all of the fingers and its bones. Some examples of bone data that can be accessed is shown below.</p> <p></p> <p>Alongside hand and finger detection, the Leap Motion Controller can additionally track hand gestures. The Leap Software Development Kit (SDK) offers support for four basic gestures:</p> <ul> <li>Circle: a single finger tracing a circle</li> <li>Swipe: a long, linear movement of a finger</li> <li>Key tap: a tapping movement by a finger as if tapping a keyboard key</li> <li>Screen tap: a tapping movement by the finger as if tapping a vertical computer screen</li> </ul> <p></p> <p>The controller also offers other data that can be used to manipulate elements:</p> <ul> <li>Grab strength: the strength of a grab hand pose in the range [0..1]</li> <li>An open hand has a grab strength of zero and a closed hand (fist) has a grab strength of one</li> <li>Pinch strength: the strength of a pinch pose between the thumb and the closet finger tip as a value in the range [0..1]</li> <li>As the tip of the thumb approaches the tip of a finger, the pinch strength increases to one</li> </ul> <p>The Leap Motion Controller offers much functionality and a plethora of guides online on how to use it. It\u2019s recommended that the user keep the implementation of an application simple as having too many gestures doing too many things can quickly complicate an algorithm.</p> <p>Leap Motion Setup</p> <p>The Leap Motion Controller can be connected to the computer using a USB-A to Micro USB 3.0 cable. After it\u2019s connected, you\u2019ll need to get the UltraLeap hand tracking software, available here. You may need to use older versions of the software since V4 and V5 only offer support and code in C. We found that V2 of the UltraLeap software best fit our needs as it offered Python support and still included a robust hand-tracking algorithm. We installed the necessary software and SDK for V2 from here. Once we had all the necessary hardware and software set-up, we used the UltraLeap Developer documentation for Python at this site to begin creating the algorithm for our project.</p> <p>To start using the Leap Motion Controller, you\u2019ll need to connect the device to your computer via a USB-A to Micro USB 3.0 cable. You\u2019ll need to install the Leap software from their website at https://developer.leapmotion.com/. We found that we had to install older versions of the tracking software as the latest versions (V4 and V5) only supported C and that the Python version had been deprecated. For our needs, V2 worked best as it offered support for creating Python and came with a robust tracking system. After that, you\u2019ll need to install the Leap SDK, with more details on how to set this up below. (See Setup for more details on how to set up Leap for this project specifically)</p>"},{"location":"reports/2022/mini_scouter/#alexa-skills-kit","title":"Alexa Skills Kit","text":"<p>A widely known IOT home device, Amazon Alexa is Amazon\u2019s cloud-based voice service, offering natural voice experiences for users as well as an advanced and expansive collection of tools and APIs for developer use.</p> <p>When a phrase is said to Alexa, it\u2019s first processed through the Alexa service, which uses natural language processing to interpret the users \u201cintent\u201d and then the \u201cskill logic\u201d (as noted down below) handles any further steps once the phrase has been interpreted.</p> <p></p> <p>We created an Alexa skill for this project as a bridge between the user and the robot for voice commands. The Alexa skill also utilizes AWS Lambda, a serverless, event-driven computing platform, for it\u2019s intent handling. The Lambda function sends messages to the AWS SQS Queue that we used for Alexa-specific motion commands.</p> <p>For example, if a user makes a request, saying \u201cAlexa, ask mini scout to move forward\u201d, the Alexa service identifies the user intent as \u201cMoveForward\u201d. Once identified, the lambda function is activated, and it uses a \u201chandler\u201d specific to the command to send a dictionary message to the queue.</p> <p></p>"},{"location":"reports/2022/mini_scouter/#aws-simple-queue-service","title":"AWS Simple Queue Service","text":"<p>AWS Simple Queue Service is a managed message queuing service used to send, store and retrieve multiple messages for large and small scale services as well as distributed systems.</p> <p>The robot, Alexa, and the Leap motion controller all utilize the AWS Simple Queue Service to pass commands to each other. There are two options when creating queues, a standard queue, and a \u201cFIFO\u201d queue - first in first out, i.e. messages will only be returned in the order they were received. We created two FIFO queues, one for Leap, called \u201cLeapMotionQueue\u201d, and one for Alexa, called \u201cAlexaMotionQueue\u201d.</p> <p>In the Alexa lambda function and the Leap script, a boto3 client is created for SQS, which is used to connect them to the queue.</p> <p>On VNC, the robot makes the same connection to SQS, and since all of our work is carried out through one AWS account, it\u2019s able to access the data pushed to both queues upon request.</p>"},{"location":"reports/2022/mini_scouter/#boto3","title":"Boto3","text":"<p>Boto3 is the latest version of the AWS SDK for python. This SDK allows users to integrate AWS functionalities and products into their applications, libraries and scripts. As mentioned above, we used Boto3 to create a SQS \u2018client\u2019, or connection, so that our Leap script and our Alexa skill could access the queues.</p>"},{"location":"reports/2022/mini_scouter/#discussion-of-interesting-algorithms-modules-techniques","title":"Discussion of interesting algorithms, modules, techniques","text":""},{"location":"reports/2022/mini_scouter/#working-with-aws","title":"Working with AWS","text":"<p>Initially, we planned on having the Leap Motion Controller plugged directly into the robot, but after some lengthy troubleshooting, it was revealed that the Raspberry Pi the robot uses would not be able to handle the Leap motion software. Instead of giving up on using Leap, we thought of other ways that would allow communication between our controller and the robot.</p> <p>At the suggestion of a TA (thanks August!) to look into AWS, we looked for an AWS service that might be able to support this type of communication. The Simple Queue Service was up to the task - it allowed our controller to send information, and allowed the robot to interpret it directly. (Deeper dive into this issue in the Problems/Pivots section)</p>"},{"location":"reports/2022/mini_scouter/#leap-motion-gestures","title":"Leap Motion Gestures","text":"<p>One of the first and most important design choices we had to consider on this project was which gestures we should use to complete certain actions. We had to make sure to use gestures that were different enough so that the Leap Motion Controller would not have difficulty distinguishing between two gestures, as well as finding gestures that could encompass all the actions we wanted to complete. We eventually settled on the following gestures that were all individual enough to be identified separately:</p> <ul> <li>Hand pitch forward (low pitch): move backward</li> <li>Hand pitch backward (high pitch): move forward</li> <li>Clockwise circle with fingertip: turn right</li> <li>Counter-clockwise circle with fingertip: turn left</li> <li>Grab strength 1 (hand is a fist): stop all movement</li> </ul> <p>Another aspect we had to take into consideration when creating the algorithm was how much to tune the values we used to detect the gestures. For example, with the gesture to move forward and backward, we had to decide on a pitch value that was not so sensitive that other gestures would trigger the robot to move forward/backward but sensitive enough that the algorithm would know to move the robot forward/backwards. We ran the algorithm many times with a variety of different values to determine which would result in the greatest accuracy across all gestures. Even with these considerations, the algorithm sometimes was still not the most accurate, with the robot receiving the wrong command from time to time.</p> <p></p> <p>We implemented this algorithm on the local side. Within the code, we created a Leap Motion Listener object to listen to commands from the Leap Motion Controller. The Leap Listener takes in data such as number of hands, hand position in an x, y, z plane, hand direction, grab strength, etc. This data is then used to calculate what gestures are being made. This information then makes a request to the queue, pushing a message that looks like the following when invoked:</p> <pre><code>sqs_message = {\n        \"Current Time\":current_time.strftime(\"%H:%M:%S\"),\n        \"Motion Input\": \"Leap\", \n        \"Motion\": motion\n        }\n</code></pre> <p>On the VNC side, the motion is received from the SQS queue and turned back into a dictionary from a string. The VNC makes use of a dictionary to map motions to a vector containing the linear x velocity and angular z velocity for twist. For example, if the motion is \u201cMove forward,\u201d the vector would be [1,0] to indicate a linear x velocity of 1 and an angular z velocity of 0. A command is published to cmd_vel to move the robot. The current time ensures that all of the messages are received by the queue as messages with the same data cannot be received more than once.</p>"},{"location":"reports/2022/mini_scouter/#alexa-skill-and-lambda","title":"Alexa Skill and Lambda","text":"<p>For the Alexa skill, many factors had to be considered. We wanted to implement it in the simplest way possible, so after some troubleshooting with Lambda we decided on using the source-code editor available in the Alexa developer portal.</p> <p>In terms of implementing the skill, we had to consider the best way handle intents and slots that would not complicate the way messages are sent to the queue. We settled on giving each motion an intent, which were the following:</p> <ul> <li>MoveForward</li> <li>MoveBackward</li> <li>RotateLeft</li> <li>RotateRight</li> <li>Stop</li> </ul> <p>In order for these \u2018intents\u2019 to work with our robot, we also had to add what the user might say so that the Alexa service could properly understand the intent the user had. This meant adding in several different \u201cutterances\u201d, as well as variations of those utterances to ensure that the user would be understood. This was very important because each of our intents had similar utterances. MoveForward had \u201cmove forward\u201d, MoveBackward has \u201cmove backwards\u201d, and if the Alexa natural language processor only has a few utterances to learn from it could easily confuse the results meant for one intent for a different intent.</p> <p></p> <p>Once the intent was received, the Lambda function gets to work. Each handler is a class of it\u2019s own. This means that if the intent the Alexa service derives is \u201cRotateRight\u201d, it invokes only the \u201cRotateRightHandler\u201d class. This class makes a request to the queue, pushing a message that looks like the following when invoked.</p> <pre><code>sqs_message = {\n        \"Motion Input\": \"Alexa\", \n        \"Intent\": intent_name, \n        \"Current Time\":current_time.strftime(\"%H:%M:%S\")\n        }\n</code></pre> <p>Once this reaches VNC, the \u2018intent_name\u2019 - which would be a string like \u2018MoveForward\u2019 - is interpreted by converting the string into a dictionary, just like the Leap messages. We had to add the current time as well, because when queue messages are identical they are grouped together, making it difficult to pop them off the queue when the newest one arrives. If a user requested the robot to turn right twice, the second turn right message would not make it to the queue in the same way as the first one since those message are identical. Adding the time makes each message to the queue unique - ensuring it reaches the robot without issue.</p>"},{"location":"reports/2022/mini_scouter/#vnc","title":"VNC","text":"<p>The way that motions would be handled from two different sources in VNC was a huge challenge at one point during this project. Once we decided we\u2019d incorporation voice control into our robots motion, it was clear that switching back and forth between leap commands and voice commands would be an issue. At first, we thought we\u2019d try simultaneous switching between the two, but that quickly proved to be difficult due to the rate in which messages are deleted from each queue. It was hard to ensure that each command would used while reading from both of the queues, as messages could arrive at the same time. So we made the executive decision that voice commands take priority over Leap commands.</p> <p>This means that when an Alexa motion is pushed onto the queue, if the script that\u2019s running on VNC is currently listening to the Leap queue, it will pause the robot and proceed to listen to commands from the Alexa queue until it receives a stop command from Alexa.</p> <p>This simplified our algorithm, allowing for the use of conditionals and calls to the SQS class we set up to determine which queue was to be listened to.</p> <p>On the VNC, there are 3 classes in use that help the main function operate.</p> <ul> <li><code>MiniScoutAlexaTeleop</code></li> <li><code>MiniScoutLeapTeleop</code></li> <li><code>SQS_Connection</code></li> </ul> <p>The <code>MiniScoutMain</code> file handles checking each queue for messages and uses conditionals to determine which queue is to be listened to as well as making calls to the two teleop classes for twist motion calculation.</p> <p>The <code>SQS_Connection</code> class handles queue connection, and has a few methods that assist the main class with it. The method <code>has_queue_message()</code> returns whether the requested queue has a message or not. This method makes a request to the selected queue, asking for the attribute <code>ApproximateNumberOfMessages</code> . This was the only way we could verify how many messages were present in the queue, but it proved to be a challenge as this request only returns the approximate number of messages in the queue. At any time during the request could the number change. This meant that we had to set a couple of time delays in the main script as well as checking the queue more than once to accomodate for this possibility. The method <code>get_sqs_message()</code> makes a request to the queue for a message, but only is called in the main function if <code>has_queue_message()</code> returns <code>True</code> . This helps insure that the request does not error out and end our scripts execution.</p> <p>The <code>MiniScoutAlexaTeleop</code> class handles incoming messages from the Alexa queue and converts them into twist messages based on the intent received in the dictionary that are then published to <code>cmd_vel</code> once returned.</p> <p>The <code>MiniScoutLeapTeleop</code> class takes the dictionary that is received from the SQS queue. It makes use of this dictionary to map motions to a vector containing the linear x velocity and angular z velocity for twist, and then returns it.</p>"},{"location":"reports/2022/mini_scouter/#setup-guide-on-how-to-use-the-code","title":"Setup (guide on how to use the code)","text":"<ol> <li>Clone the Mini Scout repository (Be sure to switch to the correct branch based on your system)</li> <li>https://github.com/campusrover/mini_scouter</li> <li> <p>Install the Leap Motion Tracking SDK</p> <p>Note: Software installation is only needed on one 'local' device. If your laptop runs on MacOS, clone the MacOS branch and follow the MacOS instructions. If your laptop runs on Windows, clone the Windows branch and follow the Windows instructions</p> <ol> <li>Mac (V2): https://developer-archive.leapmotion.com/v2</li> <li>Windows (Orion, SDK included): https://developer.leapmotion.com/tracking-software-download</li> </ol> </li> </ol>"},{"location":"reports/2022/mini_scouter/#mac-os-installation","title":"Mac OS Installation","text":"<ol> <li> <p>Note: The MacOS software does not currently work with macOS Monterey, but there is a hack included below that does allow it to work</p> <p>https://developer.leapmotion.com/tracking-software-download     2.  Once installed, open the Leap Motion application to ensure correct installation.</p> <p></p> <p>A window will pop up with controller settings, and make sure the leap motion icon is present at the top of your screen.     3.  Plug your leap motion controller into your controller via a USB port.</p> <p>     4.  From the icon dropdown menu, select visualizer.</p> <p></p> <p>The window below should appear.</p> </li> </ol> <p></p> <ol> <li> <p>Note: If your MacOS software is an older version than Monterey, skip this step. Your visualizer should display the controller\u2019s cameras on it\u2019s own.</p> <p>Restart your computer, leaving your Leap controller plugged in. Do not quit the Leap Application. (so it will open once computer is restarted)     6. Once restarted, the computer will try to configure your controller. After that is complete, the cameras and any identifiable hand movement you make over the controller should appear.</p> </li> </ol>"},{"location":"reports/2022/mini_scouter/#windows-installation","title":"Windows Installation","text":"<ol> <li>Note: the Leap Motion Controller V2 is not compatible with a certain update of Windows 10 so you\u2019ll have to use the Orion version of the Leap software</li> <li>With V2, you can go into Program Files and either replace some of the .exe files or manually change some of the .exe files with a Hex editor</li> <li>However, this method still caused some problems on Windows (Visualizer could see hands but code could not detect any hand data) so it is recommended that Orion is used</li> <li>Orion is slightly worse than V2 at detecting hand motions (based off of comparing the accuracy of hand detection through the visualizer</li> <li>Orion is fairly simple to set up; once you install the file, you just run through the installation steps after opening the installation file</li> <li>Make sure to check off \u201cInstall SDK\u201d when running through the installation steps</li> <li> <p>After the installation is completed, open \u201cUltraLeap Tracking Visualizer\u201d on your computer to open the visualizer and make sure the Leap Motion Controller is connected</p> <p>The window below should look like this when holding your hands over the Leap Motion Controller:</p> <p> 3. AWS Setup    1. Create a AWS Account (if you have one already, skip this step)    2. Create an SQS Queue in AWS       1. Search for \u201cSimple Queue Service\u201d       2.  Click \u201cCreate Queue\u201d</p> <p>       3.  Give your queue a name, and select \u2018FIFO\u2019 as the type.</p> <p>       4.  Change \u201cContent-based deduplication\u201d so that its on. There is no need to change any of the other settings under \u201cConfiguration\u201d</p> <p>       5.  Click \u201cCreate Queue\u201d. No further settings need to be changed for now.</p> <p>This is the type of queue you will use for passing messages between your robot, the leap controller, alexa and any additional features you decide to incorporate from the project.       6. Creating your Access Policy  1. Click \u201cEdit\u201d  2.  Under \u201cAccess Policy\u201d, select advanced, and then select \u201cPolicy Generator\u201d</p> <pre><code> ![Imgur](https://i.imgur.com/zlaPKAu.png)\n</code></pre> <ol> <li>This will take you to the AWS Policy Generator. The Access Policy you create will give your Leap software, Alexa Skill and robot access to the queue.</li> <li> <p>Add your account ID number, which can be found under your username back on the SQS page, and your queue ARN, which can also be found on the SQS page.</p> <p></p> <ol> <li>Select \u201cAdd Statement\u201d, and then \u201cGenerate Policy\u201d at the bottom. Copy this and paste it into the Access Policy box in your queue (should be in editing mode).<ol> <li>Repeat step 1-5 once more, naming the queue separately for Alexa.</li> </ol> </li> <li>Create an AWS access Key (Start at \u201cManaging access keys (console)\u201d)</li> </ol> </li> </ol> </li> </ol> <p>https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html</p> <p>Save the details of your AWS access key somewhere safe.    4. Installing AWS CLI, boto3   1.  Follow the steps at the link below:</p> <pre><code>  [https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html)\n</code></pre> <ol> <li> <p>Once completed, downgrade your pip version with:</p> <p><code>sudo easy_install pip==20.3.4</code></p> <p>(this is needed since the LEAP portion of the project can only run on python2.7)       3. Run the following commands  1. <code>pip install boto3</code>  2. <code>pip freeze</code>  3. check that <code>boto3</code> is actually there!  4. <code>python -m pip install --user boto3</code> 4. Adding your credentials to the project package    1. In your code editor, open the cloned gesture bot package    2. navigate to the \u201ccredentials\u201d folder    3. using your saved AWS Access Key info, edit the fields       * Your \u201c<code>OWNER_ID</code>\u201d can be found in the top right hand corner of your aws console       *   \u201c<code>lmq_name</code>\u201d is the name of your queue + \u201c<code>.fifo</code>\u201d</p> <p>i.e. <code>leapmotionqueue.fifo</code>    4. change the name of <code>Add_Credentials.py</code> to <code>Credentials.py</code>    5.  You will need to do this step with each credentials file in each package</p> </li> </ol> <p>IMPORTANT</p> <p>UNDER NO CONDITION should this file be uploaded to github or anywhere else online, so after making your changes, run <code>nano .git/info/exclude</code> and add <code>Credentials.py</code></p> <p>Please make sure that it is excluded by running <code>git status</code> and then making sure it\u2019s listed under <code>Untracked files</code> 5.  Running the script</p> <p>Before running this script, please make sure you have Python2.7 installed and ready for use</p> <ol> <li>navigate to the \u2018<code>scripts</code>' folder and run \u2018<code>python2.7 hello_robot_comp.py</code>'</li> <li>If everything is installed correctly you should see some output from the controller!</li> <li>to run the teleop script, run \u2018<code>LeapGestureToSQS.py</code>\u2019 while your controller is plugged in and set up!</li> <li>Using the Alexa Skill</li> <li>This step is a bit more difficult.</li> <li>Go to the Alexa Developer Console</li> </ol> <p>https://developer.amazon.com/alexa/console/ask    3.  Click \u201cCreate Skill\u201d</p> <p>    4. Give your skill a name. Select Custom Skill. Select \u2018Alexa-Hosted Python\u2019 for the host.    5.  Once that builds, go to the \u2018Code\u2019 tab on the skill. Select \u2018Import Code\u2019, and import the \u201cVoiceControlAlexaSkill.zip\u201d file from the repository, under the folder \u201cAlexa\u201d</p> <p>    6. Import all files.    7.  In Credentials.py, fill in your credentials, ensuring you have the correct name for your queue.</p> <p>\u201c<code>vcq_name</code>\u201d is the name of your queue + \u201c<code>.fifo</code>\u201d    8. Click \u2018Deploy\u2019    9. Once Finished, navigate to the \u2018Build\u2019 tab. Open the <code>intents.txt</code> file from the repository\u2019s \u2018Skill\u2019 folder (under the Alexa folder), and it\u2019s time to build our intents.    10. Under invocation, give your skill a name. This is what you will say to activate it. We recommend mini scout! Be sure to save!    11. Under intents, click \u2018Add Intent\u2019 and create the following 5 intents:</p> <ol> <li>Move</li> <li>MoveForward</li> <li>MoveBackward</li> <li>RotateLeft</li> <li>RotateRight</li> </ol> <p>    12. Edit each intent so that they have the same or similar \u201cUtterances\u201d as seen in the <code>intents.txt</code> file. Don\u2019t forget to save!    13. Your skill should be good to go! Test it in the Test tab with some of the commands, like \u201cask mini scout to move right\u201d 7. Using MiniScout in ROS    1. Clone the branch entitled vnc into your ROS environment    2. install boto3 using similar steps as the ones above    3. run <code>catkin_make</code>    4. give execution permission to the file <code>MiniScoutMain.py</code>    5. change each <code>Add_Credentials.py</code> file so that your AWS credentials are correct, and change the name of each file to <code>Credentials.py</code> 8. running hello robot!    1. run the hello_robot_comp.py script on your computer    2. run the hello_robot_vnc.py script on your robot</p>"},{"location":"reports/2022/mini_scouter/#clear-description-and-tables-of-source-files-nodes-messages-actions-and-so-on","title":"Clear description and tables of source files, nodes, messages, actions and so on","text":"<pre><code>MacOS\npath on computer/mini_scouter\n\u2502\n\u251c\u2500\u2500 Alexa\n\u2502   \u251c\u2500\u2500 VoiceControlAlexaSkill.zip\n\u2502   \u251c\u2500\u2500 VoiceControlAlexaSkill\n\u2502       \u251c\u2500\u2500 interactionModels\n\u2502           \u251c\u2500\u2500 custom / en-US.json\n\u2502       \u251c\u2500\u2500 lambda\n\u2502           \u251c\u2500\u2500 Credentials.py\n\u2502           \u251c\u2500\u2500 lambda_function.py\n\u2502           \u251c\u2500\u2500 requirements.txt\n\u2502           \u251c\u2500\u2500 utils.py\n\u2502       \u2514\u2500\u2500 skill.json\n\u2502   \u2514\u2500\u2500 intents.txt\n\u251c\u2500\u2500 src\n\u2502   \u251c\u2500\u2500 credentials\n\u2502       \u251c\u2500\u2500 __init__.py\n\u2502       \u2514\u2500\u2500  Add_Credentials.py\n\u2502   \u251c\u2500\u2500 MacOSLeap\n\u2502       \u251c\u2500\u2500 Leap.py\n\u2502       \u251c\u2500\u2500 LeapPython.so\n\u2502       \u2514\u2500\u2500 libLeap.dylib\n\u251c\u2500\u2500 hello_robot_comp.py\n\u251c\u2500\u2500 LeapGestureToSQS.py\n\u2514\u2500\u2500 .gitignore\n\nWindows\npath on computer/mini_scouter\n\u2502\n\u251c\u2500\u2500 Alexa\n\u2502   \u251c\u2500\u2500 VoiceControlAlexaSkill.zip\n\u2502   \u251c\u2500\u2500 AlexaSkillUnzipped\n\u2502       \u251c\u2500\u2500 interactionModels\n\u2502           \u251c\u2500\u2500 custom / en-US.json\n\u2502       \u251c\u2500\u2500 lambda\n\u2502           \u251c\u2500\u2500 Credentials.py\n\u2502           \u251c\u2500\u2500 lambda_function.py\n\u2502           \u251c\u2500\u2500 requirements.txt\n\u2502           \u251c\u2500\u2500 utils.py\n\u2502       \u2514\u2500\u2500 skill.json\n\u2502   \u2514\u2500\u2500 intents.txt\n\u251c\u2500\u2500 src\n\u2502   \u251c\u2500\u2500 credentials\n\u2502       \u251c\u2500\u2500 __init__.py\n\u2502       \u2514\u2500\u2500  Credentials.py\n\u2502   \u251c\u2500\u2500 WindowsLeap\n\u2502       \u251c\u2500\u2500 Leap.py\n\u2502       \u251c\u2500\u2500 LeapPython.so\n\u2502       \u2514\u2500\u2500 libLeap.dylib\n\u251c\u2500\u2500 LeapGestureToSQS.py\n\u2514\u2500\u2500 .gitignore\n\nvnc\ncatkin_ws/src/mini_scout_vnc\n\u2502\n\u251c\u2500\u2500 src\n\u2502   \u251c\u2500\u2500 credentials\n\u2502       \u2514\u2500\u2500  Credentials.py\n\u2502   \u251c\u2500\u2500 hello_robot\n\u2502       \u2514\u2500\u2500  src\n\u2502           \u251c\u2500\u2500 Credentials.py\n\u2502           \u2514\u2500\u2500 hello_robot_vnc.py\n\u2502   \u251c\u2500\u2500 sqs\n\u2502       \u2514\u2500\u2500 sqs_connection.py\n\u2502   \u251c\u2500\u2500 teleop\n\u2502       \u251c\u2500\u2500 MiniScoutAlexaTeleop.py\n\u2502       \u2514\u2500\u2500 MiniScoutLeapTeleop.py\n\u2502   \u2514\u2500\u2500 MiniScoutMain.py\n\u251c\u2500\u2500 CMakeLists.txt\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 package.xml\n\u2514\u2500\u2500 .gitignore\n</code></pre>"},{"location":"reports/2022/mini_scouter/#story-of-the-project","title":"Story of the project","text":""},{"location":"reports/2022/mini_scouter/#our-initial-idea","title":"Our initial idea","text":"<p>When we first started our project, we had the idea of creating a seeing-eye dog robot that would guide the user past obstacles. We decided we wanted to use the Leap Motion Controller in our project and we planned to have the user direct the robot where to go using Leap. We would then have the robot detect obstacles around it and be able to localize itself within a map. Our final project has definitely deviated a lot from our original project. At the beginning, we made our project more vague to just become a \u201cgesture-controlled bot\u201d since we felt that our project was taking a different direction than the seeing-eye dog bot we had planned on. After implementing features for the robot to be commanded to different positions through hand gestures and voice commands, we settled on creating a mini scouter robot that can \u201cscout\u201d ahead and help the user detect obstacles or dangerous objects around it.</p>"},{"location":"reports/2022/mini_scouter/#problems-pivots-and-concluding-remarks","title":"Problems, Pivots and Concluding Remarks","text":"<p>Our first main challenge with the Leap Motion Controller was figuring out how to get it connected. We first attempted to connect the controller directly to the robot via USB. However, the computer on the turtlebot3 ended up not being strong enough to handle Leap, so we had to consider other methods. August recommended using AWS IOT and we began looking into AWS applications we could use to connect between the local computer and VNC. We settled on using AWS SQS to send messages from the local computer to the VNC and the robot and we used boto3 to add SQS to our scripts.</p> <p>Our next challenge was with getting the Leap Motion Controller setup. The software we needed for Leap ended up not being compatible with Monterey (the OS Nazari\u2019s laptop was on) and it didn\u2019t seem to work when attempting to set it up on Windows (Helen\u2019s laptop). For Mac, we found a workaround (listed above under setup) to get V2 of the Leap software working. For Windows, we had to try a couple different versions before we found one that worked (Orion).</p> <p>There were also challenges in finding the best way to represent the gestures within the algorithm for moving the robot. Our gestures were not clear at times and having the robot constantly detect gestures was difficult. We had to make changes to our code so that the robot would see a gesture and be able to follow that command until it saw another gesture. This way, Leap would only have to pick up on gestures every few seconds or so - rather than every second - making the commands more accurate.</p> <p>When we started implementing a voice control system into the robot, we had challenges with finding a system that wasn\u2019t too complex but worked well with our needs. We first explored the possibility of using pocketsphinx package but this ended up not being the most accurate. We decided to use Amazon Alexa and AWS lambda to implement voice recognition.</p> <p>Overall, we are very satisfied with the progress on this project. It was quite tricky at times, but working with AWS and finding a new way for the controller to communicate with the robot was an unexpected \u201cpivot\u201d we took that worked out well in the long run. The entire project was truly a learning experience, and it\u2019s exciting to watch our little Mini Scouter move as it uses the code we created over the last three months.</p>"},{"location":"reports/2022/not-play-catch/","title":"not-play-catch","text":"<p>Veronika Belkina Robotics Independent Study Fall 2022</p>"},{"location":"reports/2022/not-play-catch/#links","title":"Links","text":"<ul> <li>not-play-catch github</li> <li>arm_control github</li> <li>arm documentation</li> <li>camera calibration</li> <li>apriltag setup</li> <li>video</li> </ul>"},{"location":"reports/2022/not-play-catch/#introduction","title":"Introduction","text":"<p>Objectives</p> <ul> <li>Document the Interbotix X100 arm and create a program that would send commands to the arm through messages and help explore the capabilities and limitations of the arm.</li> <li>Create a program that would push a ball towards a target and also to guess if a ball is under a goal and push it out.</li> <li>The original purpose of this project was to create an arm that could play catch with the user. This would have involved catching a ball that was rolling towards it, and also pushing the ball towards a target.</li> </ul>"},{"location":"reports/2022/not-play-catch/#what-was-created","title":"What was created","text":""},{"location":"reports/2022/not-play-catch/#technical-description","title":"Technical Description","text":""},{"location":"reports/2022/not-play-catch/#arm-control","title":"<code>arm-control</code>","text":"<ul> <li>arm-control is a basic command line program that allows the user to send commands to the arm through a ROS message. There are two nodes: send_command.py and arm_control.py. The arm must be turned on, connected to the computer, and the ROS launch files launched.</li> <li>send_command.py allows for a user to input a command (from a limited pool of commands) such as moving the arm to a point, opening or closing the gripper, moving to the sleep position, etc. There are some basic error checks in place such as whether the command is correct or whether the inputs are the correct type. The user can enter one command at a time and see what happens. The node will then publish the proper message out. There are 8 message types for the various commands to simplify parsing through and figuring out what is happening. When the user decides to exit the program, the sleep message will be sent out to put the arm in the sleep position and exit out of the send_command program.</li> <li>Arm_control subscribes to the same messages that the send_command node publishes. It connects to the arm and initiates in a pose. From there, it will wait for a message. When it receives a message of a certain type, it will execute that command, and then stay in that position until another command comes.</li> </ul>"},{"location":"reports/2022/not-play-catch/#not-play-catch_1","title":"<code>not-play-catch</code>","text":"<p>This is the setup for play-catch. There is a board that is taped off to indicate the boundaries of the camera. The robot is placed in the center of the bottom half and its base-link is treated as the origin (world). There is a ball that is covered in green tape for colour detection, and Apriltags that are placed on top of goals. The camera is situated around 80 cm above the board to get a full view of the board.</p> <p>Camera and Computer Vision</p> <p>Camera.py</p> <p>This node does the main masking and filtering of the images for colour detection. It then publishes a variety of messages out with the various image types after processing. It also allows for dynamic reconfiguration so that it is easier to find the colour that you need.</p> <p>Ball_distance.py</p> <p></p> <p>This node finds the green ball and determines its coordinates on the x-y plane. It publishes those coordinates and publishes an Image message that displays the scene with a box around the ball and the coordinates in centimeters in the corner.</p> <p>Sending Commands</p> <p>Pickup.py</p> <p>The pickup node calculates the position of the ball and picks it up. It first turns to the angle in the direction of the ball and then moves in the x-z direction to pickup the ball. It then turns to the target Apriltag and puts the ball down 15cm away in front of the Apriltag. The calculations for the angles are down in the same way \u2013 determining the atan of the y/x coordinates. The steps are all done at trajectory time of 2.0 sec.</p> <p>Push.py</p> <p>The push node pushes the ball towards the target. It starts from the sleep position and then turns back to Apriltag target. Since it knows that the ball is already in position, it can push the ball with confidence after closing its gripper. The steps are all done at a trajectory time 0.9 sec to give extra impact to the ball and let it roll.</p> <p>Guess.py</p> <p>The guess node is a similar idea to the previous nodes, however, there is some random chance involved. The arm guesses between the two Apriltags, and then tries to push the ball out from under it. If a ball rolls out and is seen by the camera, then the arm will do a celebration dance. If there is no ball, then it will wait for the next turn. For this node, I had to make some adjustments to the way I publish the coordinates of the ball because when the ball wasn\u2019t present, the colour detector was still detecting some colour and saying that there was a ball. So I adjusted the detector to only work when the detected box is greater than 30 pixels, otherwise, the node will publish (0,0,0). Since the ball is (x, y, 0.1) when it is being seen, there shouldn\u2019t be a chance for an error with the ball being at (0,0,0). It would be (0,0,0,1) instead.</p> <p>Miscellaneous</p> <p>Main.py</p> <p>The node that initiates the game with the user and the one that is run in the roslaunch file. From here, the other nodes are run. It also keeps track of the score.</p> <p>Ball_to_transform.py</p> <p>This node creates the transform for the ball using the coordinates that are published from ball_distance.py. It uses pose.yaml file to see what the name of the frame is, the parent frame, and the message topic the Pose is being published to.</p> <p>Guide on how to use the code written</p> <ul> <li> <p><code>roslaunch arm_control command_center.launch</code></p> </li> <li> <p>If you\u2019re working with the actual arm, ensure that it is turned on and plugged into the computer before starting. Otherwise, if you\u2019re working in simulation, then uncomment <code>&lt;arg name=use_sim value=true /&gt;</code> line in the launch file.</p> </li> <li> <p>If you want to just run this file and send commands to the arm, then you can also run: <code>rosrun arm_control send_command.py</code></p> </li> <li> <p><code>roslaunch playcatch play-catch.launch</code></p> </li> </ul> <p>Tables</p> <p><code>arm_control</code></p> Nodes Description send_command.py Receives command line input from user and sends it to arm_control as a message arm_control.py Receives command line input from user and sends it to arm_control as a message <p><code>not-play-catch</code></p> Nodes Description main.py Brings everything together into a game pickup.py Locates the ball, determines if it can pick it up, and sends a series of commands to the arm to pick and place the ball in the correct angle for the Apriltag. push.py Pushes the ball towards the Apriltag using a series of commands. guess.py Guesses a box and pushes the ball out to see if it is there or not. If it is, then it does a celebratory dance. If it\u2019s not, then it will wait for the next round sadly. ball_distance.py Calculates the location of the ball on the x-y plane from the camera and publishes a message with an image of the ball with a box around it and the coordinates of the ball. ball_to_transform.py Creates a transform for the ball. camera.py Does all the masking and filtering and colour detection to make the computer vision work properly. <p>Messages</p> Message Type Description /arm_control/point Pose Pose message which tells the arm where to go /arm_control/pose Pose Pose message which sets the initial pose of the arm /arm_control/home Bool Sends the arm to the home position /arm_control/sleep Bool Sends the arm to the sleep position /arm_control/gripper String Opens or closes the gripper /arm_control/exit Bool Exits the program and sends the arm to the sleep position /arm_control/time Float32 Sets the trajectory time for movement execution /arm_control/celebrate Bool Tells the arm to celebrate! <p>Other Files/Libraries</p> Name Description apriltag library A library that allows for users to easily make use of Apriltags and get their transforms. catch_settings.yaml Apriltags settings to specify the family of tags and other parameters. catch_tags.yaml The yaml file for specifying information about the Apriltags. pose.yaml The yaml file for specifying information about the ball. cvconfig.cfg The dynamic reconfiguration file for colour detection and dynamically changing the colour that is being detected."},{"location":"reports/2022/not-play-catch/#story-of-the-project","title":"Story of the project","text":"<p>The idea for this project started a while ago, during the time when we were selecting what project to do in COSI119. I had wanted to play catch with a robot arm then, however, we ended up deciding on a different project at that time. But the idea still lived on inside me, and when Pito bought the PX100 over the summer, I was really excited to give the project a try.</p> <p>The beginning was a little unorganized and a little difficult to get familiar with the arm because I had to stay home due to personal reasons for a month. I worked primarily on the computer vision aspects of the project at that time, setting up the Apriltag library and developing the colour detection for the ball and publishing a transform for it.</p> <p>When I was able to finally return to the lab, it still felt hard to get familiar with the arm\u2019s capabilities and limitations, however, after talking to Pito about it, I ended up developing the arm_control program to send commands and see how the ROS-Python API works more properly. This really helped push the project forward and from there, things felt like they went smoother. I was able to finish developing the pickup and pushing nodes. However, this program also allowed for me to see that the catching part of the project was not really feasible with the way things were. The commands each took several seconds to process, there had to be time between sending messages otherwise they might be missed, and it took two steps to move the arm in the x, y, z directions. It was a bit of a letdown, but not the end of the world.</p> <p>As an additional part of the project, I was going to do a pick and place node, but it didn\u2019t seem very exciting to be honest. So, I was inspired by the FIFA World Cup to create a game for the robot to play which developed into the guessing game that you see in the video.</p> <p>This project had a lot of skills that I needed to learn. A big part of it was breaking down the task into smaller more manageable tasks to keep myself motivated and see consistent progress in the project. Since this project was more self-dictated than other projects that I have worked on, it was a challenge to do sometimes, but I\u2019m happy with the results that I have achieved.</p> <p>As I was writing this report, I had an insight that I felt would have made the program run much faster. It is a little sad that I hadn\u2019t thought of it earlier which is simply to add a queue into the program to act as a buffer that holds commands as they come in. However, I wouldn\u2019t have had to time to implement in a couple of days because it would require to rework the whole structure of the arm_control node. When I thought of the idea, I really didn\u2019t understand how I didn\u2019t think of it earlier, but I suppose that is what happens when you are in the middle of a project and trying to get things to work. Since I had written the arm_control program for testing and discovery and not for efficiency. But then I used it since it was convenient to use, but I didn\u2019t consider that it was not really an optimized program. So next semester, I would like to improve the arm_control program and see if the arm runs the not-play-catch project faster.</p> <p>Either way, I really enjoyed developing this project and seeing it unfold over time, even if it\u2019s not exactly what I had aspired to do in the beginning. Hopefully, other people will also enjoy creating exciting projects with the arm in the future!</p> <p>(I didn\u2019t really want to change the name of the project, so I just put a not in front of it\u2026)</p>"},{"location":"reports/2022/waiter-bot/","title":"waiter-bot.md","text":""},{"location":"reports/2022/waiter-bot/#team-ben-soli-bsolibrandeisedu-and-harry-zhuh-zhuhbrandeisedu","title":"Team: Ben Soli (bsoli@brandeis.edu) and Harry Zhuh (zhuh@brandeis.edu)","text":""},{"location":"reports/2022/waiter-bot/#date-552022","title":"Date: 5/5/2022","text":""},{"location":"reports/2022/waiter-bot/#github-repo-httpsgithubcomcampusroverwaiter_bot","title":"Github repo: https://github.com/campusrover/waiter_bot","text":"<p>Introduction\\</p> <p>For this project, we wanted to create an automated catering experience. More and more, customer service jobs are being performed by chatbots and voicebots, so why not bring this level of automation to a more physical setting. We figured the perfect domain for this would be social events. Waiter bot serves as a voice activated full-service catering experience. The goal was to create a general purpose waiter robot, where all the robot needs is a map of a catering location, a menu, and coordinates of where to pick-up food and or drinks.</p> <p>Original Objectives:\\</p> <ul> <li>Capable of speech to text transcription and intent understanding.</li> <li>Has conversational AI.</li> <li>Can switch between 3 states \u2013 Wander, Take_Order, and Execute \u2013 to achieve waiter-like behavior.</li> <li>Able to localize itself and navigate to given locations on a map.</li> <li> <p>Relevant literature:</p> <ul> <li>The Alexa Skills Kit SDK for Python</li> <li>actionlib SimpleActionClient class</li> </ul> <p>Core Functionality:</p> <p>This application was implemented as two primary nodes. One node serves as the user-interface, which is a flask implementation of an Alexa skill. This node is integrated with a ngrok subdomain. This node processes the speech and sends a voice response to Alexa while publishing necessary information for navigation.</p> <p>We created a custom message type called Order, which consists of three std_msg/String messages. This stores the food for the order, the drink for the order, and the name of the On the backend, a navigation node processes these orders, and looks up the coordinates for where the food and drink items are located. We created a map of the space using SLAM with AMCL. The robot uses this map along with move-base to navigate to its goal and retrieve this item. While Alexa works great for taking orders, one of its flaws is that it cannot be programmed to speak unprompted. When the robot arrives to retrieve the item, the navigation node supplies the alexa-flask node with the item the robot needs.</p> <p>A worker can ask Alexa what the robot needs, and Alexa tells the worker what is needed. The user then tells Alexa that the robot has the item, and it proceeds to its next stop. Once the robot has collected all of the items, it returns to the location where it originally received the voice request. The navigation node then supplies the alexa-flask node with a message for the order it is delivering. The user can then ask Alexa if that is their order, and Alexa will read back the order to the user. After this, the order is deleted from the node, allowing the robot to take more orders.</p> <p>Due to the physical size constraints of the robot, it can only carry at most three orders at a time. The benefit of using the flask-ask module, is that the Alexa voice response can take this into account. If the robot has too many orders, it can tell the user to come back later. If the robot is currently in motion, and the user begins to make a user, a String message is sent from the alexa-flask node to the navigation node telling the robot to stop. Once Alexa has finished processing the order, the alexa-node sends another String message to the navigation node telling the robot to continue moving. If the robot has no orders to fulfill, it will roam around, making itself available to users.</p> <p>In terms of overall behavior, the WaiterBot alternates between wandering around the environment, taking orders and executing the orders. After initialization, it starts wandering around its environment while its order_log remains empty or while it\u2019s not stopped by users. It randomly chooses from a list of coordinates from wander_location_list.py, turns it into a <code>MoveBaseGoal</code> message using the <code>goal_pose()</code> function, and has actionlib\u2019s <code>SimpleActionClient</code> send it using <code>client.send_goal()</code>. And instead of calling <code>client.wait_for_result()</code> to wait till the navigation finishes, we enter a while loop with <code>client.get_result() == None</code> as its condition.</p> <p>This has 2 benefits \u2013 it makes the navigation interruptible by new user order and allows WaiterBot to end its navigation once it\u2019s within a certain tuneable range of its destination. We noticed during our experimentation that while having <code>SimpleActionClient</code> send <code>MoveBaseGoal</code> works really well in gazebo simulation, it always results in WaiterBot hyper-correcting itself continuously when it arrives at its destination. This resulted in long wait times which greatly hampers the efficiency of WaiterBot\u2019s service. Therefore, we set it up so that the robot cancels its <code>MoveBaseGoal</code> using <code>client.cancel_goal()</code> once it is within a certain radius of its goal coordinates.</p> <p>When a user uses the wake phrase \u201cAlexa, tell WaiterBot (to do something)\u201d in the proximity of the WaiterBot, the robot will interrupt its wandering and record an order. Once the order has been published as an <code>Order</code> message to the <code>/orders</code> topic, the robot will initiate an <code>order_cb</code> function, store the order on an order_log, and start processing the first order. Before it looks up the coordinates of the items, it will store an Odometry message from /odom topic for later use. It then looks at <code>Order</code> message\u2019s data field food and look up its coordinates in <code>loc_string.py</code>. After turning it into a <code>MoveBaseGoal</code>, it uses <code>client</code> to send the goal. Once the Waiterbot arrives at its destination, it will broadcast a message, asking for the specific item it is there to collect. It will not move on to its next destination till it has received a String message from the <code>order_picked_up</code> topic.</p> <p>After it has collected all items, it uses the Odometry it stored at the beginning of the order and set and send another <code>MoveBaseGoal</code>. Once it returns to its original location, to the user. If <code>order_log</code> is empty, it will being wandering again.</p> <p>How to Use the Code:</p> <p><code>git clone https://github.com/campusrover/waiter_bot.git</code></p> <p><code>cm</code></p> <ul> <li>Download ngrok and authorize ngrok token <code>pip3 install flask-ask</code> and <code>sudo apt-install ros-noetic-navigation</code></li> <li>Have waiterbot skill on an Alexa device</li> <li>ssh into your robot</li> <li>In waiter_bot/src bash ngrok_launch.sh</li> </ul> <p><code>roslaunch waiter_bot waiter_bot.launch</code>\\</p> <p>Tables and Descriptions of nodes, files, and external packages:</p> ROS Nodes Descriptions waiter_bot_navigation.py Master node. Has 3 functions Wander, Take_Order and Execute, which serve as states waiter_bot_alexa_hook.py Receives speech commands from the user, and sends them to the navigation node Files Description bm.pgm An image file of the lab basement created using SLAM of the gmapping package bm.yaml Contains map information such as origin, threshold, etc. related to bm.pgm menu_constants.py contains the food menu and the drink menu loc_dictionary.py Contains a dictionary mapping food and drink items to their coordinates wander_location_list.py Contains a list of coordinates for the robot\u2019s wandering behavior Order.msg A custom message containing three String messages: food, drink and name External Packages Descriptions Flask-ASK A Python module simplfying the process of developing the backend for Alexa skills using a Flask endpoint Ngrok Creates a custom web address linked to the computer's local IP address for https transfers <p>Story of the Project:\\</p> <p>Our original project intended to have at least two robots, one working as a robot waiter, the other working as a robot kitchen assistant. Initially, we planned to create our NLP system to handle speech and natural language understanding. This quickly became a problem, as we could only find a single dataset that contained food orders and its quality was questionable. At the same time, we attempted to use the Google Speech API to handle out speech-to-text and text-to-speech. We found a ROS package for this integration, but some dependencies were out of data and incompatible. Along with this, the firewalls the lab uses, made integration with this unlikely. We then attempted to use PocketSphinx which is an off-the-shelf automated speech recognition package which can run offline. Initial tests of this showed an incredibly high word-error-rate and sentence-error-rate, so we abandoned this quickly. Previous projects had success integrating Alexa with ROS nodes, so we decided to go in that direction.</p> <p>Building off of previous work using Flask and ngrok for Alexa integration was fairly simple and straightforward thanks to previous documentation in the lab notebook. Building a custom Alexa skill for our specific purpose was accomplished quickly using the Alexa Developer Console. However, previous methods did not allow a feature that we needed for our project, so we had to find a new approach to working with Alexa.</p> <p>At first, we tried to change the endpoint for the skill from an ngrok endpoint to a lambda.py provided by Amazon, but this was difficult to integrate with an ROS node. We returned to an approach that the previous projects had used, but this did not allow us to build Alexa responses that involved considering the robots state and asking the user for more information in a way that makes sense. For example, due to the physical size of the robot, it can probably never handle more than three orders. An order consists of a food, a drink, and name to uniquely identify the order. From the Alexa Developer Console, we could prompt the user for this information, then tell the user if the robot already had too many orders to take another one, but this felt like bad user-interface design. We needed a way to let the user know that the robot was too busy before Alexa went through the trouble of getting all this information. Then we found a module called Flask-ASK, which seamlessly integrated flask into the backend of an Alexa Skill. This module is perfect for Alexa ROS integration. On the Alexa Developer Console, all you need to do is define your intents and what phrases activate them. Then within your ROS, you can treat those intents like functions. Within those functions you can define voice responses based on the robot\u2019s current state and also publish messages to other nodes, making the voice integration robust and seamless.</p> <p>Navigation-wise, we experimented with different ways of utilizing <code>SimpleActionClient</code> class of the <code>actionlib</code> package. As we have mentioned above, we originally used <code>client.wait_for_result()</code> so that WaiterBot can properly finish navigating. <code>wait_for_result()</code>, however, is a black box by itself and prevented us from giving WaiterBot a more nuanced behavior, i.e. capable of interrupting its own navigation. So we looked into the documentation of <code>actionlib</code> and learnt about the <code>get_result()</code> function, which allowed us to tear away the abstraction of <code>wait_for_result()</code> and achieve what we want.</p> <p>It was surprising (though it should not be) that having a good map also proved to be a challenge. The first problem is creating a usable map using <code>SLAM</code> method of the <code>Gmapping</code> package only, which is not possible with any sizeable area. We searched for software to edit the pgm file directly and settled on GIMP. Having a easy-to-use graphical editor proved to be highly valuable since we are free to tweak any imperfections of a raw SLAM map and modify the image as the basement layout changes.</p> <p>Another challenge we had concerned the sizes of the maps and how that affect localization and navigation. At one point, we ended up making a map of half the basement since we wanted to incorporate WaiterBot into the demo day itself: WaiterBot would move freely among the guests as people chat and look at other robots and take orders if requested. The large map turned out to be a problem, however, since the majority of the space on the map is featureless. This affects two things: the amcl algorithm and the local map. To localize a robot with a map, <code>AMCL</code> starts with generating a normally distributed initalpose, which is a list of candidate poses. As the robot moves around, <code>AMCL</code> updates the probabilities of these candidate poses based on <code>LaserScan</code> messages and the map, elimating poses whose positions don't align their corresponding predictions. This way, <code>AMCL</code> is able to narrow down the list of poses that WaiterBot might be in and increase the accuracy of its localization. <code>AMCL</code> thus does not work well on a large and empty map since it has few useful features for predicting and deciding which poses are the most likely. We eventually settled down on having a smaller, more controlled map built with blocks.</p> <p>An issue unique to a waffle model like Mutant is that the four poles interfere with the lidar sensor. We originally tried to subcribe to <code>/scan</code> topic, modify the <code>LaserScan</code> messages, and publish it to a new topic called <code>/scan_mod</code> and have all the nodes which subscribe to <code>/scan</code> such as <code>AMCL</code> to subscribe to <code>/scan_mod</code> instead. This was difficult because of the level of abstraction we had been operating on by roslaunch <code>Turtlebot3_navigation.launch</code> and it was not convenient to track down every node which subscribe to <code>/scan</code>. Instead, we went straight to the source and found a way to modify the <code>LaserScan</code> message itself so that its <code>scan.range_min</code> definition is big enough so that it encompasses the four poles. We learnt that we need to modify the <code>ld08_driver.cpp</code> file on the TurtleBot itself and learnt how to access it through a vnc terminal. We then used <code>nano</code> editor to edit <code>LaserScan</code> <code>scan.range_min</code> directly.</p> <p>Self Assessment</p> <p>Overall, we consider this project a success. Through this project, we were able to integrate conversational AI with a robotic application, learning more about Alexa skill development, mapping, localization, and navigation. While there are a few more things we wish we had time to develop, such as integrating a speaker to allow the robot to talk unprompted, or using a robotic arm to place food and drink items on the robot, this project served as a lesson in properly defining the scope of a project. We learned how to identify moments where we needed to pivot, and rapidly prototyped any new features we would need. In the beginning, we wanted to be evaluated on our performance with regards to integrating an NLP system with a robot, and we more than accomplished that. Our Alexa skill uses complex, multi-turn dialog, and our robotic application shows what may be possible for future automation. This project has been a combination of hardware programming, software engineering, machine-learning, and interaction design, all vital skills to be a proper roboticist.</p> </li> </ul>"},{"location":"reports/2023/Autopilot/","title":"Autopilot","text":""},{"location":"reports/2023/Autopilot/#project-report","title":"Project Report","text":""},{"location":"reports/2023/Autopilot/#team-members-zihao-liu-junhao-wang-long-yi","title":"Team members: Zihao Liu, Junhao Wang, Long Yi**","text":""},{"location":"reports/2023/Autopilot/#github-repo-httpsgithubcomcampusroverautopilot","title":"Github Repo: https://github.com/campusrover/autopilot","text":""},{"location":"reports/2023/Autopilot/#date-may-2023","title":"Date: May 2023","text":""},{"location":"reports/2023/Autopilot/#introduction","title":"Introduction","text":"<p>The idea of the autopilot robot was simple \u2013 it\u2019s a mobile robot augmented with computer vision, but it\u2019s a fun project to work on for people wishing to learn and combine both robotics and machine learning techniques in one project. Evenmore, one can see our project as miniature of many real world robots or products: robot vacuum (using camera), delivery robot and of course, real autopilot cars; so this is also an educative project that can familiarize one with the techniques used, or at least gives a taste of the problems to be solved, for these robots/products.</p>"},{"location":"reports/2023/Autopilot/#problem-statement-original-objectives","title":"Problem statement (original objectives)","text":"<ul> <li>Able to follow a route</li> <li>Able to detect traffic signs</li> <li>Able to behave accordingly upon traffic signs</li> </ul>"},{"location":"reports/2023/Autopilot/#relevant-literature","title":"Relevant literature","text":"<p>Thesis Traffic and Road Sign Recognition</p> <p>What was created</p> <ol> <li>Technical descriptions, illustrations</li> </ol> <p>There are five types and two groups of traffic signs that our robot can detect and react to. Here are descriptions of each of them and the rules associated with them upon the robot\u2019s detection.</p> <p>Traffic light (See Fig1)</p> <ol> <li> <p>Red light: stops movement and restarts in two seconds.</p> </li> <li> <p>Orange light: slows down</p> </li> <li> <p>Green light: speeds up</p> </li> </ol> <p>Turn signs (See Fig2)</p> <ol> <li> <p>Left turn: turn left and merge into the lane</p> </li> <li> <p>Right turn: turn right and merge into the lane</p> </li> <li> <p>All turns are 90 degrees. The robot must go straight and stay in the lane if not signaled to turn.</p> </li> </ol> <p></p> <pre><code>**Fig 1. Traffic light examples**\n</code></pre> <p></p> <pre><code>**Fig 2a. Initial turn sign examples**\n</code></pre> <p></p> <pre><code>**Fig 2b. Revised turn sign examples**\n</code></pre> <ol> <li>Discussion of interesting algorithms, modules, techniques</li> </ol> <p>The meat of this project is computer vision. We soon realized that different strategy can be used the two groups of traffic signs as they have different types of differentiable attributes (color for traffic lights and shape for turning signs).Therefore, we used contour detection for traffic lights, and we set up a size and circularity threshold to filter noise out noise. Turning signs classification turned out to be much harder than we initially expected in the sense that the prediction results seem to be sometimes unstable, stochastic, and confusing (could be because of whatever nuance factors that differ in the training data we found online and the testing data (what the robot sees in lab).</p> <p>We tried SVM, Sift, and CNN, and we even found a web API for traffic signs models that we integrated into our project \u2013 none of them quite work well.  </p> <p>Therefore we decided to pivot to another more straightforward approach: we replaced our original realistic traffic signs (Fig2a) with simple arrows (Fig2b), and then what we are able to do is that:</p> <ul> <li>Detect the blue contour and crop the image from robot\u2019s camera to a smaller size </li> <li>Extract the edges and vertices of the arrow, find the tip and centroid of it</li> <li>Classify the arrow\u2019s direction based on the relative position of the tip to the centroid</li> </ul> <p>Two more things worth mentioning on lane following:</p> <ol> <li>Our robot follows a route encompassed by two lines (one green and one red) instead of following one single line. Normally the robot uses contour detection to see the two lines and then computes the midpoint of them. But the problem comes when the robot turns \u2013 in this case, the robot can only see one colored line. The technique we use is to only follow that line and add an offset so that the robot turns back the the middle of the route (the direction of the offset is based on the color of that line)</li> <li>How we achieve turning when line following is constantly looking for a midpoint of a route. What we did is that we published three topics that are named \u201c/detect/lane/right_midpoint\u201d, \u201c/detect/lane/midpoint\u201d, \u201c/detect/lane/left_midpoint\u201d. In this case there are multiple routes (upon an intersection), the robot will choose to turn based on one of midpoint position, based on the presence turning sign. By default, the robot will move according to the \u201c/detect/lane/midpoint\u201d topic.</li> <li> <p>Guide on how to use the code written</p> <p>Run \u201croslaunch autopilot autopilot.launch\u201d, which starts the four nodes used in our project.</p> </li> <li> <p>Clear description and tables of source files, nodes, messages, actions</p> </li> </ol>  Node     Messages     Self defined?     Usage     Cmd_vel     Twist     NO     Publish movement     /raspicam_node/image/compressed     CompressedImage     NO     Receive camera data     /detect/lane/midpoint     MidpointMsg <p> [int32 x <p> int32 y <p> int32 mul]     YES     Midpoint of a lane     /detect/lane/right_midpoint     MidpointMsg     YES     Midpoint of a left lane (when there\u2019s a branch or cross)     /detect/lane/left_midpoint     MidpointMsg     YES     Midpoint of a right lane (when there\u2019s a branch or cross)     /detect/sign     SignMsg <p> [string direction <p> float32 width <p> float32 height]     YES     Detection of the      /detect/light     LightMsg <p> [string color <p> float32 size]     YES     Detection of a traffic light     <p>Table 1. Nodes and Messages description</p> <p>Story of the project</p> <ol> <li>How it unfolded, how the team worked together</li> </ol> <p>At the very beginning of our project, we set these three milestones for our work:</p> <pre><code>**Milestone-1:** Be able to detect and recognize the signs in digital formats (i.e. jpg or png) downloaded online. No ROS work needs to be done at this stage.\n\n\n**Milestone-2:** Complete all the fundamental functionalities, and be able to achieve what\u2019s described in a simulation environment.\n\n\n**Milestone-3:** Be able to carry out the tasks on a real robot in a clean environment that\u2019s free of noises from other objects. We will use blocks to build the routes so that laser data can be used to make turns. We will print out images of traffic signs on A4 white paper and stick them onto the blocks.\n</code></pre> <p>And we have been pretty much following this plan, but as one can tell, the workload for each phase is not evenly distributed. </p> <p>Milestone 1 is something we\u2019ve already done in assignment, but here we use two lines to define a route instead of just one single line, and the complexity is explained above. Traffic light classification was pretty straightforward in the hindsight. We simply used color masking and contour detection. To avoid confusing the robot between red green lines that define the route, and the red green traffic signs, we set up a circularity and area threshold. We spent the most time on milestone 3. We experimented with various methods and kept failing until we decided to simply our traffic signs \u2013 more details are explained in section 8.  Our team worked together smoothly. We communicated effectively and frequently. As there are three of us, we divided the work into lane detection and following, traffic light classification, and sign classification and each of us works on a part to maximize productivity. After each functionality is initially built, we worked closely to integrate them and debug together.</p> <ol> <li>problems that were solved, pivots that had to be taken</li> </ol> <p>The first problem or headache we encountered is that we use a lot of colors for object detection in this project, so we had to spend a great amount of time tuning HSV values of our color-of-interest, as slight changes in background lighting/color mess up with it.</p> <p>The more challenging part is that, as alluded to above, turning signs classification is more difficult than we initially expected. We tried various model architectures, including CNN, SVM, SIFT, and linear layers. No matter what model we choose, the difficulty comes from the inherent instability of machine learning models. Just to illustrate it: we found a trained model online that was trained on 10000 traffic signs images, for 300 epochs, and reached a very low loss. It is able to classify various traffic signs \u2013 stop sign, rotary, speed limit, you name it. But sometimes, it just failed on left turn and right turn classification (when the image is clear and background is removed), and we didn\u2019t figure out a great solution on remediating this. So instead we piloted away from using machine learning models and replaced curved traffic signs (Fig 2a) with horizontal arrows (Fig 2b). The detail of the algorithm is discussed in section 4.</p> <ol> <li>Your own assessment</li> </ol> <p>We think that this has been a fun and fruitful experience working on this project. From this semester\u2019s work, we\u2019ve all had a better appreciation of the various quirky problems in robotics \u2013 we had to basically re-pick all the parameters when we migrated from gazebo world the the real lab, and also we literally had to re-fine-tune the hsv values everytime we switched to a new robot because of the minute difference in background lighting and each robot\u2019s camera. So if one thing we learned from this class, it\u2019s real Robots Don't Drive Straight. But more fortunately, we\u2019ve learned a lot more than this single lesson, and we\u2019re able to use our problem solving and engineering skills to find work-arounds for all those quirky problems we encountered. Therefore, in summary we learned something from every failure and frustration, and after having been through all of these we\u2019re very proud of what we achieved this semester. </p>"},{"location":"reports/2023/BowlingBot/","title":"BowlingBot (Spring 2023)","text":"<p>Team Members: Michael Jiang (michaeljiang@brandeis.edu) &amp; Matthew Merovitz (mmerovitz@brandeis.edu)</p>"},{"location":"reports/2023/BowlingBot/#background","title":"Background:","text":"<p>The project aimed to create a robot that could mimic the game of bowling by grabbing and rolling a ball towards a set of pins. The robot was built using a Platform bot with a pincer attachment. The ball is a lacrosse ball and the \u201cpins\u201d are either empty coffee cups or empty soda cans.</p> <p>At first, we imagined that in order to achieve this task, we would need to create an alley out of blocks, map the space, and use RGB for ball detection. None of this ended up being how we ultimately decided to implement the project. Instead, we opted to use fiducial detection along with line following to implement ball pickup and bowling motion. The fiducial helps a lot with consistency and the line following allows the robot to be on track. </p>"},{"location":"reports/2023/BowlingBot/#objective","title":"Objective:","text":"<p>The primary objective of the BowlingBot project was to develop a robot that could perform the task of bowling, including locating the ball, grabbing it, traveling to the bowling alley, and knocking down the pins with the ball. The project aimed to use a claw attachment on the Platform bot to grab and roll the ball towards the pins. The robot was designed to use fiducial markers for ball detection and orientation, and line detection for pin alignment.</p>"},{"location":"reports/2023/BowlingBot/#algorithms","title":"Algorithms:","text":"<p>The BowlingBot project utilized several algorithms to achieve its objectives. These included:</p> <ol> <li>Fiducial Detection: The robot was designed to use fiducial markers to detect the location and orientation of the ball. The fiducial markers used were ArUco markers, which are widely used in robotics for marker-based detection and localization. In addition, we used odometry to ensure precision, allowing us to consistently pick up the ball. </li> </ol> <p></p> <ol> <li>Line Detection: The robot utilized line detection techniques to align itself with the pins. This involved using a camera mounted on the robot to detect the yellow line on the bowling alley and align itself accordingly. This also uses OpenCV.</li> </ol> <p></p> <ol> <li>Pincer Attachment: The BowlingBot utilized a pincer attachment to pick up the ball and roll it towards the pins. </li> </ol>"},{"location":"reports/2023/BowlingBot/#description-of-the-code","title":"Description of the Code:","text":"<p>The code is centered around a logic node that controls the logic of the entire bowling game. It relies on classes in fiducialbowl.py, pincer.py, bowlingmotion.py. </p> <p>Fiducialbowl.py: This class has two modes. Travel mode is for traveling to the ball in order to pick it up and then traveling back to the bowling alley. Bowling mode is for ensuring centering of the robot along the line with the assistance of the yellow line. </p> <p>Pincer.py: Controls the opening and closing of the pincer attachment on the platform bot. </p> <p>Bowlingmotion.py: This node turns until the yellow line is seen, centers the robot along the line, and then performs the quick motion of moving forwards and releasing the ball. </p>"},{"location":"reports/2023/BowlingBot/#blockers-and-roadblocks","title":"Blockers and Roadblocks:","text":"<p>During our time developing the BowlingBot, we encountered several obstacles and challenges that we were forced to overcome and adjust for. Initially, the crowded lab hours slowed down our camera and sensors, peaking at about a minute delay between the camera feed and what was happening in real time. We decided collectively to instead come during office hours when it was less crowded in order to avoid the severe network traffic, which allowed us to test the camera properly. </p> <p>Another roadblock was finding an appropriate ball, which held up our testing of the bowling motion. We knew we needed a ball that was heavy enough to knock over pins, but small enough to fit inside the pincer. We initially bought a lacrosse ball to test with, but it was too grippy and ended up catching on the ground and rolling under the robot, forcing us to pivot to using a field hockey ball, which was the same size and weight but crucially was free from the grippiness of the lacrosse ball.</p> <p>Another challenge we had to adapt to was the actual bowling motion. We initially proposed that the robot should implement concepts from the line follower so that it could adjust its course while rolling down the alley. However, we discovered that the robot\u2019s bowling motion was too quick for it to reliably adjust itself in such a short time. To resolve this, we pivoted away from adjusting in-motion and instead implemented aligning itself as best as possible before it moves forward at all.</p>"},{"location":"reports/2023/BowlingBot/#story-of-the-project","title":"Story of the Project:","text":"<p>The BowlingBot project was inspired by the team's love for robotics and bowling. The team was intrigued by the idea of creating a robot that could mimic the game of bowling and we spent lots of time researching and developing the project.</p> <p>The project was challenging and required us to utilize several complex algorithms and techniques to achieve our objectives. We faced several obstacles, including issues with ball detection and alignment, and had to continuously iterate and refine our design to overcome these challenges.</p> <p>Despite the challenges, the team was able to successfully develop a functional BowlingBot that could perform the task of bowling. We are excited to showcase our project to our peers and hold a demonstration where participants can compete against the robot.</p> <p>Overall, the BowlingBot project was a success, and the team learned valuable lessons about robotics, problem-solving, and combining several class concepts into a more complex final product.</p>"},{"location":"reports/2023/cargoclaw/","title":"Cargo Claw","text":"<ul> <li>Benjamin Blinder benjaminblinder@brandeis.edu  </li> <li>Ken Kirio kenkirio@brandeis.edu  </li> <li> <p>Vibhu Singh vibhusingh@brandeis.edu  </p> </li> <li> <p>Github Repository</p> </li> </ul>"},{"location":"reports/2023/cargoclaw/#introduction","title":"Introduction","text":""},{"location":"reports/2023/cargoclaw/#problem-statement","title":"Problem Statement","text":"<p>The project coordinates a stationary arm robot and a mobile transportation robot to load and unload cargo autonomously. We imagine a similar algorithm being used for loading trucks at ports or in factory assembly lines.</p>"},{"location":"reports/2023/cargoclaw/#demonstration","title":"Demonstration","text":"<p>Watch the demo </p> <p>There are three phases: mapping, localization, and delivery.  </p> <p>First, the robot drives around and makes a map. Once the map is made, a person will manually drive the robot to a delivery destination and press a button to indicate its location to the robot. Then the person will manually have to drive the robot back to the loading point and mark that location as well. With this setup, the robot can start working as the delivery robot and transporting cubes from the loading zone to the arm.  </p> <p>In the delivery phase, a person loads a piece of cargo - we used a 3D printed cube - onto the robot and then presses a button to tell the robot that it has a delivery to make. Once there, computer vision is used to detect the cargo and calculate its position relative to the arm. Once the arm receives the coordinates, it determines what command would be necessary to reach that position and whether that command is physically possible. If the coordinates aren\u2019t valid, the transportation robot attempts to reposition itself so that it is close enough for the arm. If the coordinates are valid, the arm will then grab the cube off the loading plate on the robot and place it in a specified go. Finally, the transportation robot will go back to the initial home zone, allowing the process to repeat.</p>"},{"location":"reports/2023/cargoclaw/#learning-objectives","title":"Learning Objectives","text":"<ul> <li>Mapping</li> <li>Create a map</li> <li>Localize on a premade map</li> <li>Perform autonomous navigation between two fixed points</li> <li>Image processing</li> <li>Recognize the cargo</li> <li>Calculate the physical location of the cargo</li> <li>Robotic arm</li> <li>Coordinate commands on a robot that does not run ROS natively</li> </ul>"},{"location":"reports/2023/cargoclaw/#literature","title":"Literature","text":"<ul> <li>ROS Navigation Tuning</li> </ul>"},{"location":"reports/2023/cargoclaw/#what-was-created","title":"What Was Created","text":"<p> A flowchart of message passing that must occur between the three main components of this project.</p>"},{"location":"reports/2023/cargoclaw/#algorithms-techniques","title":"Algorithms &amp; Techniques","text":""},{"location":"reports/2023/cargoclaw/#mapping-slam-amcl-move_base","title":"Mapping: SLAM, AMCL, move_base","text":"<p>We needed the robot to be able to move autonomously and orient itself to the arm, so we decided to use SLAM to create a map and the AMCL and move_base packages to navigate. Using SLAM, we use the LiDAR sensor on the robot to create a map of the room, making sure we scan clear and consistent lines around the arm and the delivery zone. We then save the map and use AMCL to generate a cost map. This also creates a point cloud of locations where the robot could be according to the current LiDAR scan. Then, we manually moved the robot through the map in order to localize the robot to the map and ensure accurate navigation. We then capture the position and orientation data from the robot for a home and delivery location, which is then saved for future navigation. Finally, we can tell the robot to autonomously navigate between the two specified locations using the move_base package which will automatically find the fastest safe route between locations, as well as avoid any obstacles in the way.</p>"},{"location":"reports/2023/cargoclaw/#image-processing-color-filtering-conversion-to-physical-units-transformation-to-arm-commands","title":"Image Processing: color filtering, conversion to physical units, transformation to arm commands","text":"<p>Cargo is detected via color filtering. We filter with a convolutional kernel to reduce noise, then pass the image through bitwise operators to apply the color mask. The centroid is converted from pixels to physical units by measuring the physical dimensions captured by the image.</p> <p>Since commands to the arm are sent in polar coordinates, the cartesian coordinates obtained from the image are then converted into polar. These coordinates must then be transformed into the units utilized by the arm, which have no physical basis. We took measurements with the cargo in various positions and found that a linear equation could model this transformation, and utilized this equation to determine the y-coordinate. However, the x-coordinate could not be modeled by a linear equation, as the arm's limited number of possible positions along this axis made it impossible to determine a strong relationship between the coordinates. In addition, whether the cargo was parallel or at an angle to the arm affected the location of the ideal point to grab on the cargo. Instead we calculated the x-position by assinging the position and orientation of the cargo to states, which were associated with a particular x-coordinate.</p>"},{"location":"reports/2023/cargoclaw/#how-to-run","title":"How to Run","text":""},{"location":"reports/2023/cargoclaw/#setup","title":"Setup","text":"<ol> <li>Arm robot  </li> </ol> <p>The arm robot needs to be elevated so that it can pick up the cube. Ensure that the setup is stable even when the arm changes positions, using tape as necessary. Determine the z-command required to send the arm from the \"home\" position (see Veronika's guide to manually controlling the arm) to the height of the cargo when it is loaded on the transportation robot. This will be one parameter of the launch file.</p> <ol> <li>Camera  </li> </ol> <p>The camera should be set in a fixed position above the loading zone, where the lens is parallel to the ground. The lab has a tripod for such purposes. Record the resolution of the image (default 640x480) and the physical distance that the camera captures at the height of the cargo when placed on the transportation robot. These will also be parameters for the launch file.</p>"},{"location":"reports/2023/cargoclaw/#execution","title":"Execution","text":"<p>Run <code>roslaunch cargoclaw transport_command.launch</code>. Separately, run <code>roslaunch cargoclaw arm_command.launch</code> with the following parameters:</p> <ul> <li>arm_z: z-command from the arm's position to pick up the cargo from the transportation robot (Setup #1)</li> <li>width_pixels/height_pixels: Image resolution (Setup #2)</li> <li>width_phys/height_phys: Physical dimensions captured in the image at the height of the cargo (Setup #2)</li> </ul> <p>Mode 1: Mapping  Use teleop to drive the robot between the loading and unloading zones. Watch rviz to ensure adequate data is being collected.</p> <p>Mode 2: Localizing  Use teleop to drive the robot between the loading and unloading zones. Watch rviz to ensure localization is accurate. Press the \"Set Home\" button when the robot is at the location where cargo will be loaded. Press the \"Set Goal\" button when the robot is at the location of the arm.</p> <p>Mode 3: Running  When the cargo is loaded, press \"Go Goal\". This will send the robot to the loading zone where the loading/unloading will be done autonomously. The robot will then return to the home position for you to add more cargo. Press \"Go Goal\" and repeat the cycle. The \"Go Home\" button can be used if the navigation is not working, but should not be necessary.</p>"},{"location":"reports/2023/cargoclaw/#summary-of-nodes-messages-and-external-packages","title":"Summary of Nodes, Messages, and External Packages","text":"<p>Nodes</p>  Node   Description   arm_cam   Accepts image of the delivery zone from the camera, detects the cargo with color filtering, and calculates and publishes its physical location   box_pickup   Accepts the coordinates published from arm_cam and uses a linear regression to determine the y transform and a set of ranges to determine the x transform  cargo_bot   Directly controls the motion of the Alien Bot using cmd_vel and move_base  cargo_gui   User interface which allows control of the robot as well as setting the mapping and navigation points <p>Messages</p>  Message   Description   alien_state   True if the transportation robot has finished navigating to the delivery zone. Published by `cargo_bot` and subscribed to by `arm_cam`.   cargo_point   The physical position of the cargo. Published by `arm_cam` and subscribed to by `box_pickup`.   UI   A string of information relaying the state, position, and time since the last input for the robot which is published to `cargo_gui` for display   keys   A string that directs robot movement. Published from `cargo_gui` to `cargo_bot`.   arm_status   A string relaying whether the location of the cargo can be picked up by the arm, and if so, when the arm has finished moving the cube. This informs the transportation robot's movement in the delivery zone. Published by `box_pickup` and subscribed to by `cargo_bot`."},{"location":"reports/2023/cargoclaw/#process","title":"Process","text":"<p>We began by trying to send commands to the robotic arm through ROS. While the arm natively runs proprietary software, Veronika Belkina (hyperlink?) created a package that allows for ROS commands to control the arm which we decided to utilize. The arm is controlled via USB so first we had to set up a new computer with Ubuntu and install ROS. Our first attempt was unsuccessful due to an unresolved driver error, but our second attempt worked. This step took a surprising amount of time, but we learned a lot about operating systems while troubleshooting in this stage.</p> <p>Simultaneously, we researched the arm capabilities and found that the arm can pick up a maximum weight of 30g. This required that we create our own 3D printed cubes to ensure that the cargo would be light enough. All cubes were printed in the same bright yellow color so that we could use color filtering to detect the cube.</p> <p>Next, we tested our code and the cubes by coding the arm to pick up a piece of cargo from a predetermined location. This step was difficult because commands for the x/z-coordinates are relative, while commands for the y-coordinate are absolute. After this test worked, we made the node more flexible, able to pick up the cargo from any location.</p> <p>With the arm working, we then began working on the image processing to detect the cargo and calculate its location. First we had to set up and calibrate the USB camera using the <code>usb_cam</code> package. Then we began working on cargo detection. Our first attempt used color filtering, but ran into issues due to the difference in data between the USB camera and the robots' cameras. We then tried applying a fiducial to the top of the transportation robot and using the aruco_detect package. However, the camera was unable to reliably detect the fiducial, leading us to return to the color detection method. With some additional image transformation and noise reduction processing, the color detection was able to reliably identify the cargo and report its centroid. Then, since the height of the transportation robot is fixed we were able to calculate the physical location of the cargo based on the ratio of pixels to physical distance at that distance from the camera. Using this information, we created another node to determine whether the cube is within the arm's range to pick up. We found converting the physical coordinates to the commands utilized by the arm to be extremely difficult, ultimately deciding to take many data points and run a regression to find a correlation. With this method we were relatively successful with the y-coordinate, which controlled the arm's rotation.</p> <p>Simultaneously, we coded the transportation robot's autonomous navigation. We realized our initial plan to combine LIDAR and fiducial data to improve localization would not work, as <code>move_base</code> required a cost map to calculate a trajectory while <code>fiducial_slam</code> saved the coordinates of each detected fiducial but did not produce a cost map. We decided to move forward by using <code>slam_gmapping</code>, which utilized lidar data and did produce a cost map. This allowed us to use <code>move_base</code> to reach the approximate location of the arm, and fine-tune the robot's position using fiducial detection as necessary. However, upon testing, we realized <code>move_base</code> navigation was already very accurate, and trading control between the <code>move_base</code> and a fiducial-following algorithm resulted in unexpected behavior, so we decided to just use <code>move_base</code> alone.</p> <p>Finally, we attempted to combine each of these components. Although <code>move_base</code> allowed for good navigation, we found even slight differences in the orientation of the robot in the delivery zone resulted in differences in the coordinates necessary to grab the cargo. We decided to make the image processing component more robust by pivoted to using polar rather than cartesian coordinates when calculating the commands to send to the arm, as the arm's x- and y-coordinates correspond to its radius and angle. We then used different approaches for transforming the x- and y-coordinates into commands. The arm had very limited range of motion along its x-axis, making it practical to measure the cargo's location at each position and pick the x-command which most closely corresponded to the cargo's location. However, the robot had a much greater range along its y-axis, leading us to take measurements and perform a cubic regression to get an equation of the command for a given y-coordinate. In addition, we added further tuning after this step, as the cargo's orientation required that the arm move increasingly along its y-axis to pick up a piece of cargo that is angled compared to the arm's gripper.</p> <p>Working as a team was very beneficial for this project. Although we all planned the structure of the project and overcame obstacles as a team, each person was able to specialize in a different area of the project. We had one person work on the arm, another on image processing, and another on the transportation robot.</p>"},{"location":"reports/2023/command-control/","title":"Command and Control Dashboard","text":""},{"location":"reports/2023/command-control/#about","title":"About","text":""},{"location":"reports/2023/command-control/#team","title":"Team","text":"<p>Naimul Hasan (naimulhasan@brandeis.edu | LinkedIn) Jimmy Kong (jameskong@brandeis.edu | LinkedIn) Brandon J. Lacy (blacy@brandeis.edu | LinkedIn)</p>"},{"location":"reports/2023/command-control/#submission-date","title":"Submission Date","text":"<p>May 5, 2023</p>"},{"location":"reports/2023/command-control/#github-repository","title":"GitHub Repository","text":""},{"location":"reports/2023/command-control/#demo-video","title":"Demo Video","text":""},{"location":"reports/2023/command-control/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction </li> <li>Campus Rover Dashboard<ul> <li>Overview</li> <li>System Architecture</li> <li>Web-Client<ul> <li>ReactJS</li> <li>ROSBridge and ROSLIBJS</li> </ul> </li> <li>GPS<ul> <li>GPS2IP</li> </ul> </li> <li>ROS<ul> <li>Nodes<ul> <li>GPS</li> <li>img_res</li> <li>ROS Topic List</li> </ul> </li> </ul> </li> </ul> </li> <li>Walkthrough<ul> <li>GitHub<ul> <li>Architecture</li> <li>Contribution Policy</li> </ul> </li> <li>Tutorial</li> </ul> </li> <li>Story of Project<ul> <li>Overview</li> <li>Team Structure<ul> <li>Separation of Duties</li> </ul> </li> <li>Timeline<ul> <li>Major Hurdles</li> <li>Major Milestones</li> </ul> </li> </ul> </li> <li>Conclusion</li> </ol>"},{"location":"reports/2023/command-control/#_1","title":"Command and Control Dashboard","text":""},{"location":"reports/2023/command-control/#introduction","title":"Introduction","text":"<p>\u201cYou can\u2019t be an expert in everything. Nobody\u2019s brain is big enough.\u201d A quote that\u2019s been stated by Pito at least twice this semester and once last semester in his software entrepreneurship course, but one that hadn\u2019t truly sunk in until the time in which I, Brandon, sat down to write this report with my teammates, Naimul and Jimmy.</p> <p>I\u2019ve greatly enjoyed the time spent on group projects within my professional experiences, but always erred on the side of caution with group work in college as there\u2019s a much higher chance an individual won\u2019t pull their weight. It\u2019s that fear of lackluster performance from the other team members that has always driven me to want to work independently. I\u2019d always ask myself, \u201cWhat is there to lose when you work alone? </p> <p>You\u2019ll grow your technical expertise horizontally and vertically in route to bolster your resume to land that job you\u2019ve always wanted, nevermind the removal of all of those headaches that come from the incorporation of others. There is nothing to lose, right?\u201d </p> <p>Wrong.</p> <p>You lose out on the possibility that the individuals you could\u2019ve partnered with are good people, with strong technical skills that would\u2019ve made your life a whole lot easier throughout the semester through the separation of duties. You lose out on a lot of laughs and the opportunity to make new friends. </p> <p>Most importantly, however, you lose out on the chance of being partnered with individuals who are experts in areas which are completely foreign to you.There is nobody to counter your weaknesses. You can\u2019t be an expert in everything, but you can most certainly take the gamble to partner with individuals to collectively form one big brain full of expertise to accomplish a project of the magnitude in which we\u2019ve accomplished this semester. </p> <p>I know my brain certainly wasn\u2019t big enough to accomplish this project on my own. I\u2019m grateful these two men reached out to me with interest in this project. Collectively, we\u2019ve set the foundation for future iterations of the development of campus rover and we think that\u2019s pretty special considering the long term impact beyond the scope of this course. So, I guess Pito was right after all; one brain certainly isn\u2019t big enough to carry a project, but multiple brains that each contribute different areas of expertise. Now that\u2019s a recipe for success. </p> <p></p>"},{"location":"reports/2023/command-control/#campus-rover-dashboard","title":"Campus Rover Dashboard","text":""},{"location":"reports/2023/command-control/#overview","title":"Overview","text":"<p>The command and control dashboard, otherwise known as the campus rover dashboard, is a critical component to the development of the campus rover project at Brandeis University as it\u2019s the medium in which remote control will take place between the operator and the robot. It is the culmination of three components: a web client, robot, and GPS. Each component is a separate entity within the system which leverages the inter-process communications model of ROS to effectively transmit data in messages through various nodes. There are various directions in which the campus rover project can progress, whether it be a tour guide for prospective students or a package delivery service amongst faculty. The intention of our team was to allow the command and control dashboard to serve as the foundation for future development regardless of the direction enacted upon in the future. </p> <p></p>"},{"location":"reports/2023/command-control/#system-architecture","title":"System Architecture","text":""},{"location":"reports/2023/command-control/#web-client","title":"Web-Client","text":""},{"location":"reports/2023/command-control/#reactjs","title":"React.js","text":"<p>The front-end and back-end of our web client is built with React.js. The app.js file is used to manage routes and some React-Bootstrap for the creation of certain components, such as buttons. The code structure is as follows: app.js contains the header, body, and footer components. The header component hosts the web client\u2019s top-most part of the dashboard which includes the name of the application with links to the different pages. The footer component hosts the web-client\u2019s bottom-most part of the dashboard which includes the name of each team member. The body component is the meat of the application which contains the routes that direct the users to the various pages. These routes are as follows: about, help, home, and settings. Each of these pages utilize components that are created and housed within the rosbridge folder. You can think of them as parts of the page such as the joystick or the video feed. The ros_congig file serves as the location for default values for the settings configurations. We use local storage to save these settings values and prioritize them over the default values if they exist. </p> <p></p>"},{"location":"reports/2023/command-control/#rosbridge-and-roslibjs","title":"ROSbridge and ROSLIBJS","text":"<p>ROSBridge is a package for the Robot Operating System (ROS) that provides a JSON-based interface for interacting with ROS through WebSocket protocol (usually through TCP). It allows external applications to communicate with ROS over the web without using the native ROS communication protocol, making it easier to create web-based interfaces for ROS-based robots. ROSLIBJS is a JavaScript library that enables web applications to communicate with ROSBridge, providing a simple API for interacting with ROS. It allows developers to write web applications that can send and receive messages, subscribe to topics, and call ROS services over websockets. ROSBridge acts as a bridge between the web application and the ROS system. It listens for incoming WebSocket connections and translates JSON messages to ROS messages, and the other way around. ROSLIBJS, on the other hand, provides an API for web applications to interact with ROSBridge, making it easy to send and receive ROS messages, subscribe to topics, and call services. In a typical application utilizing ROSBridge and ROSLIBJS, it would have the following flow:</p> <ol> <li>Web client uses ROSLIBJS to establish a WebSocket connection to the ROSBridge server, through specifying a specific IP address and port number.</li> <li>The web client sends JSON messages to ROSBridge, which converts them to ROS messages and forwards them to the appropriate ROS nodes.</li> <li>If the node has a reply, the ROS nodes send messages back to ROSBridge, which converts them to JSON and sends them over the WebSocket connection to the web client.</li> <li>ROSLIBJS API processes the incoming JSON messages so it can be displayed/utilized to the web client</li> </ol> <p></p>"},{"location":"reports/2023/command-control/#gps","title":"GPS","text":"<p>GPS, or Global Positioning System, is a widely-used technology that enables precise location tracking and navigation anywhere in the world through trilateration, a process which determines the receiver\u2019s position by measuring the distance between it and several satellites. Though it has revolutionized the way we navigate and track objects, its accuracy can vary depending on a variety of factors that are beyond the scope of this paper. However, there are various companies which have embraced the challenge of a more accurate navigation system that incorporates other data points into the algorithm responsible for the determination of the receiver position. Apple is notorious for the pin-point accuracy available within their devices which incorporate other sources such as nearby Wi-Fi networks and cell towers to improve its accuracy and speed up location fixes. The utilization of a more sophisticated location technology is critical for this project to be able to navigate routes within our university campus. Therefore, we\u2019ve chosen to leverage an iPhone 11 Pro placed on our robot with the iOS application GPS2IP open to leverage the technology available in Apple\u2019s devices to track our robot\u2019s movement. </p> <p></p>"},{"location":"reports/2023/command-control/#gps2ip","title":"GPS2IP","text":"<p>GPS2IP is an iOS application that transforms an iPhone or iPad into a wireless GPS transmitter over the network. It possesses various features such as high configurability, background operation on the iPhone, and standard NMEA message protocols. There is a free lite version available for testing and system verification as well as a paid version with all available features. </p> <p></p>"},{"location":"reports/2023/command-control/#ros","title":"ROS","text":""},{"location":"reports/2023/command-control/#nodes","title":"Nodes","text":""},{"location":"reports/2023/command-control/#gps_1","title":"/gps","text":"<p>The node\u2019s purpose is to listen to GPS2IP through a socket and publish the data as a gps topic. The GPS2IP iOS application provides the functionality of a portable, highly sophisticated, GPS receiver that provides a very accurate location when the iPhone is placed on the robot. The GPS data is sent as a GLL NMEA message from a given IP address and corresponding port number. The /gps node leverages the socked package in Python to listen to the messages published at that previously emphasized IP address and port number. Through this connection, a stream of GLL messages can be received, parsed, and transformed into the correct format to be interpreted by the web client. The main transformation is for the latitude and longitude from decimal and minutes to decimal degrees format. The node will publish continuously until the connection through the socket is disturbed. The topic has a data type of type String, which is a serialized version of JSON through the utilization of the json package to dump the Python dictionary. </p>"},{"location":"reports/2023/command-control/#code","title":"Code","text":"<p><pre><code>#!/usr/bin/env python\n\n'''\nA module with a GPS node.\n\nGPS2IP: http://www.capsicumdreams.com/gps2ip/\n'''\n\nimport json\nimport re\nimport rospy\nimport socket\n\nfrom std_msgs.msg import String\n\nclass GPS:\n    '''A node which listens to GPS2IP Lite through a socket and publishes a GPS topic.'''\n    def __init__(self):\n        '''Initialize the publisher and instance variables.'''\n        # Instance Variables\n        self.HOST = rospy.get_param('~HOST', '172.20.38.175')\n        self.PORT = rospy.get_param('~PORT', 11123)\n\n        # Publisher\n        self.publisher = rospy.Publisher('/gps', String, queue_size=1)\n\n    def ddm_to_dd(self, degrees_minutes):\n        degrees, minutes = divmod(degrees_minutes, 100)\n        decimal_degrees = degrees + minutes / 60\n        return decimal_degrees\n\n    def get_coords(self):\n        '''A method to receive the GPS coordinates from GPS2IP Lite.'''\n        # Instantiate a client object\n        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n            s.connect((self.HOST, self.PORT))\n            # The data is received in the RMC data format\n            gps_data = s.recv(1024)\n\n        # Transform data into dictionary\n        gps_keys = ['message_id', 'latitude', 'ns_indicator', 'longitude', 'ew_indicator']\n        gps_values = re.split(',|\\*', gps_data.decode())[:5]\n        gps_dict = dict(zip(gps_keys, gps_values))\n\n        # Cleanse the coordinate data\n        for key in ['latitude', 'longitude']:\n            # Identify the presence of a negative number indicator\n            neg_num = False\n\n            # The GPS2IP application transmits a negative coordinate with a zero prepended\n            if gps_dict[key][0] == '0':\n                neg_num = True\n\n            # Transform the longitude and latitude into decimal degrees\n            gps_dict[key] = self.ddm_to_dd(float(gps_dict[key]))\n\n            # Apply the negative if the clause was triggered\n            if neg_num:\n                gps_dict[key] = -1 * gps_dict[key]\n\n        # Publish the decoded GPS data\n        self.publisher.publish(json.dumps(gps_dict))\n\nif __name__ == '__main__':\n    # Initialize a ROS node named GPS\n    rospy.init_node(\"gps\")\n\n    # Initialize a GPS instance with the HOST and PORT\n    gps_node = GPS()\n\n    # Continuously publish coordinated until shut down\n    while not rospy.is_shutdown():\n        gps_node.get_coords()\n</code></pre> </p>"},{"location":"reports/2023/command-control/#img_res","title":"img_res","text":"<p>The node\u2019s purpose is to alter the quality of the image through a change in resolution. It\u2019s done through the utilization of the OpenCV package, CV2, and cv_bridge in Python. The cv_bridge package, which contains the CvBridge() object, allows for seamless conversion from a ROS CompressedImage to a CV2 image and vice versa. There are two different camera topics that can be subscribed to depending upon the hardware configuration, /camera/rgb/image/compressed or /raspicam_node/image/compressed. Additionally, the /image_configs topic is subscribed to to receive the specifications for the resolution from the web client. A new camera topic is published with the altered image under the topic name /camera/rgb/image_res/compressed or /raspicam_node/image_res/compressed depending upon the hardware configuration. The web client subscribes to this topic for the camera feed. </p>"},{"location":"reports/2023/command-control/#code_1","title":"Code","text":"<p><pre><code>#!/usr/bin/env python\n\n'''\nA module for a node that alters the quality of the image.\n'''\n\nimport cv2\nimport numpy as np\nimport re\nimport rospy\n\nfrom cv_bridge import CvBridge\nfrom sensor_msgs.msg import CompressedImage\nfrom std_msgs.msg import String\n\n\nclass Compressor:\n    ''''''\n    def __init__(self):\n        '''Initialize necessary publishers and subscribers.'''\n        # Instance Variables\n        # Initialize an object that converts OpenCV Images and ROS Image Messages\n        self.cv_bridge = CvBridge()\n        # Image Resolution\n        self.img_res = {\n            'height': 1080,\n            'width': 1920\n        }\n\n        # Publisher - https://answers.ros.org/question/66325/publishing-compressed-images/\n        # self.publisher = rospy.Publisher('/camera/rgb/image_res/compressed', CompressedImage, queue_size=1)\n        self.publisher = rospy.Publisher('/raspicam_node/image_res/compressed', CompressedImage, queue_size=1)\n\n        # Subscribers\n        # self.subscriber_cam = rospy.Subscriber('/camera/rgb/image_raw/compressed', CompressedImage, self.callback_cam, queue_size=1)\n        self.subscriber_cam = rospy.Subscriber('/raspicam_node/image/compressed', CompressedImage, self.callback_cam, queue_size=1)\n        self.subscriber_set = rospy.Subscriber('/image_configs', String, self.callback_set, queue_size=1)\n\n\n    def callback_cam(self, msg):\n        '''A callback that resizes the image in line with the specified resolution.'''\n        # Convert CompressedImage to OpenCV\n        img = self.cv_bridge.compressed_imgmsg_to_cv2(msg)\n\n        # Apply New Resolution\n        img = cv2.resize(img, (self.img_res['height'], self.img_res['width']))\n\n        # Convert OpenCV to CompressedImage\n        msg_new = self.cv_bridge.cv2_to_compressed_imgmsg(img)\n\n        # Publish\n        self.publisher.publish(msg_new)\n\n    def callback_set(self, msg):\n        '''A callback to retrieve the specified resolution from the web client.'''\n        img_set = re.split(',', msg.data)\n\n        self.img_res['height'] = int(img_set[0])\n        self.img_res['width'] = int(img_set[1])\n\n\nif __name__ == \"__main__\":\n    rospy.init_node(\"img_comp\")\n    comp = Compressor()\n    rospy.spin()\n</code></pre> </p>"},{"location":"reports/2023/command-control/#rostopiclist","title":"/rostopiclist","text":"<p>The node\u2019s purpose is to retrieve the output generated by the terminal command \u201crostopic list\u201d to receive a list of currently published topics. It performs this functionality through the use of the subprocess package in Python, which will create a new terminal to execute the command. The output is then cleaned to be published over the topic \u201c/rostopic_list\u201d. The topic has a data type of type String. It is subscribed too by the web client and parse the data based upon the comma delimiter. The content is used to create a scrollable menu that displays all of the available rostopics. </p>"},{"location":"reports/2023/command-control/#code_2","title":"Code","text":"<p><pre><code>#!/usr/bin/env python\n\nimport rospy\nfrom std_msgs.msg import String\nimport subprocess\n\ndef get_ros_topics():\n    \"\"\"\n    Returns a list of active ROS topics using the `rostopic list` command.\n    \"\"\"\n    command = \"rostopic list\"\n    process = subprocess.Popen(command.split(), stdout=subprocess.PIPE)\n    output, error = process.communicate()\n    topics = output.decode().split(\"\\n\")[:-1] # remove empty last element\n    return topics\n\ndef rostopic_list_publisher():\n    \"\"\"\n    ROS node that publishes the list of active ROS topics as a string message.\n    \"\"\"\n    rospy.init_node('rostopic_list_publisher', anonymous=True)\n    pub = rospy.Publisher('/rostopic_list', String, queue_size=10)\n    rate = rospy.Rate(1) # 1 Hz\n\n    print(\"[INFO] Robot.py node has been started!\")\n\n    while not rospy.is_shutdown():\n   1     topics = get_ros_topics()\n        message = \",\".join(topics)\n        pub.publish(message)\n        rate.sleep()\n\nif __name__ == '__main__':\n    try:\n        rostopic_list_publisher()\n    except rospy.ROSInterruptException:\n        pass\n</code></pre> </p>"},{"location":"reports/2023/command-control/#walkthrough","title":"Walkthrough","text":""},{"location":"reports/2023/command-control/#github","title":"GitHub","text":""},{"location":"reports/2023/command-control/#architecture","title":"Architecture","text":"<p>The command-control repository can be effectively broken down into six branches, with one deprecated branch, to create a total of five active branches: </p> <p>main: The branch contains the stable version of the project. It is locked, which means no one can directly push to this branch without another person reviewing and testing the changes. </p> <p>ros-api: The branch was initially committed to develop an API through FastAPI, but was later deprecated because the functionality could be implemented with ROSBridge and ROSLIBJS. </p> <p>ros-robot: The branch focuses on the development and integration of the Robot Operating System (ROS) with a physical or simulated robot. It includes the launch files responsible for the run of the gps, dynamic image compression, and ROSBridge. </p> <p>test-old-videofeed: The branch was created to test and debug the previous implementation of the dashboard, which was built five years ago. We imported the video from the old dashboard to our dashboard but later deprecated this implementation as it used additional redundant dependencies which can be implemented with ROSBridge and ROSLIBJS. </p> <p>test-old-web-client: The branch is to test and understand the previous version of the web client dashboard. It helped us realize areas for improvement, such as a way to set the robot\u2019s IP address within the web application. It also set the tone for our simplified dashboard format. </p> <p>web-client: The branch is for ongoing development of the new web client. It includes the latest features, improvements, and bug fixes for the web client, which may eventually be merged into the main branch after thorough testing and review. </p> <p></p>"},{"location":"reports/2023/command-control/#contribution-policy","title":"Contribution Policy","text":"<p>In this section it will guide you through the process of contributing to a GitHub repository following the guidelines outlined in the contributing.md file. This file specifies that the main branch is locked and cannot be directly pushed to, ensuring that changes are reviewed before being merged. </p> <ol> <li>Update the Main Branch</li> <ol> <li>Ensure you are on the main branch and pull down the new changes.</li> <li>Resolve any conflicts which may arise at this time.</li> </ol> <li>Create a New Branch</li> <ol> <li>Create a new branch for the feature or issue and switch to it.</li> <li>Verify the current branch is the correct branch before you begin work.</li> </ol> <li>Build Feature</li> <ol> <li>Develop the feature or fix the issue on the new branch.</li> </ol> <li>Check for Conflicts within the Main Branch</li> <ol> <li>Ensure there are no conflicts with the main branch prior to the creation of the pull request. Update the main branch and merge the changes into your feature branch.</li> </ol> <li>Create a Pull Request (PR)</li> <ol> <li>Push the changes to the remote repository and create a pull request on GitHub. Provide a clear and concise description of the purpose of the feature as well as how to test it. Ensure you mention any related issue numbers.</li> </ol> <li>Request a Review</li> <ol> <li>Ask another team member to review your pull request and run the code locally to ensure there are no errors or conflicts.</li> </ol> <li>Merge the Branch</li> <ol> <li>Once the review is complete and the changes are approved, the feature branch can be merged into the main branch through the pull request.</li> </ol> </ol> <p></p>"},{"location":"reports/2023/command-control/#tutorial","title":"Tutorial","text":""},{"location":"reports/2023/command-control/#installation-and-run","title":"Installation and Run","text":"<ol> <li>Clone the GitHub repository which contains the web client code.</li> <li>Install the necessary dependencies by running ```npm install``` in the terminal.</li> <li>Start the web client by running ```npm start``` in the terminal.</li> <li>Open a web browser and type in http://localhost:3000.</li> <li>Navigate to the settings page on the web client to set them according to your intentions.</li> </ol>"},{"location":"reports/2023/command-control/#web-client-settings","title":"Web Client Settings","text":"<ul> <li>The IP address of the ROSBridge server. It is the server that connects the web application to the ROS environment.</li> <li>Port: The port number for the ROSBridge server.</li> <li>Video Resolution Width: The width of the video resolution for the ROS camera feed.</li> <li>Video Resolution Height: The height of the video resolution for the ROS camera feed.</li> <li>Video Frame Width: The width of the video frame for the ROS camera feed.</li> <li>Video Frame Height: The height of the video frame for the ROS camera feed.</li> <li>Show Battery Status: A toggle to display the battery status in the web application.</li> <li>Manual Input Teleoperation: A toggle to enable or disable the manual teleoperation control input to replace the joystick.</li> </ul>"},{"location":"reports/2023/command-control/#run-with-ros-robot","title":"Run with ROS Robot","text":"<ol> <li>SSH into the robot.</li> <li>Open the GPS2IP iOS application. In the application settings, under \u201cNMEA Messages to Send\u201d, solely toggle \u201cGLL\u201d. Once set, toggle the \u201cEnable GPS2IP\u201d on the homepage of the application. Additionally, turn off the iOS auto-lock to prevent the application connection from being interrupted. Place the device on the robot. </li> <li>Launch the necessary ROS launch file to run the vital nodes which include gps, img_res, and rostopiclist. You may use the sim.launch file for a simulation in Gazebo and real.launch for real world.</li> <li>Run any other nodes that you desire.</li> <li>Ensure the web client has the correct ROSBridge Server IP Address and Port set within the settings page. You can check this with the \u201ccurrent configurations\u201d button.</li> <li>If you want the camera to show up, make sure that the correct campera topic is set within the img_res node module. It can vary dependent upon the hardware. The two options are listed within the file with one commented out and the other active. Select one and comment out the other.</li> <li>Enjoy the web client!</li> </ol>"},{"location":"reports/2023/command-control/#story-of-project","title":"Story of Project","text":""},{"location":"reports/2023/command-control/#overview_1","title":"Overview","text":"<p>I, Brandon, had initially intended to work on this project independently but was later reached out to by Naimul and Jimmy to form the team as presently constructed. The project was initially framed as a simple remote teleoperation application in which a user could control the robot with an arrow pad. However, it has evolved into a more sophisticated product with the expertise added from my fellow teammates. The project is now boasted as a dashboard that leverages a joystick for a more intuitive control mechanism with a live camera feed and GPS localization which far exceeded the initial outline as a result of the additional requirements associated with a group of three individuals in a team. Pito proposed various ideas associated with campus rover and it was on our own accord that we chose to combine the dashboard with the GPS localization to create a strong command center for a user to control a robot of choice. </p> <p></p>"},{"location":"reports/2023/command-control/#team-structure","title":"Team Structure","text":"<p>The team was constructed based upon a single common interest, participation within a final project that challenged our software engineering skills within a new domain that would have a long-term impact beyond this semester. It was coincidental that our individual areas of expertise balanced out our weaknesses. The team was balanced and all members were extremely active participants throughout weekly stand ups, discussions, and lab work. </p> <p></p>"},{"location":"reports/2023/command-control/#separation-of-duties","title":"Separation of Duties","text":""},{"location":"reports/2023/command-control/#naimul-hasan","title":"Naimul Hasan","text":"<p>Throughout the semester, my responsibilities evolved according to the team's requirements. Initially, after Jimmy provided a visual representation of the previous dashboard, I designed an intuitive and user-friendly Figma layout. I then examined the code and relevant research, which led me to employ ROSLIBJS and RosBridge for the project. I dedicated a week to establishing a workflow standard for the team, including locking down the main branch and authoring a CONTRIBUTING.md document to ensure everyone understood and followed the desired workflow. After gaining an understanding of websocket communication between ROSLIBJS and ROSBridge, I focused on developing the Home page, which now features a video feed, map, and teleop joystick component. To address the issue of multiple websocket connections for each component, I transitioned to using a React Class Component with a global ROS variable. This enabled the components to subscribe and publish messages seamlessly. Additionally, I modified one of the ROSLIBJS dependencies to prevent app crashes and created a default settings configuration for simulation mode, which Jimmy later integrated with localStorage. The initial configuration consumed a significant portion of time, followed by the development of the map component. After exploring various options, such as react-leaflet and Google Maps, I chose Mapbox due to its large community and free API for GPS coordinates. Simultaneously, James and I resolved a dashboard crash issue stemming from localStorage data being empty and causing conversion errors. Collaborating with Brandon, I worked on integrating GPS data into the dashboard. We encountered a problem where the frontend couldn't correctly read the message sent as a string. After a week, we devised a solution in the Python code, converting the GPS data from Decimal Degrees and Minutes (DDM) to Decimal Degrees (DD). This adjustment enabled us to track movements on the map accurately. Additionally, I was responsible for deploying the app, initially attempting deployment to Netlify and Heroku. After evaluating these options, I ultimately settled on Azure, which provided the most suitable platform for our project requirements. </p> <p>It is worth noting that regardless of whether the app is deployed locally, within the Kubernetes cluster on campus, or on Azure, the primary requirement for successful communication between the laptop/client and the robot is that they are both connected to the same network, since dashboard relies on websocket communication to exchange data and control messages with the robot, which is facilitated by the ROSLIBJS and ROSBridge components. Websocket connections can be established over the same local network, allowing for seamless interaction between the robot and the dashboard. </p>"},{"location":"reports/2023/command-control/#jimmy-kong","title":"Jimmy Kong","text":"<p>During the beginning of the semester, my work started out as researching the old web client to see if we could salvage features. I was able to get the old web client working which helped us research and analyze the ways in which rosbridge worked and how we would have our web client connect to it as well. After we got a good foundational understanding from the web client, I started working on a settings page which would provide the user with a more convenient and customizable experience. The settings page went through multiple iterations before it became what it is today, as it started out very simple. The following weeks I would work on adding additional components such as the dropdown menu that shows the list of ros topics, the dropdown menu that shows the current settings configuration, the manual input teleoperation form that allows the user to manually input numbers instead of using the joystick, the robot battery status indicator that shows the battery status of the robot, clear button to clear the input boxes, reset to default button to clear locally stored values, placeholder component that displays a video error message, and last but not least dark mode for theming the website and for jokes since we basically wrapped up the project with a few days to spare. For the dropdown menu that shows the list of ros topics, I also had to create a python node that would send the \u201crostopic list\u201d command and then use that output to then publish as a string separated by commas as a row topic. I would then subscribe to that topic and parse it via commas on the web client side for the rostopiclist button. After adding all these components, I also had to add the corresponding buttons (if applicable like dark mode or battery status) to the settings page to give the user the option to disable these features if they did not wish to use them. Inserting forms and buttons into the settings page made us have to think of a way to save the settings configurations somehow to survive new browser sessions, refreshes, and the switching of pages. To solve this issue, I utilized localStorage which worked great. I wrote logic for deciding when to use localStorage and when to use default values stored in our ros_configs file. After finishing the implementation of local storage, I also decided to add some form validation along with some quality of life error indicator and alert messages to show the user that they are putting in either correct or incorrect input. Aside from focusing on the settings page, I also handled the overall theming and color scheme of the website, and made various tweaks in CSS to make the website look more presentable and less of a prototype product. I also modified the about page to look a bit better and created a new help page to include instructions for running the web client. In summary, I was responsible for creating multiple components for the web client and creating the appropriate setting configuration options within the settings page, working on the aesthetics of the website, and creating the help page. Lastly, I also edited the demo video. </p>"},{"location":"reports/2023/command-control/#brandon-j-lacy","title":"Brandon J. Lacy","text":"<p>The contribution to this project on my behalf can be broken down into two different phases: team management and technical implementation. I was responsible for the project idea itself and the formation of the team members. I delegated the duties of the project amongst the team members and led the discussions within our weekly discussions outside of the classroom. I also set the deadlines for our team to stay on track throughout the semester. The technical contribution revolves around the GPS implementation in which I selected the GPS receiver, an iPhone with the GPS2IP iOS application. I was responsible for the Python code to receive the data from the GPS receiver over the internet. Lastly, I was responsible for the node that changes the resolution of the image that is displayed on the web client.Lastly, I wrote 75% of the report and the entire poster. </p> <p></p>"},{"location":"reports/2023/command-control/#timeline","title":"Timeline","text":""},{"location":"reports/2023/command-control/#major-hurdles","title":"Major Hurdles","text":"<p>Initially, the dashboard was hosted on Naimul's laptop for testing purposes, which allowed the team to evaluate its functionality and performance in a controlled environment. When we  tried to deploy the dashboard on Netlify, we encountered several issues that hindered the deployment process and the app's performance.</p> <p>One of the primary issues with Netlify was its inability to run a specific version of Node.js, as we want to avoid version mismatch, required by the dashboard. This limitation made it difficult to build the app using Netlify's default build process, forcing the us to manually build the app instead. However, even after successfully building the dashboard manually, the we faced additional problems with the deployment on Netlify. The app would not load the map component and other pages as expected, which significantly impacted the app's overall functionality and user experience. </p> <p>As we were trying to find a new platform to host the dashboard so it can run 24/7 and pull down any changes from the main branch. Azure allowed us to download specific version of dependencies for the dashboard, while leaving overhead in resource availability if there is a lot of traffic. As mentioned above, regardless of whether the app is deployed locally, within the Kubernetes cluster on campus, or on Azure, the primary requirement for successful communication between the laptop/client and the robot is that they are both connected to the same network, since dashboard relies on websocket communication to exchange data and control messages with the robot, which is managed by the ROSLIBJS and ROSBridge components. Websocket connections can be established over the same local network, allowing for seamless interaction between the robot and the dashboard.</p>"},{"location":"reports/2023/command-control/#major-milestones","title":"Major Milestones","text":""},{"location":"reports/2023/command-control/#public-web-client-hosting","title":"Public Web Client hosting","text":"<p>Hosting the web client publically so that anyone can access the website with a link was a major milestone that we reached since just hosting through local host was not practical since its just for development usage and would not be very useful. More is explained within the major hurdle discussed above.</p>"},{"location":"reports/2023/command-control/#gps_2","title":"GPS","text":"<p>The implementation of the GPS was a major milestone in our project as it was the most difficult to design. When the project was initialized, there was no intention to incorporate GPS into the dashboard. However, the requirements for the project were increased when the team expanded to three people. The GPS portion of the project was dreaded, quite frankly, so when the Python code was completed for a ROS node that listened to the GPS data being published from the GPS2IP iOS application it was a huge celebration. The kicker was when the topic was subscribed to on the front-end and the visualization on the map was perfectly accurate and moved seamlessly with our movement across campus. </p>"},{"location":"reports/2023/command-control/#dark-mode","title":"Dark Mode","text":"<p>Dark mode, a seemingly small feature, turned out to be a monumental milestone in our project. Its implementation was far from smooth sailing, and it presented us with unexpected challenges along the way. Little did we know that this seemingly simple concept would revolutionize the entire user experience and become the most crucial component of our project. Initially, we underestimated the complexity of integrating dark mode into our dashboard. We had to navigate through a labyrinth of CSS styles, meticulously tweaking each element to ensure a seamless transition between light and dark themes. Countless hours were spent fine-tuning color schemes, adjusting contrasts, and experimenting with different shades to achieve the perfect balance between aesthetics and readability. The moment dark mode finally came to life, everything changed. It was as if a veil had been lifted, revealing a whole new dimension to our dashboard. The sleek, modern interface exuded an air of sophistication, captivating users and immersing them in a visually stunning environment. It breathed life into every element, enhancing the overall user experience. It quickly became apparent that dark mode was not just another feature; it was the heart and soul of our project. The dashboard transformed into an oasis of productivity and creativity, with users effortlessly gliding through its streamlined interface. Tasks that were once daunting became enjoyable, and the project as a whole gained a newfound momentum. In fact, we can boldly assert that without dark mode, our dashboard would be practically unusable. The blinding glare of the bright background would render the text illegible, and the stark contrast would induce eye strain within minutes. Users would be left squinting and frantically searching for a pair of sunglasses, rendering our carefully crafted functionalities utterly useless. Dark mode's significance cannot be overstated. It has redefined the essence of our project, elevating it to new heights. As we reflect on the struggles, the countless lines of code, and the sleepless nights spent perfecting this feature, we cannot help but celebrate the impact it has had. Dark mode, the unsung hero of our project, has left an indelible mark on our journey, forever changing the way we perceive and interact with our dashboard. </p> <p></p>"},{"location":"reports/2023/command-control/#conclusion","title":"Conclusion","text":"<p>We sought a deeper understanding of the inter-process communications model of ROS to be able to allow our web application to take advantage of this model and we\u2019ve successfully developed this knowledge. Additionally, it allowed us to integrate our software development knowledge with ROS to create a complex system which will draw attention to our respective resumes. We are proud of the product created in this course and see it as a major improvement from the previous implementation of the dashboard. The only aspect in which we are not content is the lag present on the camera feed and joystick. However, there are areas for improvement in which we recommend to be addressed by the next team to tackle this project. The lag issue previously described is one of them as it decreases usability of the product. It would also be useful to create a map of the campus with way points in which the GPS can be utilized to create a path from one place to another. There are a variety of improvements to be made to this application to continue to shape campus rover into an implemented product. It\u2019s up to the next generation of students to decide where to take it next. </p>"},{"location":"reports/2023/dynamaze/","title":"Dynamaze.md","text":""},{"location":"reports/2023/dynamaze/#spring-2023-aiden-dumas-and-muthhukumar-malaiiyyappan","title":"Spring 2023, Aiden Dumas and Muthhukumar Malaiiyyappan","text":""},{"location":"reports/2023/dynamaze/#github-httpsgithubcomcampusroverdynamaze","title":"Github: https://github.com/campusrover/dynamaze","text":""},{"location":"reports/2023/dynamaze/#introduction","title":"Introduction","text":"<p>We started this project with a simple assumption. Solving a maze is an easy task. If you enter a maze and place your hand on one wall of the maze and traverse the maze without ever removing your hand you\u2019ll reach the exit. What happens though if the maze is\u2026 dynamic? How would you solve a maze that shifts and changes? The problem is itself intractable. There is no guaranteed way to solve such a maze, but fundamentally we believe that a brute force algorithm is a poor solution to the problem and with the possibility of cycles arising in such a maze; sometimes not a solution at all. We believe that in a dynamic maze it is possible to come up with a solution that given an infinite amount of time, can solve the maze by using knowledge about the structure of the maze as a whole. </p> <p>This problem essentially is a simplification of a much more complicated problem in robotics: given a starting point and a known endpoint with an unknown location in an unknown environment, navigate from point A to point B with the added complication that the environment itself is not consistent. Think something comparable to a delivery robot being dropped off at an active construction site. There is no time to stop construction to allow the robot to scan and build a map of the site. There are going to be constantly moving vehicles and materials around the site. A worker tells the robot to go to the manager\u2019s office and gives the robot some description to identify it. The robot must get from where it is to the office despite the scenario it is placed in. This is essentially the problem we are solving. </p>"},{"location":"reports/2023/dynamaze/#objectives","title":"Objectives","text":"<ul> <li>Problem: Develop an algorithm to autonomously navigate and solve a dynamically changing maze.</li> <li>Explore the maze and build a representation of it as a graph data structure with intersections as nodes and distances between them as edges</li> <li>Create a navigation system based on this graph representation to travel effectively around the maze, updating the graph as changes are detected in real-time. Essentially developing our own SLAM algorithm. (SLAM being an algorithm than automates navigation based on a known map)</li> <li>Efficiently search our graph for the exit to the maze its dynamic property making the graph unreliable</li> </ul>"},{"location":"reports/2023/dynamaze/#relevant-sources","title":"Relevant Sources","text":"<ul> <li>A Helpful Resource for Overall Maze Methodology: https://www.pololu.com/file/0J195/line-maze-algorithm.pdf</li> <li>Some Help With Tuning Your PID: https://www.crossco.com/resources/technical/how-to-tune-pid-loops/</li> <li>Lidar Methodology: https://github.com/ssscassio/ros-wall-follower-2-wheeled-robot/blob/master/report/Wall-following-algorithm-for-reactive%20autonomous-mobile-robot-with-laser-scanner-sensor.pdf</li> <li>Entity Used for Consulting: https://chat.openai.com</li> </ul>"},{"location":"reports/2023/dynamaze/#files-how-to-use","title":"Files &amp; How to Use","text":"<p>Use: All that is needed to use the code is to clone the github repository and put your robot in a maze!</p> File Name Purpose laserUtils.py Process Lidar Data pid_explore.py Navigating Maze"},{"location":"reports/2023/dynamaze/#methodology","title":"Methodology","text":"<p>From a birds eye view, the solution we came up with to solve a dynamic maze mirrors a very interesting problem in graph algorithms. With the assumption that the key to our solution lies in building an understanding of the structure of the maze as a whole, we build a map on the fly as we traverse the maze. We represent the maze as a graph data structure, labeling each intersection in the maze we reach as a node and each edge as the distance between these intersections in the maze. We create these nodes and edges as we encounter them in our exploration and delete nodes and edges as we come across changes in the maze. This problem can then be seen in actuality as a graph algorithms problem. Essentially we are running a graph search for a node with an unknown entry point into an unknown graph that itself is unreliable or rather, \u201clive\u201d and subject to modification as it is read. There are then 2 stages to solving this problem. The first is learning the graph for what it is as we explore, or rather create a full picture of the graph. Once we have explored everything and have not found the exit we know searching for the exit is now the problem of searching for change. The crux of solving this problem is characterizing the changes themselves. It is by understanding how changes take place that we devise our graph search from here to find these changes. For example, a graph whose changes occur randomly with equal probability of change at each node would have an efficient search solution of identifying the shortest non cyclical path that covers the most amount of nodes. The change behavior we used for our problem was that changes happen at certain nodes repeatedly. To solve the maze, we then use the previous solution of searching the most nodes as quickly as possible for changes if we fully explore the maze without identifying anything, then once a changing node is found we label it as such so we can check it again later, repeating these checks until new routes in the maze open up. </p> <p>As a note, some simplifications that we use for time\u2019s sake: Width of corridors in the maze are fixed to .5m Turns with always be at right angles</p> <p>We designed an intelligent maze-solving robot navigation system that employs a depth-first search (DFS) algorithm for effective graph traversal. Our solution is structured into two primary components: laserutils and pid_explore, which work together seamlessly to facilitate the robot's navigation and exploration within the maze.</p> <p>The laserutils module is a collection of essential utility functions for processing laser scan data and assisting in navigation. It calculates direction averages, simplifies 90-degree vision frameworks, detects intersections, and implements a PID controller to maintain a straight path. Moreover, it incorporates functions for converting yaw values, managing node formatting, and handling graph data structures, ensuring a streamlined approach to processing and decision-making.</p> <p>The pid_explore module serves as the core of the robot's navigation system. It subscribes to laser scan and odometry data and employs the laserutils functions for efficient data processing and informed decision-making. The robot operates in three well-defined states: \"explore,\" \"intersection,\" and \"position,\" each contributing to a smooth and strategic navigation experience.</p> <p>In the \"explore\" state, the robot uses the PID controller to navigate straight, unless it encounters an intersection, prompting a transition to the \"intersection\" state. Here, the robot intelligently labels the intersection, assesses the path's continuation, and determines its next move. If the robot is required to turn, it shifts to the \"position\" state, where it identifies the turn direction (left or right) and completes the maneuver before resuming its straight path.</p> <p>Our navigation system employs a Graph class to store and manage the maze's structure, leveraging the power of the DFS algorithm to find the optimal path through the maze. The result is a sophisticated and efficient maze-solving robot that strategically navigates its environment, avoiding any semblance of brute force in its problem-solving approach.</p>"},{"location":"reports/2023/dynamaze/#story-of-project","title":"Story of Project","text":"<p>We built this project to address the challenges faced by robots in navigating unpredictable and dynamic environments, such as active construction sites or disaster zones. Traditional maze-solving algorithms, based on brute force and heuristics, are inadequate in such situations due to the constantly changing nature of the environment.</p> <p>Our goal was to develop an intelligent navigation system that could adapt to these changes in real-time and continue to find optimal paths despite the uncertainties. By creating a program that can autonomously drive a robot through a changing maze, we aimed to advance robotic navigation capabilities and make it more efficient and useful in a wide range of applications.</p> <p>With our innovative approach of \"on the fly\" map creation and navigation, we sought to enable robots to traverse dynamic spaces with minimal prior knowledge, making them more versatile and valuable in industries that require real-time adaptability. By simultaneously building and updating the map as the robot navigates, we ensured that the robot could rely on its current understanding of the environment to make informed decisions.</p> <p>Ultimately, we built this project to push the boundaries of robotic navigation, opening up new possibilities for robots to operate in complex, ever-changing environments, and offering solutions to a variety of real-world challenges.</p> <p>The project began our first week with a theoretical hashing out of how to approach the problem and what simplifications to make. We started by learning how to build simulations in Gazebo and implementing our Lidar methods. We then were able to create simple demos of an initial explore algorithm we created with the \u201cexplore\u201d, \u201cintersection\u201d, \u201cposition\u201d approach. Gazebo issues were encountered early on and were largely persistent throughout the course of the project with walls losing their physics upon world resets. At this point we built larger models in Gazebo to test with and implemented a PID algorithm to smooth the robot\u2019s traversal. This PID would go through several versions as the project went on especially as we tuned navigation to be more precise as we adjusted our hall width from 1 meter down to .5. We then formulated and integrated our graph data structure and implemented a DFS algorithm to automatically find paths from node to node to navigate. We then added node labeling and cycle detection to our graph to recognize previously visited nodes. One of our group member\u2019s github became permanently disconnected from our dev environment in the last stage of the process but all challenges are meant to be overcome. We finished by accounting for dynamic change in our algorithm and designed a demo. </p>"},{"location":"reports/2023/dynamaze/#final-product","title":"Final Product","text":"<p>Despite having trouble in our Gazebo simulations we were surprised to see that our algorithm worked well when we moved our tests over to the physical robots. PID especially, which had proven to be trouble in our testing in fact worked better in the real world than in simulations. There were a few corners cut in robustness (ie: with the simplifications agreed upon such as only right angle turns), but overall the final product is very adaptable within these constraints and accounts for a number of edge cases. Some of the potential improvements we would have liked to make is to expand past these simplifications of only having right-angled intersections and limited hall width. The prior would greatly change the way we would have to detect and encode intersections for the graph. A next algorithmic step could be having two robots traverse the maze from different points at the same time and sharing information between each other. This would be the equivalent of running two concurrent pointers in the graph algorithm.  Overall, though an initially simple project on the surface, we believe that the significance of this problem as a graph algorithms problem not only makes it an important subject in robotics (maze traversal) but also an unexplored problem in the larger world of computer science; searching an unreliable/live modifying graph. </p>"},{"location":"reports/2023/guard_robot/","title":"Guard Robot","text":"\ud83e\udd16\ud83e\udd16 Guard Robot (by Liulu Yue, Rongzi Xie, and Karen Mai) \ud83e\udd16\ud83e\udd16\ud83e\udd16  <p> For our team's COSI119a Autonomous Robotics Term Project, we wanted to tackle a challenge that was not too easy or not hard. All of our members have only recently learned ROS, so to create a fully demo-able project in 8 weeks sounded impossible. We started off having a few ideas: walking robot that follows an owner, robot that plays tag, and a pet robot that accompanies one. There were a lot of blockers for those ideas from limitation of what we know, tech equipments, time constraints, and mentor's concern that it will not be able to. We had to address issues like:   In the end, we decided to work on a robot that guards an object and stop an intruder from invading a specified region. We pivoted from having one robot to having multiple robots so that they are boxing out the object that we are protecting. We tested a lot of design decisions where we wanted to see which method would give us the desired result with the most simple, least error prone, and high perfomring result.    There were a lot of learnings throughout this project, and while even during the last days we were working to deploy code and continue to test as there is so much that we want to add.   </p> <p>Video demo: https://drive.google.com/drive/folders/1FiVtw_fQFKpZq-bJSLRGotYXPQ-ROGjI?usp=sharing</p> <p></p> <p> As we will be approaching this project with an agile scrum methodology, we decided to take the time in developing our user stories, so that at the end of the week during our sprint review we know if we are keeping us on track with our project. After taking a review of the assignment due dates for other assignments of the class, we thought that having our own group based deadlines on Friday allows us to make these assignments achievable on time. Below is a brief outline of our weekly goals. </p> <p>Here is a \u23f0 timeline \u23f3 , we thought were were going to be able to follow: - March 3: Finish the Movie Script That Explains Story of Waht we want to do and Proposal Submission - March 10: Figure out if tf can be done in gazebo - March 17: Run multiple real robots - March 24: Detect the object - March 31: Aligning to be protecting it - circle or square like formation - April 7: Detecting if something is coming at it - April 14: Making the robot move to protect at the direction that the person is coming at it from  - April 21: Testing - April 28: Testing - May 3: Stop writing code and work on creating a poster - May 10 Continuing to prepare for the presentation</p> <p>Here is a timeline of what we actually did:  - March 3: Finished all Drafting and Proposal - Submitted to get Feedback and Confirmation That it is Good To Go (Karen, LiuLu, Rongzi) - March 10: Figure out if tf can be done in gazebo (Rongzi), Creating Github Repo (Karen), Drawing Diagrams (Liulu), Explore Gazebo Worlds - Get Assests and Understand Structure (Karen) - March 17: Run multiple real robots on Gazebo (Karen), Created multi robot gazebo launch files (Karen), Wrote Code on Patroling Line (Liulu), Create box world (Rongzi), Make Behavior Diagram (Karen) - March 24: Continue to write code on patroling the line for multirobot (Liulu), Explored fiducials to integrate (Karen), Made better Gazebo world (Rongzi) - March 31: Aligning to be protecting it - circle or square like formation - April 7: Detecting if something is coming at it - April 14: Making the robot move to protect at the direction that the person is coming at it from  - April 21: Testing - April 28: Testing - May 3: Stop writing code and work on creating a poster - May 10 Continuing to prepare for the presentation</p> Before After <p>Here is another breakdown of the timeline that we actually followed:  </p> <p></p>  Preliminary \u2753 Questions \u2753 Answered   <p> To address blockers, we had to start asking ourselves questions and addressing issues like:  </p> <ol> <li>What are the major risky parts of your idea so you can test those out first?</li> <li>What robot(s) will you want to use?</li> <li>What props (blocks, walls, lights) will it need? </li> <li>What extra capabilities, sensors, will your robot need? How you have verified that this is possibler?</li> <li>And many other questions.</li> </ol> <p>Here was the original plan of what we wanted to display:  Seeing object they want to protect             |  Going towards the object and boxing it out :-------------------------:|:-------------------------:   |  </p> Blocking the intruder when one robot detects it Notifies robot to go over to intruder <p>Though things do not often go to plan as we did not realize that step 2 to step 3 was going to be so much harder as there was all these edge cases and blockers that we discovered (concurrency, network latency, camera specs). We ended up really only focusing on the patrolling and block stages. </p> <p></p> <p>There are many states to our project. There is even states withhin states that need to be broken down. Whenever we were off to coding, we had to make sure that we were going back to this table to check off all the marks about control of logic. We were more interested in getting all the states working, and so we did not use any of the packages of state management. Another reason why we did not proceed with using those state pacakges was because we needed to have multiple files as one python file represented one node so we were not able to be running multiple robot through one file. </p> <p>Here are the states that we were planning to have.  </p> <p>Finding State: Trigger: Lidar detection/Fiducial Detection/Opencv: Recognizing the target object, consider sticking a fiducial to the target object, and that will help the robot find the target object. The state is entered when Lidar finds nothing but the target object with a fiducial.</p> <p>Patrolling State: Trigger:  Lidar detection: When there is nothing detected in the right and left side of the robot, the robot will keep patrolling and receive callbacks from the subscriber.</p> <p>Signal State: Trigger:  Lidar detection: Finding intruders, which means the lidar detect something around and will recognize it as the intruder New topic: If_intruder: Contain an array that formed by four boolean values corresponds to each robot\u2019s publisher of this topic or the position of another robot. The robot will publish to a specific index of this array with the boolean value of the result of their lidar detection. If there is nothing, the robot will send False, and if it can get into the Signal State, it will send it\u2019s position and wait for the other robot\u2019s reaction. </p> <p>Form Line State:  Trigger: The message from topic  if_intruder: When a robot receives a position message, it will start to go in that direction by recognizing the index of the position message in the array, and they\u2019ll go to that corresponding colored line. Behavior Tree: A behavior will be created that can help the robot realign in a colored line.</p> <p>Blocking State:  Trigger: tf transformation:All the robots have the information that there is an intruder. All the robot will go to the direction where the intruder is and realign and try to block the intruder</p> <p>Notify State:  Trigger: Lidar Detection: If the lidar keeps detecting the intruder and the intruder doesn\u2019t want to leave for a long time, the robot will keep blocking the intruder without leaving. If the intruder leaves at some time, the robot will get back to the blocking state and use lidar to detect and make sure there is no intruder here and turn back to the finding state</p> <p>In the end, these were the only states that we actually tackled:</p> <p></p> <p></p> <p>Get correct color for line following in the lab Line follower may work well and easy to be done in gazebo because the color is preset and you don't need to consider real life effect. However, if you ever try this in the lab, you'll find that many factors will influence the result of your color.</p> <p>Real Life Influencer: 1. Light: the color of the tage can reflect in different angles and rate at different time of a day depend on the weather condition at that day.  2. Shadow: The shadow on the tape can cause error of color recognization 3. type of the tape: The paper tage is very unstable for line following, the color of such tape will not be recognized correctly. The duct tape can solve most problem since the color that be recognized by the camera will not be influenced much by the light and weather that day. Also it is tougher and easy to clean compare to other tape. 4. color in other object: In the real life, there are not only the lines you put on the floor but also other objects. Sometimes robots will love the color on the floor since it is kind of a bright white color and is easy to be included in the range. The size of range of color is a trade off. If the range is too small, then the color recognization will not be that stable, but if it is too big, robot will recognize other color too. if you are running multiple robots, it might be a good idea to use electric tape to cover the red wire in the battery and robot to avoid recognizing robot as red line.</p> <p>OpenCV and HSV color: Opencv use hsv to recognize color, but it use different scale than normal. Here is a comparison of scale: normal use  H: 0-360, S: 0-100, V: 0-100 Opencv use  H: 0-179, S: 0-255, V: 0-255</p> <p>So if we use color pick we find online, we may need to rescale it to opencv's scale.</p> <p>Here is a chart that talks about how we run the real robots live with those commands. On the right most column that is where we have all of the colors for each line that the robot home base should be: </p> <p></p> <p></p> <p>If you are interested in launching on the real turtlebot3, you are going to have to ssh into it and then once you have that ssh then you will be able to all bringup on it. There is more detail about this in other FAQs that can be searched up. When you are running multirobots, be aware that it can be a quite bit slow because of concurrency issues. </p> <p>These 3 files are needed to run multiple robots on Gazebo. In the object.launch that is what you will be running roslaunch. Within the robots you need to spawn multiple one_robot and give the position and naming of it. </p> <p></p> <p>Within the object.launch of line 5, it spawns an empty world. Then when you have launched it you want to throw in the guard_world which is the one with the multiple different colors and an object to project in the middle. Then you want to include the file of robots.launch because that is going to be spawning the robots. </p> <p></p> <p>For each robot, tell it to spawn. We need to say that it takes in a robot name and the init_pose. And then we would specify what node that it uses.</p> <p></p> <p>Within the robots.launch, we are going to have it spawn with specified position and name.  </p> <p></p> We had it run for more than one minute and it started sensing each other as the object and trying to stay away from it, so it was not exactly ideal. It would start going further and further say from each other. The perimeter started getting bigger and better which is a problem as we do not know when the intruder will come. Pros of this approach is that it will not hit the object. It is good enough in which it stays within the perimeter. It may become a problem if multiple run this because we would need to find a way to edit the auro detect so that all four runs that. Line follower is the best algorithm we found to deal with patrolling around the object after tried out different strategies and compare the performance. It allows stable protection of the object: the robot strictly follows the designed path indicated by line and would not run away from the object. The simulation of patrolling environment in gazebo should work better than real world environment since the color can be set to pure green, yellow, red and blue and there is no shadow or reflection that can cause error on color recognition <p> Basic Idea:</p> <p>Guard object has a Twist() which will control the velocity of the robot. It also has a subscriber that subscribe to scan topic to receive LaserScan message and process the message in scan_cb() which is the callback function of the scan topic. The ranges field of the LaserScan message gives us an array with 360 elements each indicating the distance from the robot to obstacle at that specific angle. The callback function takes the ranges field of the LaserScan message, split into seven regions unevenly as shown below\uff1a</p> <p>The minimum data of each region is stored in a dictionary. Then change_state function is called inside the scan_cb, which will check the region dictionary and update the intruder state as needed.</p> <p>Work with real robot:</p> <ul> <li>Dealing with signal noise</li> </ul> <p>The laser scan on real robot is not always stable. Noisy data is highly likely to occur which can influence our intruder detection a lot. To avoid the influence of scan noise, we processed the raw ranges data to get a new ranges list which each element is the average of itself and 4 nearby elements in original ranges data. This data smoothing step can help us get a more reliable sensor data.</p> <ul> <li>Sensor on real robot</li> </ul> <p>The sensor of each robot is build differently. In gazebo, if nothing is scanned at that angle, inf is shown for the corresponding element in the ranges. However, some real-life robot have different LaserScan data. Nothing detected within the scan range will give a 0 in ranges data. To make our code work correctly on both Gazebo and real robot, please follow our comment in the code. There are only two lines of code that need to be changed. </p> <p>Also, the laserScan data for Rafael has different number of elements. So the region division based on index is a little different for Rafael. We have different Python file for each robot, so this part is taken care of in line_F4.py which is only for Rafael.</p> <p> A new message is created for our robots to communicate about intruder detection. The message contains four boolean values each associated with one of our guardbot. When a guardbot detects the intruder, a new message will be created with that associated boolean value set to True, and the new message will be published to a topic called see_intruder. Each robot also has a subscriber that subscribes to this topic and a callback function that will check the passed-in message and get the information about which robot is seeing the intruder.  </p> <p>The CMakeLists.txt and package.xml are also modified to recognize this newly created message.</p> <p>We had to watch a lot of videos to figure out how to do this. We made an msg file which stored the type of data that we will hold which are boolean values on if the robot sees an intruder. We had to edit the cmake file and then had to edit the xml because we need to say that there is this new created message that the robots may communicate with and then have these structure look through the folder to see how it is created. </p> <p>We need our own messades and tonics for the rohots to communicate. Here are the new messages:</p> <ul> <li> <p>see intruder: the message contains four std msgs/Boolean, each associated with a specific robot. When an intruder is detected by one robot. its' associated Boolean will be set to True. Only one robot can be true.</p> </li> <li> <p>stop order: the message contains a list of std_msgs/String which would record the name of the robots in the stop order, and an std msas/Int8 which would record the current index of the list for the next stop or move.</p> </li> </ul> <p>Here is a chart talking about what we are interested in: </p> <p>Here are the links that we used quite a lot: https://wiki.ros.org/ROS/Tutorials/CreatingMsgAndSrv  https://www.theconstructsim.com/solve-error-importerror-no-module-named-xxxx-msg-2/ </p> <p></p> <p>This project does not end here, there is a lot more that we can add. For an example here are a couple of other features that cam be added:  - Having more sides so that there is faster converage - Making it run faster and still be as accurate with its patrolling  - Finding a better system so that it does not need the order ahead of time if possible  - Try to make it so that the robots all patrolling around instead of it being on a single like - Adding extra technologies where robots are connecting to like an Alexa can tell them to stop  - Add a way to have it be stopped by human input and have it overrided</p> <p> We feel like as a team we suceeded a lot in this project. We had great communication and determination that makes us such good teammates. We did not know each other before doing this project and at the beginning of the week we were a bit hesitant. We </p> <p>\ud83d\udc69\u200d\ud83d\udcbb\ud83d\udc69\u200d\ud83d\udcbb\ud83d\udc69\u200d\ud83d\udcbb\ud83d\udc69\u200d\ud83d\udcbb\ud83d\udc69\u200d\ud83d\udcbb\ud83d\udc69\u200d\ud83d\udcbb\ud83d\udc69\u200d\ud83d\udcbb\ud83d\udc69\u200d\ud83d\udcbb\ud83d\udc69\u200d\ud83d\udcbb\ud83d\udc69\u200d\ud83d\udcbb\ud83d\udc69\u200d\ud83d\udcbb\ud83d\udc69\u200d\ud83d\udcbb\ud83d\udc69\u200d\ud83d\udcbb\ud83d\udc69\u200d\ud83d\udcbb\ud83d\udc69\u200d\ud83d\udcbb\ud83d\udc69\u200d\ud83d\udcbb\ud83d\udc69\u200d\ud83d\udcbb\ud83d\udc69\u200d\ud83d\udcbb\ud83d\udc69\u200d\ud83d\udcbb\ud83d\udc69\u200d\ud83d\udcbb\ud83d\udc69\u200d\ud83d\udcbb\ud83d\udc69\u200d\ud83d\udcbb\ud83d\udc69\u200d\ud83d\udcbb\ud83d\udc69\u200d\ud83d\udcbb\ud83d\udc69\u200d\ud83d\udcbb\ud83d\udc69\u200d\ud83d\udcbb\ud83d\udc69\u200d\ud83d\udcbb\ud83d\udc69\u200d\ud83d\udcbb\ud83d\udc69\u200d\ud83d\udcbb\ud83d\udc69\u200d\ud83d\udcbb\ud83d\udc69\u200d\ud83d\udcbb\ud83d\udc69\u200d\ud83d\udcbb\ud83d\udc69\u200d\ud83d\udcbb\ud83d\udc69\u200d\ud83d\udcbb\ud83d\udc69\u200d\ud83d\udcbb\ud83d\udc69\u200d\ud83d\udcbb\ud83d\udc69\u200d\ud83d\udcbb\ud83d\udc69\u200d\ud83d\udcbb\ud83d\udc69\u200d\ud83d\udcbb\ud83d\udc69\u200d\ud83d\udcbb\ud83d\udc69\u200d\ud83d\udcbb\ud83d\udc69\u200d\ud83d\udcbb\ud83d\udc69\u200d\ud83d\udcbb\ud83d\udc69\u200d\ud83d\udcbb</p>"},{"location":"reports/2023/multi-robot-surveillance/","title":"Multi-Robot Home Surveillance","text":""},{"location":"reports/2023/multi-robot-surveillance/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Problem Statement</li> <li>Challenges</li> <li>Video Demo</li> <li>Program Structure</li> <li>TF Tree</li> <li>RViz Interface</li> <li>Leader Election Algorithm</li> <li>Resiliency and Node Failure Detection</li> <li>Limitations</li> <li>Applications</li> <li>Team Members</li> <li>Special Thanks</li> </ul>"},{"location":"reports/2023/multi-robot-surveillance/#problem-statement","title":"Problem Statement","text":"<ul> <li>The home above is divided into four security zones.</li> <li>We can think of them as a sequence, ordered by their importance: [A, B, C, D].</li> <li>If we have n live robots, we want the first n zones in the list to be patrolled by at least one robot.</li> <li>n== 2 \u2192 patrolled(A, B); </li> <li>n == 3 \u2192 patrolled(A, B, C)</li> <li>1 &lt;= n &lt;= 4</li> <li>The goal is for this condition to be an invariant for our program, despite robot failures.</li> <li>Let robot<sub>z</sub> refer to the robot patrolling zone Z.</li> <li>Illustration:</li> <li>n == 4 \u2192 patrolled(A, B, C, D)</li> <li>robotA dies</li> <li>n == 3 \u2192 patrolled(A, B, C)</li> <li>robotB dies</li> <li>n == 2 \u2192 patrolled(A, B)</li> <li>robotA dies</li> <li>n == 1 \u2192 patrolled(A)</li> </ul>"},{"location":"reports/2023/multi-robot-surveillance/#challenges","title":"Challenges","text":"<ul> <li>We need to:</li> <li>map the home<ul> <li>SLAM (Simultaneous Localization and Mapping) with the gmapping package</li> </ul> </li> <li>have multiple robots patrol the security zone in parallel<ul> <li>AMCL (Adaptive Monte Carlo Localization) and the move_base package</li> <li>One thread per robot with a SimpleActionClient sending move_base goals</li> </ul> </li> <li>have the system be resilient to robot failures, and adjust for the invariant appropriately<ul> <li>Rely on Raft\u2019s Leader Election Algorithm (\u2018LEA\u2019) to elect a leader robot.</li> <li>The remaining robots will be followers.</li> <li>The leader is responsible for maintaining the invariant among followers.</li> </ul> </li> <li>More on system resiliency:</li> <li>The leader always patrols zone A, which has the highest priority.</li> <li>On election, the leader assigns the remaining zones to followers to maintain the invariant.</li> <li>On leader failure: <ul> <li>global interrupts issued to all patrol threads (all robots stop).</li> <li>the LEA elects a new leader</li> </ul> </li> <li>On follower failure: <ul> <li>local interrupts issued by leader to relevant patrol threads (some robots stop).</li> <li>Example:</li> <li>n==4 \u2192 patrol(A, B, C, D)</li> <li>robotB dies</li> <li>leader interrupts robotD</li> <li>leader assigns robotD to zone B</li> <li>n==3 \u2192 patrol(A, B, C)</li> </ul> </li> </ul>"},{"location":"reports/2023/multi-robot-surveillance/#video-demo","title":"Video Demo","text":"<p>Watch the System in Action!</p>"},{"location":"reports/2023/multi-robot-surveillance/#program-structure","title":"Program Structure","text":"<ul> <li>Two points of entry:  real_main.launch and sim_main.launch.</li> <li>real_main.launch:</li> <li>used for real-world surveillance of our home environment</li> <li>runs the map_server with the home.yaml and home.png map files obtained by SLAM gmapping.<ul> <li>All robots will share this map_server</li> </ul> </li> <li>launches robots via the real_robots.launch file (to be shown in next slide)</li> <li>launches rviz with the real_nav.rviz configuration</li> <li>sim_main.launch:</li> <li>used for simulated surveillance of gazebo turtlebot3_stage_4.world</li> <li>works similarly to real_main.launch except it uses a map of stage_4 and sim_robots.launch to run gazebo models of the robots via tailored urdf files.</li> </ul>"},{"location":"reports/2023/multi-robot-surveillance/#tf-tree","title":"TF Tree","text":""},{"location":"reports/2023/multi-robot-surveillance/#rviz-interface","title":"RViz Interface","text":""},{"location":"reports/2023/multi-robot-surveillance/#leader-election-algorithm","title":"Leader Election Algorithm","text":"<ul> <li>Based on Diego Ongaro et al. 2014, \u201cIn Search of an Understandable Consensus Algorithm\u201d</li> <li>The full Raft algorithm uses a replicated state machine approach to maintain a set of nodes in the same state. Roughly:</li> <li>Every node (process) starts in the same state, and keeps a log of the commands it is going to execute.</li> <li>The algorithm elects a leader node, who ensures that the logs are properly replicated so that they are identical to each other.</li> <li> <p>Raft\u2019s LEA and log replication algorithm are fairly discrete. We will only use the LEA.</p> </li> <li> <p>Background:</p> <ol> <li>Each of the robots is represented as an mp (member of parliament) node.</li> <li>mp_roba, mp_rafael, etc.</li> <li>Each node is in one of three states: follower, candidate, or leader.</li> <li>If a node is a leader, it starts a homage_request_thread that periodically publishes messages to the other nodes to maintain its status and prevent new elections.</li> <li>Every mp node has a term number, and the term numbers of mp nodes are exchanged every time they communicate via ROS topics.</li> <li>If an mp node\u2019s term number is less than a received term number, it updates its term number to the received term number.</li> </ol> </li> <li>LEA:<ol> <li>On startup, every mp node is in the follower state, and has:<ol> <li>a term number; and</li> <li>a duration of time called the election timeout.</li> </ol> </li> <li>When a follower node receives no homage request messages from a leader for the duration of its election timeout, it:<ol> <li>increments its term number;</li> <li>becomes a candidate;</li> <li>votes for itself; and</li> <li>publishes vote request messages to all the other mp nodes in parallel.</li> </ol> </li> <li>When a node receives a request vote message it will vote for the candidate iff:<ol> <li>the term number of the candidate is at least as great as its own; and</li> <li>the node hasn\u2019t already voted for another candidate.</li> </ol> </li> <li>Once a candidate mp node receives a majority of votes it will:<ol> <li>become a leader;</li> <li>send homage request messages to other nodes to declare its status and prevent new elections.</li> </ol> </li> <li>The election timeout duration of each mp node is a random quantity within 2 to 3 seconds.</li> <li>This prevents split votes, as it\u2019s likely that some follower will timeout first, acquire the necessary votes, and become the leader. </li> <li>Still, in the event of a split vote, candidates will:<ol> <li>reset their election timeout durations to a random quantity within the mentioned interval;</li> <li>wait for their election timeout durations to expire before starting a new election.</li> </ol> </li> </ol> </li> </ul>"},{"location":"reports/2023/multi-robot-surveillance/#resiliency-and-node-failure-detection","title":"Resiliency and Node Failure Detection","text":"<ul> <li>The election timeout feature is part of what enables leader resiliency in my code. <ul> <li>When a leader node is killed, one of the follower nodes time out and become the new leader.</li> </ul> </li> <li>roscore keeps a list of currently active nodes. So:<ul> <li>followers, before transitioning to candidate state, update their list of live mp nodes by consulting roscore.</li> <li>leaders do the same via a follower_death_handler thread, which runs concurrently with main thread and the homage_request thread, and maintains the invariant.</li> </ul> </li> </ul>"},{"location":"reports/2023/multi-robot-surveillance/#limitations","title":"Limitations","text":"<ul> <li>The mp nodes are all running on the remote VNC computer, which also runs the roscore.<ul> <li>So killing the mp node is just a simulation of robot failure, not true robot failure.<ul> <li>We\u2019re just killing a process on the vnc computer, not the robot itself</li> </ul> </li> <li>We can remedy this by binding the mp node\u2019s life to any other nodes that are crucial to our robot fulfilling its task. <ul> <li>E.g., for our project the /roba/amcl and /roba/move_base nodes are critical.</li> <li>So we can consult roscore periodically to see if any of these nodes die at any given point. And if they do, we can kill mp_roba.</li> <li>We can also define and run more fine-grained custom nodes that keep track of any functional status of the robot we\u2019re interested in, and bind the life of those custom nodes to the mp nodes.</li> </ul> </li> </ul> </li> <li>Running each mp node on its corresponding robot is a bad idea!<ul> <li>Only if a robot gets totally destroyed, or the mp process fails, will the algorithm work.</li> <li>Performance will take a hit: </li> <li>bandwidth must be consumed for messages between mp nodes</li> <li>robot\u2019s computational load will increase</li> </ul> </li> <li>Our system has a single point of failure; the VNC computer running roscore.<ul> <li>So we have to keep the VNC safe from whatever hostile environment the robots are exposed to.</li> <li>Maybe this can be remedied in ROS2, which apparently does not rely on a single roscore. </li> </ul> </li> <li>Our patrol algorithm is very brittle and non-adversarial<ul> <li>Depends on AMCL and move_base, which do not tolerate even slight changes to the environment, and which do not deal well with moving obstacles.</li> <li>There are no consequences to disturbing the patrol of the robots, or the robots themselves (e.g. no alarm or \u2018arrests\u2019)</li> </ul> </li> </ul>"},{"location":"reports/2023/multi-robot-surveillance/#applications","title":"Applications","text":"<ul> <li>The LEA is fairly modular, and the algorithm can be adapted to tasks other than patrolling.<ul> <li>It would work best for tasks which are clearly ordered by priority, and which are hazardous, or likely to cause robot failures.</li> <li>Completely fanciful scenarios:<ul> <li>Drone strikes: Maybe certain military targets are ordered by priority, and drones can be assigned to them in a way that targets with a higher priority receive stubborn attention.</li> <li>Planetary exploration: Perhaps some areas of a dangerous planet have a higher priority that need to be explored than others. Our system could be adapted so that the highest priority areas get patrolled first.</li> </ul> </li> </ul> </li> </ul>"},{"location":"reports/2023/multi-robot-surveillance/#team-members","title":"Team Members","text":"<ul> <li>James Lee (leejs8128@brandeis.edu)</li> </ul>"},{"location":"reports/2023/multi-robot-surveillance/#special-thanks","title":"Special Thanks","text":"<ul> <li>A special thanks to Adam Ring, the TA for the course, who helped with the project at crucial junctures.</li> </ul>"},{"location":"reports/2023/object_sorter/","title":"Project Report Object Sorter","text":"<ul> <li>Names: Isaac Goldings (isaacgoldings@brandeis.edu), Jeremy Huey (jhuey@brandeis.edu), David Pollack (davidpollack@brandeis.edu)</li> <li>Instructor: Pito Salas (rpsalas@brandeis.edu) </li> <li>Date: 05/03/2023</li> <li>Github repo: https://github.com/campusrover/COSI119-final-objectSorter</li> </ul>"},{"location":"reports/2023/object_sorter/#introduction","title":"Introduction","text":"<p>This Object Sorter robot uses qrcode/fiducial detection and color detection to find cans and then pick them up with a claw and drop them off at designated sorted positions. In this implementation, the robot is mobile and holds a claw for grabbing objects. The robot executes a loop of finding an item by color, grabbing it, searching for a dropoff fiducial for the correct color, moving to it, dropping it off, and then returning to its pickup fiducial/area. </p> <p>The project runs via the ros environment on linux, requires some publically available ros packages (mostly in python), and runs on a custom \"Platform\" robot with enough motor strength to hold a servo claw and camera. Ros is a distributed environment running multiple processes called nodes concurrently. To communicate between them, messages are published and subscribed to. The robot itself has a stack of processes that publish information about the robot's state, including its camera image, and subscribes to messages that tell it to move and open and close the claw. </p> <p>Our project is divided up into a main control node that also includes motion publishing, and two nodes involved with processing subscribed image messages and translating them into forward and angular error. These are then used as inputs to drive the robot in a certain direction. The claw is opened and closed when the color image of the soda can is a certain amount of pixel widths within the camera's image. </p> <p>The project runs in a while loop, with different states using control from the two different image platforms at different states until it is finished with a finished parameter, in the current case, when all 6 cans are sorted after decrementing.  </p> <p></p>"},{"location":"reports/2023/object_sorter/#problem-statement-including-original-objectives","title":"Problem Statement including original objectives","text":"<ul> <li>Our plan for this project is to build a robot that will identify and sort colored objects in an arena.</li> <li>These objects will be a uniform shape and size, possible small cardboard boxes, empty soda cans, or another option to be determined. The robot will be set up in a square arena, with designated locations for each color. </li> <li>Upon starting, The robot will identify an object, drive towards it, pick it up using the claw attachment, and then navigate to the drop off location for that color and release it. It will repeat this until all the objects have been moved into one of the designated locations.</li> </ul>"},{"location":"reports/2023/object_sorter/#relevant-literature","title":"Relevant literature","text":"<p>Color detection robots and robotic arms are a large and known factor of industrial workflows. We wanted to incorporate the image detection packages and show its use in a miniature/home application. </p> <ul> <li>Color Detection</li> <li>Aruco Fiducial Detection and explanation</li> <li>Home/Miniature Sorter</li> <li>Industrial Sorter</li> </ul>"},{"location":"reports/2023/object_sorter/#what-was-created","title":"What was created","text":"<p>The project has been realized almost exactly to the specifications. We found that a bounding arena was not neccesary if we assumed that the travelling area was safe to the robot and the project. This implementation melds two detection camera vision algorithms and motion control via error feedback loop. </p> <p>The current version uses three cans of two different colors, sorts the 6 cans, and finishes.  We found some limitations with the project, such as network traffic limiting the refresh information of the visual indicators. </p> <ol> <li>The first vision algorithm is to detect fiducials, which look like qr codes. The fiducials are printed to a known dimension so that the transformation of the image to what is recognized can be used to determine distance and pose-differential. The aruco_detect package subscribes to the robot's camera image. It then processes this image and returns an array of transforms/distances to each fiducial. The package can recognize the distinctness between fiducials and searches for the correct one neede in each state. The transform is then used to determine the amount of error between then robot and it. Then the robot will drive ion that direction, while the error is constantly updated with new processed images. (See Section: Problems that were Solved for issues that arose.)</li> </ol> <p></p> <ol> <li>The second vision algorithm is to detect color within the bounds of the camera image. The camera returns a color image, and the image is filtered via HSV values to seek a certain color (red or green). A new image is created with non-filtered pixels (all labelled one color) and the rest is masked black. A contour is drawn around the largest located area. Once the width of that contour is a large enough size in the image, the claw shuts. The processing thus can detect different objects, and the robot will go towards the object that is the clsoest due to its pixel count. (See Section: Problems that were Solved for issues that arose.)</li> </ol> <p> The above picture shows (starting with the upper most left picture and going clockwise): </p> <ol> <li>A centroid (light blue dot) that is in the middle of the colored can of which is used as a target for the robot to move towards. The centroid is calcuated using the second vision algorithm by calculating the middle of the contour that is drawn around the largest located area.</li> <li>The contour that is drawn around the largest located error.</li> <li>The image that is created with one color isolated and all other colored pixels masked in black.</li> </ol>"},{"location":"reports/2023/object_sorter/#technical-description","title":"Technical Description","text":"<p>Our project is to get a platform robot with a claw mount to sort soda cans into two different areas based on color. The test area is set up with two fiducials on one side and one fiducial on the other, approximately 2 meters apart. The two fiducials are designated as the red and green drop-off points, and the one is the pickup point. Six cans are arranged in front of the pickup point with the only requirements being enough spacing so the robot can reverse out without knocking them over, and a direct path to a can of the color the robot is targeting (e.x. so it doesn\u2019t have to drive through a red can to get to a green one). The robot starts in the middle of the area facing the pickup point. </p> <p>The robot\u2019s behavior is defined by three states, \u201cfind item,\u201d \u201cdeliver item,\u201d and \u201creturn to start.\u201d For \u201cfind item\u201d the robot uses computer vision to identify the closest can of the target color, and drive towards it until it is in the claw. Then it closes the claw and switches state to \u201cdeliver item.\u201d For this state, the robot will rotate in place until it finds the fiducial corresponding to the color of can it is holding, then drive towards that fiducial until it is 0.4 meters away. At that distance, the robot will release the can, and switch to \u201creturn to start.\u201d For this last state, the robot will rotate until it finds the pickup fiducial, then drive towards it until it is 1 meter away, then switch back to \u201cfind item.\u201d The robot will loop through these three states until it has picked up and delivered all the cans, which is tracked using a decrementing variable. </p> <p></p>"},{"location":"reports/2023/object_sorter/#guide-to-codebase-and-algorithms","title":"Guide to Codebase and algorithms","text":"<p>This project consists of three main nodes, each with their own file, main_control.py, contour_image_rec.py, and fiducial_recognition.py. </p> <ul> <li> <p>contour_image_rec.py uses the raspicam feed to calculate the biggest object of the target color within the field of view, and publishes the angle necessary to turn to drive to that object. It does this by removing all pixels that aren\u2019t the target color, then drawing a contour around the remaining shapes. It then selects the largest contour, and draws a rectangle around it. After that it calculates the middle of the rectangle and sends uses the difference between that and the center of the camera to calculate the twist message to publish.</p> </li> <li> <p>Fiducial_recognition.py just processes the fiducials and publishes the transforms. </p> </li> <li> <p>main_control.py  subscribes to both of the other nodes, and also contains the behavior control for the robot. This node is in charge of determining which state to be in, publishing twist values to cmd_vel based on the input from the other nodes, and shutting the robot down when the cans are all sorted.</p> </li> <li> <p>Prod_v5.launch is our launch file, it starts the main_control node, the image recognition node, the fiducial recognition node, and the aruco_detect node that our fiducial recognition node relies on.</p> </li> </ul>"},{"location":"reports/2023/object_sorter/#story-of-the-project","title":"Story of the project.","text":"<p>We wanted to do a project that had relatable potential and worked on core issues with robotics. Settling on this topic, we found that having to integrate two different camera algorithms provided a higher level of interest. In deciding to sort cans, we liked that these were recognizable objects. It was hoped that we could detect based on the plain color of the cans, Coke, Canada Dry Raspberry, Sprite, AW Root Beer. For simplicity, we did tape the tops of the green cans to give more consistency, but it was noted that the Sprite cans would work well still with some minor tweaking. </p> <p>For motion control, using the direct error from the two detected objects provided reasonable levels of feedback.  We put the components into a state controller main node, and then wrapped all the camera outputs, nodes into a single launch node. The launch node also automatically starts aruco_detect. </p>"},{"location":"reports/2023/object_sorter/#how-it-unfolded-how-the-team-worked-together","title":"How it unfolded, how the team worked together","text":"<p>We worked collaboratively on different parts of the project together. We slowly got piece by piece working, then combined them together into a working roslaunch file. We focused on creating a working production launch file and continued to work on an in-progress one for new material. By breaking down the porject into separate parts, we were able to work separately asynchronously, while meeting up to collaborate on the design of the project.  It was a great time working with the other members. </p>"},{"location":"reports/2023/object_sorter/#project-in-pieces","title":"Project in pieces:","text":"<ul> <li>Collecting aruco_detect, rqt_image_view, main_control, fiducial_recognition, contour_image_rec into one file. Controlling version for production. </li> <li>Considering alternate control flows: behavior trees, finite state machines (not used). </li> <li>Getting fiducial recognition to work, move and alternate between different fiducials. </li> <li>Detecting and masking by HSV range, contouring the largest clump, bounding that with a rectangle, sending a centroid.  </li> <li>Grabbing cans with the claw (sending/recieving servo commands), using the width of the rectangle to time closing of the claw. </li> <li>Developing a loop control between states. </li> <li>Troubleshooting physical problems, such as the cans blocking the fiducial image. </li> </ul>"},{"location":"reports/2023/object_sorter/#problems-that-were-solved-pivots-that-had-to-be-taken","title":"problems that were solved, pivots that had to be taken","text":"<ol> <li>Fidicial-based issues:  Recognition only worked to a certain distance, after that, the image resolution is too low for certainty. To resolve this, we allowed the bounds of the project to remain within 3 meters. </li> </ol> <p>Network traffic (more other robots/people) on the network would introduce significant lag to sending images across the network from the robot to the controlling software (on vnc on a separate computer). This would cause things such as overturning based on an old image, or slow loading of the fiducial recognition software. This can be worked around by: a. increasing the bandwidth related to the project, b. buffering images, c. reducing the size or performing some calculations on the robot's Raspberry Pi chip (this is limited by the capacity of that small chip), d. changing and reducing speed parameters when network traffic is high manually (allowing less differential between slow images and current trajectory). </p> <p>Positions where the entirely of the fiducial is too close and gets cropped. Fiducial recognition requires the entire image to be on screen. This information can be processed however and saved, either as a location (which can be assessed via mapping) or as previous information of error. To combat this issue, we raised the positions of the fiducials so that they could be seen above the carried soda can and had the drop off point at 0.4m away from the fiducial. </p> <ol> <li> <p>Color-based issues: Different lighting conditions (during night vs day coming in from the window) affect color detection. Therefore it is best to get some colors that are very distinct and non-reflective (becomes white). Thus red and green. Another issue that would arise is if the robot approaches the cans from the side and the can remains on the side, then the width might not increase enough to close the claw. Some conditional code can be used to clode the claw with smaller width if the contour is in some part of the screen. </p> </li> <li> <p>Other/Physical issues: The platform robot is a big chonker! It also has meaty claws. The robot being large causes it to take up more room and move farther. In the process it sometimes will knock over other features. To combat this, we added a reversing -0.05m to the turning motion when the robot begins to switch to seeking a dropoff fiducial. The meaty claws is one of the causes of requiring a larger robot with stronger motors. </p> </li> </ol> <p></p>"},{"location":"reports/2023/object_sorter/#reflections","title":"Reflections","text":"<p>In sum, we had an enjoyable time working with our Platform Robot, and it was great to see it complete its task with only the press of a single button. The autonimity of the project was a great experience. </p>"},{"location":"reports/2023/robot_race/","title":"Robot Race","text":"<p>Project Report for Robot Race  Team: Sampada Pokharel (pokharelsampada@brandeis.edu) and Jalon Kimes (jkimes@brandeis.edu)  Date: May 4, 2023 Github repo: https://github.com/campusrover/robot_race</p>"},{"location":"reports/2023/robot_race/#introduction","title":"Introduction","text":""},{"location":"reports/2023/robot_race/#background","title":"Background","text":"<p>For our final project, we wanted to create a dynamic and intuitive project that involved more than one robot. To challenge ourselves, we wanted to incorporate autonomous robots. As the technology for self-driving cars continues to advance, we are curious about the process involved in creating these vehicles and wanted to explore the autonomous robot development process. To explore this further, we have come up with the concept of a race between two autonomous robots. The goal for this project was to program robots into racing in the track indefinitely all while following certain rules and avoiding obstacles throughout the track.</p>"},{"location":"reports/2023/robot_race/#original-objectives","title":"Original Objectives","text":"<p>Our main objective is for the robots to autonomously move itself in the race track with 2 lanes (4 lines) avoiding any collision. The plan was to do so by following the center of a lane made by 2 lines, slowing down, and switching into a different lane. We were able to use the contour system to detect multiple objects within a mask and draw centroids on them. For our purposes we wanted it to detect the two lines that make up the lane. With the centroids we could calculate the midpoint between the lines and follow that. We got a prototype running, however it struggled to turn corners. For the sake of time we pivoted to only following one line per robot. The current Robot Race is an algorithm that allows two autonomous robots to race each other, collision free, on a track with two lines, speed bumps, and obstacles along the way.</p>"},{"location":"reports/2023/robot_race/#what-was-created","title":"What was created","text":""},{"location":"reports/2023/robot_race/#technical-description-and-illustrations","title":"Technical Description and Illustrations","text":"<p>We designed our code to have the robot recognize a line by its color and follow it. While it is doing that it is also looking for yellow speed bumps along the track. When it detects yellow on the tract the robot will slow down until it passes over the speed bump. Furthermore, when the robot detects an obstacle in front of it, it will switch lanes to move faster avoiding the obstacle.</p>"},{"location":"reports/2023/robot_race/#discussion-of-interesting-algorithms-modules-techniques","title":"Discussion of interesting algorithms, modules, techniques","text":"<p>The main package that drives our project is the opencv stack. This contains a collection of tools that allow us to use computer vision to analyze images from the camera on the robot. We used the CV bridge package to process the images. As the name suggests CV bridge allows us to convert the ROS messages into OpenCV messages. The camera onboard the robot publishes the images as ROS messages into different types of CV2 images. For our use case, we converted the images from the camera into color masks. To create the color masks we used the HSV color code to set upper and lower bounds for the range of colors we want the mask to isolate. For example, for red the lower bound was: HSV [0,100,100] and the upper bound was: HSV [10,255,255]. The mask will then block out any color values that are not in that range.</p> <p></p> <p>Figure 1</p> <p>To figure out the color code we had to calibrate the camera for both robots using the cvexample.py file with the RQT app to adjust the HSV mask range live. After that we convert the color mask to a grayscale image and it's ready for use.</p> <p>We created masks for the green and red lines that the robot will follow and the yellow speed bumps scattered along the track. We set each robot to follow a specific line and also be scanning for yellow at the same time. This was achieved by drawing centroids on the line (red or green) for the robot to follow.</p> <p>We also use lidar in our project. Each robot is scanning the environment for obstacles. The robots have the ability to switch lanes by swapping the red and green masks out when it detects an obstacle on the track with lidar. We subscribed to the laser scan topic to receive ros messages from the equipped Lidar sensor. After some testing and experimenting we decided If It detects an object within 1.5 meters of the front of the robot it will switch lanes. We had to make sure that the distance threshold was far enough for the robot to switch lanes before crashing but not too far to detect the chairs and tables in the room.</p>"},{"location":"reports/2023/robot_race/#guide-on-how-to-use-the-code-written","title":"Guide on how to use the code written","text":"<ol> <li>Clone our repository: \u201cgit clone https://github.com/campusrover/robot_race.git\u201d</li> <li>ssh into two robots    for each robot run:    <pre><code>     $(bru mode real)\n    $(bru name &lt;robot name&gt; -m &lt;myvpnip&gt;)\n    multibringup\n</code></pre></li> <li>Go into vnc and run roslaunch robot_race follow_demo.launch</li> </ol> <p>This should get the robots to move on their respective lines.</p>"},{"location":"reports/2023/robot_race/#source-files-nodes-messages-and-actions","title":"Source files, nodes, messages, and actions","text":"<p>Launch File Follow_demo.launch - The main launch file for our project</p> <p>Nodes Follow.py - main programming node</p> <p></p>"},{"location":"reports/2023/robot_race/#story-of-the-project","title":"Story of the project","text":""},{"location":"reports/2023/robot_race/#how-it-unfolded","title":"How it unfolded","text":"<p>At the very beginning, we were dancing with the idea of working on two robots and having them follow the traffic rules as they start racing on the track for our final project. However, as we started learning more about image processing and worked on the line following PA, we wanted to challenge ourselves to work with two autonomous robots. With the autonomous robots in mind, we then decided to discard the traffic lights and just create a simple track with speed bumps and walls on the lanes.</p> <p>We faced many problems throughout this project and pivoted numerous times. Although the project sounds simple, it was very difficult to get the basic algorithm started. One of our biggest challenges was finding a way for the robot to stay in between two lanes. We tried to use the HoughLines algorithm to detect the lines of our lanes, however, we found it extremely difficult to intricate our code as HoughLines detected odd lines outside of our track. When HoughLines didn\u2019t work, we pivoted to using a contours-based approach. Contours allow you to recognize multiple objects in a mask and draw centroids on them. We used this to draw centroids on the two lines and calculated the midpoint in between two lines for the robot to follow. While we were successful in creating a centroid in between the lines for the robot to follow on the gazebo, when we tried to run the program on the actual robot, the robot did not stay in between the two lanes when it needed to turn a corner(Figure 2). At last, we pivoted and decided to replace the lanes with two different tapes, red and green, for the robots to follow.</p> <p></p> <p>Figure 2</p> <p>Furthermore, we wanted to create an actual speed bump using cardboards but we soon realized that would be a problem since we wanted to use the small turtle bots for our project and they were unable to slow down and go up the speed bump. Furthermore, the lidars are placed at the top of the robot and there is little to no space at the bottom of the robot to place a lidar. A solution we came up with was to use yellow tape to signal speed bumps.</p> <p>Once we were able to get the robots to follow the two lines, we had difficulty getting the robot to slow down when it detected the speed bump. We had separated the main file and the speed bump detector file. The speed bump detector file created a yellow mask and published a boolean message indicating if yellow was detected in the image. After discussing it with the TA and some friends from class we found out that we could not pass the boolean messages from the speedbump node to the main file without changing the xml file. We originally planned to do the same for object detection but did not want to run into the same problem. Our solution was to combine everything into one file and the speed bump and object detector nodes became methods.</p>"},{"location":"reports/2023/robot_race/#self-assessment","title":"Self assessment","text":"<p>Overall, we consider this project to be a success. We were able to create and run two autonomous robots simultaneously and have them race each other on the track all while slowing down and avoiding obstacles. Although we faced a lot of challenges along the way, we prevailed and were able to finish our project on time. The knowledge and experience gained from this project are invaluable, not only in the field of self-driving cars but also in many other areas of robotics and artificial intelligence. We also learned the importance of teamwork, communication, and problem-solving in the development process. These skills are highly transferable and will undoubtedly benefit us in our future endeavors.</p>"},{"location":"reports/2023/typinator/","title":"Typinator.md","text":""},{"location":"reports/2023/typinator/#cosi-119a-autonomous-robotics","title":"Cosi 119A - Autonomous Robotics","text":""},{"location":"reports/2023/typinator/#team-elliot-siegel-elliotsiegelbrandeisedu-and-kirsten-tapalla-ktapallabrandeisedu","title":"Team: Elliot Siegel (elliotsiegel@brandeis.edu) and Kirsten Tapalla (ktapalla@brandeis.edu)","text":""},{"location":"reports/2023/typinator/#date-may-2023","title":"Date: May 2023","text":""},{"location":"reports/2023/typinator/#github-repo-httpsgithubcomcampusrovertypinator2023","title":"Github repo: https://github.com/campusrover/typinator2023","text":""},{"location":"reports/2023/typinator/#introduction","title":"Introduction","text":"<p>What is a better use of a robot than to do your homework for you? With the surging use of ChatGPT among struggling, immoral college students, we decided to make cheating on homework assignments even easier. Typinator is a robot that uses computer vision and control of a robotic arm to recognize a keyboard, input a prompt to ChatGPT, and type out the response.</p>"},{"location":"reports/2023/typinator/#original-problem-statement","title":"Original Problem Statement","text":"<p>Demonstrate the robotic arm by having it type its own code. The arm will hold an object to press keys with. The robot will hopefully write code quickly enough so that people can watch it write in real time. The robot could also type input to chatgpt, and have a separate program read the output to the robot. It will then cmd+tab into a document and type the answer. Both of these demonstrations will show the arm \u201cdoing homework,\u201d which I think is a silly butchallenging way to demonstrate the robot\u2019s capabilities.</p>"},{"location":"reports/2023/typinator/#learning-objective","title":"Learning Objective","text":"<p>The goal of this project is to demonstrate the capability of the robotic arm in conjunction with computer vision. </p> <p>While the end result of the project is not actually useful in a real world setting, the techniques used to produce the end result have a wide variety of applications. Some of the challenges we overcame in computer vision include detecting contours on images with a lot of undesired lines and glare, reducing noise in contoured images, and transforming image coordinates to real world coordinates. </p> <p>Some of the challenges we overcame in using a robotic arm include moving to very specific positions and translating cartesian coordinates to polar coordinates since the arm moves by rotating. We also worked extensively with ROS.</p>"},{"location":"reports/2023/typinator/#original-goals","title":"Original Goals","text":"<ul> <li>Text Publishing</li> <li>Publish large chunks of text with ROS</li> <li>Clean text into individual characters that can be typed</li> <li>Send and receive data from ChatGPT</li> <li>Computer Vision</li> <li>Recognize keys on a keyboard</li> <li>Detect letters on a keyboard (this was not used in the final project due to inconsistency)</li> <li>Transform image coordinates to real world coordinates</li> <li>Arm Control</li> <li>Move the arm to specific positions with high accuracy</li> </ul>"},{"location":"reports/2023/typinator/#what-was-created","title":"What was created","text":""},{"location":"reports/2023/typinator/#technical-description","title":"Technical Description","text":""},{"location":"reports/2023/typinator/#project-structure-diagram","title":"Project structure diagram","text":"<p>The project works by having a central coordinating node connect arm motion, image recognition, keyboard input, and text input. All of these functionalities are achieved through ROS actions except for the connection to the keyboard input, which is done through a simple function call (coordinator --&gt; keys_to_type_action_client). Our goal with designing the project in this way was to keep it as modular as possible. For example, the chatgpt_connection node could be replaced by another node that publishes some other text output to the <code>generated_text</code> ROS topic. A different arm_control node could be a server for the arm_control action if a different robotic arm requiring different code was used. Any of the nodes connected with actions or topics could be substituted, and we often used this to our advantage for debugging.</p>"},{"location":"reports/2023/typinator/#arm-motion","title":"Arm Motion","text":"<p>Arm motion diagram</p> <p>As shown above, the arm moves with rotation and extension to hover above different keys. To achieve this movement from a flat image with (x,y) coordinates, we converted the coordinates of each key into polar coordinates. From (x,y) coordinates in meters, the desired angle of the arm is determined by $\u03b8=atan(x/y)$. The desired extension from the base of the arm is found by $dist=y/cos(\u03b8)$. We calculate this relative to the current extension of the arm so $\u0394dist=dist-currentDistance$.</p> <p>It was found that the arm does not accurately move in this way. As the angle increases, the arm does not adjust its extension by a large enough distance. This can be corrected for, but an exact correction was not found. One correction that works to an acceptable degree of accuracy for angles less than 45 degrees is: $((delta)/math.cos(delta)) - delta$, with $delta$ being some value between 0.07 and 0.09 (the optimized value was not determined and it seems as if the right value changes based on how far the arm is currently extended and to which side the arm is turned). This correction is then added to $dist$.</p>"},{"location":"reports/2023/typinator/#keyboard-recognition","title":"Keyboard Recognition","text":"<p>Keyboard contouring example 1 Keyboard contouring example 2, with a more difficult keyboard to detect</p> <p>Keyboard recognition was achieved with OpenCv transformations and image contouring. The keys of many different keyboards are able to be recognized and boxes are drawn around them. We were able to achieve key recognition with relatively little noise.</p> <p>The first transformation applied to an image is optional glare reduction with <code>cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))</code>. Then, the image is converted to greyscale with <code>cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)</code>. A threshold is then applied to the image. Different thresholds work for different images, so a variety of thresholds must be tried. One example that often works is <code>cv2.THRESH_OTSU | cv2.THRESH_BINARY_INV</code>. This applies an Otsu threshold followed by a binary inverse threshold. Finally, dilation and blur are applied to the image with customizable kernel sizes.</p> <p>After the transformations are applied, OpenCV contouring is used with <code>contours, hierarchy = cv2.findContours(img, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)</code>. External contours (those with child contours and no parent contours) are ignored to reduce noise.</p>"},{"location":"reports/2023/typinator/#running-the-code","title":"Running the code","text":"<p>Before running the project, connect the arm and a usb webcam to a computer with ROS and the software for the Interbotix arm installed (this project will not work in the vnc). More details on the arm software are provided here.</p> <p>Launch the arm: <code>roslaunch interbotix_xsarm_control xsarm_control.launch robot_model:=px100 use_sim:=false</code></p>"},{"location":"reports/2023/typinator/#calibration","title":"Calibration","text":"<p>Follow these instructions if the keyboard you are using has not already been calibrated.</p> <p>Position the usb camera in the center of the keyboard, facing directly down. The whole keyboard should be in frame. Ideally the keyboard will have minimal glare and it will be on a matte, uniform surface. The width of the image must be known, in meters. Alternatively, an image of the keyboard can be uploaded, cropped exactly to known dimensions (i.e. the dimensions of the keyboard).</p> <p>Run <code>roslaunch typinator image_setup.launch</code> to calibrate the image filters for optimized key detection. A variety of preset filters will be applied to the image. Apply different combinations of the following parameters and determin which filter preset/parameter combination is the best. An optimal filter will show keys with large boxes around them, not have a lot of noise, and most importantly detect all of the keys. If there is a lot of noise, calibration will take a long time.</p> Parameter Default Use image_file False Input the path to the image file you would like to use if not using the camera reduce_glare False Set to True to apply glare-reduction to the image blur 1 Apply blur to the image before finding contours. 1 results in no blur. kernel 1 Apply a dilation kernel to the image. 1 has no effect (1 pixel kernel) <p>Once the tranformation preset, reduce_glare, blur, and kernel have been selected, the arm can be calibrated and run. Run <code>roslaunch typinator typinator.launch</code> with the appropriate parameters:</p> Parameter Default Use calibrate True Set to false if loading the keyboard from a preset keyboard_preset \"temp_preset.json\" In calibration mode, save the keyboard to this file. Load this keyboard file if calibrate=False. chatgpt \"Generate random text, but not lorem ipsum\" Text input to chatgpt img_width 0 MUST SET THIS VALUE DURING CALIBRATION: Set to the the real world width of the image in meters img_height img_width*aspect ratio Set this to the real world height of the image if necessary arm_offset 0 Set this to the distance from the bottom edge of the image to the middle of the base of the arm image_file False Input the path to the image file you would like to use if not using the camera reduce_glare False Set to True to apply glare-reduction to the image blur 1 Apply blur to the image before finding contours. 1 results in no blur. kernel 1 Apply a dilation kernel to the image. 1 has no effect (1 pixel kernel) thresh_preset 1 Apply this threshold preset to the image. <p>The arm will press each key it detects and save its position! It will then type the output from ChatGPT! You can open a file you want it to type into, even while calibrating.</p> <p>Running from a preset</p> <p>If a specific keyboard has already been calibrated, position the arm appropriately and run <code>roslaunch typinator typinator.launch calibrate:=False keyboard_preset:=\"filename.json\"</code> Also set <code>image_file</code> if you want boxes to display current keys, and <code>chatgpt</code> if you want a custom input to chatgpt.</p>"},{"location":"reports/2023/typinator/#source-files","title":"Source Files","text":""},{"location":"reports/2023/typinator/#keys_to_type_serverpy","title":"keys_to_type_server.py","text":"<p>indiv_keys():  This is the callback function of Key service. It processes the text that it\u2019s passed as its goal, and separates it by key/letter so that only one will be typed at a time. It also takes into account capital letters and inserts \u2018caps_lock\u2019 before and after the letter is added to the result list, since a keyboard only types in lower case. Furthermore, it takes into account new lines and passes \u2018Key.enter\u2019 in place of logging \u2018\\n\u2019.</p>"},{"location":"reports/2023/typinator/#keys_to_type_clientpy","title":"keys_to_type_client.py","text":"<p>This class subscribes to the 'generated_text' topic, which consists of the text generated by ChatGPT that the arm is meant to type. cb_to_action_server(): This is the function that sends/sets the goal for the action server, and waits for the action to completely finish before sending the result. It makes the ChatGPT message the goal of the action, and waits for the text/letter separation to complete before sending the list of individual letters of the the text as the result. get_next_key(): This function returns the first key in the deque, which is the letter that needs to be typed by the arm next. </p>"},{"location":"reports/2023/typinator/#key_pospy","title":"key_pos.py","text":"<p>This class is used to convert key pixel positions to arm target x,y coordinates from an image of a keyboard.  img_cb() and img_cb_saved():  These functions use cv2 image processing to find the individual keys on a keyboard from an image. It sets some of the class variables, such as height and width, which is then later used to calculate the conversion rates used to turn the key pixel positions into meter positions. The img_cb() function is used when it's subscribing to, and processing images from, a camera topic. Then, the img_cb_saved() function is used when an already saved image of a keyboard is being used instead of one received from the camera. Both functions essentially have/use the same code.  find_key_points():  This function converts key pixel locations in the image to the physical x,y meter positions that can then be used to move the real arm so that it can hit the keys as accurately as possible on the actual keyboard. print_points():  This function prints each calculated point that has been stored in the class. The points are where we need the robot arm to move to in order to press a key.  practice_points():  This function returns the entire list of points saved in the class, with the x and y meter values of the keys' positions on the physical keyboard.  key_pos_action and action_response():  This is the KeyPos action and its callback function. The purpose of the action is to send the physical positions of the each key. It send real x, y, and z coordinates along with image x, image y, image height, and image width coordinates.</p>"},{"location":"reports/2023/typinator/#chat_gptpy","title":"chat_gpt.py","text":"<p>This node connects to ChatGPT through an API key to generate a message/string for the arm to type onto the keyboard. It then publishes the text to the 'generated_text' topic to be used by other nodes.</p>"},{"location":"reports/2023/typinator/#arm_controlpy","title":"arm_control.py","text":"<p>This file contains the sever for the 'arm_control' action, and it connects to the robotic arm. The MoveArm class acts as the link between the coordinates that the server receives and the motion of the arm. - calculate_distance() converts cartesian coordinates to polar coordinates. - set_xy_position(x, y) moves the arm to the specified cartensian x,y coordinates over the keyboard - press_point(z) moves the arm down to and back up from the specified depth coordinate - move_then_press(x,y,z) calls set_xy_position(x,y) followed by pres_point(z) - move_arm(msg) is an action callback function that calls for an instance of the MoveArm class to move the arm to the specified coordinates. It returns \"True\" when done.</p>"},{"location":"reports/2023/typinator/#coordinatorpy","title":"coordinator.py","text":"<p>This file connects the other ROS nodes together. It runs the \"coordinator\" node. In \"calibrate\" mode, the main() uses the Coordinator class to get the image of the keyboard and find the positions of individual keys. Since these keys are not laballed, it then has the arm press each found contour. After each press, it gets the last pressed key with coordinator.get_key_press(). This uses the KeyPress action to get the stored last key press. If a new key was pressed, it is mapped to the location that the arm went to. After the arm has gone to every found position, it saves the key to position mappings in a .json file.</p> <p>In \"type\" mode, the main() function loads one of the stored key mappings into a python dictionary. It gets the next letter to type one at a time using the get_next_key() method of a KeysActionClient. It uses the dictionary to find the location of the key, and then it send the arm to that location.</p>"},{"location":"reports/2023/typinator/#key_boxespy","title":"key_boxes.py","text":"<p>This file contains the FindKeys class. This class is used to find the positions of keyboard keys. - transform_image(img, threshold, blur, dilate) applies a transformation to an image of a keyboard with the specified threshold, blur kernel, and dilation kernel. - contour_image(img) applies image contouring and noise reduction to find the keys. It returns the contours as instances of the Key class.</p>"},{"location":"reports/2023/typinator/#keyspy","title":"keys.py","text":"<p>This file contains the Key class that stores the data of each individual key found from the FindKeys class. center() returns the center of a key.</p>"},{"location":"reports/2023/typinator/#key_pubpy","title":"key_pub.py","text":"<p>This file listens for keyboard input with pynput. The KeyListener class keeps track of the most recent key release and acts as the server for the KeyPress action. When called, it returns the most recent key that was pressed.</p>"},{"location":"reports/2023/typinator/#nodes","title":"Nodes","text":"Node File coordinator coordinator.py arm_control arm_control.py keys_to_type_action_client keys_to_type_client.py keys_to_type_action_server keys_to_type_server.py chatgpt_connection chat_gpt.py key_pub key_pub.py key_pos_action key_pos.py"},{"location":"reports/2023/typinator/#actions","title":"Actions","text":"Action Type Data arm_control ArmControl float32 xfloat32 yfloat32 z---string success---string status key_pos_action KeyPos string mode---float32[] positionsint32[] img_positions---bool status key_press KeyPress string modebool print---string key---bool status keys_to_type_action Keys string full_text---string[] keys---bool status"},{"location":"reports/2023/typinator/#topics","title":"Topics","text":"Topic Type /generated_text String"},{"location":"reports/2023/typinator/#story-of-the-project","title":"Story of the project","text":""},{"location":"reports/2023/typinator/#how-it-unfolded-how-the-team-worked-together","title":"How it unfolded, how the team worked together","text":"<p>We started the project with high hopes for the capabilities of the arm and image recognition software. We planned to have the arm hold a pen and press keys on the keyboard. We hoped that the keys would be recognized through letter detection with machine learning.</p> <p>Initially, we mainly set up the Interbotix arm software in our vnc\u2019s, tested the movement of the arm, and began working on keyboard recognition. Our preliminary tests with detecting the keys on the keyboard were promising. Letter detection for the letter keys was promising, however, it was not working for the special keys or keys with more than one symbol on them.</p> <p>We decided that to get the main functionality of the project working, we would divide the work between ourselves. </p> <p>Kirsten worked on connecting to ChatGPT, publishing the response from ChatGPT as characters on the keyboard, and transforming pixel coordinates in an image into real world coordinates in meters. We discovered that ROS nodes that perform actions will block for too long to receive improperly timed callbacks, so in order to avoid issues with this, Kirsten worked on converting letter publishing ROS topics into ROS actions. By designing our code this way, we were able to have a central ROS node that called ROS actions and got responses sequentially.</p> <p>Elliot worked on moving the robotic arm so that it could move to specific cartesian coordinates and then press down. He also worked on refining the keyboard recognition and designing the central node to bring the project together. After many attempts at reducing noise with statistical methods when finding the outlines of keys on a keyboard (with OpenCV contouring), he discovered that the contour hierarchy was a much more effective way of reducing noise. With contour hierarchy and number of customizable image transformations applied to the images, he was able to consistently find the keys on keyboards.</p> <p>After reexamining letter detection with machine learning, we decided that it was not consistent enough to work for the desired outcome of our project. We realized we could \u201ccalibrate\u201d the arm on the keyboard by pressing each key that was found through OpenCV contouring and listening for keystrokes. In theory, this could provide 100% accuracy.</p> <p>We encountered many difficulties when putting our pieces of the project together and running it on the real arm. As can be expected, a few small quirks with ROS and a few small bugs caused us a lot of difficulty. For example, when we first ran the project as a whole, it started but nothing happened. We eventually found that our text publishing node had a global variable declared in the wrong place and was returning <code>None</code> instead of the letters it received from ChatGPT.</p> <p>One major issue we encountered is that the arm motor turns itself off any time it thinks it is in one position but is actually in a different position. This usually only happened after around 10 key presses, so to fix it we have the arm return to its sleep position every 6 keystrokes. Another major issue is that the arm holds down on the keys for too long. We solved this problem by turning off key repeats in the accessibility settings of the computer controlling the arm. We found that the arm has some limitations with its accuracy which affects its typing accuracy, since the keys are such small targets. It also does not seem to accurately go to specific positions. It always goes to the same positions at a given angle, but its extension and retraction varies based on its current position. It was challenging, but we were able to find a function of the angle of the arm that offsets this error fairly accurately for angles less than 45 degrees, which is what we needed. If we were starting from scratch, we would place a fiducial on the arm and track its motion to try to get better accuracy.</p>"},{"location":"reports/2023/typinator/#our-assessment","title":"Our assessment","text":"<p>We were mostly successful in achieving our original project goals. We are able to recognize a keyboard, train the arm on that keyborad, and type an output from ChatGPT. We were not successful in using letter recognition to press found keys in real time, however, with our method of calibrating the arm on a specific keyboard, the arm was able press any key on almost any keyboard. If we manually input arm positions for each key on a specific keyboard we would likely get better typing accuracy, but this would not be in line with our initial goal. Our use of image contouring to find keys and transformations to convert pixels to real coordinates makes the project applicable to any keybaord without manual input.</p>"},{"location":"reports/2024/AgriculturalRobot/","title":"Agricultural Robotics System","text":""},{"location":"reports/2024/AgriculturalRobot/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Introduction<ul> <li>Project Goals</li> <li>Inspiration</li> <li>Objective</li> </ul> </li> <li>Challenges<ul> <li>Flask Server</li> <li>Plant Detection</li> <li>Water Sprayer Signaling</li> <li>Autonomous Navigation</li> </ul> </li> <li>ROS Structure</li> <li>Project Story</li> </ul>"},{"location":"reports/2024/AgriculturalRobot/#introduction","title":"Introduction","text":""},{"location":"reports/2024/AgriculturalRobot/#autonomous-turtlebot-with-precision-watering-system","title":"Autonomous Turtlebot with Precision Watering System","text":"<p>This research project focuses on the development of an autonomous Turtlebot platform equipped with a precision water-dispensing system. The robot will navigate a structured indoor environment to identify and assess plants, determining both the necessity and appropriate quantity of water to apply. This system integrates advanced robotic navigation, computer vision, and AI-driven plant recognition to mimic the efficiency of precision agricultural technologies.</p> <p></p>"},{"location":"reports/2024/AgriculturalRobot/#project-goals","title":"Project Goals","text":"<p>The Turtlebot will:</p> <ul> <li>Autonomously navigate within a defined space using systematic navigation algorithms.</li> <li>Use computer vision techniques (e.g., fiducial marker detection via OpenCV) for localization and orientation.</li> <li>Leverage OpenAI-based models for plant type identification, enabling tailored watering strategies based on plant-specific hydration needs.</li> <li>Activate a water sprayer actuator to deliver a precise volume of water upon successful identification and analysis of the plant.</li> <li>Remote controlled via an web-based application</li> </ul>"},{"location":"reports/2024/AgriculturalRobot/#inspiration","title":"Inspiration","text":"<ul> <li>Technological inspiration: This project draws inspiration from advanced agricultural robotics, such as sniper robots capable of treating vast crop fields with unparalleled efficiency while using 95% fewer chemicals.</li> <li>Sniper robot treats 500k plants per hour with 95% less chemicals</li> <li>Personal relevance: The project also addresses the common issue of plant neglect, inspired by Jiahao's experience with underhydrated and dying plants.</li> </ul> <p>By combining robotics, AI, and precision actuation, this initiative explores innovative solutions for sustainable plant care in agricultural and domestic contexts.</p>"},{"location":"reports/2024/AgriculturalRobot/#objective","title":"Objective","text":"<p>The objective of this project is to explore the integration of robotics, artificial intelligence, and precision actuation to solve real-world problems related to plant care and resource management. By leveraging autonomous navigation, image recognition, and intelligent decision-making systems, the project aims to:</p> <ul> <li>Demonstrate the feasibility of using robotics for sustainable, resource-efficient plant care.</li> <li>Develop a scalable framework for integrating AI-driven plant identification into robotic systems.</li> <li>Investigate and optimize strategies for precision watering to minimize waste while meeting plant-specific needs.</li> <li>Provide insights into the potential of robotic platforms in precision agriculture and domestic gardening.</li> <li>Design and implement a system that will handle communication between user input from UI to our robot</li> </ul> <p>This project serves as a proof of concept for innovative approaches to automated plant care, with broader implications for agricultural and environmental sustainability.</p>"},{"location":"reports/2024/AgriculturalRobot/#challenges","title":"Challenges","text":""},{"location":"reports/2024/AgriculturalRobot/#flask-server-and-react-frontend-integration-for-plant-care-robot","title":"Flask Server and React Frontend Integration for Plant Care Robot","text":"<p>This section explains how to integrate a Flask backend with a React frontend to manage a plant care robot that detects plants, updates instructions, and provides real-time information. Below is the detailed breakdown of the implementation.</p>"},{"location":"reports/2024/AgriculturalRobot/#flask-server","title":"Flask Server","text":"<p>The Flask server provides APIs for handling robot instructions and plant data. It also serves the React frontend and handles cross-origin requests using <code>flask_cors</code>.</p>"},{"location":"reports/2024/AgriculturalRobot/#system-overview","title":"System Overview","text":""},{"location":"reports/2024/AgriculturalRobot/#key-features-of-the-flask-server","title":"Key Features of the Flask Server","text":"<ol> <li> <p>Serving the React App:</p> </li> <li> <p>Serves the static React build files and ensures the app loads correctly.</p> </li> <li> <p>Managing Instructions:</p> </li> <li> <p>Handles instructions like <code>GO_HOME</code>, <code>SCAN_ALL</code>, or navigating to specific plants.</p> </li> <li> <p>Includes robust validation to ensure valid instructions are processed.</p> </li> <li> <p>Managing Plant Data:</p> </li> <li> <p>Provides a list of detected plants and allows updating the plant list dynamically.</p> </li> <li> <p>Instruction Feedback Loop:</p> </li> <li>Periodically prints the current instruction for debugging purposes.</li> </ol>"},{"location":"reports/2024/AgriculturalRobot/#flask-code-implementation","title":"Flask Code Implementation","text":"<pre><code>from flask import Flask, request, jsonify, send_from_directory\nimport os\nimport base64\nimport time\nimport threading\nfrom flask_cors import CORS\n\n# Initialize Flask app with static folder for serving React app\napp = Flask(__name__, static_folder='build', static_url_path='')\nCORS(app)\n\n# Global variables\nALL_INSTRUCTION = ['GO_HOME', 'GO_TO_PLANT_', 'SCAN_ALL', 'NONE']\nINSTRUCTION = 'NONE'\n\n# Example plant data (testing purposes)\ndef jpg_to_base64(image_path):\n    with open(image_path, \"rb\") as image_file:\n        return base64.b64encode(image_file.read()).decode('utf-8')\n\navailable_plants = [] # [[id, type, img]]\n\n# Function to print current instruction every 5 seconds\ndef print_instructions():\n    while True:\n        print(f\"Current instruction: {INSTRUCTION}\")\n        time.sleep(5)\n\nthreading.Thread(target=print_instructions, daemon=True).start()\n\n# Routes\n@app.route('/')\ndef serve_react_app():\n    return send_from_directory(app.static_folder, 'index.html')\n\n@app.route('/get_plants', methods=['GET'])\ndef get_available_plants():\n    return jsonify(available_plants), 200\n\n@app.route('/get_instruction', methods=['GET'])\ndef get_instruction():\n    global INSTRUCTION\n    return jsonify({\"instruction\": INSTRUCTION}), 200\n\n@app.route('/update_instruction', methods=['POST'])\ndef update_instruction():\n    global INSTRUCTION\n    data = request.get_json()\n    if not data:\n        return jsonify({\"error\": \"Invalid input\"}), 400\n    instruction = data.get('instruction')\n    if instruction not in ALL_INSTRUCTION and not instruction.startswith('GO_TO_PLANT_'):\n        return jsonify({\"error\": \"Invalid instruction\"}), 400\n    INSTRUCTION = instruction\n    return jsonify({\"message\": \"Instruction updated successfully\"}), 200\n\n@app.route('/update_plants', methods=['POST'])\ndef update_available_plants():\n    global available_plants\n    data = request.get_json()\n    if not data or 'available_plants' not in data:\n        return jsonify({\"error\": \"Invalid input\"}), 400\n    available_plants = data['available_plants']\n    return jsonify({\"message\": \"Available plants updated successfully\"}), 200\n\nif __name__ == '__main__':\n    app.run(debug=False)\n</code></pre>"},{"location":"reports/2024/AgriculturalRobot/#react-frontend","title":"React Frontend","text":"<p>The React frontend fetches data from the Flask API and provides an interactive UI for monitoring and controlling the robot.</p> <p></p>"},{"location":"reports/2024/AgriculturalRobot/#key-functions-of-the-react-frontend","title":"Key functions of the React Frontend","text":"<pre><code>  useEffect(() =&gt; {\n    const fetchPlants = () =&gt; {\n      fetch(`${url}/get_plants`)\n        .then((response) =&gt; response.json())\n        .then((data) =&gt; {\n          // data type: [fiducial_id, plant_type, base_64_image]\n          setPlants(data);\n        })\n        .catch((error) =&gt; {\n          console.error('Error fetching plants:', error);\n          showNotification('Failed to fetch plants');\n        });\n    };\n\n    fetchPlants(); // Initial fetch\n    const intervalId = setInterval(fetchPlants, 3000); // Fetch every 3 seconds\n\n    return () =&gt; clearInterval(intervalId); // Cleanup on unmount\n  }, []);\n\n  const safeFetch = async (func, params) =&gt; {\n    try {\n      const response = await fetch(`${url}/get_instruction`);\n      const res = await response.json();\n\n      if (res.instruction === 'NONE') {\n        await func(params); // Add await in case func is async\n      } else {\n        showNotification('Robot is busy');\n      }\n    } catch (error) {\n      showNotification('Failed to fetch robot status');\n    }\n  };\n  const handleBackToBase = () =&gt; {\n    console.log('Sending instruction to go home');\n    fetch(`${url}/update_instruction`, {\n      method: 'POST',\n      headers: {\n        'Content-Type': 'application/json',\n      },\n      body: JSON.stringify({ instruction: 'GO_HOME' }),\n    })\n      .then(() =&gt; showNotification('Robot is returning to base'))\n      .catch(() =&gt; showNotification('Failed to send robot home'));\n  };\n\n  const safeHandleBackToBase = () =&gt; {\n    safeFetch(handleBackToBase);\n  };\n\n  const handleDetectPlants = () =&gt; {\n    console.log('Detecting plants');\n    fetch(`${url}/update_instruction`, {\n      method: 'POST',\n      headers: {\n        'Content-Type': 'application/json',\n      },\n      body: JSON.stringify({ instruction: 'SCAN_ALL' }),\n    })\n      .then(() =&gt; showNotification('Detecting plants'))\n      .catch(() =&gt; showNotification('Failed to detect plants'));\n  };\n\n  const safeHandleDetectPlants = () =&gt; {\n    safeFetch(handleDetectPlants);\n  };\n\n  const handleSprayPlant = (fiducialId) =&gt; {\n    console.log(`Spraying plant ${fiducialId}`);\n    showNotification(`Spraying plant ${fiducialId}`);\n\n    fetch(`${url}/update_instruction`, {\n      method: 'POST',\n      headers: {\n        'Content-Type': 'application/json',\n      },\n      body: JSON.stringify({ instruction: `GO_TO_PLANT_${fiducialId}` }),\n    })\n      .then(() =&gt;\n        showNotification(\n          `Going to ${fiducialId}, plant: ${\n            plants.find((plant) =&gt; plant[0] === fiducialId)[1]\n          }`\n        )\n      )\n      .catch(() =&gt; showNotification(`Failed to spray plant ${fiducialId}`));\n  };\n\n  const safeHandleSprayPlant = (fiducialId) =&gt; {\n    safeFetch(handleSprayPlant, fiducialId);\n  };\n\n  const showNotification = (message) =&gt; {\n    setSnackbarMessage(message);\n    setOpenSnackbar(true);\n  };\n\n  const resetRobot = () =&gt; {\n    fetch(`${url}/reset_all`, {\n      method: 'POST',\n      headers: {\n        'Content-Type': 'application/json',\n      },\n      // body: JSON.stringify({ instruction: 'NONE' }),\n    })\n      .then(() =&gt; showNotification('Robot reset'))\n      .catch(() =&gt; showNotification('Failed to reset robot'));\n  };\n</code></pre> <ol> <li> <p>Plant Grid:</p> </li> <li> <p>Displays the detected plants with images, IDs, and types.</p> </li> <li> <p>Allows users to spray a specific plant by clicking a button.</p> </li> <li> <p>Robot Controls:</p> </li> <li> <p>Provides buttons to reset the robot, detect plants, and return to base.</p> </li> <li> <p>Notifications:</p> </li> <li>Displays success or error messages for user actions.</li> <li>Safe requests:</li> <li>Make sure the robot ensures its in idle state before sending instructions</li> </ol>"},{"location":"reports/2024/AgriculturalRobot/#plant-detection","title":"Plant Detection","text":"<p>Ensuring that the Turtlebot reliably detects the target plant under varying lighting conditions (e.g., shadows or brightness changes) is a critical component of this project. The initial approach involved integrating a YOLOv5 model for real-time plant detection, leveraging resources such as the Plant Leaf Detection and Classification model. However, this approach revealed several limitations:</p> <ul> <li>Accuracy: The model struggled with detecting plants under diverse lighting conditions and varying angles.</li> <li>Versatility: It lacked adaptability to different environmental settings, which limited its practical application.</li> <li>Computational Load: The YOLOv5 model was computationally intensive, making it less suitable for real-time deployment on a resource-constrained platform like the Turtlebot.</li> </ul> <p>To address these challenges, the project transitioned to using the OpenAI GPT-4o-mini model with advanced vision capabilities. This model demonstrated significantly improved performance by:</p> <ul> <li>Providing consistent detection accuracy, achieving nearly 100% reliability across diverse conditions.</li> <li>Offering restricted outputs tailored to specific plant identification, enhancing precision.</li> <li>Operating with greater computational efficiency, making it a more practical option since computation was offloaded to an external API.</li> </ul> <p></p>"},{"location":"reports/2024/AgriculturalRobot/#implementation-code-plant-detection-with-openai-gpt-4o-mini","title":"Implementation Code: Plant Detection with OpenAI GPT-4o-mini","text":"<p>The <code>Detector</code> class leverages the GPT-4o-mini model to identify plant types or confirm the presence of a plant in an image. Below is the implementation:</p> <pre><code>import base64\nfrom openai import OpenAI\nfrom dotenv import load_dotenv\nimport os\n\n# Load environment variables\nload_dotenv()\n\n# Retrieve the OpenAI API key from the environment variables\nopen_api_key = os.getenv(\"OPEN_API_KEY\")\n\nclass Detector:\n    def __init__(self):\n        self.client = OpenAI(api_key=open_api_key)  # Initialize OpenAI client\n        self.MAX_RETRIES = 10  # Maximum retries for API requests\n        self.plant_types = ['Cactus', 'Basil', 'Thyme', 'Parsley', 'Gatorade']  # Recognized plant types\n\n    def detect_plant(self, image):\n        \"\"\"\n        Detects the type of plant or Gatorade in an image.\n\n        Args:\n            image (str): Base64 encoded string of the image.\n\n        Returns:\n            tuple: (bool, str) indicating whether detection was successful and the identified plant type.\n        \"\"\"\n        for i in range(self.MAX_RETRIES):\n            try:\n                response = self.client.chat.completions.create(\n                    model=\"gpt-4o-mini\",\n                    messages=[\n                        {\n                            \"role\": \"user\",\n                            \"content\": [\n                                {\"type\": \"text\", \"text\": \"Output the plant type or Gatorade and only the plant type in one word: 'Cactus', 'Basil', 'Thyme', 'Parsley', or 'Gatorade' if the image's object of interest contains the plant or Gatorade\"},\n                                {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image}\"}}\n                            ],\n                        }\n                    ],\n                )\n                res = response.choices[0].message.content.strip()\n\n                if res in self.plant_types:\n                    return True, res\n                else:\n                    return False, None\n            except Exception as e:\n                print(f\"Failed attempt {i}: {e}\")\n</code></pre>"},{"location":"reports/2024/AgriculturalRobot/#explanation-of-key-components","title":"Explanation of Key Components","text":""},{"location":"reports/2024/AgriculturalRobot/#plant-type-detection","title":"Plant Type Detection:","text":"<p>The <code>detect_plant</code> method sends a base64-encoded image to the OpenAI GPT-4o-mini model, instructing it to identify the object of interest from a predefined list of plant types (e.g., 'Cactus', 'Basil', etc.) or Gatorade.</p>"},{"location":"reports/2024/AgriculturalRobot/#retry-mechanism","title":"Retry Mechanism:","text":"<p>Both methods implement a retry mechanism to handle potential API request failures, ensuring robust performance in real-world applications.</p> <p>This refinement in plant detection methodology highlights the importance of balancing model accuracy, versatility, and computational feasibility in robotics applications. The GPT-4o-mini model proved to be a game-changer, ensuring robust and reliable plant identification for the Turtlebot's precision watering tasks. It can also detect a variety of objects outside of the plant constraints, including a Gatorade bottle.</p>"},{"location":"reports/2024/AgriculturalRobot/#water-sprayer-signaling-system","title":"Water Sprayer Signaling System:","text":"<p>Create a signaling channel for the turtlebot to control the sprayer through ROS. This requires hardware level development.</p> <p>One of the biggest challenge for this project was to tackle hardware modifications as a team who has no experience in hardward work.</p> <p>From a high level, the message transmission path that controls the sprayer goes from</p> <pre><code>publisher -&gt; subscriber (on rasberry pi) -&gt; arduino uno -&gt; relay -&gt; sprayer\ntransmission type: ROS -&gt; serial -&gt; GPIO pin\n</code></pre>"},{"location":"reports/2024/AgriculturalRobot/#components","title":"Components","text":"<p>There are three main components that goes into making the sprayer remote controllable using ROS publisher.</p>"},{"location":"reports/2024/AgriculturalRobot/#1-arduino-uno","title":"1. arduino uno","text":"<p>The arduino is responsible for receiving messages from rasberry pi and controlling the relay.    </p>"},{"location":"reports/2024/AgriculturalRobot/#2-rasberry-pi","title":"2. rasberry pi","text":"<p>The rasberry pi is where the ROS subscriber is run. It listens to published messages and passes it down to arduino uno. </p>"},{"location":"reports/2024/AgriculturalRobot/#3-relay","title":"3. relay","text":"<p>The relay is responsible for controlling the open and close of the circuit loop which triggers the power of the sprayer.</p> <p>There are 6 ports on the relay. Each of them except NC is required for our setup</p> <ul> <li>IN: Connects to the Arduino's GPIO pin (e.g., pin 7), this port handles recieving commands from arduino uno.</li> <li>DC+: Connects to Arduino 5V, along with DC- this port provides the power to trigger the relay.</li> <li>DC-: Connects to Arduino GND.</li> <li>COM: Connects to the live wire or signal going to the load (e.g., a light bulb or motor).</li> <li>NO: The load should be OFF by default and turn ON when the relay is activated.</li> <li>NC: Since we want the relay to be OFF by default, this port is not necessary.</li> </ul> <p></p>"},{"location":"reports/2024/AgriculturalRobot/#soldering","title":"Soldering","text":"<p>We also learned soldering in part of this modification.</p> <p>To control the sprayer with the relay, we need to intercept the power source of the sprayer. Since the sprayer is powered by serial connection batteries, we can just cut the wires and reconnect both ends to the relay COM and NO ports. When the relay is on, the connection will be established, completing the circuit loop and trigger the sprayer.</p> <p></p> <p>Below is a visual representation of what's happening to the circuit loop.</p> <p></p> <p>More details on how we implemented this including the code implementation is in the faq section. external_actuator_control</p>"},{"location":"reports/2024/AgriculturalRobot/#autonomous-navigation-and-spraying","title":"Autonomous Navigation and Spraying:","text":"<p>To enable users to freely navigate the robot to a desired object to spray water shown in Flask Server, fiducials are used to keep track of the location and the type of the plant that the robot has interacted.</p>"},{"location":"reports/2024/AgriculturalRobot/#water-spraying-mechanism-control","title":"Water Spraying Mechanism Control:","text":"<p>Verifying that the water spraying actuator can be accurately triggered at the right time and location. The challenge is to time the activation properly and test its range to ensure it only targets specific areas. </p> <p>To tackle this, we  1. Fine tuned the robot navigation algorithm from the mapped fiducial coordinates to the robots current position so that it is precisely a set distance away from the plant. 2. Move the robot to the fiducial, turn the robot facing slightly away from the fidcuial so the sprayer is pointing at the plant, initiate the plant detection. </p>"},{"location":"reports/2024/AgriculturalRobot/#ros-structure","title":"ROS structure","text":""},{"location":"reports/2024/AgriculturalRobot/#clear-description-and-tables-of-source-files-nodes-messages-actions-and-so-on","title":"Clear description and tables of source files, nodes, messages, actions and so on","text":"<p>1. Overview of Source Files | File Name                  | Description                                                                                         | |--------------------------------|---------------------------------------------------------------------------------------------------------| | <code>camera_control.py</code>            | Subscribes to <code>/raspicam_node/image/compressed</code> and generates a snapshot for image detection | | <code>detector.py</code>                  | A script that sends the captured images to open API obejct detection   | | <code>mapper_real.py</code>               | Maps scanned fiducials to the tf tree so we can retreive the scanned frames | | <code>my_odom.py</code>                   | A helper node that returns the distance travelled and yaw| | <code>sprayer_publisher.py</code>         | Composes by the majority of our application logic, including server interaction, navigation, sprayer control, and detection.|</p> <p>2. Nodes and Topics</p> Node Name File Published Topics Subscribed Topics Description <code>/relay_subscriber</code> <code>relay_subscriber.py</code> N/A <code>/String</code> Located on rasberry pi, sends the recieved message to arduino <code>/relay_publisher</code> <code>/relay_publisher</code> <code>/String</code> <code>/odom</code> publishes \"RELAY_ON\"/\"RELAY_OFF\" message"},{"location":"reports/2024/AgriculturalRobot/#story-of-the-project","title":"Story of the project.","text":"<p>We started by planning to use the Turtlebot's robotic claw to operate a handheld sprayer. However, we couldn't find suitable sprayers and wanted to push beyond simple mechanical solutions into custom hardware integration.</p> <p>We connected with Tim Herbert, head of the Automation Lab at Brandeis University, who became our project mentor. The Automation Lab became our workshop, where we spent half our development time learning hardware modifications. We mastered soldering and hardware-ROS integration, evolving our project from basic robotics into a custom hardware-software system with reliable relay-controlled spraying.</p> <p>As we built momentum, we decided to develop a complete full-stack solution. We built a Flask backend server, React frontend, and implemented real-time communication protocols to create a web interface for controlling the robot. Users can now monitor plants, control the robot, and manage watering operations through their browser. The project grew from a simple robotics exercise into a professional-grade agricultural automation system, combining hardware engineering, robotics, and web development into one cohesive platform.</p>"},{"location":"reports/2024/AutoTaxiSimulation/","title":"Auto Taxi Simulation","text":""},{"location":"reports/2024/AutoTaxiSimulation/#introduction","title":"Introduction","text":"<p>As autonomous driving technology rapidly advances and is being adopted by major electric vehicle companies, the future of transportation stands at a fascinating crossroads. This project aims to simulate aspects of autonomous driving systems using the Robot Operating System (ROS), drawing inspiration from real-world implementations and innovations.</p> <p>The project was particularly motivated by the growing presence of autonomous taxis in Seattle and mainland China, where companies are successfully deploying self-driving vehicles in complex urban environments. These real-world applications demonstrate the practical viability of autonomous systems and their potential to revolutionize urban mobility.</p> <p>A key inspiration for this project comes from Beijing's sophisticated traffic management system, which integrates real-time signal light information into navigation applications. This system provides drivers with precise timing data about traffic signals, including countdown information for red lights. This integration closely parallels ROS's topic-based communication architecture, making it an ideal candidate for simulation in a ROS environment.</p> <p>This project focuses on implementing three crucial features of autonomous driving systems:</p> <p>Autonomous navigation within a pre-mapped environment, emphasizing efficient path planning and execution</p> <p>Intelligent interaction with traffic control elements, including stop signs and traffic signals</p> <p>A passenger service system that handles customer pickup and dropoff operations</p>"},{"location":"reports/2024/AutoTaxiSimulation/#what-was-created","title":"What was created","text":""},{"location":"reports/2024/AutoTaxiSimulation/#technical-descriptions-illustrations","title":"Technical descriptions, illustrations","text":"<p>The environment mapping utilizes the gmapping package, implementing Simultaneous Localization and Mapping (SLAM) techniques. This approach combines laser scan data with the robot's odometric information to construct a detailed 2D occupancy grid map of the environment. The resulting map serves as the foundation for autonomous navigation and path planning.</p> <p>Navigation System Navigation is handled by the move_base framework, which provides:</p> <p>Global path planning using Dijkstra's algorithm to determine optimal routes Local path planning for real-time obstacle avoidance Dynamic trajectory adjustment based on sensor feedback Integration with the robot's velocity controllers for smooth motion execution</p> <p>Traffic Control Recognition The system employs the aruco_detect library for fiducial marker detection and interpretation. This component:</p> <p>Processes video feed from the robot's camera in real-time Identifies and decodes fiducial markers representing traffic elements Stores marker positions in the robot's odometry system for future reference</p> <p>Obstacle Detection and Avoidance Safety and collision avoidance are managed through a LIDAR-based perception system that:</p> <p>Continuously scans the environment for dynamic obstacles and pedestrians Creates point cloud representations of detected objects Integrates with the navigation stack to enable real-time path adjustments Implements configurable safety zones and stopping distances</p> <p>The system maintains a hierarchical control structure where high-level navigation goals can be modified based on real-time sensor data and traffic control information, ensuring safe and efficient autonomous operation in dynamic environments.</p>"},{"location":"reports/2024/AutoTaxiSimulation/#discussion-of-interesting-algorithms-modules-techniques","title":"Discussion of interesting algorithms, modules, techniques","text":"<ol> <li> <p>Basic Mapping and Navigation For this project, I integrated three key components to achieve autonomous navigation: gmapping for environment mapping, move_base for path planning, and Lidar sensing for real-time perception. The gmapping package implements SLAM techniques to construct a global map as the robot explores it prior to navigation. Using LIDAR sensor data, the system captures any components that weren't scanned during the mapping phase and uses it to create a local map to avoid unmapped obstacles. With the environment mapped, the move_base package takes charge of navigation tasks. It processes the map data to generate a cost map, which helps the robot understand where it can safely travel. Before initiating any forward movement, the robot first rotates to align itself with its planned path direction, ensuring smooth and efficient navigation. This navigation system not only plans efficient routes but also ensures compliance with traffic rules detected through fiducial markers. The robot's LIDAR sensor continuously monitors its surroundings, enabling real-time detection and avoidance of dynamic obstacles that weren't present during the initial mapping phase, such as crossing pedestrians.</p> </li> <li> <p>Dealing with Signals &amp; Stop Signs The traffic control system uses fiducial markers to represent signal lights and stop signs. When the robot approaches a signal light fiducial, it subscribes to a ROS topic that updates the light status every 30 seconds. Upon detecting a red light, the system stores the current move_base goal and temporarily halts navigation. Navigation automatically resumes when the signal turns green. For stop sign management, the system maintains an array of vehicles waiting at the intersection. When a robot encounters a stop sign, it adds itself to this queue and comes to a complete stop. After a mandatory 3-second wait, the first robot in the queue is permitted to proceed, and its entry is removed from the array. Subsequent robots must remain stopped until they reach the front of the queue, implementing a first-come-first-serve traffic management system.</p> </li> <li> <p>Pickup passengers For passenger management, the system maintains a queue of pickup and dropoff locations read from a text file, each stored as XY coordinates. The robot processes these locations sequentially, navigating to each pickup/drop off point in order. Once a passenger location is reached, that destination is removed from the queue, and the robot proceeds to the next location. This simple yet effective system ensures orderly passenger service on a first-come-first-served basis.</p> </li> </ol>"},{"location":"reports/2024/AutoTaxiSimulation/#demo-video-for-real-life-demo","title":"Demo Video for Real Life demo","text":"<p>https://drive.google.com/file/d/1nfp36Xyu62P1gIS2hcw-TmGPWURKG40f/view?usp=sharing</p> <p>I also explained my program logic in the video. This first half of demo video contain green light scenario for traffic signal. The later half contain red light scenario.</p>"},{"location":"reports/2024/AutoTaxiSimulation/#guide-on-how-to-use-the-code-written-in-real-life","title":"Guide on how to use the code written in real life","text":"<p>To map the environment, use command <pre><code>roslaunch turtlebot3_slam turtlebot3_slam.launch slam_methods:=gmapping\n</code></pre> And you may also use the teletop function to navigate robot around while mapping. <pre><code>roslaunch turtlebot3_teletop turtlebot3_teletop_keys.launch\n</code></pre></p> <p>To run the program, use command <pre><code>roslaunch ros_auto_taxi ros_auto_taxi.launch\n</code></pre></p> <p>To run the auto navigation, use command <pre><code>roslaunch turtlebot3_navigation turtlebot3_navigation map_file:= map_file_name_here.yaml\n</code></pre></p> <p>To run the program by itself with demo passenger locations, use this command after run <pre><code>rosrun ros_auto_taxi way_point.py\n</code></pre></p> <p>It will lead the robot to passenger position and take it back to start postition.</p>"},{"location":"reports/2024/AutoTaxiSimulation/#clear-description-and-tables-of-source-files-nodes-messages-actions-and-so-on","title":"Clear description and tables of source files, nodes, messages, actions and so on","text":"File name Description taxi_solo.py include reaction towards signal/stop sign mapper.py mapping fiducial location my_odom_solo.py retrieve useful odom information traffic_signal.py publish signal lights shift stop_sign.py publish stop sign information way_point.py publish passenger location and destination Message Name Description Message Type /scan LIDAR scan data for obstacle detection and mapping sensor_msgs/LaserScan /move_base/status Navigation status updates from move_base actionlib_msgs/GoalStatusArray /move_base/goal Target pose for navigation move_base_msgs/MoveBaseActionGoal /move_base/cancel Cancel current navigation goal actionlib_msgs/GoalID /cmd_vel Robot velocity commands geometry_msgs/Twist /traffic_signal Traffic light status updates std_msgs/Bool /stop_sign Stop sign detection and management std_msgs/Int32 /odom Robot odometry data (position/velocity) nav_msgs/Odometry /my_odom A simplified odom topic std_msgs/Float32MultiArray"},{"location":"reports/2024/AutoTaxiSimulation/#story-of-the-project","title":"Story of the project.","text":"<p>My initial vision for this project was ambitious - I wanted to create a multi-robot system operating in a mapped road network. I imagined two robots coordinating to serve passengers, with the nearest robot responding to pickup requests. These robots would navigate while following traffic rules by recognizing Fiducial markers representing traffic signals and stop signs. They would also handle real-world scenarios like pedestrian crossings and yielding to emergency vehicles. However, as I delved deeper into the implementation, I realized that developing and coordinating two robots would be too complex for a solo project. This led me to strategically narrow my focus to perfecting a single robot's operation within the traffic system.</p> <p>For the navigation system, I considered two potential approaches. My primary plan utilized move_base with gmapping, which offered realistic navigation and flexibility in handling new destinations. I also kept a simpler backup plan using line or wall following algorithms in case the primary approach proved too challenging.</p> <p>As I progressed with the implementation, I encountered several technical hurdles. During the mapping phase, I discovered that gmapping's default parameters weren't well-suited for small environments. The system would scan distant objects too early, and noise significantly impacted the mapping quality. This led me to develop optimized parameters and compile troubleshooting tips, which I later documented in the FAQ section.</p> <p>The move_base implementation presented its own set of challenges, particularly in confined spaces like narrow mazes with approximately 20cm wide paths. The default path-finding algorithms, while effective in larger spaces like rooms, performed poorly in these tight conditions. I spent considerable time fine-tuning the system, which required understanding the complex relationships between costmap parameters, DWA planner settings, and move_base configurations. While these components operate independently, their effectiveness depends on careful synchronization - a challenge that proved both frustrating and enlightening as I worked to optimize the system's performance.</p>"},{"location":"reports/2024/AutoTaxiSimulation/#your-own-assessment","title":"Your own assessment","text":"<p>The project successfully implements basic autonomous taxi functionality, demonstrating effective integration of mapping, navigation, and traffic rule compliance. The system achieves its core objectives of autonomous movement, traffic sign response, and passenger management.</p> <p>There are two main areas for potential improvement: implementing Computer Vision for more realistic traffic sign detection beyond fiducial markers, and expanding to a multi-robot system as originally envisioned. These enhancements would bring the system closer to real-world autonomous taxi operations.</p> <p>Also, it would be better to develop my own version of path finding. The current move_base package works poorly on small environment. Besides, move_base is a black box that the internal algorithm is not clearly showed. The only way to improve its effeciency is to tune it through changing parameters. For future improvement, a self designed path finding algorithm is needed.</p>"},{"location":"reports/2024/CleaningRobot/","title":"Smart Cleaning Robot Final Report","text":"<p>This is the final report of project Smart Cleaning Robot, in this report, you will know:</p> <ul> <li>Brief Introduction</li> <li>Core Algorithms</li> <li>Project Modules and Detailed Explanations</li> <li>Story Behinds the Project</li> </ul> <p>And all other informations about the project.</p>"},{"location":"reports/2024/CleaningRobot/#brief-introduction-and-overview","title":"Brief Introduction and Overview","text":""},{"location":"reports/2024/CleaningRobot/#contributors","title":"Contributors","text":"<ul> <li>Pang Liu: Panel Module, Voice Control, Cleaning Module II.</li> <li>Zhenxu Chen: Mapping Module, Cleaning Module I.</li> </ul>"},{"location":"reports/2024/CleaningRobot/#key-features","title":"Key Features","text":"<p>This is a ROS-based autonomous cleaning robot that integrates indoor mapping, voice control, and innovative cleaning functionalities. This project features expandable modules, making it a valuable tool for research, education, and real-world applications.</p> <p>Develope and Test Environment: ROS Noetic, Python3, Ubuntu 20.04.6</p> <ul> <li>GUI Control Panel: A user-friendly interface to manage all modules without command-line interaction.</li> <li>Voice Control: Real-time voice recognition for hands-free operation.</li> <li>Mapping: Efficient exploration and map saving using the Explore_Lite package.</li> <li>Cleaning: Two cleaning modules for full-coverage path planning, including a fully self-designed solution.</li> </ul>"},{"location":"reports/2024/CleaningRobot/#demonstration","title":"Demonstration","text":"<ul> <li>Demo Link: https://jeffliulab.github.io/youtube_links/ros119.html   Note: If the link expires, you can find youtube channel \"JeffLiuLab\" and see all related videos.</li> </ul>"},{"location":"reports/2024/CleaningRobot/#problem-statement-and-original-objectives","title":"Problem Statement and Original Objectives","text":"<p>Background</p> <p>With the rise of smart home technology, users now seek cleaning robots with greater autonomy and interactive capabilities beyond basic cleaning. Traditional cleaning robots face significant challenges in dynamic environments, such as avoiding obstacles or adapting to changes in the environment, and often lack the precision needed for complete coverage. This creates a gap in meeting user expectations for intelligent and efficient cleaning experiences.</p> <p>Original Objectives</p> <p>The project aims to develop a ROS-based cleaning robot that integrates autonomous exploration, complete coverage path planning (CCPP), and voice control. The following objectives were identified to address the problem:</p> <ol> <li>Enable Autonomous Indoor Exploration and Mapping: Utilize Simultaneous Localization and Mapping (SLAM) and Frontier-Based Exploration (FBE) algorithms to allow the robot to autonomously create a 2D map of an unknown environment.</li> <li>Design Complete Coverage Path Planning (CCPP): Ensure the robot can cover all accessible areas without missing or re-cleaning spaces, achieving efficient and thorough cleaning.</li> <li>Integrate Real-Time Obstacle Avoidance: Equip the robot with the ability to detect and avoid dynamic obstacles, such as pedestrians, ensuring smooth and safe cleaning operations.</li> <li>Build an Intelligent Interaction System: Develop capabilities to adjust cleaning strategies based on user commands, such as stopping and returning to the base, continuing cleaning, or remapping the environment.</li> <li>Establish an Expandable System Architecture: Design the system to be scalable, with interfaces for future module expansions such as mobile app control and smart feedback systems.</li> </ol> <p>Original Goal</p> <p>The overarching goal is to create a smart autonomous cleaning robot that can:</p> <ul> <li>Quickly explore and map the environment using SLAM and FBE algorithms.</li> <li>Plan and execute efficient cleaning routes for complete coverage.</li> <li>Recognize and respond to voice commands in real-time.</li> <li>Detect and avoid dynamic obstacles during operation.</li> </ul> <p>This solution aims to overcome the limitations of traditional cleaning robots, providing users with a highly autonomous and interactive cleaning experience.</p>"},{"location":"reports/2024/CleaningRobot/#user-guide","title":"User Guide","text":""},{"location":"reports/2024/CleaningRobot/#program-entry","title":"Program Entry","text":"<p>To start the program:</p> <ol> <li>For Simulation:</li> </ol> <p>Remember to set localhost settings in ~/.bashrc</p> <pre><code>   roslaunch control_panel panel_sim.launch\n</code></pre> <p>No additional commands are required for simulation environment, this launch file handles all controls.</p> <ol> <li>For Real Robot:</li> </ol> <p>Remember to set real IP settings in ~/.bashrc and update turtlebot3's settings.</p> <p>For Real Environment, you need to run roscore on PC firstly:</p> <pre><code>roscore\n</code></pre> <p>Before bringup, if the environment has no Internet access, or no RTC module on turtlebot3, you need sync time.</p> <p>For Real Environment, you need to ssh to turtlebot3 and bring up it secondly:</p> <pre><code>$ roslaunch turtlebot3_bringup turtlebot3_robot.launch\n</code></pre> <p>After above instructions, you can now run the panel to start the program:</p> <pre><code>   roslaunch control_panel panel_real.launch\n</code></pre> <p>After running, you must do following to avoid data loss or hardware damage:</p> <pre><code>sudo shutdown -h now\n</code></pre>"},{"location":"reports/2024/CleaningRobot/#guide-for-build-a-test-environment","title":"Guide for build a test environment","text":"<p>This is also the environment of final demo at lab</p> <p>For the final demonstration at lab, we will replicate a realistic cleaning scenario in a controlled environment. The setup ensures seamless communication and modular scalability, simulating a robust system for future multi-robot collaboration.</p> <p>Hardware Configuration</p> <ul> <li>TurtleBot3 (Cleaning Robot)</li> <li>Role: A single cleaning robot performing mapping, exploration, and cleaning tasks.</li> <li> <p>Connectivity: Communicates with the central system via the router's local network.</p> </li> <li> <p>No Internet Access Local Network Router (Network Center)</p> </li> <li>Role: Acts as the network center to establish a local LAN for communication.</li> <li> <p>Features:</p> <ul> <li>Provides stable IP addresses for devices.</li> <li>Enables consistent communication across the network.</li> </ul> </li> <li> <p>Computer with Linux OS (Computing Center and Control Terminal)</p> </li> <li>Role: Acts as the central computing tower and the portable control panel.<ul> <li>Running the GUI control panel (represents portable control panel).</li> <li>Managing mapping, exploration, and cleaning processes.</li> <li>Collecting and visualizing data in real-time.</li> </ul> </li> </ul>"},{"location":"reports/2024/CleaningRobot/#essential-algorithms-self-designed","title":"Essential Algorithms (Self Designed)","text":"<p>This part introduces the essential algorithms and techniques built by ourselves.</p>"},{"location":"reports/2024/CleaningRobot/#core-algorithms-in-route-plan","title":"Core Algorithms in Route Plan","text":"<p>The route planning in cleaning module II is implemented at a low level and without relying on move_base or other integrated ROS packages. This approach provides more flexibility and control over the cleaning coverage path. The main processing logic is:</p> <ol> <li>Map Preprocessing (Dilation Algorithm)</li> <li>Generate Sampling Points (Uniform Sampling)</li> <li>Find Valid Connections (Bresenham's Algorithm)</li> <li>Generate Complete Path (Greedy Algorithm)</li> <li>Visualization &amp; Save (Arrow Path Visualization)</li> </ol> <p>Note: The codes here are simplified and hide some lines to focus on algorithm ideas.</p>"},{"location":"reports/2024/CleaningRobot/#1-dilation-algorithm","title":"&lt;1&gt; Dilation Algorithm","text":"<ul> <li>Use the dilation algorithm to expand the obstacle area.</li> <li>Calculate the safety distance based on the robot radius.</li> <li>Convert the map into a three-value representation: -1 (obstacle), 0 (not visited), 1 (visited).</li> </ul> <pre><code>def preprocess_map(self, map_array):\n    processed_map = np.full_like(map_array, MapData.OBSTACLE, dtype=np.int8)\n\n    # Set feasible area\n    processed_map[map_array == 0] = MapData.UNVISITED\n\n    # Expansion processing considering safety distance\n    safety_kernel_size = int(2.0 * self.robot_radius / self.grid_resolution)\n    safety_kernel = np.ones((safety_kernel_size, safety_kernel_size), np.uint8)\n\n    # Inflate obstacles and mark safe areas\n    obstacle_map = (processed_map == MapData.OBSTACLE).astype(np.uint8)\n    dilated_obstacles = cv2.dilate(obstacle_map, safety_kernel, iterations=1)\n    processed_map[dilated_obstacles == 1] = MapData.OBSTACLE\n</code></pre>"},{"location":"reports/2024/CleaningRobot/#2-uniform-sampling","title":"&lt;2&gt; Uniform Sampling","text":"<ul> <li>Use a fixed interval to uniformly sample the feasible area.</li> <li>Convert the sampling point coordinates from grid coordinates to world coordinates.</li> <li>Assign a unique ID and status to each sampling point.</li> </ul> <pre><code>def generate_path_points(self, processed_map):\n    path_points = []\n    height, width = processed_map.shape\n    point_id = 0\n\n    # Generate waypoints using sampling interval\n    for x in range(0, width, self.SAMPLING_INTERVAL):\n        for y in range(0, height, self.SAMPLING_INTERVAL):\n            if processed_map[y, x] == MapData.UNVISITED:\n                world_x, world_y = self.grid_to_world(x, y)\n                path_points.append({\n                    'id': point_id,\n                    'grid_x': x, 'grid_y': y,\n                    'world_x': world_x, 'world_y': world_y,\n                    'status': MapData.UNVISITED\n                })\n                point_id += 1\n</code></pre>"},{"location":"reports/2024/CleaningRobot/#3-bresenhams-algorithm","title":"&lt;3&gt; Bresenham's Algorithm","text":"<ul> <li>Use Bresenham algorithm to generate all grid points between two points.</li> <li>Determine pixel position by error accumulation.</li> <li>Only use integer operations to improve efficiency.</li> </ul> <pre><code>def get_line_points(self, x1, y1, x2, y2):\n    points = []\n    dx = abs(x2 - x1)\n    dy = abs(y2 - y1)\n    x, y = x1, y1\n    sx = 1 if x1 &lt; x2 else -1\n    sy = 1 if y1 &lt; y2 else -1\n\n    if dx &gt; dy:\n        err = dx / 2.0\n        while x != x2:\n            points.append((x, y))\n            err -= dy\n            if err &lt; 0:\n                y += sy\n                err += dx\n            x += sx\n</code></pre>"},{"location":"reports/2024/CleaningRobot/#4-greedy-algorithm","title":"&lt;4&gt; Greedy Algorithm","text":"<ul> <li>Use a greedy strategy to select the next visit point.</li> <li>Prioritize the nearest unvisited point.</li> <li>Ensure that the path does not pass through obstacles.</li> <li>Handle disconnected areas.</li> </ul> <pre><code>def find_valid_connections(self, path_points):\n    connections = []\n    visited = set()\n    current_point = path_points[0]\n    visited.add(0)\n\n    while len(visited) &lt; len(path_points):\n        best_distance = float('inf')\n        best_next_point = None\n\n        for i, point in enumerate(path_points):\n            if i in visited or not self.are_adjacent(current_point, point):\n                continue\n            if self.line_crosses_obstacle(current_point, point):\n                continue\n\n            distance = ((current_point['grid_x'] - point['grid_x']) ** 2 + \n                      (current_point['grid_y'] - point['grid_y']) ** 2) ** 0.5\n\n            if distance &lt; best_distance:\n                best_distance = distance\n                best_next_point = point\n</code></pre>"},{"location":"reports/2024/CleaningRobot/#5-arrow-path-visualization","title":"&lt;5&gt; Arrow Path Visualization","text":"<ul> <li>Use heatmap to display map information.</li> <li>Use vector calculation to generate arrow paths.</li> <li>Use color mapping to display different states.</li> <li>YAML serialization to save path data.</li> </ul> <pre><code>def visualize_plan(self, processed_map, path_points):\n    display_map = np.full_like(processed_map, fill_value=1.0, dtype=float)\n    display_map[processed_map == MapData.OBSTACLE] = 0.8\n\n    plt.imshow(display_map, cmap='gray')\n\n    for point in path_points:\n        plt.scatter(point['grid_x'], point['grid_y'], c='red', s=30)\n\n    connections = self.find_valid_connections(path_points)\n    for point1, point2 in connections:\n        plt.arrow(point1['grid_x'], point1['grid_y'],\n                 point2['grid_x'] - point1['grid_x'],\n                 point2['grid_y'] - point1['grid_y'],\n                 head_width=2, head_length=2, fc='blue', ec='blue', alpha=0.5)\n</code></pre>"},{"location":"reports/2024/CleaningRobot/#core-algorithms-in-route-follow","title":"Core Algorithms in Route Follow","text":"<p>Process Flow Initialization \u2192 Load Path Points \u2192 Start Route Following Loop:</p> <ol> <li> <p>Check localization accuracy</p> </li> <li> <p>Find next target point:</p> </li> </ol> <p>Priority: follow planned route order</p> <p>Fallback: find nearest accessible point</p> <ol> <li> <p>Check path safety (obstacle avoidance)</p> </li> <li> <p>Move to target point:</p> </li> </ol> <p>First rotate to target orientation</p> <p>Then move in straight line</p> <ol> <li> <p>Update point status (visited/obstacle)</p> </li> <li> <p>Repeat loop until all points are completed</p> </li> </ol> <p>Special Features</p> <ol> <li>State Machine</li> </ol> <p>Uses enumerated states (UNVISITED/VISITED/OBSTACLE) for point tracking</p> <p>Enables systematic progress monitoring and recovery</p> <ol> <li>Visualization Capabilities</li> </ol> <p>Real-time display of point status and path connections</p> <p>Color-coded visualization for different point states</p> <p>Path visualization with directional indicators</p> <ol> <li>Real-time Safety Monitoring</li> </ol> <p>Continuous LiDAR data processing for obstacle detection</p> <p>Dynamic path adjustment for obstacle avoidance</p> <p>Safety distance maintenance for pedestrian protection</p> <p>Configurable safety parameters for different environments</p> <p>Note: The codes here are simplified and hide some lines to focus on algorithm ideas.</p>"},{"location":"reports/2024/CleaningRobot/#1-localization-accuracy-check","title":"&lt;1&gt; Localization Accuracy Check","text":"<p>Verifies AMCL positioning accuracy to ensure reliable navigation.</p> <pre><code>def check_localization_accuracy(self):\n    if not self.current_pose:\n        return False, \"No pose data\"\n    return True, \"Localization accuracy sufficient\"\n</code></pre>"},{"location":"reports/2024/CleaningRobot/#2-target-point-finding","title":"&lt;2&gt; Target Point Finding","text":"<p>Implements hierarchical point selection strategy - first tries planned route, then falls back to nearest accessible point.</p> <pre><code>def get_next_planned_point(self, current_point_id):\n    for start_id, end_id in self.path_connections:\n        if start_id == current_point_id:\n            for point in self.path_points:\n                if point['id'] == end_id and point['status'] == MapData.UNVISITED:\n                    if self.check_path_safety(point) and self.is_point_reachable(point):\n                        return point\n    return None\n</code></pre>"},{"location":"reports/2024/CleaningRobot/#3-path-safety-verification","title":"&lt;3&gt; Path Safety Verification","text":"<p>Uses LiDAR data to validate path safety by checking obstacle proximity.</p> <pre><code>def check_path_safety(self, target_point):\n    path_start = np.array([self.current_pose[0], self.current_pose[1]])\n    path_end = np.array([target_point['world_x'], target_point['world_y']])\n    path_vector = path_end - path_start\n\n    danger_points = 0\n    for obs_point in self.obstacle_points:\n        obs_vector = np.array(obs_point) - path_start\n        projection = np.dot(obs_vector, path_vector) / np.linalg.norm(path_vector)\n        if 0 &lt;= projection &lt;= np.linalg.norm(path_vector):\n            distance = abs(np.cross(path_vector, obs_vector)) / np.linalg.norm(path_vector)\n            if distance &lt; self.SAFETY_DISTANCE:\n                danger_points += 1\n                if danger_points &gt; 5:\n                    return False\n    return True\n</code></pre>"},{"location":"reports/2024/CleaningRobot/#4-reachability-check","title":"&lt;4&gt; Reachability Check","text":"<p>Determines if target point is within maximum reachable distance.</p> <pre><code>def is_point_reachable(self, point):\n    distance = math.hypot(\n        point['world_x'] - self.current_pose[0],\n        point['world_y'] - self.current_pose[1]\n    )\n    return distance &lt; self.MAX_REACHABLE_DISTANCE\n</code></pre>"},{"location":"reports/2024/CleaningRobot/#5-movement-control","title":"&lt;5&gt; Movement Control","text":"<p>Implements two-phase movement: rotation alignment followed by linear motion.</p> <pre><code>def move_to_point(self, target_point):\n    while not rospy.is_shutdown():\n        dx = target_point['world_x'] - self.current_pose[0]\n        dy = target_point['world_y'] - self.current_pose[1]\n        distance = math.hypot(dx, dy)\n        target_angle = math.atan2(dy, dx)\n\n        if distance &lt; self.POSITION_TOLERANCE:\n            self.stop_robot()\n            return True\n\n        angle_diff = target_angle - self.current_pose[2]\n        if abs(angle_diff) &gt; self.ANGLE_TOLERANCE:\n            cmd_vel.angular.z = self.ANGULAR_SPEED if angle_diff &gt; 0 else -self.ANGULAR_SPEED\n        else:\n            cmd_vel.linear.x = min(self.LINEAR_SPEED, distance)\n            cmd_vel.angular.z = 0.5 * angle_diff\n</code></pre>"},{"location":"reports/2024/CleaningRobot/#essential-modules","title":"Essential Modules","text":"<p>Module Structures: </p>"},{"location":"reports/2024/CleaningRobot/#1-panel-module","title":"1. Panel Module","text":"<ul> <li>Developer: Pang Liu</li> <li>Description:</li> <li>Self-designed GUI for controlling all program modules.</li> <li>Provides buttons and voice control integration for seamless operation.</li> <li>Key Features:</li> <li>Start SLAM, exploration, and cleaning processes.</li> <li>Save and load maps.</li> <li>Route analysis and visualization in RViz.</li> <li>Robot movement control.</li> <li>Developer-friendly logs for debugging.</li> <li>Real running illustration:</li> <li></li> </ul> <p>Control Panel Buttons and Functions</p> <ul> <li>Build Map: Launches SLAM and RViz.</li> <li>Start/Stop Exploration: Begins or halts autonomous exploration.</li> <li>Save Map: Saves the current map to the <code>/maps</code> directory.</li> <li>Analyze Route: Uses <code>route_plan.py</code> to plan paths based on the saved map.</li> <li>Show Route: Visualizes the planned route in RViz.</li> <li>Start Cleaning: Executes the cleaning routine (based on the selected cleaning module).</li> <li>Robot Control: Allows manual control of the robot via <code>/cmd_vel</code>.</li> <li>Quit Program: Shuts down the system.</li> </ul>"},{"location":"reports/2024/CleaningRobot/#2-voice-control-module","title":"2. Voice Control Module","text":"<ul> <li>Developer: Pang Liu</li> <li>Description:</li> <li>Real-time voice recognition using the Vosk model.</li> <li>Publishes recognized commands to the <code>voice_commands</code> topic.</li> <li>Enables voice-activated control of exploration and cleaning.</li> <li></li> </ul>"},{"location":"reports/2024/CleaningRobot/#3-mapping-module","title":"3. Mapping Module","text":"<ul> <li>Developer: Zhenxu Chen</li> <li> <p>Description:</p> </li> <li> <p>Based on the Explore_Lite package, customized for fast exploration and map saving.</p> </li> <li> <p>Workflow:</p> </li> <li> <p>Start SLAM: Launches <code>turtlebot3_slam.launch</code> for SLAM and RViz.</p> </li> <li> <p></p> </li> <li> <p>Start Exploration: Begins autonomous exploration using <code>explore.launch</code>.</p> </li> <li> <p></p> </li> <li> <p>Visualization Markers:</p> <ul> <li>\ud83d\udd35 Blue Points (Frontier Exploration Markers)</li> <li>Technical Meaning: Valid frontier points indicating unexplored boundaries</li> <li>Simple Description: These points show the boundary between mapped and unmapped areas - like a border between known and unknown territory on a map. They are the potential areas for the robot to explore next.</li> <li>\ud83d\udd34 Red Points (Frontier Exploration Markers, Not showned on demo)</li> <li>Technical Meaning: Blacklisted frontier points that failed exploration attempts</li> <li>Simple Description: These are \"no-go\" areas that the robot tried to reach before but couldn't. Think of them like marking an X on a map where there might be obstacles or unreachable spots.</li> <li>\ud83d\udfe2 Green Spheres (Frontier Exploration Markers)</li> <li>Technical Meaning: Initial points of frontiers, with sphere size inversely proportional to frontier cost</li> <li>Simple Description: These balls mark the starting points of unexplored areas. The bigger the ball, the more interesting that area is for exploration - like highlighting the most promising spots on a treasure map.</li> <li>\ud83d\udfe3 Pink Path (Path Planning Markers, 0.05 width)</li> <li>Technical Meaning: Global plan from DWAPlannerROS (/move_base/DWAPlannerROS/global_plan)</li> <li>Simple Description: This is like the overall route plan on a GPS - it shows the complete path the robot plans to take from its current location to its destination.</li> <li>\ud83d\udc9b Yellow Path (Path Planning Markers, 0.03 width)</li> <li>Technical Meaning: Local plan from DWAPlannerROS (/move_base/DWAPlannerROS/local_plan)</li> <li>Simple Description: This is like watching your next few steps carefully - it shows the immediate path the robot plans to take while paying attention to nearby obstacles and adjusting its movement.</li> <li></li> <li></li> </ul> </li> <li> <p>Save Map: Saves the map as <code>.pgm</code> and <code>.yaml</code> files in the <code>/maps</code> directory.</p> </li> <li> <p></p> </li> <li> <p>Finish Mapping: Stops SLAM and exploration nodes.</p> </li> <li> <p></p> </li> </ul>"},{"location":"reports/2024/CleaningRobot/#4-1-cleaning-module-i","title":"4-1. Cleaning Module I","text":"<ul> <li>Developer: Zhenxu Chen</li> <li>Description:</li> <li>Based on the CCPP package for full-coverage path planning and cleaning.</li> <li>Utilizes <code>move_base</code> for navigation.</li> <li>Note: <code>&lt;Cleaning Module I&gt;</code> is developed at branch <code>backup</code></li> <li>CCPP Package: https://wiki.ros.org/full_coverage_path_planner</li> <li>The CCPP package will use saved map to plan a full coverage route and allow the robot following the route.</li> <li>Video: https://drive.google.com/file/d/1F1Hh0JKD9KMvRVsC_EX5ZwptzUVWLEi8/view?usp=drive_link</li> <li></li> </ul>"},{"location":"reports/2024/CleaningRobot/#4-2-cleaning-module-ii","title":"4-2. Cleaning Module II","text":"<ul> <li>Developer: Pang Liu</li> <li>Description:</li> <li>Fully self-designed cleaning functionality split into two submodules:<ul> <li>Route Analysis Submodule:</li> <li>Reads saved maps and analyzes routes using a three-value map (-1 for obstacles, 0 for uncleaned areas, 1 for cleaned areas).</li> <li>Plans paths using sampling intervals and a greedy algorithm to find valid connections.</li> <li>Route Follow Submodule:</li> <li>Executes the planned path, marking cleaned areas in real-time (still under debugging).</li> </ul> </li> </ul>"},{"location":"reports/2024/CleaningRobot/#sub-module-i-route-analysis","title":"Sub Module I: Route Analysis","text":"<ul> <li> <p>Detailed introduction of <code>route_plan.py</code> (core script):</p> </li> <li> <p>Get the latest map (map data of <code>OccupancyGrid</code> message type) through <code>/map</code> topic.</p> </li> <li> <p></p> </li> <li> <p>Convert <code>OccupancyGrid</code> data to a grid map represented by a NumPy array.</p> </li> <li> <p>Perform obstacle expansion on the map (taking into account the safety distance of the robot).</p> </li> <li> <p></p> </li> <li> <p>Generate a three-value map: <code>-1</code>, <code>0</code>, and <code>1</code> are used to represent obstacles, unvisited areas, and visited areas respectively.</p> </li> <li>Generate path points in the map through a fixed sampling interval. Each path point includes world coordinates and grid coordinates.</li> <li> <p>Use greedy algorithm to find valid connections between path points and check whether there are obstacles between two points.</p> </li> <li> <p></p> </li> <li> <p>After the connection is completed:</p> </li> <li> <p>Use <code>matplotlib</code> to draw the path points and connected line segments and save them as an image.</p> </li> <li>The logic of finding valid connections:<ul> <li>Each path point can only be connected to the path points adjacent to it.</li> <li>Definition of connection: up, down, left, and right.</li> <li>Isolated path points are not considered in the connection.</li> </ul> </li> <li>Use RViz and route_show (button [Show Route]) to see the points and route:</li> <li></li> <li></li> </ul>"},{"location":"reports/2024/CleaningRobot/#sub-module-ii-route-follow","title":"Sub Module II: Route Follow","text":"<ul> <li> <p>Main Logic</p> </li> <li> <p>(1) Follow the route based on route_plan analyzed</p> </li> <li>(2) When reach a red point, that point will turn to green</li> <li>(3) If the robot found the red point is not reachable, might be a wall, might be a moving obstacle, then the point will turn to black.</li> <li> <p>The full logic of <code>route_follow.py</code>:</p> </li> <li> <p></p> </li> <li> <p>black point demo:</p> </li> <li> <p></p> </li> <li> <p>red point turn to green point demo:</p> </li> <li> <p></p> </li> <li></li> </ul>"},{"location":"reports/2024/CleaningRobot/#codes-and-files-explanations","title":"Codes and Files Explanations","text":""},{"location":"reports/2024/CleaningRobot/#directory-structures","title":"Directory Structures","text":"<p> Note: The Cleaning Module I (CCPP) is not in branch <code>master</code>, please see its implementation in branch <code>backup</code>. For this project final demonstration, we mainly use our self designed Cleaning Module II.</p>"},{"location":"reports/2024/CleaningRobot/#directory-and-scripts-table","title":"Directory and Scripts Table","text":"Folder/File Description Topics and Messages maps/ Store map files built by mapping module src/ Main source code directory \u251c\u2500\u2500 cleaning_bot/ Explore lite package files \u251c\u2500\u2500 control_panel/ Control panel directory \u2502   \u251c\u2500\u2500 launch/ Launch file directory \u2502   \u2502   \u251c\u2500\u2500 panel_real.launch Program entrance in real environment \u2502   \u2502   \u251c\u2500\u2500 panel_sim.launch Program entrance in simulation \u2502   \u2514\u2500\u2500 src/ \u2502   \u2502   \u251c\u2500\u2500 control_panel_gui.py Main control panel program scripts Subscribe: /voice_commands \u2502   \u2502   \u2502 Publish: /cmd_vel \u2502   \u2502   \u2502 Publish: /control_move_base \u2502   \u2502   \u2502 Publish: /control_explore \u2502   \u2502   \u2502 Publish: /control_gmapping \u2502   \u2502   \u2502 Publish: /control_amcl \u2502   \u2502   \u2502 Publish: /control_route_show \u2502   \u2502   \u2502 Publish: /control_route_follow \u2502   \u2502   \u251c\u2500\u2500 voice_vosk.py Voice recognition program Subscribe: control commands \u2502   \u2502   \u2502 Publish: /voice_commands \u2502   \u2502   \u251c\u2500\u2500 SLAM_controller.py SLAM controller Subscribe: /control_gmapping \u2502   \u2502   \u251c\u2500\u2500 AMCL_controller.py AMCL controller Subscribe: /control_amcl \u2502   \u2502   \u251c\u2500\u2500 move_base_controller.py Move base controller Subscribe: /control_move_base_controller \u2502   \u2502   \u251c\u2500\u2500 explore_controller.py Exploration controller Subscribe: /control_explore \u2502   \u2502   \u251c\u2500\u2500 controller_route_show.py Route display controller Subscribe: /control_route_show \u2502   \u2502   \u251c\u2500\u2500 controller_route_follow.py Route following controller Subscribe: /control_route_follow \u2502   \u2502   \u2514\u2500\u2500 vosk-model-small-en-us/ Voice recognition model files \u251c\u2500\u2500 simulation_world/ Simulation environment directory \u251c\u2500\u2500 sweep/ Core cleaning directory \u2502   \u251c\u2500\u2500 config/ RViz configuration files \u2502   \u251c\u2500\u2500 debug/ Processed map files \u2502   \u251c\u2500\u2500 pathfiles/ Path information files \u2502   \u2514\u2500\u2500 src/ \u2502   \u2502   \u251c\u2500\u2500 route_map_server.py Supporting files for following scripts \u2502   \u2502   \u251c\u2500\u2500 route_plan.py Route planner Subscribe: /map topic \u2502   \u2502   \u251c\u2500\u2500 route_show.py Route visualization Publish: /path_visualization \u2502   \u2502   \u2502 Publish: /path_connections \u2502   \u2502   \u2514\u2500\u2500 route_follow.py Route following program Subscribe: /amcl_pose \u2502   \u2502   \u2502 Subscribe: /scan \u2502   \u2502   \u2502 Publish: /cmd_vel \u2502   \u2502   \u2502 Publish: /route_status \u2502   \u2502   \u2502 Publish: /route_status_visualization \u2502   \u2502   \u2502 Publish: /current_path \u2514\u2500\u2500 turtlebot3/ Turtlebot3 core files"},{"location":"reports/2024/CleaningRobot/#story-behinds-the-project","title":"Story behinds the project","text":""},{"location":"reports/2024/CleaningRobot/#how-our-team-was-formed","title":"How Our Team Was Formed","text":"<p>At the beginning, we worked on our own projects separately. Pang was working on a project for a robotic guide dog, while Zhenxu focused on a floor-cleaning robot. Pang's project aimed to enable a robot to move indoors, avoid obstacles, guide visually impaired people to exits, and be controlled by voice commands. On the other hand, Zhenxu's project focused on autonomous map creation for the robot.</p> <p>After the initial project setup, we had an in-depth discussion and quickly realized we could combine the features of both projects. We decided to create an autonomous floor-cleaning robot that integrates voice control and automatic mapping. Before the second project update, we worked together to create a proposal for the autonomous cleaning robot.</p> <p>The initiative to undertake a cleaning robot project comes from a keen interest in delving deeper into ROS technologies. Cleaning robots encompass advanced concepts like SLAM, navigation, and control. Initially, we observed that SLAM typically requires manual control to let the robot move under commands. However, expecting users to manually control the robot is impractical. After researching various materials, we decided to implement frontier-based mapping to achieve autonomous mapping. The way to do so is to integrate the explore_lite package into our project.</p> <p>For the sweeping phase, after saving the generated map, we initially considered leveraging existing packages such as full-coverage path planning. However, we ultimately chose to develop our own module instead of relying on pre-built solutions. This decision was based on the recognition that the sweeping module is the core component of our project, and building it ourselves aligns with our goals for innovation and mastery.</p> <p>Our project progressed smoothly, largely because we invested in our own TurtleBot3, which minimized external interference from other groups. Throughout the development process, team members collaborated effectively, analyzing challenges and debugging together to ensure steady progress.</p>"},{"location":"reports/2024/CleaningRobot/#assessments","title":"Assessments","text":"<p>Participating in this cleaning robot project has been a valuable learning experience and a rewarding challenge. We have gained deeper insights into advanced ROS technologies, such as SLAM, autonomous navigation, and control. This project pushed us to explore the intricacies of frontier-based mapping and the design of a custom sweeping module, alongside integrating an acoustic recognition module and UI design &amp; implementation.</p> <p>Developing the sweeping module from scratch was both challenging and fulfilling. It required us to think critically about the way to realize an effective algorithm to plan the path that enables the robot to cover all the explored areas, as well as how to integrate various components like the voice control module seamlessly. This hands-on experience improved our ability to analyze requirements, break down complex tasks, and implement practical solutions. Teamwork played a crucial role in our success. Our group demonstrated excellent collaboration and communication, which helped us overcome obstacles and refine our approach. Overall, it is a very successful project.</p>"},{"location":"reports/2024/CleaningRobot/#chanllenges-and-problem-solving","title":"Chanllenges and Problem Solving","text":"<p>Our challenges mainly came from two aspects: integrating and adjusting existing packages, and designing custom code. The first type of challenge arose from the \"black box\" nature of many existing packages, which often caused unexpected conflicts. Fine-tuning the parameters was also particularly demanding and consumed a significant amount of time. The second type of challenge came from designing code almost entirely from scratch, which felt like building a house brick by brick on our own.</p> <p>The primary challenge in integrating frontier-based autonomous mapping lies in tuning the numerous parameters associated with local and global path planners, move_base, SLAM, and exploration modules. The huge volume of these parameters makes it time-consuming to achieve satisfactory results in Gazebo simulations. However, even after extensive tuning, the outcomes in the real-world environment often fall short of expectations.</p> <p>To address this, we plan to use teleop commands during the real robot demonstration to assist with mapping, particularly in situations where the robot becomes stuck during the autonomous mapping process. This approach ensures smoother operation while we continue refining the autonomous functionality.</p> <p>The biggest challenge in coding stage came from designing the autonomous algorithm for the floor-cleaning robot. From planning the program flow to writing pseudocode, implementing the final code, integrating it, and debugging, we faced numerous difficulties. Transitioning from simulation to the real robot also presented significant challenges.</p> <p>One issue worth highlighting involved problems that didn\u2019t appear in simulation but arose in the real-world application. Since we built our own robot and used our custom Ubuntu system instead of the school\u2019s VNC setup, we encountered various unexpected connection errors. Eventually, we identified the root cause as a time synchronization issue. We used a standalone router to connect the computer and the robot, but the Raspberry Pi 4B lacks an RTC (Real-Time Clock) module, so the system time didn\u2019t update after booting. To resolve this, I configured the computer as a time synchronization server and set up a time synchronization mechanism on the TurtleBot3\u2019s system.</p> <p>Another issue involved the LiDAR. During simulation testing, the robot performed flawlessly. However, in the real-world setup, it couldn\u2019t follow the path to clean effectively. We eventually discovered that the LiDAR data contained many points with a value of 0, which weren\u2019t handled properly. Since all the code for the cleaning module II was custom-designed without relying on existing packages like move_base or gmapping, we encountered these fundamental but critical problems.</p> <p>The turtlebot3 we built: </p>"},{"location":"reports/2024/CleaningRobot/#relevant-literature-and-references","title":"Relevant Literature and References","text":"<p>Related research, tutorial and reference for algorithms:</p> <ul> <li>Frontier Based Exploration: chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://arxiv.org/pdf/1806.03581</li> <li>Dilation Algorithm: https://homepages.inf.ed.ac.uk/rbf/HIPR2/dilate.htm</li> <li>Greedy Algorithm: https://en.wikipedia.org/wiki/Greedy_algorithm</li> <li>Bresenham's Line Algorithm: https://en.wikipedia.org/wiki/Bresenham%27s_line_algorithm</li> <li>Mapping, localization and planning: chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://gaoyichao.com/Xiaotu/resource/refs/PR.MIT.en.pdf</li> </ul> <p>Technical reference for integration and structures:</p> <ul> <li>ROS Frontier Exploration: https://wiki.ros.org/frontier_exploration</li> <li>ROS Explore Lite: https://wiki.ros.org/explore_lite</li> <li>VOSK: https://alphacephei.com/vosk/</li> <li>ROS Full Coverage Path Planning: https://wiki.ros.org/full_coverage_path_planner</li> <li>Turtlebot3: https://emanual.robotis.com/</li> <li>OpenCV: https://opencv.org/</li> </ul>"},{"location":"reports/2024/CleaningRobot/#future-plans","title":"Future Plans","text":"<ul> <li>Optimize the Route analyze and Route Follow submodule.</li> <li>Open-source the project to foster collaboration on smart cleaning robot innovations.</li> <li>Create a tutorial for building autonomous cleaning robots step-by-step.</li> <li>Expand the frontier exploration module with self-designed algorithms.</li> </ul>"},{"location":"reports/2024/CleaningRobot/#special-thanks","title":"Special Thanks","text":"<p>I would like to express my heartfelt gratitude to Professor Pito Salas and the Course 119 team and classmates for introducing me to the field of robotics and helping me rediscover my passion from earlier years. Professor Salas has been incredibly dedicated to supporting students in learning practical skills, going above and beyond to create a rich learning environment. His efforts include inviting industry professionals to share insights on the latest developments in the field.</p> <p>119 course has rich resources on helping us explore ROS and other things. Here\u2019s one of my favorite picture captured from the course:</p> <p></p>"},{"location":"reports/2024/DungeonAndRobot/","title":"Dungeons and Robot","text":"<p>Yutian (Tim) Fan: yutianfan@brandeis.edu</p>"},{"location":"reports/2024/DungeonAndRobot/#introduction","title":"Introduction","text":""},{"location":"reports/2024/DungeonAndRobot/#problem-statement","title":"Problem Statement","text":"<p>The objective of this project is to enable a robot to autonomously map an unknown environment in real time without relying on a pre-existing map. While mapping the environment dynamically, the robot must also explore it autonomously, identifying and interacting with specific elements like frontiers and loot objects. This necessitated the use of SLAM (Simultaneous Localization and Mapping) combined with frontier exploration.</p>"},{"location":"reports/2024/DungeonAndRobot/#relevant-literature","title":"Relevant Literature","text":"<p>The concept of frontier exploration and its implementation were sourced from Kai Nakamura's project on autonomous mapping robots, as detailed on the webpage: https://kainakamura.com/project/slam-robot. Nakamura\u2019s explanation of frontier exploration and associated code served as the foundation for integrating autonomous exploration in this project.</p>"},{"location":"reports/2024/DungeonAndRobot/#what-was-created","title":"What Was Created","text":""},{"location":"reports/2024/DungeonAndRobot/#1-technical-descriptions-and-illustrations","title":"1. Technical Descriptions and Illustrations","text":""},{"location":"reports/2024/DungeonAndRobot/#slam-simultaneous-localization-and-mapping","title":"SLAM (Simultaneous Localization and Mapping)","text":"<p> Figure 1: SLAM process visualized, with the robot dynamically mapping its environment as it explores.</p> <p>This project utilizes SLAM, specifically the GMapping package, which enables the robot to self-locate and construct a map of its environment in real time. The robot employs LIDAR to measure distances to surrounding objects, allowing it to infer its location relative to a global reference frame while simultaneously updating a map of its environment. This dynamic process ensures the robot can explore and navigate unknown environments efficiently without requiring a pre-existing map.</p>"},{"location":"reports/2024/DungeonAndRobot/#2-discussion-of-interesting-algorithms-modules-techniques","title":"2. Discussion of Interesting Algorithms, Modules, Techniques","text":"<p>Figure 2: The occupancy grid map generated by SLAM, showing explored, unexplored, and obstacle areas.</p> <ol> <li>Frontier Exploration and Path Planning</li> <li>Frontier exploration identifies unexplored regions on the map and determines optimal paths to explore them.</li> <li> <p>The project employs a costmap-based path planner combined with the A* algorithm for generating efficient paths. The pure pursuit algorithm ensures smooth navigation along these paths by using a look-ahead point further in the path to reduce oscillation.</p> </li> <li> <p>Object Mapping with Camera and LIDAR Integration</p> </li> <li>The camera processes images to detect objects marked with deep blue tape.</li> <li>LIDAR measurements provide depth information to accurately position these objects in the map frame.</li> <li> <p>Fiducials (tags for marking points of interest) are detected and pinned in the map with their TFs continuously broadcasted. These techniques are inspired by the Fiducial Navigation PA and Line Following PA, integrated into this project for object detection and mapping.</p> </li> <li> <p>Exploration and Exit Strategy</p> </li> <li>The robot alternates between exploring frontiers and collecting mapped loot.</li> <li>When sufficient loot is collected or no frontiers remain, the robot navigates to an exit fiducial to complete the task.</li> </ol>"},{"location":"reports/2024/DungeonAndRobot/#3-guide-on-how-to-use-the-code","title":"3. Guide on How to Use the Code","text":"<p>Follow these steps to set up and run the project:</p> <ol> <li> <p>Launch Gazebo Simulation (for simulation only):    Start the Gazebo simulation environment with the TurtleBot3 and the pre-configured world:    <pre><code>roslaunch dungeon_explorer turtlebot3_world.launch\n</code></pre>    For real robots, instead of launching Gazebo, bring up your robot accordingly.</p> </li> <li> <p>Start SLAM:    Launch the GMapping package to begin SLAM. This initializes the <code>/map</code> topic, enabling the robot to self-localize and dynamically update the map:    <pre><code>roslaunch turtlebot3_slam turtlebot3_gmapping.launch\n</code></pre></p> </li> <li>Launch RViz:    Start RViz for visualization. It is crucial to manually launch RViz instead of using pre-configured launch files to avoid algorithm conflicts:    <pre><code>rviz\n</code></pre>    In RViz:</li> <li>Add the map under <code>/map</code>.</li> <li>Add TF and select only the <code>map</code> and <code>base_link</code> frames.</li> <li> <p>Note that new TFs for loot and fiducials will be added automatically as they are broadcasted during the exploration process.</p> </li> <li> <p>Launch Fiducial Detection and Mapping:    Start the fiducial detection node to map fiducials and loot objects. This node will continuously broadcast TF frames for detected objects, making them accessible for path planning:    <pre><code>roslaunch dungeon_explorer fiducials_real.launch\n</code></pre></p> </li> <li>Start Pure Pursuit:    Run the pure pursuit script to enable path-following functionality. This node listens for paths generated by the exploration loop and executes them to guide the robot along the planned trajectories:    <pre><code>rosrun dungeon_explorer pure_pursuit.py\n</code></pre></li> <li>Start the Frontier Exploration Loop:    Finally, initiate the high-level exploration loop. This script manages the entire exploration process, including loot searching, frontier exploration, and path planning. The loop runs until the robot finishes exploring or finds the exit:    <pre><code>rosrun dungeon_explorer frontier_exploration.py\n</code></pre></li> </ol>"},{"location":"reports/2024/DungeonAndRobot/#4-names-and-purposes-of-components","title":"4. Names and Purposes of Components","text":""},{"location":"reports/2024/DungeonAndRobot/#python-scripts","title":"Python Scripts","text":"Script Name Purpose <code>frontier_exploration.py</code> Contains the high-level exploration loop, including frontier exploration, loot searching, and path generation. <code>mapper.py</code> Detects fiducials and objects, maps their positions, and broadcasts their TF frames. <code>path_planner.py</code> Provides path planning utilities, including A* algorithm and C-space calculation. <code>pure_pursuit.py</code> Implements the pure pursuit algorithm for smooth path following by the robot. <code>my_odom.py</code> Adjusts and customizes odometry data for internal use. <code>frontier_search.py</code> Searches for unexplored regions (frontiers) in the map and prioritizes them for exploration."},{"location":"reports/2024/DungeonAndRobot/#topics-and-their-messages","title":"Topics and Their Messages","text":"Topic Name Message Type Purpose <code>/pure_pursuit/paths</code> <code>dungeon_explorer/PathList</code> Sends the list of paths for the pure pursuit node to execute. <code>/cmd_vel</code> <code>geometry_msgs/Twist</code> Sends velocity commands to control the robot's movement. <code>/finished_paths</code> <code>std_msgs/Bool</code> Indicates whether all paths have been completed. <code>/odom</code> <code>nav_msgs/Odometry</code> Provides odometry data for robot localization. <code>/map</code> <code>nav_msgs/OccupancyGrid</code> Provides the current map for path planning and frontier exploration. <code>/mapper/max_id</code> <code>std_msgs/Int32</code> Publishes the ID of the most recently detected object. <code>/pure_pursuit/enabled</code> <code>std_msgs/Bool</code> Enables or disables the pure pursuit functionality. <code>/pure_pursuit/lookahead</code> <code>geometry_msgs/PointStamped</code> Publishes the lookahead point for the pure pursuit algorithm."},{"location":"reports/2024/DungeonAndRobot/#custom-messages","title":"Custom Messages","text":"Message Name Fields Purpose <code>Frontier.msg</code> <code>geometry_msgs/Point centroid</code>, <code>float32 size</code> Represents a single frontier detected during exploration. <code>FrontierList.msg</code> <code>dungeon_explorer/Frontier[] frontiers</code> Contains a list of detected frontiers for exploration. <code>PathList.msg</code> <code>nav_msgs/Path[] paths</code> Represents a list of paths for the pure pursuit node."},{"location":"reports/2024/DungeonAndRobot/#3-story-of-the-project","title":"3. Story of the Project","text":"<p>This project started with a very ambitious idea. I wanted to create a game-like experience where a human could design a dungeon for a robot to explore. The robot would use SLAM to discover the map step by step, similar to how a player uncovers a map in a video game. My original plan included two types of objects: monsters and loot. The robot would recognize these objects using deep learning-based computer vision. A robot arm would pick up loot and knock over monsters, and the robot would eventually find its way to an exit fiducial.</p> <p>But being a solo project, I had to scale things down quickly. Time was limited, especially with much of my semester dedicated to graduate school applications. The first compromise was to drop deep learning-based computer vision. Training a model wasn\u2019t realistic, and running it on a Raspberry Pi would likely fail. I decided to focus on only one object type instead of distinguishing between monsters and loot. The robot arm idea was also cut, as it wasn\u2019t reliable enough to depend on for the final presentation. Instead, I decided the robot would only navigate to loot objects and treat interaction as reaching their location.</p> <p>Next, I had to refine how the robot would explore. I initially thought of navigating to the centroids of unexplored areas, but I quickly realized this wouldn\u2019t work\u2014unexplored space is effectively infinite. After some research, I found Nakamura's frontier exploration algorithm, which uses boundaries between explored and unexplored areas. This approach fit much better with the occupancy grid provided by SLAM, which represents unknown, free, and occupied spaces as -1, 0, and 100, respectively.</p> <p>Another challenge was object localization. My original idea was to use a depth camera, which would provide straightforward measurements. But I discovered that most depth camera tutorials are for ROS2, and ROS1 options lacked detailed guides. After spending a week troubleshooting, I switched to my backup plan: using a regular camera to align the robot with an object and LIDAR to measure the distance. This allowed me to map objects by combining LIDAR distance with the robot\u2019s position in the map frame. To simplify the setup further, I replaced monsters with tall blocks wrapped in blue tape to represent loot.</p> <p>Once the plan was finalized, I started implementing the project. I learned a lot more about ROS, like how topics, TF buffers, and nodes work together. I also had to explore new concepts like custom messages and building a custom Gazebo simulation. It wasn\u2019t smooth sailing\u2014debugging and fine-tuning took time\u2014but I made steady progress.</p> <p>In the end, the project didn\u2019t always work perfectly due to the robot\u2019s physical limitations, but I\u2019m happy with how far it came. This project taught me the importance of adapting to constraints and coming up with backup plans. It also reinforced how much progress comes from consistent effort\u2014without a solid understanding of earlier assignments, especially the line_follower PA, fiducial_nav PA, and tf tutorials, I wouldn\u2019t have been able to complete this project on my own.</p> <p> Figure 3: Visualization of the robot navigation in the level.</p>"},{"location":"reports/2024/GestureBot/","title":"Introduction:","text":"<p>Body language is an important part of human interaction and communication. Often, we rely on gestures for expression and occasionally instruction. With robotics becoming more prevalent every year and companies aiming towards producing humanoid and companion-like robots, we believed that it would be interesting to see the viability of creating such a robot. Our primary objective was to create the GestureBot, a robot that would read hand gestures and perform instructions accordingly. We aimed to give it both basic and complex instructions, and for it to perform without needing preplanned environments or mapping, so that it could simply be deployed and immediately be in use.</p> <p>Our basic instructions were simple movement commands. However, for complex instructions we aimed to either have the robot follow the user or have the robot memorize object locations and traverse back to them even if led somewhere completely different. Having the robot follow a person turned out to be significantly more difficult, however, when putting obstacles into consideration and especially when other people are near the robot. Therefore, we decided to implement the latter complex instruction instead. Given that this idea was based off of the GOAT (Go to Any Thing) robot shown in class, it will be called mini GOAT for short.</p>"},{"location":"reports/2024/GestureBot/#what-was-created","title":"What was created:","text":""},{"location":"reports/2024/GestureBot/#gestures","title":"Gestures:","text":"<p>The primary feature of our robot is its ability to recognize gestures. Our original idea for implementing this was using object segmentation to find the hand, and then another algorithm for figuring out the gestures: but this would likely use up an enormous amount of processing power. After further research, we found a better solution using Google\u2019s MediaPipe Hand Landmarker tool combined with a custom neural network made with Keras for simplicity. The hand landmarker takes in an image and produces an output of the hand\u2019s \u201ckey points\u201d, which are mainly the joints in each finger and the wrist of the hand. Given that this produces exactly 21 points every time it finds a hand in the image, we could create a neural network that takes in these 21 inputs and produces an output gesture based on hand training data that we could create. </p> <p>Model layers: <pre><code>model = tf.keras.models.Sequential([\n    tf.keras.layers.Input((21 * 2, )),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(20, activation='relu'),\n    tf.keras.layers.Dropout(0.4),\n    tf.keras.layers.Dense(10, activation='relu'),\n    tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')\n])\n</code></pre></p> <p>To obtain our training data, we filled a CSV file with a list of keypoints and a class binded to a key that we would press to add a new datapoint to the file. We did this until we had around 5000 data points. Our neural network\u2019s layers were chosen by trial and error - we began with a neural network that had already been written in an older project written by us for classification and then changed the parameters until it worked consistently. After training the model, it can be used for gesture classification.</p> <p>Logging to CSV: <pre><code>csv_path = /csv_path\nwith open(csv_path, 'a', newline=\"\") as f:\n    writer = csv.writer(f)\n    writer.writerow([number, *landmark_list])\n    print(f\"logged {number}\")\n</code></pre></p>"},{"location":"reports/2024/GestureBot/#mini-goat-command","title":"Mini GOAT command:","text":"<p>The real GOAT\u2019s implementation involved object segmentation and depth estimation to produce an estimation of the object\u2019s position on the map, and then used a system extremely similar to move_base to go to the object. We wanted to implement this from scratch, so we decided to first follow in their footsteps and see where we could go from there.</p> <p>We found that there were three major issues:</p> <ul> <li> <p>Accurate depth estimation is NOT possible using a single camera. </p> <ul> <li>Multiple different real-time monocular depth algorithms were tested with the robot (https://github.com/nianticlabs/monodepth2, https://github.com/Ecalpal/RT-MonoDepth, https://github.com/atapour/monocularDepth-Inference to list a few), and all of them were either too slow or infeasible for the project for various reasons.</li> </ul> </li> <li> <p>Object segmentation does not differentiate between unique objects.</p> <ul> <li>If there were 20 unique books in front of the robot, it will not remember the difference between book 1 and book 20</li> </ul> </li> <li>These algorithms are very expensive.<ul> <li>The object segmentation algorithm takes 1.5 seconds to run.</li> <li>This was the fastest algorithm that we found that had OK accuracy.</li> </ul> </li> </ul> <p>We will address them in order, beginning with our issues with depth estimation. The original GOAT used depth estimation in order to accurately localize the object within its map. This is not possible for us because real-time monocular depth algorithms with metric depth do not exist (without taking 15 seconds to run) despite them claiming they do, such as this one by Apple that claims it can create a depth map in less than one second (many users with high quality GPUs reported they could not recreate the same speed). This poses a problem because we need to find a way to record the pose of the object in order to navigate to it.</p> <p>Our solution to this issue comes from the realization that the object\u2019s pose is almost always going to be near the robot\u2019s pose: and considering that the robot must be facing towards the object for the object segmentation algorithm to work, we can simply record the robot\u2019s pose as the object\u2019s pose. This allows us to travel back to the exact position where the robot sees the object, and then we can have the robot travel directly towards the object until we reach its actual position.</p> <p>This poses another problem: some objects are not within the LiDAR\u2019s range; that is, they may be smaller than the robot or above the robot. In this instance, it will not be possible to use sensor distance as a metric to determine how close the robot is to the object. This is why we must still use the depth estimation algorithm to determine an extremely rough guess of how close the robot is. While it is impossible to obtain an accurate depth reading from a monocular depth algorithm, it is still possible to get a relative reading, where closer objects will have a high value and distant objects will have a low value. Additionally, our object segmentation algorithm provides bounding boxes that indicate where the object is in our camera frame, which allows us to select just the object\u2019s relative depth. In this way we are able to get closer to the object by getting the bounding box of the object, calculating the average of the object\u2019s depth, travelling towards it, and then making sure our relative depth estimate isn\u2019t too high. In the case that the object disappears from view for too long or the depth estimate goes above the threshold, we simply stop the robot because we assume either we are too close or the object is no longer in our view due to elevation or movement.</p> <p>The object\u2019s depth is calculated very simply: we take the median of all depths in the bounding box (the majority should obviously belong to the object) and filter out all depths less than the median. We are likely then left with all of the depths belonging to the object. We then take the mean of all those depths, and are left with our assumed relative distance of the object.</p> <p>Sample Code: <pre><code>box = self.predictions['bboxes'][object_prediction_index]\nx_min = int(box[0])\ny_min = int(box[1])\nx_max = int(box[2])\ny_max = int(box[3])\nsubarray = self.disparity_map[x_min:x_max, y_min:y_max]\nmedian = np.median(subarray)\nmask = subarray &gt;= median\nsubarray = subarray[mask]\ndepth = np.mean(subarray)\n</code></pre></p> <p>Our second issue is, unfortunately, a restriction on the robot\u2019s ability. If we wanted to add unique object mapping to the robot, we would have to add feature recognition in some way to evaluate traits of objects and assign them to some sort of dictionary or tree to \u201crecall\u201d previous unique objects. This would add to the already high computational cost of a depth, classification and segmentation algorithm running at the same time. So unfortunately, it will not be feasible to add this to the robot.</p> <p>Our third issue can only be resolved by decreasing the callback rate of our functions. Limiting the object segmentation algorithm to one call every 1.5 seconds helps in preventing constant expensive operations. Additionally, the algorithm is only ever called when either searching for the object after navigation or when memorizing an object\u2019s position. The depth algorithm is forced to run only when the segmentation algorithm runs, so it does not take up resources. The gesture recognition must be on constantly, but the callback rate can be reduced so that it does not become too expensive. </p> <p>We used rospy.Timer to change the callback rate: <pre><code>rospy.Timer(rospy.Duration(1.5), self.segmentation_callback) #start object recognition callback\nrospy.Timer(rospy.Duration(1.5), self.depth_callback) #start depth estimation callback\n</code></pre></p> <p>Now that these issues have been addressed, we can explain the full scope of the mini GOAT instruction.</p> <p>There are two steps to this process: memorization and navigation. Memorization only occurs when given a specific gesture. Upon recognition, the robot will immediately memorize eight objects within its view with the highest confidence scores, filtering out people, chairs, tables, and other common objects that are not to be remembered. It also filters out objects based on the relative depth estimates, so distant objects will not be relevant to prevent a fire extinguisher that is 20 feet away from being memorized as a bottle. After the filtering process, the remaining objects will become keys in a dictionary and assigned the robot\u2019s pose as a value.</p> <p>For the filtering process, we had to implement Non-Maximum Suppression due to the many overlapping guesses that the segmentation algorithm produced. Non-Maximum Suppression is used to output the best \u201cbounding box\u201d out of a set of overlapping bounding boxes; where bounding box refers to a rectangle that represents the object\u2019s location in the frame. This is done by calculating the Intersection over Union (Area of intersection / Combined Area) of all bounding boxes and taking the highest confidence box that is above the Intersection over Union \u201cthreshold\u201d, which determines if a box is significantly overlapping or not.</p> <p>Sample Code: <pre><code>def non_max_suppression(self, boxes, scores, threshold):\n    order = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)\n    keep = []\n    while order:\n        i = order.pop(0)\n        keep.append(i)\n        for j in order:\n            # iou between the two boxes\n            intersection = max(0, min(boxes[i][2], boxes[j][2]) - max(boxes[i][0], boxes[j][0])) * \n                            max(0, min(boxes[i][3], boxes[j][3]) - max(boxes[i][1], boxes[j][1]))\n            union = (boxes[i][2] - boxes[i][0]) * (boxes[i][3] - boxes[i][1]) + \n                    (boxes[j][2] - boxes[j][0]) * (boxes[j][3] - boxes[j][1]) - intersection\n            iou = intersection / union\n\n            if iou &gt; threshold:\n                order.remove(j)\n    return keep\n</code></pre></p>"},{"location":"reports/2024/GestureBot/#navigation","title":"Navigation","text":"<p>The navigation system combines SLAM mapping and movebase. These components work together to allow a robot to autonomously explore an unknown environment, plan efficient paths, and adapt to changes in real-time. By Implementing Simultaneous Localization and Mapping (SLAM), the robot was able to build and maintain a map of an unknown environment and be able to recognize where it was located within the dynamically made map. Combined together with the built-in amcl within move base using global and local costmaps based on odom and lidar scans, it was able to effectively navigate to saved objects.</p>"},{"location":"reports/2024/GestureBot/#slam-gmapping","title":"SLAM gmapping","text":"<p>In environments where no pre-existing maps are available, the robot needs to simultaneously map the surroundings and localize itself. SLAM solves the \"chicken and egg\" problem: you need a map to localize and localization to create a map. SLAM allows for adaptability in dynamic environments where obstacles or features may change over time.   </p> <p>Purpose: To create a map of the environment while simultaneously determining the robot\u2019s location within it.  Importance: Maps are essential for navigation in unknown environments.  Localization is key to determining the robot's pose relative to obstacles and goals.   </p> <p>How it Works:  The Gmapping node subscribes to sensor data (e.g., laser scans) and odometry data to update the map.  <code>odom_frame</code>: Tracks the robot\u2019s movement relative to the map.  <code>map_update_interval</code>: Adjusts how often the map is updated, balancing accuracy and computation cost.   </p>"},{"location":"reports/2024/GestureBot/#frontier-exploration","title":"Frontier Exploration","text":"<p>While SLAM enables mapping, the robot needs to autonomously decide where to explore next to cover the environment effectively. Frontier exploration identifies regions that are on the boundary between explored and unexplored areas, allowing the robot to focus on expanding the map systematically.  </p> <p>Purpose: Autonomous exploration of an unknown environment.  Importance: Automates navigation to unvisited areas by identifying frontiers which are boundaries between known and unknown regions.  Saves manual effort and accelerates mapping of the environment.  How it Works:  The <code>explore_lite</code> node generates navigation goals to frontiers.  These goals are sent to the <code>move_base</code> node for execution.  </p>"},{"location":"reports/2024/GestureBot/#costmaps","title":"Costmaps","text":"<p>Navigation requires both global and local planning to ensure safety and efficiency: The global costmap is used for long-term planning and navigation across large areas. The local costmap is used for real-time adjustments to avoid dynamic obstacles like people or moving objects.  </p> <p>Costmaps are grid-based representations of the environment, used for path planning and obstacle avoidance.  </p> <p>Global Costmap:  Purpose: Provides a high-level view of the entire environment for long-distance path planning.  global_frame: Uses the map frame to align with SLAM-generated maps.    static_map: Set to false in order to dynamically make the map  Importance: Enables efficient navigation to distant goals while avoiding large obstacles.   </p> <p>Local Costmap:  Purpose: Provides a localized, real-time view for immediate obstacle avoidance.  rolling_window: Updates the local costmap as the robot moves.  width, height: Define the area covered by the local costmap.  Importance: Ensures the robot avoids unexpected obstacles in its immediate vicinity.   </p> <p>Common Costmap Parameters: Define how obstacles are detected and inflated for safety:  obstacle_range: Maximum detection range for obstacles.  inflation_radius: Adds a buffer zone around obstacles for safety.  robot_radius: defines how large the robot is so it knows which path is safe and where to make it more costly.   </p>"},{"location":"reports/2024/GestureBot/#path-planning","title":"Path Planning","text":"<p>Robots need to compute paths that are both efficient (shortest path) and safe (avoiding obstacles). Integration of SLAM and costmaps allows the robot to dynamically adapt its path when new obstacles are detected. This is effectively and more optimally done within the movebase as it localizes itself using its odom and lidar sensors.   </p> <p>Purpose: To compute and execute paths from the robot\u2019s current position to a specified goal.  Importance:  Integrates global and local planning for robust navigation.   Adapts dynamically to changes in the environment using costmaps.  How it Works:  Global planner for high-level paths.  Local planner for immediate movements and obstacle avoidance.  Configurations (base_local_planner_params.yaml):  max_vel_x: 0.45  min_vel_x: 0.1  max_vel_theta: 1.0  acc_lim_theta: 3.2  acc_lim_x: 2.5  These ensure smooth and safe movement by limiting velocities and accelerations.  </p>"},{"location":"reports/2024/GestureBot/#manual-waypoint-navigation","title":"Manual Waypoint Navigation","text":"<p>Purpose: Allows users to manually save poses and command the robot to navigate to those poses.  Importance:  Used for our gesture recognition to remember a specific pose when it recognizes an object and be able to navigate back to it.   </p>"},{"location":"reports/2024/GestureBot/#usage","title":"Usage","text":"<p>To run the robot, first bring up the master node on the robot. Then do <code>cd movebase/launch</code> and run <code>roslaunch move_base.launch</code>. After, do <code>cd real</code> and run <code>python3 roscam.py</code>. We only use an image subscriber and publisher for debugging.</p>"},{"location":"reports/2024/GestureBot/#story","title":"Story","text":"<p>The GestureBot project began with a straightforward idea: combining gesture recognition with basic movement commands to emulate a robot that responds to human gestures. The initial inspiration stemmed from the desire to create a robot with functionality akin to a pet: intelligent and interactive, but devoid of auditory command reliance. Jeffrey and I both lacked firsthand experience with pets, particularly dogs, which made the concept of creating a robot capable of \"fetching\" or \"following\" based on gestures an exciting challenge.</p> <p>We initiated the project with an extensive research phase. Gesture recognition stood out as the project's base, so we explored various methodologies, such as Leap Motion sensors. However, after discovering that the Leap device we had access to was incompatible due to proprietary changes, we pivoted to a solution involving Google MediaPipe. This tool, combined with a custom neural network built using Keras, enabled efficient and lightweight gesture classification.</p> <p>Once gesture recognition was operational, we expanded the project's scope to include elements inspired by the GOAT (\"Go to Any Thing\") system from class. This involved implementing advanced features like object recognition, depth estimation, and autonomous navigation. At this point, we divided responsibilities: Jeffrey focused on object recognition and depth estimation, while I worked on live mapping, localization, and path planning. This division of labor allowed us to address the project's complex requirements in parallel.</p> <p>Our teamwork was characterized by clear communication and mutual respect for each other's expertise. Although we worked on separate components, we regularly synchronized our efforts to ensure seamless integration. Debugging sessions became collaborative problem-solving exercises, where one person's fresh perspective often revealed overlooked issues.</p> <p>This collaboration extended beyond technical implementation. For example, Jeffrey's insights into reducing computation overhead significantly influenced my approach to SLAM and costmap configurations. Similarly, my work on creating a modular navigation framework informed Jeffrey's decisions regarding depth estimation and object segmentation integration.</p>"},{"location":"reports/2024/GestureBot/#problems-that-were-solved","title":"Problems That Were Solved:","text":"<p>Gesture Recognition and Data Collection: Initial attempts at gesture recognition revealed a need for consistent and comprehensive training data. We decided to use a CSV file to spam thousands of datapoints to train our neural network. A simple function was created that, upon pressing a number on the keyboard, would write all keypoints that the camera identified along with the pressed number into the CSV file, which represented the inputs for the neural network and the output class. This way, we were able to log datapoints with the press of a button.</p> <p>Depth Estimation Challenges: Depth estimation emerged as a major hurdle due to the limitations of monocular camera systems. After testing multiple algorithms, we realized that accurate metric depth could not be achieved in real-time. Our workaround involved recording the robot's pose as a proxy for the object's location and using relative depth estimation to refine navigation.</p> <p>Resource Management: The computational demands of running multiple algorithms simultaneously posed a risk to system stability. We optimized callback rates for object segmentation and depth estimation, reducing resource usage without compromising functionality. This adjustment was crucial for maintaining smooth robot operations.</p> <p>Object Recognition and Differentiation: The segmentation algorithm's inability to distinguish unique objects led us to implement Non-Maximum Suppression (NMS) to refine bounding box selections. Although true object differentiation remained beyond our scope, the NMS-based filtering significantly improved recognition reliability.</p> <p>SLAM Integration: Integrating SLAM with gesture-based navigation required careful tuning of parameters like obstacle inflation and robot dimensions. By balancing accuracy and computational efficiency, we ensured that the robot could navigate dynamic environments without frequent errors.</p>"},{"location":"reports/2024/GestureBot/#pivots","title":"Pivots","text":"<p>We knew that there were sensors that performed gesture recognition, so initially we wanted to use the LeapMotion sensor for that part of our project. However, the sensor did not work with our laptops, and we figured that it would be far too much work to try and get it working, especially since we did not know whether or not the sensor was actually any good. We decided to look for a software solution instead, which is where we found Google's Hand Landmarking tool from MediaPipe Solutions. It is a very fast, very accurate model that identifies \"keypoints\" in the hand, which are mostly just joints. From this, we decided that combining it with a neural network would give relatively decent outputs in a short period of time. </p> <p>Our original idea for mapping was to create our own SLAM algorithm, where we would use a \"frontier search\" algorithm, which looked for frontiers (or the boundaries of the map), using BFS (Breadth First Search). We also needed to create a path planner, which would use A* to plan paths; a path following algorithm, which would follow the path using a point slightly in front of the robot; and a \"frontier exploration\" algorithm, which would coordinate these three classes. It was also necessary to create a custom occupancy grid and custom messages for the frontier search algorithm.</p> <p>However, we found that implementing these four classes ourselves caused the robot to malfunction and have increased latency because of the amount of overhead; it also only worked 50% of the time. This caused us to pivot to using movebase with SLAM gmapping and yaml costmaps because it was more optimally written and created less overhead in the robot. SLAM gmapping used lidar and odom to keep track of where the robot was relative to its surroundings, as well as creating the map that was used by move base in the global and local costmaps for autonomous navigatation to the object. </p>"},{"location":"reports/2024/GestureBot/#self-assessment","title":"Self Assessment","text":"<p>The project achieved its goals of gesture recognition and autonomous navigation to an object. We did not expect the cluster to be able to handle multiple heavy algorithms at the same time, so being able to successfully run object segmentation, depth estimation and gesture recognition at the same time on top of a ROS navigation stack is quite unexpected. Most of the time, a lot of issues occur when translating outputs between algorithms, so it was very pleasing to see that most of the data was actually extremely organized and very easy to both use and interpret. Because of this, the way that the algorithms interact is actually quite smooth. Overall, given our limitations and workarounds, we are happy to see that the robot works and functions as intended. </p> <p>GestureBot represents a synthesis of cutting-edge techniques in gesture recognition, object segmentation, and autonomous navigation. By tackling real-world constraints with creative solutions, we transformed an ambitious idea into a functional prototype. Beyond the technical achievements, this project underscored the value of teamwork, adaptability, and the iterative design process in robotics development.</p>"},{"location":"reports/2024/HideAndSeek/","title":"HideAndSeek","text":"<p>CS119A - Autonomous Robotics - Professor Pito Salas - December 10, 2024</p> <ul> <li>Daphne Pissios</li> <li>Chloe Wahl-Dassule</li> <li>Jungju Lee</li> </ul>"},{"location":"reports/2024/HideAndSeek/#final-report-hide-and-seek","title":"Final Report - Hide and Seek","text":""},{"location":"reports/2024/HideAndSeek/#github-repository","title":"Github Repository","text":"<p>https://github.com/AnotherDaphne/robots_hide_and_seek</p>"},{"location":"reports/2024/HideAndSeek/#introduction","title":"Introduction","text":"<p>Although there are many different tasks that robots can perform, arguably all of these tasks involve the actuation of the robot or sensors of the robot. After going through the current curriculum in the Autonomous Robotics course, a project that our group decided that can best demonstrate the utilization of actuators and sensors was to perform a Hide and seek game.</p> <p>Hide and seek game consists of two separate teams; Hiders and Seekers. Hider's main objective is to find the best hiding spot that will prevent them from being found for the longest time. There is no limit to using sensory devices but there is a limit to actuators, specifically motor speed. There also is no limit to using any ROS topics. Same rules apply for Seekers. The Seeker's main objective is to find the Hiders. To better detect Hiders, Hiders will have fiducials attached on all four sides of the robot. This imposes one additional mandatory task for seekers which is having to seek for fiducials at all times.</p> <p>To better demonstrate the development of hide and seek algorithms, final deliverables will consist of video that shows how algorithms have been making improvements; multiple generations of algorithms based on trial and failure. As per this paper, however, there will only be initial proposals of each hide and seek algorithms by participants.</p>"},{"location":"reports/2024/HideAndSeek/#rules","title":"Rules","text":"<p>Following are basic rules that will be imposed during the hide and seek game. - All robots have a limit to x coordinate linear velocity at 0.5 on code. - All robots have a limit to z coordinate angular velocity at 0.5 on code. - Under move_base topic operation, no velocity limit is imposed. - Once the game starts, Hiders will have 5 minutes of hiding time. Then, seekers will start searching.</p>"},{"location":"reports/2024/HideAndSeek/#environment-control","title":"Environment Control","text":"<p>The environment that our team has been assigned to perform the hide and seek had multiple objects that would interfere with the Lidar sensors; chairs, bricks with holes, round trash bin, or a table that has an empty space just enough for the robot to pass through. To control this environment, cardboard boxes, wooden walls were used to block under the desk, and hide any potential interference.</p> <p>Although the room was already in a very unique shape which provides a lot of blind spots for hiders, our team has decided that the room was narrow and did not consist of enough variables. To provide more options for robots to hide, artificial pockets were created. These pockets were located both in the middle of the room, or tangent to a wall.</p>"},{"location":"reports/2024/HideAndSeek/#seeker-algorithms-and-implementations-james","title":"Seeker Algorithms and Implementations - James","text":""},{"location":"reports/2024/HideAndSeek/#guide-on-how-to-use-the-code","title":"Guide on how to use the code","text":"<ol> <li>Run <code>roslaunch &lt;package-name&gt; fiducial.launch</code></li> <li>Run <code>rosrun &lt;package-name&gt; mapper.py</code></li> <li>Search Algorithm ( either one ) 3-1. Run <code>rosrun &lt;package-name&gt; random_search.py</code> 3-2. Run <code>rosrun &lt;package-name&gt; wall_follow_search_v0.py</code></li> </ol>"},{"location":"reports/2024/HideAndSeek/#fiducials","title":"Fiducials","text":"<p>Since computer vision was a very sophisticated and niche feature, it was not yet to be implemented for searching other robots. One easy workaround was to attach fiducials on all four sides of the robot, then treat the hider to be 'found' once the seeker detects the fiducial. If multiple hiders were to perform hiding at the same time, this would reduce the confusion as unique fiducials would allow the seeker to distinguish between two turtlebots that looks nearly identical.</p> <p>Aruco D7 Fiducials were attached to all four sides of the robot. Initially, these fiducials can easily be detected once 'aruco_detect' ros-package was launched. List of fiducials to look for was detected before seeking starts. From the buffer of tf messages, fiducials message was seeked and once specific fiducial was found, the hider is considered 'found' and the seeker removes the 'found' fiducial from seeking list and continues searching if there is any more hiders to find.</p>"},{"location":"reports/2024/HideAndSeek/#algorithms-pseudo-random-search","title":"Algorithms - (Pseudo) Random Search","text":"<p>This random search algorithm is based off of the following logic; When playing hide and seek in person at a completely random location, the most reasonable thing that one does is moving around to a random location, hoping to find hiders. However, this algorithm consists multiple problem. As the randomness of this algorithm suggests, it could be highly inefficient and run into a problem where the algorithm runs unluckily, and robot does not reach the other side of the room, or fails to look for one specific corner even after a long random search. To prevent this, the algorithm will not be completely random but rather make decisions based off of the lidar information.</p> <p>From 0 degree to 360 degree, robot will fetch Lidar distance information with 30 degree interval, being total of 12 candidates. Then, out of 12 candidates, robot will randomly select one direction out of three directions that has the longest distance read. Then, robot will turn to this direction with -15 to 15 degree offset that is randomly selected.</p> <p>After direction has been determined, the robot will perform a linear movement. This movement will also be randomly decided between 50% and 90% approach to the wall; time that it performs linear movement will be proportional to its distance from the wall.</p>"},{"location":"reports/2024/HideAndSeek/#algorithms-wall-follow-search","title":"Algorithms - Wall Follow Search","text":"<p>This algorithm is based off of the following logic; To search the entire room visually, every single blind spot will be visited if one follows along the wall. This algorithm is rather simpler than Random Search. Robot will perform a wall follow mainly utilizing its Lidar sensor. While performing a wall follow, searching for fiducial will not stop. Robot will also periodically stop, turn around to scan the whole room for fiducial scan; this is regarding the pockets in the middle of the room or missed blindspots that could be behind the robot.</p> <p>There is a lot more leniency than having to perform an actual wall follow. As the robot has to visually scan the whole room, we can allow robot to perform wall search with some leniency to its distance from the wall. This should be fine tuned, although, as there is a chance that robot runs into a pocket and considers it as a wall, causing problems with the wall follow itself.</p> <p>There also is a lot to be optimized for this algorithm. For example, the OV5647 camera module that is used for Raspicam, which is attached to the turtlebot has default of 54 degree of horizontal field of view. This means when the robot is parallel to the wall, 27 degree to the left of the robot is already being scanned. When robot decides to turn around for full room scan, it has to only turn 153 degree instead of 180 degree.</p> <p>Also, depending on the size of fiducial, the capability of camera to detect fiducial could vary. Once measuring the maximum fiducial detection distance, robot could use it as an interval. For example, if the maximum fiducial detection distance is 3 meters, robot can stop after every 6 meters travel, do a full room search. This will cover circular area with diameter of 6 meters.</p>"},{"location":"reports/2024/HideAndSeek/#hider-algorithms-and-implementation-chloe","title":"Hider Algorithms and Implementation - Chloe","text":""},{"location":"reports/2024/HideAndSeek/#guide-on-how-to-use-the-code_1","title":"Guide on how to use the code","text":"<ol> <li> <p>Robot must have AMCL running using the following command</p> </li> <li> <p><code>roslaunch turtlebot3_navigation turtlebot3_navigation.launch map_file:=\"filename\"</code></p> </li> <li> <p>Give robot accurate starting point</p> </li> <li> <p>Run<code>hider_real.py</code> or <code>hider_sim.py</code></p> </li> <li> <p>Run <code>scan_sim.py</code></p> </li> <li> <p>Run <code>timer.py</code></p> </li> </ol> <p>Files must be run in this order</p>"},{"location":"reports/2024/HideAndSeek/#algorithm-overview","title":"Algorithm overview","text":"<p>This algorithm works by attempting to recognize pockets, small areas walled in on 3 sides, in LiDAR data, calculating coordinates to the pockets, assessing the best pocket, and navigating to the coordinate at the end of the hiding time.</p>"},{"location":"reports/2024/HideAndSeek/#recognizing-pockets-scan_simpy","title":"Recognizing pockets - scan_sim.py","text":"<p>As the robot is moving around the space, it will be regularly checking the data from LiDAR and looking for a group of points that look like pockets. The diagram on the right shows what pockets look like in the data. The algorithm is a large set of if statements that comb through the points looking for local minima, maxima and plateaus. I had to rewrite this section many times as my understanding of how the pockets showed up on the LiDAR evolved. I eventually settled on looking for points that were all close together, started with an increase, had a maximum, although there could be multiple points at the same level of the y axis and those would all be counted as the maximum (added to the plateau array), and ended with a decrease. For the most part, this technique does identify pockets in the LiDAR data though it will also identify areas that are incorrect. I manage that at a later point in the algorithm.</p>"},{"location":"reports/2024/HideAndSeek/#calculating-the-coordinates-scan_simpy","title":"Calculating the coordinates - scan_sim.py","text":"<p>Once the pockets are identified, the algorithm calculates where on the map the pockets are. This is done through basic trigonometry. First the algorithm gets the first recorded localmin_1 the center of plateau and the last recorded localmin_2. Then it finds the coordinates of these points. It does this by getting the robot's coordinates and yaw from get_robot_position() and the distance reading from the LiDAR. Using simple trigonometry, the coordinates of the points are identified. Then the centroid coordinates are calculated and published. This was the most challenging part of the project for me because I was incorrectly trying to use the degrees from LiDAR as the angle for the trigonometry functions without accounting for the yaw of the robot. This meant that everytime I calculated the coordinates, they were always nearby but rotated incorrectly. This bug took me a long time to track down and understand. However, once I figured it out I was able to get accurate coordinates and publish them.</p>"},{"location":"reports/2024/HideAndSeek/#ranking-pockets-hider_simpy-or-hider_realpy","title":"Ranking pockets - hider_sim.py or hider_real.py","text":"<p>Once the coordinates are received they are screened against previously visited coordinates to avoid revisiting the same places. Coordinates are only added to the queue of places to visit if they are not within the set radius of any points that have already been visited, or points that are currently in the queue. Coordinates are also not added if they match any that are in the preempted array but they can be nearby those points.</p> <p>When a coordinate is visited, assuming the goal isn\u2019t preempted, the algorithm takes the average of all the LiDAR data as a simple measure of how enclosed the space is. This number is stored along with the coordinate.</p> <p>As previously mentioned, the pocket detection does make mistakes and will send the robot to places that are incorrect. Additionally, sometimes calculated coordinates are in a wall or close enough to a wall that move_base cannot navigate to it. In order to minimize the time spent on these goals, I implemented a time limit that uses odometry to see how much the robot has moved in the last publication. I have two tiers, one that checks if the robot has not moved at all and one that checks if the robot has moved a small amount (actual values differ between sim and real). If the robot has moved too little and triggers these conditions, the goal is preempted and the robot moves on to the next goal. This decision is purely strategic for the hide and seek game. I found that the recovery behavior of move_base took far too much time and was triggered a lot, meaning that the robot would spend well over a minute trying to navigate to a difficult coordinate instead of continuing to explore. Ultimately, it was more effective and efficient for the robot to give up on that specific coordinate, hoping that the pocket would be detected again, and a better coordinate would be calculated. This is the part of my algorithm I am least happy about. With unlimited time I would continue to work on this problem and try to find a more elegant solution.</p>"},{"location":"reports/2024/HideAndSeek/#game-logic","title":"Game logic","text":"<p>The game logic of my program is very simple, once the timer is running, the robot begins to move to its first coordinate which is always 0,0 if possible. I did this because the center of the map should give the robot the most starting options in the most number of situations. As long as there are pockets in the queue, it will continue to go to those. As the robot is continuously looking for pockets the queue is rarely empty. Once 5 minutes have passed, the algorithm loops through all the successful visited spots and sorts them based on which one had the lowest average of all LiDAR data. Then it will move there in the remaining minute and end the program.</p>"},{"location":"reports/2024/HideAndSeek/#hider-algorithms-and-implementation-daphne","title":"Hider Algorithms and Implementation - Daphne","text":"<p>To run this code</p> <ol> <li> <p><code>roslaunch turtlebot3_navigation turtlebot3_navigation.launch map_file:=\"filename\"</code></p> </li> <li> <p>Run the <code>hider_explored.py</code> file</p> </li> </ol> <p>The script defines a Hider class that creates a robot navigation strategy designed to explore an environment and ultimately hide in a corner within a 5-minute time limit. Using 3 states: \u201cexplore\u201d, \u201cfollow_wall\u201d, \u201chide\u201d, in order to guide the robot to its destination.</p> <p>The exploration phase begins with the robot waiting to receive map data and then moving around the environment. During this initial phase, the robot will move to a random location on the mpa. The goal is to locate potential walls that can be followed towards a corner. The robot has a built-in time limit for this initial exploration stage.</p> <p>When a wall is detected during exploration, the robot transitions to a \"follow_wall\" state, which is managed by a PID controller. The PID helps the robot follow the wall smoothly. The wall-following uses sensor data from the LiDAR to adjust the robot's movement, attempting to stay roughly one meter from the wall while moving forward.</p> <p>As the robot follows the wall, it continuously checks for corner detection. A corner is identified when the LiDAR detects very short distances on both the left and right sides of the robot, indicating it has approached a corner of a room or enclosed space. Once a corner is detected, the robot transitions to the \"hide\" state, where it goes farther into the corner and then stops moving to effectively \"hide\".</p> <p>If the robot is in the \u201cfollow_wall\u201d state for too long, it returns to the explore phase for some time to try and find a fall with more potential places to hide.</p> <p>The entire program is constrained by a 5 minute deadline. If the robot fails to find a good hiding place, it will stop moving wherever it is, making it a lot easier for the hider to find it if the robot cannot complete its goal in time.  </p> <p>A challenge that has come up during the development of this program has been the corner detection, and the robot\u2019s positioning to hide. I have been using the LiDAR to detect the corners, but the detection can be imprecise. Getting the robot into the corner it has decided to hide in can also be difficult. Not only are some of the hiding spaces in the environment relatively small compared to the size of the robot, the calculation of getting the robot hidden sufficiently within the corner can also be imprecise and may result in the robot being positioned farther away from the hiding corner than is practical for a hide and seek game.</p>"},{"location":"reports/2024/HideAndSeek/#challenges-and-hardships","title":"Challenges and Hardships","text":"<p>As this is a game that is performed by multiple robots, our main hardship was that we could not perform this in a Gazebo simulation. This also caused some problems with imposing time limits or deadlines to certain team members, causing delays to deliverables.</p> <p>A challenge within gazebo was that only certain maps worked when trying to create maps with move_base. Because of this issue, it was more difficult to test the algorithms in diverse environments to see if any issues in a real life scenario would arise</p> <p>We also faced issues with testing in the lab due to not having enough turtlebots available. We originally wanted to have three turtlebots, each running one of the programs, in the room at the same time. However, because there were never three turtlebots available at once, we modified our project to work with one turtlebot. After each hider algorithm has finished, instead of leaving the turtlebot there, we put a block with a fiducial on it to save its place. This way the turtlebot was available for the next hider algorithm. This meant that the seeker was looking for fiducials on blocks instead of on turtlebots.</p>"},{"location":"reports/2024/HideAndSeek/#source-files","title":"Source Files","text":""},{"location":"reports/2024/HideAndSeek/#james","title":"James","text":"File name Description Subscribers Publishers /launch/fiducials.launch Launch file that runs aruco detection. Must be run before running any other python files. /src/mapper.py Mapper for Fiducials. Must be run before running seeker algorithm. Searches the fiducual from tf2_buffer and published the pin through the broadcaster topic. tf2_buffer tf2_broadcaster /src/random_search_v0.py Multiple versions of random_search algorithms. Relies on pre-programmed pseudo random decision trees. tf2_buffer, /odom(Odometry), /scan(LaserScan) /cmd_vel(Twist) /src/wall_follow_search_v0.py Multiple versions of wall_follow_search algorithms. Relies on wall follow algorithm modified with certain search action per interval. tf2_buffer, /odom(Odometry), /scan(LaserScan) /cmd_vel(Twist)"},{"location":"reports/2024/HideAndSeek/#chloe","title":"Chloe","text":"File name Description Subscribers Publishers hider_real.py Filters coordinates received from my_scan subscriber, handles all movement through move_base, and contains all game logic my_scan (Float32MultiArray), my_timer (Int32), /scan (LaserScan), my_odom (Point), odom(Odometry) cmd_vel (Twist) scan_sim.py Reads lidar data and detects pockets. Calculates coordinates of pockets and publishes them to my_scan /scan (LaserScan) my_scan (Float32MultiArray) timer.py Helper program that publishes an increasing integer at the set rate to my_timer my_timer (Int32) my_odom.py Helper program that handles the robots odometry and publishes data to my_odom odom(Odometry) my_odom (Point)"},{"location":"reports/2024/HideAndSeek/#daphne","title":"Daphne","text":"File name Description Subscribers Publishers hider.py Entire hiding algorithm /map, /scan (LaserScan) cmd_vel (Twist) (move_base)"},{"location":"reports/2024/RIghtsofRobots/","title":"Right-of-Way System of Robots &amp; Yielding","text":"<p>A ROS Robot  Project</p> <p>GitHub Link: https://github.com/1192119703jzx/Right-of-Way-System-of-Robots-Yielding</p> <p>Also check FAQ, Setting the Camera Parameters section.</p>"},{"location":"reports/2024/RIghtsofRobots/#team-members","title":"Team Members","text":"<ul> <li> <p>Haochen Lin   Email: haochenlin@brandeis.edu</p> </li> <li> <p>Zixin Jiang   Email: zixinjiang@brandeis.edu</p> </li> </ul>"},{"location":"reports/2024/RIghtsofRobots/#inspiration","title":"Inspiration","text":"<p>The idea of this project comes from the real-world cross-road scenario and right-of-way problem. In the real world, it is very confusing to decide who has the right of way when you reach a crossroad with 4 stop signs on each side. Typically, people will waive their hand when they can\u2019t tell who arrives first and has the right of the way. If we can build a scenario in which a central system controls the traffic and the right of way. It gonna be a monumental mark to auto-drive technology.</p>"},{"location":"reports/2024/RIghtsofRobots/#work-distribution","title":"Work Distribution:","text":"<p>Scheduling and communication (Haochen Lin). Robot movement, Lidar sensation, and Fiducial recognition (Zixin Jiang)</p>"},{"location":"reports/2024/RIghtsofRobots/#project-overview","title":"Project Overview","text":"<p>This project simulates a real-world right-of-way system for robots navigating an intersection. The system is inspired by traffic scenarios where determining the right of way can be challenging. It employs a central scheduler to manage robot movement, mimicking real-world traffic flow and promoting advancements in autonomous driving technology.</p>"},{"location":"reports/2024/RIghtsofRobots/#problem-statement","title":"Problem Statement","text":"<p>This project focuses on creating a logic system for robot communication and navigation at intersections. Two robots approach the intersection from different directions and determine the right of way based on their arrival times, mimicking real-world traffic scenarios. Priority can also be adjusted based on predefined identities or emergency statuses. Fiducials, placed at four corners of the intersection, provide location information for the robots via their cameras, enabling recognition from the opposite direction without obstruction.</p>"},{"location":"reports/2024/RIghtsofRobots/#running-logic","title":"Running Logic","text":"<ul> <li>The robot starts from the beginning of the map and starts to move forward.</li> <li>Upon reaching the intersection, the robot stops in front of the intersection. The robot will publish a message to the scheduling node, indicating its presence along with its relative location in the intersection.</li> <li>The scheduling node applies a \"first come, first go\" rule, optionally taking robot positions into account, and informs each robot when it is its turn to proceed.</li> <li>When the robot receives the signal from the scheduling node, it enters the intersection. When it leaves the intersection, it also sends a message to the scheduling node, indicating it has left so that the scheduling node can let other waiting robots process.</li> </ul>"},{"location":"reports/2024/RIghtsofRobots/#map-setup","title":"Map Setup","text":"<p>Our real crossroad map setting</p> <ol> <li>The intersection scenario is set up on a large white paper, serving as the test map for all experiments. White paper can make the colored line more obvious and easy to detect.</li> <li>The intersection is built with black and red lines. Each road is divided into two ways, the right way for the forward direction, and the left way for the opposite direction.</li> <li>~~Four fiducial blocks are placed at four corners of the intersection. For each direction, a block is placed in the left corner right next to the black line. A fiducial with a unique ID is stuck on the block.~~</li> <li>Walls are placed behind each black line to simulate the intersection boundary.</li> <li>The robot will be placed on the right side of the two-way road, mimicking the real-world right-hand side traffic.</li> </ol>"},{"location":"reports/2024/RIghtsofRobots/#key-features-break-down","title":"Key Features Break Down","text":""},{"location":"reports/2024/RIghtsofRobots/#multi-robot-system","title":"Multi-Robot System","text":"<p>Used function: <pre><code>$ roslaunch turtlebot3_bringup turtlebot3_multi_robot.launch\nor\n$ roslaunch turtlebot3_bringup right2.launch\n</code></pre> The Project runs multiple robots at the same time. By signing one robot to another robot\u2019s IP, we can control two robots at the same time. However, due to environmental limitations, we can not run more than two robots since we don\u2019t want to interrupt others by corrupting the robot when we sign it to another robot. In this case, we have the main robot robA as the master robot, and Rafael as the follower robot. By the launch muti_robot_launch in the follower robot, both robots can take the same move or different depending on the actual launch file setup. We need to indicate the namespace of \"Rafael\" when subscribing to Rafael's node.</p>"},{"location":"reports/2024/RIghtsofRobots/#scheduling-stages","title":"Scheduling Stages","text":"<ul> <li>Entering: Robots move toward the intersection.</li> <li>Waiting: Robots queue in the scheduler.</li> <li>Passing: Robots cross the intersection when permitted.</li> <li>Left: Robots signal departure, allowing others to proceed.</li> </ul> <p>Each robot is split into 4 stages: Entering, Waiting, Passing, and Left. Initially, all robots were in the \u201cEntering\u201d stage which means they would move until they reached the cross. Then, they will turn into \u201cWaiting\u201d stage. The robot in the waiting stage will be stored in a queue in self.scheduler_robot. When the queue only has one robot and that robot's state is not \"passing\", the scheduler will set the robot's state as \"pass\" and broadcast this message with robot's name. When there are more than one robots in the queue, the second robot will not get the pass and has to wait until there is only one robot left in the queue. In the callback function which processes the message sent from each robot, the scheduler will add the robot to the queue if the message indicates its state as \"waiting\" and the robot is not in the queue. The scheduler will remove the robot from the queue if the message indicates its state as \"passing\" and the robot is in the queue.</p>"},{"location":"reports/2024/RIghtsofRobots/#navigation-challenges-solutions","title":"Navigation Challenges &amp; Solutions","text":"<p>A. We need the robot to move along the center of the space between the red and the black lines</p>"},{"location":"reports/2024/RIghtsofRobots/#difficulty","title":"Difficulty:","text":"<p>Lighting Inconsistency: Room illumination varies unpredictably. Sometimes extreme darkness can cause camera failure in red line detection. Thus, we need to establish stable environmental conditions to ensure consistent camera performance across different lighting scenarios. Camera Quality Variability: Camera specifications differ between robots We want to develop a universal Python control script for convenience. Thus, we seek a generalized solution adaptable to all robot units to ensure robust camera performance regardless of individual hardware differences.</p>"},{"location":"reports/2024/RIghtsofRobots/#solution","title":"Solution:","text":"<p>Turn on all the lights in the room when testing. (of course!) We customize each robot's camera parameters (contrast and brightness) through launch file modifications, enabling universal detection of red and black lines using standardized color range configurations in our code. See the FAQ section for more details on how to configure the robot\u2019s camera parameters.</p>"},{"location":"reports/2024/RIghtsofRobots/#limitation","title":"Limitation:","text":"<p>Our map, created on large and thin white paper, presents surface irregularities that cause reflective challenges. These uneven surfaces result in significant light reflection, which artificially brightens the black color and compromises detection accuracy.</p>"},{"location":"reports/2024/RIghtsofRobots/#implementation","title":"Implementation:","text":"<p>We convert the camera callback image from BGR to HSV color space and create a mask to detect red regions. And then we extract a slice of the mask to find a red line in front of the robot. Depending on the robot's current stage and plan, it adjusts the region of interest for line detection. We calculate the centroid of the detected line and update flags indicating whether the line is detected. The centroids are used to guide the robot's navigation. We do the same thing for the black line and get the centroid for black. Implementing PID control with the proportional part only for stabilization reasons. We calculate the current error using the formula: (w / 2 - r + (b - r) / 2)) / 100, where: w = image width, r = red line centroid, b = black line centroid. B. We need the robot to stop in front of the intersection.</p>"},{"location":"reports/2024/RIghtsofRobots/#failure-stopping-mechanism-at-intersection","title":"Failure Stopping Mechanism at Intersection:","text":"<p>Stop when the right-hand side black line is no longer detectable. Place a green stop line at the intersection entry point. Program the robot to halt when the stop line appears in the lower camera image region.</p>"},{"location":"reports/2024/RIghtsofRobots/#reason-for-failure","title":"Reason for Failure:","text":"<p>Camera placement challenges create a critical detection limitation: mounted vertically at the robot's top, the camera can only view the front floor, missing the area directly underneath. This positioning causes the robot to halt approximately 20 cm before an intersection when the green line or right-hand black line becomes undetectable at that point. Attempting to tilt the camera downward results in an excessively dark callback image due to automatic camera exposure adjustment and it is impossible to detect the fiducial from that angle.</p>"},{"location":"reports/2024/RIghtsofRobots/#solution_1","title":"~~Solution:~~","text":"<p>Utilize fiducial markers to precisely measure distance. Initiate marker detection at the onset of movement. Selectively store only fiducials facing the robot by calculating the rotation of the fiducial. Save the frame of the fiducial to a pin frame in the tf tree to prevent loss of the fiducial frame when it is detectable. Continuously monitor the distance between the robot's base_link and the pinned fiducial frame by calculating the distance at the beginning of every loop. Halting movement when the distance falls below the predetermined target threshold. </p>"},{"location":"reports/2024/RIghtsofRobots/#new-problem","title":"New Problem:","text":"<p>The previous code worked smoothly on RobA. However, after integrating multi-robot into testing, we found that the code can not work properly on Rafael.      Issue 1: We use aruco_detect package to recognize fiducial, and this package will add the fiducial as a frame to the tf tree. However, when practicing multi-robot, the fiducial recognized by rafael (child robot) is added to the tf tree of Roba (main robot). With reading through the source code of aruco_detect package we found the reason for this error. In the aruco_detect.cpp, the code subscribe to the node camera_info, which is rafael/raspicam_node/camera_info. It takes the header.frame_id of this message as the header of the fiducial frame. However, by echoing the message of this node we found that both RobA and rafael have the frame_id as \"raspicam\". On the other hand, since RobA and rafael have their own tf tree, we actually have \"rafael/raspicam\" frame on rafael's tf tree, and \"raspicam\" frame is a frame that belongs to RobA's tf tree. This is why both fiducials recognized by two robots are added to RobA's tf tree.      Solution 1: By reading through the source code of rospicam_node, we found that the header.frame_id of the camera_info node is set by the variable camera_frame_id, which is in turn a parameter passed in from the launch file rospicam_node.launch. The parameter \"camera_frame\" is set default as \"raspicam\". We can modify this parameter just like what we do to set the parameter of the camera, which means adding a line of code to the bringup launch file of rafael which sets the parameter as \"rafael/raspicam\". <code>&lt;arg name=\u201dcamera_frame_id\u201d value=\u201drafael/raspicam\u201d/&gt;</code>     Issue 2: After we done the above modification, the recognized fiducials are under each robot's tf'tree and can be saved as pin properly. However, we found that the tf works fine for RobA (main robot) but fails on rafael (child robot). For rafael, the distance between fiducial and rafael, which is calculated by the transformer between \"pin_id\" and \"base_link\" is getting larger and sometimes rising and falling as rafael is actually moving toward the fiducial in the real world. This prevents the turning left and stopping in front of the intersection from functioning correctly.     Solution 2: Unfortunately, we have not figured out the reason for this bug!!! We have to switch to use Lidar as the indicator for stopping in front of the intersection.</p>"},{"location":"reports/2024/RIghtsofRobots/#new-implementation","title":"New implementation:","text":"<p>We use Lidar. We set up walls right behind each black line to create detectable indicators for the range of intersections. Lidar will continually scan the obstacles around it. We pick the clock-wise 15 to 60 degrees as the range for the  right front. We clean the Lidar callback data and calculate the average for the smallest ten numbers. If the output is larger than a pre-define number (0.35 in this case), the robot will stop, sending a message to the scheduling node, and wait.  3. We need the robot to go ahead, turn right, and turn left at the intersection. </p>"},{"location":"reports/2024/RIghtsofRobots/#go-ahead","title":"Go Ahead:","text":"<p>Implementation: When the right-side black line becomes undetectable, navigation relies solely on the left-side red line using PID control (only proportional part). The current error is calculated as (w / 2 - (r + x)) / 100, where w represents image width, r denotes the red line centroid, and x is a predetermined threshold for the center of the way.     Difficulty: The previously used image slice reveals a horizontal red line that risks misguiding the robot into an unintended rightward turn. To ensure accurate navigation, the system must selectively capture only the vertical red line while filtering out the potentially misleading horizontal line.     Solution: When the stage transitions from \"moving\" to \"crossing\", the sliced region of the callback image shifts from the lower to the middle part (slice 2). We calculate the centroid of red using slice 2. When both red and black lines become detectable in slice 1, the robot reverts to the \"moving\" stage and signals the scheduling node of its departure.</p>"},{"location":"reports/2024/RIghtsofRobots/#turning-right","title":"Turning Right:","text":"<p>Implementation: When the right-side black line becomes undetectable, navigation relies solely on the left-side red line using PID control (only proportional part). The current error is calculated as (w / 2 - (r + x)) / 100, where w represents image width, r denotes the red line centroid, and x is a predetermined threshold for the center of the way. We use still use the old sliced region of the callback image which captures the lower part (slice 1). There is only a horizontal red line in this slice region which will guide the robot turning rightward. The predetermined threshold x must be bigger than the center of the way such that the amplitude of rotation is large enough for turning the robot 90 degrees. When both red and black lines become detectable, the robot reverts to the \"moving\" stage and signals the scheduling node of its departure.</p>"},{"location":"reports/2024/RIghtsofRobots/#turning-left","title":"~~Turning Left:~~","text":"<p>~~Failure Mechanism to Turn Left: Maintain a constant angular and linear speed for the robot to execute a circular motion with a predefined radius matching the intersection's dimensions on the map.~~</p>"},{"location":"reports/2024/RIghtsofRobots/#reason-for-failure_1","title":"~~Reason for Failure:~~","text":"<p>~~The pre-determined movement proves unreliable, failing to adapt to intersections of varying sizes and geometries. Dynamic path adjustment becomes essential to accommodate diverse intersection configurations.~~</p>"},{"location":"reports/2024/RIghtsofRobots/#solution_2","title":"~~Solution:~~","text":"<p>~~The solution involves dynamic adjustment using the fiducial marker during the navigation process. While in the \"moving\" stage, the system stores the detected fiducial marker in a pin frame within the tf tree. The initial step is to calculate the arctangent angle between the base link and the pin frame. The robot then rotates to face the fiducial marker by turning through this calculated angle. Subsequently, the system continuously calculates and monitors the distance between the base link and pin frame during each iteration. The robot progresses forward until this distance falls below a predetermined threshold. Upon stopping, the robot initiates a left turn until both red and black lines become detectable. Once these conditions are met, the robot transitions back to the \"moving\" stage and signals the scheduling node to indicate its departure.~~</p>"},{"location":"reports/2024/RIghtsofRobots/#program-structure","title":"Program Structure","text":"<p>For script, each robot has its own sciprt to deal with data separately</p> <ul> <li>Launch Files:</li> <li>~~<code>fiducial.launch</code>: Detect fiducial markers.~~</li> <li><code>road_run.launch</code>: Bringup <code>scheduler.py</code>, <code>version2.py</code> for each robot</li> <li>Scripts:</li> <li><code>version2.py</code>, <code>version2_rafael.py</code>: Control robot movement.</li> <li><code>scheduler.py</code>: Handle communication and scheduling.</li> </ul>"},{"location":"reports/2024/RIghtsofRobots/#how-to-use","title":"How to Use","text":"<ol> <li>Run the launch file:    <pre><code>roslaunch package_name road_run.launch\n</code></pre></li> </ol> <p>We only need to run one file, road_run.launch, which will automatically run all the necessary files.</p>"},{"location":"reports/2024/RIghtsofRobots/#results","title":"Results","text":"<p>So, how\u2019s the outcome? Due to the limitation of robot hardware, we can only run two robots at the same, but it is good enough to verify our concept. Now in our designed map, two robots can run as scheduler-managed. There are some delays that could potentially be caused by running two robots on the same roscore, but overall it performs everything the scheduler plans.  The other huge limitation of this project design is that the movement of the robot completely relies on the detection of the red and black lines on the map. However, the rightening condition will largely affect the accuracy of line detection. We have to configure the camera parameter, camera angle, and other environmental conditions every time we start the testing. The performance of daytime and nighttime can be different. The robot often stops during testing or switches to the next stage (moving -&gt; leaving) just because the light prevents the robot from detecting the required lines. To improve the project, we must figure out another way to provide a more stable performance. It is sorry that we could not make the fiducial-tf_tree part of the code run successfully on the child robot rafael. Fiducial-tf_tree logic provides a more stable performance in turning left and stopping in front of intersection.</p>"},{"location":"reports/2024/RIghtsofRobots/#abandoned-concepts","title":"Abandoned Concepts","text":"<p>Originally, the project was more focused on recognizing different types of robots such as Platbot or tanks using Lidar or a camera. However, this idea is abandoned since neither is efficient enough to give precise information about which type of robot it is.  Moreover, the map that we made is too small for a robot like Platbot to pass through, and the Platbot has various problems when we try to implement it to our function because of its size and the method that it operates.  Eventually, it leads to signaling information exchange. The information is not only accurate but also avoids the limitations of the robot.</p> <p>What\u2019s more, the original plan actually doesn\u2019t involve two lines but only one road, which means each robot might need to run a little bit further from the road to avoid conflict. It is eventually abandoned because the robot isn\u2019t sensitive enough to keep a safe distance from each other.</p> <p></p> <p>The beginning map concept (Abandoned)</p> <p>At last, it is pretty sad we can only run two robots within the same terminal due to budget limits and time constraints. Since each robot has its independent \u201cbashrc\u201d, it will require us to manually change the setup in each robot. In other words, we hack those robots thus others can\u2019t really use them. Nevertheless, two robots might be efficient enough to finish signed tasks.</p>"},{"location":"reports/2024/RIghtsofRobots/#conclusion","title":"Conclusion","text":"<p>This project demonstrates the feasibility of a scheduler-based right-of-way system for robots. With better hardware, the concept can scale to larger traffic scenarios, paving the way for advancements in autonomous navigation.</p>"},{"location":"reports/2024/RIghtsofRobots/#references","title":"References","text":"<ul> <li>ROS Official Tutorials</li> <li>Campus Rover Lab Notebook</li> </ul>"},{"location":"reports/2024/RIghtsofRobots/#github-repository","title":"GitHub Repository","text":"<p>Explore the full code and documentation here: https://github.com/campusrover/Right-of-Way-System-of-Robots-Yielding</p>"},{"location":"reports/2024/RIghtsofRobots/#thanks-for-the-help-from-everybody-in-the-119-robot-lab","title":"Thanks for the help from everybody in the 119 Robot Lab.","text":""},{"location":"reports/2024/StarshipClone/","title":"StarshipClone","text":"<p>title: Starship Delivery Robot Clone authors: Ephraim Zimmerman, Zach Hovatter, Artem Lavrov date: Dec 10 2024 -</p> <p>Github Repo</p>"},{"location":"reports/2024/StarshipClone/#team-members","title":"Team Members","text":"<p>Ephraim Zimmerman - ezimmerman@brandeis.edu Zach Hovatter - zhovatter@brandeis.edu Artem Lavrov - artemlavrov@brandeis.edu  </p>"},{"location":"reports/2024/StarshipClone/#introduction","title":"Introduction","text":"<p>The goal for this project was to build a clone of the Starship robots found on the Brandeis campus. These robots are capable of navigating along the campus sidewalks autonomously, with the goal of delivering food (that was placed inside of it by a human). While it wasn\u2019t feasible to do the latter, we were able to build a robot with autonomous, outdoor navigating capabilities. The key topics in the below description highlight the difficulties of localizing the robot in real time, having it navigate to nodes, as well as stay in a desired path during its traversal. The only major component not included in this that we originally planned on was obstacle avoidance. However, we later learned that Starship robots also do not contain any obstacle avoidance capabilities, they simply stop moving when confronted with such a problem. As such, our original goal still holds strong. </p>"},{"location":"reports/2024/StarshipClone/#how-the-code-work-and-how-to-use-it","title":"How the code work and how to use it","text":""},{"location":"reports/2024/StarshipClone/#what-sensors-does-the-robot-use-and-how-do-they-work","title":"What sensors does the robot use and how do they work?","text":"<p>For this algorithm, the robot uses two sensors: a GPS and a magnetometer. The GPS is used for absolute position data and the magnetometer is used for absolute orientation/heading. Using a combination of these two sensors, the robot can navigate outside by checking whether it has reached a certain set of coordinates and being able to check if it is facing the right direction to go to the next set of coordinates. </p> <p>There are many better resources to find out how these sensors work in detail, but we will give a brief overview of how they work here. </p> <p>The GPS works by communicating with at least 3 satellites. The satellites send the GPS their position data and a timestamp, which the GPS chip then uses to calculate the distance of the GPS chip from the satellite. With at least 3 satellites, you get three spheres of possible positions of the GPS receiver. This is because you know the center of the sphere (the position of the satellite) and the radius of the sphere (the GPS receiver\u2019s distance from the satellite). If you take the intersection of two of these spheres, you get a 2 dimensional circle of all the possible positions where the GPS receiver could be. If you take the intersection of these 3 spheres you get 2 possible positions where the GPS receiver could be . If you assume the GPS receiver is on the surface of the Earth, you can use the Earth as a fourth sphere and calculate the intersection of this sphere with the other 3 spheres to get a singular position where the GPS receiver could be. Typically a fourth satellite is also used to adjust for small errors in timestamps that are being received from the other 3 satellites. That is the essence of how the GPS determines its position on the earth. </p> <p> </p> <p>The magnetometer works by measuring the magnetic field of the earth relative to itself. More specifically, it measures the strength of the magnetic field in each direction relative to itself. Our robot uses a 3 axis magnetometer, meaning it measures the strength of the magnetic field in the direction of the x,y, and z axes. You can see how these axes are positioned relative to our magnetometer in the diagram below. The issue with magnetometers is that they often need to be calibrated because various materials can interfere with the magnetic field of the Earth and thus with their readings, but these can be accounted for. This is what you are doing when calibrating a magnetometer, accounting for these interference with the magnetometer.</p> <p></p>"},{"location":"reports/2024/StarshipClone/#how-does-the-robot-localize-and-move-across-space","title":"How does the robot localize and move across space?","text":"<p>Although there is a determined map recorded as a set of coordinates (lat/lon) along a path, the nature of the robot is still autonomous in its ability to pathfind and navigate to these points. The robot uses a breadth search first algorithm (BSF) to find the optimal path between nodes. The nodes themselves are labeled with a string name, and their metadata contains lat/lon coordinates, as well as the nodes they are connected to via undirected edges. This edge system, however, is not what the robot follows to navigate to the next node, it is to inform the BSF algorithm of the interconnectivity of the nodes for ease in path following. This was an interesting challenge but yields more creativity in the physical locations a robot can be placed. To navigate to the next node, a function was built to calculate the necessary turn rate to arrive at the next node from the node it resides in. We also use something called the \u201chaversine function\u201d to calculate the distance between two lat/lon points in meters. This is primarily used to localize the robot, specifically, this is most important on bring-up, since the robot needs to know which node it is currently residing in and what it\u2019s facing. </p>"},{"location":"reports/2024/StarshipClone/#configuring-the-graph","title":"Configuring the Graph","text":"<p>The graph was built in Python without using any external libraries. The nodes themselves are stored in a list of tuples, with lat/lon values. The edges exist inside of a dictionary mapping nodes to a list of connected nodes. The nodes are specified as lat/lon values, but you can also generalize them by using a separate dictionary where the key is some string value (like \u201chome\u201d, or \u201coustide_lab\u201d). The nodes are grabbed from Google Maps and local GPS data and have a threshold distance for arrival, currently set as 1 meter (to allow for some error). The key way to understand how coordinates are created is to think about at least adding a node wherever the robot needs to change direction. However, not having a node, even on a straight line for a significant distance may cause other issues, such as the robot not being able to find the node as it went too far in one direction. Since this structure is so dynamic, the robot could theoretically move in any shape (square, circle, octagon, etc.)</p>"},{"location":"reports/2024/StarshipClone/#bringing-up-the-robot-and-checking-if-it-is-working-properly","title":"Bringing up the robot and checking if it is working properly","text":"<p>There is a custom bringup for this project in the ros_mapping package. It requires the sodacan package and all of its dependencies in order to work. In order to run the bringup, first bring the robot outside and wait until your GPS is actively receiving messages from satellites. Then run  <pre><code>roslaunch ros_mapping bu.launch\n</code></pre> Wait until the first imu message is received and make sure there are no errors within the bringup. It is also recommended to make sure the magnetometer is calibrated properly by running <pre><code>rosrun ros_mapping mag_reader.py\n</code></pre> and seeing if the degree values from some other compass (like your phone compass) match up approximately with the magnetometer degree values. If there is a significant difference between these values, you may want to recalibrate the magnetometer. If not, you can skip the next section. </p>"},{"location":"reports/2024/StarshipClone/#calibrating-the-magnetometer","title":"Calibrating the Magnetometer","text":"<p>To calibrate the magnetometer,  first you need to collect a lot of raw magnetometer values. I have written a script to do this. All you have to do is run: <pre><code>./mag_logger.py &gt; &lt;file name&gt;.csv\n</code></pre> and enter the name of the file into the constant called \u201cDATA_FILE\u201d in the calibration_plotter.py file. It should be at the start of the file and look like the code below:</p> <pre><code># Make sure to install pandas, numpy and matplotlib before running this script,  it should run automatically under jupyter notebook\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport numpy as np\nfrom matplotlib.colors import LinearSegmentedColormap\nfrom io import StringIO\n\nDATA_FILE = \"magnetometer_outside.csv\"  # Change this to your actual file name\n\n# Be sure to remove your outliers and check the data to see if it looks correct. ideally there should not any dots far from the \"circles\".\n# Percentage of outliers to remove from the top of the data, increase adjust until you data looks cleaned.\nmy_top_percentile = 0.025\n# Percentage of outliers to remove from the bottom of the data\nmy_bottom_percentile = 0.025\nsave_plots_to_file = True   # Set to True to save the plots to a file\n# Adjust if need to plot faster, 1.0 = 100% of the data, 0.5 = 50% of the data, 0.1 = 10% of the data, etc.\nplot_sampling_percentage = 0.2\nfigure_size = (10, 10)  # Adjust the size of all the plots, (width, height)\n</code></pre> <p>Make sure to replace magnetometer_outside.csv with your actual file name with the data you recorded in the previous step. At this point you should be able to run the calibration_plotter.py file like so: <pre><code>python3 calibration_plotter.py\n</code></pre> and this script will bring up a bunch of matlab plots (which are interesting to look at for visualization of what the script is actually doing and how it is calibrating the magnetometer, but not actually important to understand for this step). You can ignore the plots and look at the values printed to the console. They should look something like this:</p> <p>.</p> <p>You want to copy the offset and scale values in the xyz() parentheses and paste them into the constants called \u201cOFFSETS\u201d and \u201cSCALES\u201d in the mag_corrector.py file respectively. The start of the mag_corrector.py file should look something like this after that:</p> <pre><code>#! /usr/bin/python3\nimport numpy as np\nfrom sensor_msgs.msg import MagneticField\nfrom geometry_msgs.msg import Vector3\nimport rospy\nfrom collections import deque\n\n'''Calibration Params'''\n# Replace with the offset values you get from calibration_plotter.py\nOFFSETS = [-13, -33, -33]\n# Replace with the scale values you get from calibration_plotter.py\nSCALES = [1.083333, 0.962963, 0.962963]\n\n''' Median Filter Params '''\n# Adjust this to increase the buffer size for the median filter.\n# Increasing will reduce noise, decreasing will increase noise.\nWINDOW_SIZE = 20\n\n# Create buffers for median filter\nx_buffer = deque(maxlen=WINDOW_SIZE)\ny_buffer = deque(maxlen=WINDOW_SIZE)\nz_buffer = deque(maxlen=WINDOW_SIZE)\n</code></pre> <p>After you\u2019ve completed all these steps, make sure these changes are present on your actual robot, and you can bring up the robot again and run the mag_reader.py script and your robot should be reading proper degrees that should approximately line up with your other compass (but they are not accounting for magnetic declination so they might be slightly off, that is normal). </p> <p>If there are still issues, there are likely magnetic substances in your environment that are throwing off the magnetometer readings, and you should move your robot to a new environment if possible. If that is not possible, try removing possible sources of magnetic interference. </p> <p>After all that is done, we can finally run the actual code. This is thankfully much simpler. All you have to do is run: <pre><code>roslaunch ros_mapping takeoff.launch\n</code></pre></p>"},{"location":"reports/2024/StarshipClone/#description-of-the-primary-nodes","title":"Description of the primary nodes","text":"<ol> <li>gps_reader.py     Parses raw data from the configured GPS sensor and converts it to a more usable format, and publishes this data as a NavSatFix message to the topic /gps.</li> <li>mag_corrector.py     Parses raw data from the configured magnetometer and applies the hard and soft iron correction (as determined by the calibration process) to correct the data, and publishes it as a MagneticField message on the /imu/mag_corrected topic.</li> <li>navigate_to_destination.py     This node is where the graph logic is contained. It subscribes to the /gps and /imu/mag_corrected topics. The graph must be populated manually with the latitude and longitude values for each node, and the edges for each node must also be specified. The user can then set a start and end node and BFS will be used to determine the shortest navigation path between those nodes. This program constantly checks the GPS position of the robot and publishes the desired heading that must be maintained to reach the next node in the specified navigation path as a Float64 message to the /robot/bearing topic. Once the robot gets within a 3 meter distance of the location specified in a graph node, the program publishes the heading needed to travel to the next node in the path and so forth until the last node in the path is reached. </li> <li>mag_follower.py     This node subscribes to the /robot/bearing and /imu/mag_corrected topics and handles the navigation between graph nodes. Once the node receives a new target bearing from the /robot/bearing node it publishes to /cmd_vel to turn to that bearing and starts moving toward the next graph node. A PID control is used to ensure that the robot follows the desired direction, allowing it to correct itself when going off course due to bumpy segments of road.</li> </ol>"},{"location":"reports/2024/StarshipClone/#story-of-the-project","title":"Story of the project.","text":""},{"location":"reports/2024/StarshipClone/#how-it-unfolded","title":"How it unfolded","text":"<p>The initial motivation for the project was to better understand the types of algorithms and considerations used when programming an outdoor autonomously navigating robot by replicating the functionalities of a Starship delivery robot. </p> <p>At first we planned on combining the navigation of the robot with a web interface that could send \u201corders\u201d to the robot, telling it where to navigate, but we ultimately decided to focus solely on the navigation portion of the system, which had plenty of challenges to tackle.</p> <p>The first outlines of our project required a full map of campus which would make navigation much easier, assuming the GPS was accurate enough. However, we decided that this mapping process would take too much time and would be of questionable accuracy anyways due to our limited resources, so we pivoted to using waypoint-based localization that uses a graph populated with a few important gps locations and navigational data corresponding to each next node in the graph. While we were unsure about the accuracy of this solution at first, we quickly gained confidence in this method after correctly configuring the GPS sensor and verifying the accuracy of its readings. </p> <p>After deciding on waypoint-based navigation, we only needed to decide on a method of navigating the robot to and from each waypoint, as the robot wouldn\u2019t be localized while travelling between nodes in the graph. At first we considered using computer vision to detect where the sidewalks were, allowing it to simply follow the sidewalk until reaching the next node, however this proved to be difficult, so we pivoted to relying on magnetometer data which allows the robot to know its real bearing and ensuring that there are enough graph nodes such that a safe straight path can be made between any two nodes and using the bearing between such two nodes to inform the robot on what direction to travel. </p>"},{"location":"reports/2024/StarshipClone/#how-the-team-worked-together","title":"How the team worked together","text":"<p>Given that we have three members, we split up the project into its three main problems. Ephraim worked on designing the graph object that would serve as our waypoint-based \u2018map\u2019 and populating it with data.</p> <p>Artem worked on sensor fusion, and ensuring that the sensors we needed were calibrated correctly, giving accurate data, and ensuring that the readings were consistent across sensors giving similar types of data.</p> <p>Zach worked on the navigation algorithm that allows the robot to move safely from node to node without falling off of the sidewalk.</p> <p>Given that we each were working on fairly separate parts of the project, there was initially some difficulty combining our separate programs into one coherent system, but the topic-based messaging system facilitated by ROS made collaboration and the compilation of our separate work much easier. </p>"},{"location":"reports/2024/StarshipClone/#problems-that-were-solved-pivots-that-had-to-be-taken","title":"Problems that were solved, pivots that had to be taken","text":"<p>As mentioned in the previous section, there were a number of pivots that had to be made due to limitations of time, the technology, and our knowledge.</p> <p>First and foremost we decided early on to pivot from a full map of our demo area to a waypoint graph map due to the difficulty of getting an accurate map of campus. We wouldn\u2019t have been able to use the robot to make a map as the main distinction we needed was that between the sidewalks and grass, or other off road sections. Lidar isn\u2019t great for this as the grass is roughly the same height as the sidewalk (below the lidar placement). We considered using a camera-based SLAM mapping, which was discussed briefly in class, but quickly disposed of the idea due to struggles with computer vision techniques. We also looked into creating a map manually, but decided that it would take too much time to construct an accurate map \u2018by hand\u2019, which would take away from the programming aspects of the project. Because of these challenges we pivoted to the graph map that our final demo uses, which forces us to rely on other techniques to navigate the robot through the environment.</p> <p>An early strategy that we considered for navigating between nodes in the graph was to use computer vision to follow the sidewalk. The idea was to treat the sidewalk as a line, and use a line following algorithm to help the robot follow the sidewalk, and more importantly, ensure that it didn\u2019t go off road. However, the main roadblock in this strategy was the difficulty in configuring the HSV values to get an accurate mask that distinguished between grass and the sidewalk. The camera encoded the sidewalk pixels with a wide range of hue values, making it difficult to isolate the grey colors in the environment. Furthermore, there were a number of sections of sidewalk where the immediate off road was dirt instead of pavement, which was even harder to distinguish. An alternative solution to this navigation problem was to simply have enough graph nodes such that a safe straight-line path could be made between each node. We then used magnetometer data to ensure the robot followed the desired direction between nodes and could correct itself when getting offset by rough sections of road. </p> <p>There were also a number of problems related to sensor fusion, specifically with getting accurate values from the IMU. We initially wanted to fuse GPS coordinates with IMU and odometry data to make our position estimate more accurate, but upon testing we discovered that the IMU was giving inaccurate data, throwing off the fused measurements. When trying to fix the IMU, we found that the IMU combines 3 different sensors that are fused together into a single ROS topic. Ultimately we found that the magnetometer data was completely inaccurate, which proved to be the source of the IMU error. Once we successfully calibrated the magnetometer, we realized that we didn\u2019t need any of the other sensors, as the absolute heading that the magnetometer gave us could be combined with the absolute position of the GPS to give us a complete pose of the robot at any point. We did try to fuse the magnetometer data with the other IMU sensors, but it only added more inaccuracies, so we ended up filtering the magnetometer data, which made it quite accurate.</p> <p>Ultimately there were a number of challenges in completing a project outside of the lab, as the more diverse and variable conditions rendered many of the strategies we had previously learned ineffective or in need of modification. Despite this we learned a great deal about the inner workings of sensors and the importance of being able to adapt robotic algorithms to work within and benefit from the constraints of your target environment.</p>"},{"location":"reports/2024/WarehouseRobot/","title":"Warehouse Robot Navigation","text":"<p>This project implements an autonomous robot system for warehouse navigation using ROS (Robot Operating System). The robot is capable of detecting and following a colored line, identifying fiducial markers, and returning to its starting position. It uses state-driven logic, computer vision, and odometry to achieve its objectives.</p>"},{"location":"reports/2024/WarehouseRobot/#features","title":"Features","text":"<ul> <li>Line Following: Detect and follow an orange line using a camera.</li> <li>Fiducial Detection: Identify fiducial markers and approach them precisely.</li> <li>State-Based Navigation: Transition through multiple predefined states for flexible navigation.</li> <li>Return to Home: Navigate back to the starting position after completing tasks.</li> <li>Dynamic Speed Control: Adjust speed dynamically based on proximity to fiducials and line positions.</li> <li>User Interaction: Receive user input for mission control and state transitions.</li> </ul>"},{"location":"reports/2024/WarehouseRobot/#technical-description","title":"Technical Description","text":""},{"location":"reports/2024/WarehouseRobot/#high-level-design","title":"High-Level Design","text":"<p>The robot operates in multiple predefined states, managed by the <code>calculate_movement</code> function: - LINE_FOLLOWING: Follow the orange line. - APPROACHING: Move toward a fiducial marker. - RETURN_TO_LINE: Navigate back to a stored line position after fiducial interaction. - TURN_TO_HOME: Calculate and turn toward the starting position. - RETURNING: Follow the line back to the home position. - TURN_180: Perform a 180-degree turn to prepare for the next fiducial.</p> <p>State transitions are based on sensor inputs and environmental conditions.</p>"},{"location":"reports/2024/WarehouseRobot/#key-functional-modules","title":"Key Functional Modules","text":"<ol> <li> <p>Image Processing for Line Following:    <pre><code>hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\nmask = cv2.inRange(hsv, LOWER_ORANGE, UPPER_ORANGE)\nM = cv2.moments(largest_contour)\ncx = int(M['m10'] / M['m00'])\nerr = cx - w // 2\n</code></pre></p> </li> <li> <p>PID Control for Line Following:    <pre><code>derivative = (err - self.prev_err) / delta_time\nself.control_signal = (KP * err) + (KD * derivative)\nself.twist.angular.z = -self.control_signal\n</code></pre></p> </li> <li> <p>Fiducial Detection and Localization:    <pre><code>transform = self.tf_buffer.lookup_transform('base_link', f'pin_{self.target_fiducial}', rospy.Time(0))\nself.pin_position = transform.transform.translation\nself.distance_to_pin = math.sqrt(self.pin_position.x**2 + self.pin_position.y**2)\nself.angle_to_pin = math.atan2(self.pin_position.y, self.pin_position.x)\n</code></pre></p> </li> <li> <p>State-Based Movement:    <pre><code>dx = self.temp_line_position[0] - pos.x\ndy = self.temp_line_position[1] - pos.y\ndist_to_temp = math.sqrt(dx*dx + dy*dy)\nself.twist.linear.x = min(MAX_SPEED, dist_to_temp * 0.5)\n</code></pre></p> </li> <li> <p>Return to Home:    <pre><code>target_yaw = math.atan2(dy, dx)\nyaw_diff = self.normalize_angle(target_yaw - self.current_yaw)\nself.twist.angular.z = turn_speed if yaw_diff &gt; 0 else -turn_speed\n</code></pre></p> </li> </ol>"},{"location":"reports/2024/WarehouseRobot/#error-handling","title":"Error Handling","text":""},{"location":"reports/2024/WarehouseRobot/#fiducial-loss","title":"Fiducial Loss","text":"<pre><code>rospy.logwarn(\"Lost sight of fiducial during approach\")\n</code></pre>"},{"location":"reports/2024/WarehouseRobot/#odometry-failures","title":"Odometry Failures","text":"<pre><code>try:\n    odom = rospy.wait_for_message('/odom', Odometry, timeout=1.0)\nexcept rospy.ROSException:\n    rospy.logwarn(\"Failed to get current position\")\n</code></pre>"},{"location":"reports/2024/WarehouseRobot/#usage-instructions","title":"Usage Instructions","text":""},{"location":"reports/2024/WarehouseRobot/#prerequisites","title":"Prerequisites","text":"<ul> <li>Install ROS and ensure the required topics (/cmd_vel, /odom, /raspicam_node/image/compressed) are available.</li> <li>Ensure the robot's environment has an orange line and fiducial markers (IDs: 106, 100, 108).</li> </ul>"},{"location":"reports/2024/WarehouseRobot/#running-the-robot","title":"Running the Robot","text":"<ol> <li>Launch ROS and required nodes.</li> <li>Run the script:    <pre><code>rosrun &lt;package_name&gt; warehouse_robot.py\n</code></pre></li> <li>Input the target fiducial ID when prompted:    <pre><code>Enter the target fiducial ID: 106\n</code></pre></li> </ol>"},{"location":"reports/2024/WarehouseRobot/#user-interaction","title":"User Interaction","text":"<p>During fiducial interaction, you can: - Enter <code>continue</code> to proceed to the next task. - Enter <code>shutdown</code> to stop the robot.</p>"},{"location":"reports/2024/WarehouseRobot/#workflow","title":"Workflow","text":""},{"location":"reports/2024/WarehouseRobot/#initialization","title":"Initialization:","text":"<ul> <li>The robot stores its starting position and orientation from odometry.</li> <li>Awaits user input for the target fiducial ID.</li> </ul>"},{"location":"reports/2024/WarehouseRobot/#line-following","title":"Line Following:","text":"<ul> <li>Uses the camera to detect and follow the orange line.</li> </ul>"},{"location":"reports/2024/WarehouseRobot/#fiducial-interaction","title":"Fiducial Interaction:","text":"<ul> <li>Detects and approaches the fiducial marker.</li> <li>Pauses for user input to determine the next action.</li> </ul>"},{"location":"reports/2024/WarehouseRobot/#return-to-line","title":"Return to Line:","text":"<ul> <li>Returns to the stored line position after interacting with the fiducial.</li> </ul>"},{"location":"reports/2024/WarehouseRobot/#return-to-home","title":"Return to Home:","text":"<ul> <li>Follows the line back to the starting position.</li> </ul>"},{"location":"reports/2024/WarehouseRobot/#turn-180-degrees","title":"Turn 180 Degrees:","text":"<ul> <li>Performs a 180-degree turn to align with the next fiducial or complete operations.</li> </ul>"},{"location":"reports/2024/WarehouseRobot/#project-journey","title":"Project Journey","text":""},{"location":"reports/2024/WarehouseRobot/#development-evolution","title":"Development Evolution","text":"<p>The project underwent several iterations before arriving at its current form. Initially, the scope was different, but through various pivots and refinements, it evolved into a focused warehouse navigation solution. The most significant change came from reconsidering the initial project direction multiple times to find the most practical and achievable approach.</p>"},{"location":"reports/2024/WarehouseRobot/#technical-challenges-solutions","title":"Technical Challenges &amp; Solutions","text":""},{"location":"reports/2024/WarehouseRobot/#key-challenges-overcome","title":"Key Challenges Overcome:","text":"<ol> <li> <p>Home Position Tracking: A critical breakthrough came in solving the issue of saving and utilizing the home yaw orientation. This was essential for accurate return navigation and required careful consideration of coordinate systems and orientation tracking.</p> </li> <li> <p>Fiducial Approach Logic: One of the more complex challenges was developing the logic for the robot to properly detach from the line when approaching fiducials. This required precise distance calculations and smooth transition states.</p> </li> <li> <p>Integration Complexities: While the original plan included integration with Rafael, technical constraints led to focusing solely on Roba for execution. This taught valuable lessons about scope management and adapting to technical limitations.</p> </li> </ol>"},{"location":"reports/2024/WarehouseRobot/#development-process","title":"Development Process","text":"<p>The project's success relied heavily on: - Extensive testing cycles with multiple test cases - Dedicated debugging sessions, often spanning many hours - Iterative code refinement through hands-on testing - Continuous documentation and code review</p>"},{"location":"reports/2024/WarehouseRobot/#lessons-learned","title":"Lessons Learned","text":"<ol> <li> <p>Scope Management: While the initial vision was more extensive, learning to adapt and focus on core functionality proved crucial for delivering a working solution.</p> </li> <li> <p>Testing Importance: The numerous hours spent on debugging and testing highlighted the importance of thorough testing methodologies in robotics development.</p> </li> <li> <p>Future Improvements: Areas identified for potential enhancement include:</p> </li> <li>Integration with additional robot platforms</li> <li>More sophisticated path planning algorithms</li> <li>Enhanced error recovery mechanisms</li> </ol>"},{"location":"reports/2024/WarehouseRobot/#time-investment","title":"Time Investment","text":"<p>The project required significant time investment, particularly in: - Debugging and testing sessions - Implementing and refining core functionalities - Documentation and code organization - System integration testing</p> <p>This extensive testing and debugging phase, while time-consuming, was crucial for ensuring reliable robot behavior and robust navigation capabilities.</p>"},{"location":"reports/2024/WarehouseRobot/#contributors","title":"Contributors","text":"<p>Eric Hurchey (erichurchey@brandeis.edu)</p>"},{"location":"reports/2024/counterstrikebot/","title":"Counter-Strike Bot","text":"<p>Github repo</p> <p>Harry Yu, Zared Cohen, TsonOn Kwok</p>"},{"location":"reports/2024/counterstrikebot/#introduction","title":"Introduction","text":"<p>Our goal was to create a robot simulation of the popular game franchise, Counter Strike, which is a first-person shooter game where teams of terrorists (Ts) and counter-terrorists (CTs) fight for their specific goals. The Ts must plant a bomb and defend it until detonation, and the CTs must eliminate all Ts or defuse the bomb if planted. If the round time runs out before either goal occurs, the CTs will be victorious. For our implementation, we created a 2v2 version of this game in a simulated environment (gazebo), and a 1v1 version in the real world. We also designed different personalities and guns for each robot (player), which provides a more realistic simulation of the game with an unpredictable outcome.</p>"},{"location":"reports/2024/counterstrikebot/#problem-statement-objectives","title":"Problem statement (objectives)","text":"<ul> <li>Robots can detect enemy teams and shoot </li> <li>Robots can successfully navigate the course and avoid obstacles/other robots</li> <li>Server node manages player health, game state, bomb site locations</li> <li>Communication between local (robot) nodes and server node </li> </ul>"},{"location":"reports/2024/counterstrikebot/#what-was-created","title":"What was created","text":"<ol> <li>There are a few groups of tasks we had to work on: the gazebo world simulation</li> <li>The creation of the map using SLAM, how bomb site locations, patrol points and spawn points are defined. This must be done manually.</li> <li>AMCL that allows the robots to navigate and avoid obstacles</li> <li>Personalities of robots, robot actions when situations change using FSMs (very complicated)</li> <li>Modularized launch files to be used when needed</li> </ol>"},{"location":"reports/2024/counterstrikebot/#files-overview","title":"Files Overview","text":"File/Component Description world3.world A gazebo world map of a simplified version of Inferno A site in Counter-Strike. publish_point.py Tool for generating map configuration points. Subscribes to /clicked_point. Records clicked points in RViz for map setup, saves spawn points and bomb site locations to YAML config file, launches map server and RViz for point selection. Used during the map setup/configuration phase. map_manager.py Loads and manages map configuration from CSV files. Node: map_manager. Publishes to /game/map_markers and /game/map. Handles bomb site locations and spawn points. Publishes visualization markers for map features. Creates RViz markers for bomb sites and spawn locations. Manages map coordinate system and point conversions. game_manager.py The central 'server' node that manages the game state. Handles round timers, bomb events, and robot health. Publishes game state updates to all robots. Manages combat events and damage calculations, tracks dead players and round winners. Controls round phases (PREP, ACTIVE, BOMB_PLANTED). robot_controller.py Base class for all robot behaviors. Handles movement, combat, and state management. Processes visual detection of enemies. Manages navigation and pathfinding. Interfaces with ROS navigation stack. Parent class for personality-specific behaviors. CT_normie.py, T_aggressive.py, etc. Child classes of <code>robot_controller.py</code>. Inherits all combat management and specific behavior, and uses FSMs to manage different states for different behaviors and teams. gun.py Defines weapon types and their characteristics (rifle, sniper, SMG). Manages weapon properties like damage, fire rate, and accuracy. Handles ammo management and reload timers. Provides shooting mechanics with accuracy calculations. Returns damage values based on successful hits. <p>Topics published:</p> File/Component Type Topic map_manager.py Publishes /game/map_markers Publishes /game/map game_manager.py Publishes /game/state Subscribes /game/robot_states Subscribes /game/shoots Subscribes /game/bomb_events robot_controller.py Publishes /game/robot_states Publishes /{robot_name}/cmd_vel Publishes /game/shoot"},{"location":"reports/2024/counterstrikebot/#how-to-run-the-game-in-gazebo","title":"How to Run the Game (in Gazebo)","text":""},{"location":"reports/2024/counterstrikebot/#building-the-environment","title":"Building the Environment","text":"<ol> <li>Ensure the <code>transitions</code> library is installed:    <pre><code>pip3 install transitions\npip3 install pandas\n</code></pre></li> <li>Navigate to your catkin workspace:    <pre><code>cd &lt;your-catkin-workspace&gt;\n</code></pre></li> <li>Build the workspace:    <pre><code>catkin_make\n</code></pre>    If no error messages appear, proceed to the next step.</li> </ol>"},{"location":"reports/2024/counterstrikebot/#generating-the-map","title":"Generating the Map","text":"<p>We need a robot to explore the map using SLAM to manually generate the world map for later use.</p> <ol> <li> <p>Launch SLAM using <code>gmapping</code> by running the following command:    <pre><code>roslaunch turtlebot3_slam turtlebot3_slam.launch slam_methods:=gmapping\n</code></pre></p> </li> <li> <p>Open a new terminal and control the robot with the keyboard:    <pre><code>roslaunch turtlebot3_teleop turtlebot3_teleop_key.launch\n</code></pre>    Drive the robot around the target area to scan its surroundings.</p> </li> <li> <p>Once the entire target area has been scanned, open another terminal and save the map:    <pre><code>rosrun map_server map_saver -f `rospack find cs_bot`/maps/&lt;map_name&gt;\n</code></pre>    This will generate two files: <code>&lt;map_name&gt;.yaml</code> and <code>&lt;map_name&gt;.pgm</code> in the <code>cs_bot/maps</code> directory.</p> </li> </ol>"},{"location":"reports/2024/counterstrikebot/#defining-areas","title":"Defining Areas","text":"<p>We will define the bomb sites using our script and <code>rviz</code>.</p> <ol> <li> <p>Run the following command:    <pre><code>rosrun cs_bot publish_point.py\n</code></pre></p> </li> <li> <p>Click on the map according to the prompt. The results will be saved to <code>/config/points.yaml</code>.</p> </li> </ol> <p>Then run <code>rosrun cs_bot launch.py</code> to load the yaml file into the launch file.  The coordinates saved in <code>/config/points.yaml</code> will also be used by other nodes later.</p> <p></p>"},{"location":"reports/2024/counterstrikebot/#launch-game","title":"Launch Game","text":"<p>Run the game by launching sim_2v2.launch, which launches all the nodes required including launching 4 robots and starting the game server node: <pre><code>roslaunch cs_bot sim_2v2.launch\n</code></pre> This launch file calls a few other launch files and sets up a 2v2 environment. </p> <p>Below is an image of the game running, with the T robots moving to site and planting, and the CT robots defending and dealing damage to robb(T) bot and a UI interface written in Qt.  Here is the multi-robot AMCL working: </p>"},{"location":"reports/2024/counterstrikebot/#story-of-the-project","title":"Story of the Project:","text":"<p>This was an extremely ambitious and challenging project that required the team to pivot multiple times to achieve successful completion. Our initial concept was to create a complex 2v2 robot gunfight game, with a server node managing critical elements such as health, player location, game state, round timer, alongside player-side hit detection to simulate real-world game mechanics. While the idea appeared straightforward in theory, implementing server-based gunfight mechanics proved unreliable, with inaccuracies in hit detection leading to significant challenges. This prompted our first pivot: shifting from server-based hit detection to relying solely on the shooter robot\u2019s perception of its enemy for accuracy.</p> <p>Implementing shooting mechanics was another major hurdle. The robots' cameras were mounted on their undersides, which complicated aiming while moving. Adjustments required the gun's angle to align dynamically with the target, introducing numerous variables, such as gun_angle, enemy_to_center_angle (calculated using OpenCV), and current_speed. These complexities made fine-tuning exponentially harder. To address this, we simplified the shooting mechanic by fixing the gun\u2019s aim to the robot\u2019s center of view. While this reduced the computational burden, it required robots to remain stationary while shooting\u2014a limitation reminiscent of the strategic elements in the original Counter-Strike game.</p> <p>Despite delays in real-life deployment, we believe the project achieved a high success rate, and the demo's performance exceeded our expectations. Throughout this journey, we gained valuable insights into SLAM mapping, the nuances of AMCL, the limitations of the ROS system, server-client communications, multiple complex FSM implementation for different player personalities, server-side data management and resource allocation, defining and building custom message types, reading and writing to launch files directly, fine-tuning move_base parameters, optimizing color detection, and managing Gazebo physics challenges. However, the most significant skill we developed was debugging and problem-solving. Debugging accounted for approximately 70% of our total project time due to the ambitious nature of the project and the meticulous implementation of its various components. We wish for more time to work on the project to make it perfect, but we are satisfied of what we created as is.</p>"},{"location":"reports/2024/readme/","title":"This directory contains the final reports for Cosi2019 - Autonomous Robotics - 2024","text":"Project Name Brief Description Teammate 1 Teammate 2 Teammate 3 Tom and Jerry: The Cheese Napping A game of tag played by 2 robots where one has to find and grab a block (using claw), while the other has to chase it. Vedanshi Shah Parthiv Ganguly Red Object Picker Picking up red objects (ie. a red foam block) against a black background with the use of ML Sonny George Alex Danilkovas Starship Bot Clone Create delivery bot that can navigate outside, with the ability to bring an item to the person who requested it. Artem Lavrov Zach Hovatter Ephraim Zimmerman Precision Agriculture Robot Turtlebot Spraying Water Based on Color Detection Ming-Shih Wang Jiahao Li Dungeons and Robot Robot explore map and interact with objects according to given mission Yutian (Tim) Fan Smart Cleaning Bot with Voice Control Smart Cleaning Robot: Autonomous Indoor Mapping and Coverage Path Planning for a ROS-Based Cleaning Robot with Voice Control Functions Zhenxu Chen Pang Liu Rights of Robot Robot makes decision with rights of way according to the encountering robot type. Haochen Lin Zixin Jiang Autonomous Taxi Sim Multiple bot moving in a map, picking up \"passengers\" and drop them at their destination. The Robots should also automatically deal with signal &amp; Stop sign. Chao An Hide and Seek 2 robots will go hide, and a 3rd robot will go find them Jungju Lee Daphne Pissios Chloe Wahl-Dassule Micro Mouse 2 A robot maps out a non-trivial maze and learns to solve it in the fastest time possible Sam Herman Bot Follower/Mapper It is a robot that is similar to the Turtle Bot in Gazebo where the robot will follow you based on where you are in radius to it and at the same time, it will use Slam to try to have a brief map of its environment. Eric Hurchey Myself Counter Strike Bot 2v2 robot game of shooting opponents and planting/defusing a digital bomb in the bomb site (they dont strike counters) Zared Cohen Harry Yu TsunOn Kwok Gesture Control Bot Controlling a bot using hand gestures Leo Gao Jeffrey Wang Micro Mouse 1 A robot travels through an unseen maze and finish the maze in the shortest time Ben Zhang"},{"location":"reports/2024/rl-trained_object_picker/","title":"RL-Trained Object Picker","text":"<ul> <li>Sonny George &amp; Alex Danilkovas</li> <li>Dec 10 2024</li> <li>ROS version: Noetic</li> </ul> <p>Please see this youtube video for a detailed walkthrough of the code.</p>"},{"location":"reports/2024/rl-trained_object_picker/#problem-statement","title":"Problem Statement","text":"<p>Using Gazebo simulations, can we prototype an object-picking reinforcement learning loop for a simple task that generalizes to the real px100 robot arm?</p>"},{"location":"reports/2024/rl-trained_object_picker/#motivation","title":"Motivation","text":"<p>The intersection of machine learning and robotic control is a notoriously difficult problem space. Unlike most machine learning problems that require one inference at a time, robotic control is a sequential decision-making problem where action taken not only influences the immediate outcomes, but also affects future states, creating a complex feedback loop. Furthermore, every action (control) needs to be a function of high-dimensional \"real world\" data sources that are often noisy, incomplete, and non-deterministic. Also, what characterizes a \"good\" action? Most machine learning applications are self-supervised and have data to learn from. However, what is the data in robotics?</p>"},{"location":"reports/2024/rl-trained_object_picker/#reinforcement-learning-imitation-learning","title":"Reinforcement Learning &amp; Imitation Learning","text":"<p>Much of the research in this are has focused on two paradigms: reinforcement learning (RL) and imitation learning (IL). In RL, the agent learns to interact with the environment by taking actions and observing the rewards it receives. In IL, the agent learns to mimic the behavior of an expert by observing the expert's actions. Often these methods are used in conjunction to allow a robot to learn from demonstration (IL), and then refine its policy using RL to learn how to make its policy robust to perturbations.</p>"},{"location":"reports/2024/rl-trained_object_picker/#so-when-is-reinforcment-learning-actually-useful-in-robotics","title":"So When is Reinforcment Learning Actually Useful in Robotics?","text":"<p>For tasks like picking up objects that are always placed in the same location (what we are doing), reinforcement learning is not the most efficient way to solve the problem. In these cases, a rule-based system with something like PID control to ensure precise movements is recommended.</p> <p>Reinforcement learning becomes useful when the task is complex and the environment is non-deterministic in such a way that a rule-based system would be too complex to design. However, the tradeoff is that RL requires a lot of data and time to train, and the policy learned may not be interpretable or generalizable to other tasks. Furthermore, (as we have learned in our project) it is not always guaranteed to work well within the time and compute restraints of the real-world.</p>"},{"location":"reports/2024/rl-trained_object_picker/#relevant-literature","title":"Relevant Literature","text":"<ul> <li>For a primer on reinforcement and imitation learning in robotics, see this FAQ.</li> <li>For a primer on the Python <code>gymnasium</code> package, see this FAQ.</li> <li>For the original papers proposing the learning algorithms we use, see: SAC, PPO, Behavioral Cloning.</li> </ul>"},{"location":"reports/2024/rl-trained_object_picker/#codebase-details","title":"Codebase Details","text":"<p>For our project, we created a ROS package called <code>object-picker</code> with code that, for the Interbotix px100 robot arm:</p> <ol> <li>Uses reinforcement learning (either PPO or SAC learning algorithms) to train a multi-layer perceptron policy to pick up a 'T'-shaped object</li> <li>Optionally, initialize the policy with imitation learning (behavioral cloning)</li> <li>Runs trained policies on the real px100 robot arm</li> <li>Contains pre-trained models (policies) from our training experiments</li> </ol>"},{"location":"reports/2024/rl-trained_object_picker/#code-structure","title":"Code Structure","text":"<p>Our repository is a <code>catkin</code> workspace containing a <code>src</code> directory with the <code>object-picker</code> package.</p> <p>The <code>src</code> folder of this package (<code>object-picker</code>) is structured as follows:</p> <pre><code>\ud83d\udcc1 src\n\u251c\u2500\u2500 \ud83d\udcc1 models    # our trained models\n\u251c\u2500\u2500 \ud83d\udcc4 config.py # global constants and parameters\n\u251c\u2500\u2500 \ud83d\udcc4 env.py    # training-env code (reward logic &amp; training primitives)\n\u251c\u2500\u2500 \ud83d\udcc4 gazebo.py # code handling topic and service comms with gazebo sim\n\u251c\u2500\u2500 \ud83d\udcc4 run.py    # loads and runs policies on real px100\n\u251c\u2500\u2500 \ud83d\udcc4 train.py  # main entrypoint for training\n\u2514\u2500\u2500 \ud83d\udcc4 utils.py  # generic helper functions\n</code></pre>"},{"location":"reports/2024/rl-trained_object_picker/#how-to-run-the-code","title":"How to Run the Code","text":"<p>\u2139\ufe0f Run training: To run the training script with the parameters specified in the <code>if __name__ == '__main__':</code> block, you must:</p> <ol> <li>Launch ROS with:     <pre><code>roslaunch\n</code></pre></li> <li>Start the Gazebo simulation with:     <pre><code>roslaunch interbotix_xsarm_gazebo xsarm_gazebo.launch robot_model:=px100 use_position_controllers:=true gui:=false use_rviz:=true\n</code></pre>     Or (depending on whether you want the Gazebo GUI or RViz to open)     <pre><code>roslaunch interbotix_xsarm_gazebo xsarm_gazebo.launch robot_model:=px100 use_position_controllers:=true gui:=true\n</code></pre></li> <li>Start the training script with:     <pre><code>rosrun object-picker train.py\n</code></pre>     Or, <code>cd</code> into the <code>src</code> directory and run:     <pre><code>python train.py\n</code></pre></li> </ol> <p>\u2139\ufe0f Run real px100: To run the trained models on the real px100 robot arm:</p> <ol> <li>Run:     <pre><code>rosrun object-picker run.py\n</code></pre>     Or, <code>cd</code> into the <code>src</code> directory and run:     <pre><code>python run.py\n</code></pre></li> </ol>"},{"location":"reports/2024/rl-trained_object_picker/#nodes-created","title":"Nodes Created","text":"Node Created Function <code>px100-training</code> Orchestrate policy training by:1. subscribing to the joint-state topics2. publishing to joint-control topics"},{"location":"reports/2024/rl-trained_object_picker/#topics-used","title":"Topics Used","text":"Topics and Their Messages Function <code>/px100/waist_controller/state</code> publish waist position <code>/px100/waist_controller/command</code> receive target waist position commands <code>/px100/shoulder_controller/state</code> publish shoulder position <code>/px100/shoulder_controller/command</code> receive target shoulder position commands <code>/px100/elbow_controller/state</code> publish elbow position <code>/px100/elbow_controller/command</code> receive target elbow position commands <code>/px100/wrist_angle_controller/state</code> publish wrist angle position <code>/px100/wrist_angle_controller/command</code> receive target wrist angle position commands <code>/px100/right_finger_controller/state</code> publish right finger position <code>/px100/right_finger_controller/command</code> receive target right finger position commands <code>/px100/left_finger_controller/state</code> publish right finger position <code>/px100/left_finger_controller/command</code> receive target right finger position commands"},{"location":"reports/2024/rl-trained_object_picker/#technical-details-discussion","title":"Technical Details &amp; Discussion","text":""},{"location":"reports/2024/rl-trained_object_picker/#observation-action-space","title":"Observation &amp; Action Space","text":"<p>Typically, a robot arm policy would observe:</p> <ol> <li>Joint positions (sine and cosine of joint angles)</li> <li>Joint velocities (joint angle derivatives) of all actuated joints</li> <li>Joint torques from the last time step</li> <li>Other task-specific signals</li> </ol> <p>And output joint torques as actions.</p> <p>Given our time constraints and levels of expertise, the only control interface we could reliably get to work for both the Gazebo-simulated px100 and real px100 was joint position control (and even then, the left finger didn't publish position state). That is, an API that allows for the setting and reading of joint positions.</p> <p>To retrofit this higher level of abstraction to something more akin to what is typical for trained robot-arm policies, we wrote code that (1) takes a change in joint position (positive or negative value within some small range) and then, (2) by adding this change to the current joint positions, sets the new joint positions--making our control interface analogous to <code>cmd_vel</code>, whose input space is a target velocity (position over time).</p> <p>Altogether, our simplified observation space was only the joint positions, and our action space were these target changes in joint positions per step (a velocity).</p> <p>Since the under-the-hood joint position controller uses PID, we had to consider the tradeoff between wait-time per step and movement precision. We could wait multiple seconds for every tiny step of movement--allowing the PID to steer the joint to the precise destinition--or, we could move quickly with less precision. Of course, it wouldn't make for much of a demo if the robot arm took 10 minutes to pick up an object, so we opted for low-precision movements. Of course, having a highly non-deterministic simulation--where controls are capable of producing high-variance outcomes--makes the training task much more difficult.</p>"},{"location":"reports/2024/rl-trained_object_picker/#reward-function","title":"Reward Function","text":"<p>Originally, our reward function included the following two terms:</p> <ol> <li>'Assuming lifting position' term The negative of the distance between the gripper and the object.</li> <li>'Lifting the object' term: The negative of the distance between the object and the goal position (raised in the air).</li> </ol> <p>However, having little success at encouraging the arm to approach the object in a way that would permit it to be picked up, we changed the reward function to be stateful as follows:</p> <ol> <li>'Assuming lifting position' term term:<ul> <li>'Get low' term: Z-value of gripper</li> <li>'Get close' term: Only once the gripper is low (and not before), the negative of the distance between the gripper and the object (since the forklift must get low before it can insert).<ul> <li>Once the gripper is both low and close (and not before), the 'get low' term is removed and replaced with a constant <code>1.0</code> (to prevent the 'get low' term from discouraging the lifting up of the object).</li> </ul> </li> </ul> </li> <li>'Lifting the object' term: (unchanged)</li> </ol>"},{"location":"reports/2024/rl-trained_object_picker/#learning-algorithms","title":"Learning Algorithms","text":"<p>While we originall were using SAC (Soft Actor-Critic) for training, we switched to PPO (Proximal Policy Optimization) because we believed it could be more stable without tuning hyperparameters. The only hyperparameter we modified (from the defaults in the <code>stable-baselines3</code> library) was the number of steps per epoch (from 2048 to 4096).</p> <p>Finally, we used Behavioral Cloning to initialize the policy by learning from single successful demonstration of the task. We hard-coded this demonstration (in the <code>config.py</code> file) and used it to train the policy for 100 epochs.</p>"},{"location":"reports/2024/rl-trained_object_picker/#our-results","title":"Our Results","text":""},{"location":"reports/2024/rl-trained_object_picker/#1-generation-one-sac","title":"1\ufe0f\u20e3 Generation One (SAC)","text":"<p>For our first generation, we used SAC to train the policy. We our older reward function and ran it for 670,000 steps. Unfortunately, this policy, although it would often get close to the object, struggled to get underneath the object in a way that could pick it up. This motivated us to change our reward function as previously described.</p> <p></p>"},{"location":"reports/2024/rl-trained_object_picker/#2-generation-two-ppo","title":"2\ufe0f\u20e3 Generation Two (PPO)","text":"<p>For our second generation, we used PPO to train the policy. We used our new reward function and ran it for 270,000steps.</p> <p>Here is what it looked like at the 130,000th step:</p> <p></p> <p>Obviously, it learned to \"get low\" (satisfying this aspect of the reward function). However, it is clearly struggling to approach the object once low.</p> <p>After 270,000 steps, it is not much better (arguably worse):</p> <p></p>"},{"location":"reports/2024/rl-trained_object_picker/#3-generation-three-ppo-behavioral-cloning","title":"3\ufe0f\u20e3 Generation Three (PPO + Behavioral Cloning)","text":"<p>Finally, once we introduce behavioral cloning, where we begin the learning process by training the policy on a single successful demonstration of the task before kicking off the PPO process, we see that a much more successful policy is achieved after only 40,000 PPO steps (approximately 1,400 episodes):</p> <p></p> <p>The following is a plot of the average reward per episode over the course of PPO training. As we can see, PPO is indeed increasing the average reward per episode over time.</p> <p></p> <p>Noteably, there is still a lot of variance, with a dense clusters around 0.0 and -0.5. There is a simple explanation to this. Because of the low-precision movements explained in the observation and action space section, much of the time, these unavoidable imprecisions result in a light bump to the object that topple it over. When this occurs, the episode's average reward sufferes from never receiving reward for the lifting of the object toward the target position. Crucially however, we see that the PPO is gradually learning to output controls that minimize the chance of this occuring.</p> <p>On the real robot, this learned policy does indeed pickup a 'T'-shaped object. However, there definitely seem to be slight discrepancies between the simulation and the real world (notice the nose angle of the gripper).</p> <p></p>"},{"location":"reports/2024/rl-trained_object_picker/#project-journey","title":"Project Journey","text":"<p>The journey for this project was laborious and educational. The PX100 robotic arm is unlike any other robot in the lab\u2014it\u2019s immobile and plugged into a desktop computer. Naturally, the first step was to get the arm booted and working.  </p> <p>When we powered up the computer, we were greeted by an Ubuntu 18.04 interface, without ROS installed. This immediately seemed off, as the arm had been used just last semester. Furthermore, the device wasn\u2019t set up with internet connectivity.  </p> <p>Fortunately, we resolved these issues within a couple of days and officially got started. Our first priority was to familiarize ourselves with the arm. To do this, we reviewed past projects and experimented with an API (<code>brl-pxh-api</code>) created by a previous student in the course.  </p> <p>While the API was well-designed, we discovered it was incompatible with Gazebo. It functioned only on the physical robot, so we needed to dive deeper. </p> <p>Next, we researched which Reinforcement Learning (RL) algorithm would best suit our task. After some deliberation, we selected Soft Actor-Critic (SAC) as our starting point. We chose SAC for its sample efficiency and its ability to handle sparse rewards effectively. Around this time, we also explored how others had implemented RL with Gazebo. This led us to a small but promising repository, ros-gazebo-gym. Coincidentally, its sample project also used SAC as the model of choice, which reinforced our decision.  </p> <p>Before diving too tackling the entire learning pipeline, we focused on getting the arm moving in simulation as expected. As explained further in explained in the observation and action space section, we realized that we couldn\u2019t control it the typical RL way\u2014by outputting torque to each servo. Instead, we had to rely on setting angular positions for each servo. This led to the tradeoff between precision and speed, which we discussed in the same section.</p> <p>Once we had control of the robot in simulation, we started by (1) digging our teeth into the learning pipeline--learning as much as we could from ros-gazebo-gym--and (2) implementing our own solution. We started navigating through the aforementioned example and tracing how it ran. Although ros-gazebo-gym contained far too many unnecessary and buggy features for our project, we were able to learn the basic abstraction associated with the (gymnasium)[https://github.com/Farama-Foundation/Gymnasium] framework. We came to know exactly what we needed in our solution and what we did not. After this, multiple 10+ hour days were spent writing out the training script\u2014which rather than just a simple training script turned out to be an end-to-end training pipeline using Gazebo, Gymnasium, ROS, and Stable-Baselines3, with different modules of abstraction for modeling and manipulating the simulated environment, modeling and manipulating the PX100, as well as handling the training and reward of the task at hand.</p> <p>Eventually we were able to get some sort of training running, and boy did it feel great seeing the robotic arm slowly make its way towards the red ball it wanted to pick up. However, our work was very far from done.</p> <p>Next came optimization, tuning, and debugging\u2014all of course paired with lots of experimentation. </p> <p>First matter of optimization was to increase the speed at which the simulation ran, because currently it ran at about 0.9 times real time speed. It would take far too long to train a model at this rate. Messing with the many Interbotix world-launch files, we were able to speed up the simulation to about 3x real-time speed. These speeds still weren't ideal, but still a great improvement. A little while later we were graciously granted access to a VNC much more powerful than the PC tower we were currently performing our training on. This not only helped us train at a rate of about 10x real time, but also allowed for us to test and run multiple training sessions at the same time.</p> <p>In terms of tuning, we messed with how much each joint could move at maximum in a single step, as well as joint limits, and amount of steps per episode. One crucial finding was that our original budget of 15 steps per episode was too low for the robot to realistically pick up the object.</p> <p>We adjusted and revamped the reward function many times. It started out as a simple linear 2 part reward, which then eventually transitioned into a non linear reward, which then after many more iterations became a reward with states. The results of this is described further in the reward section. All of this was done to help guide the algorithm into succeeding at the task we had given it. </p> <p>However, despite all of that all it seemed the arm ever did was get close to the block/sphere, push it around sometimes maybe, or, after enough training, it would put its fingers around the block seemingly attempt to lift it but the block would stay on the ground. This baffled us and we eventually decided to test if it was actually possible to pick up the block in simulation\u2014something we didn't even think to test beforehand (definitely an oversight on our part). </p> <p>Our findings were that while yes it was still possible, it was very difficult and there was very little room for error, way less than in real life. We found it to be an issue of pressure as well as the way the Gazebo physics engine modeled friction and other objects, which made it very difficult to do these kinds of things. There were a few things we could try, but time was starting to become a worry and we opted to change the object and remove the gripper friction issue altogether. We changed from a block to a T shape as something that the grippers would go under and scoop to pick up (like a forklift).</p> <p>Finally, we started experimenting with other algorithms as well, such as Proximal Policy Optimization (PPO) as well behavior cloning in hopes of making it easier for it to find the movements which correlate with picking up the object. Luckily, we did experience observable and empiral positive results with the combination of PPO and behavioral cloning, as described in the results section.</p> <p>Altogether, this project was profoundly challenging and deeply rewarding, requiring perseverance, creativity, and critical problem-solving. Navigating the labyrinth of technical hurdles, from setting up the hardware and software environment to designing a complex training pipeline, taught us valuable lessons in collaboration and problem solving. While the arm\u2019s ultimate mastery of the task may have been tempered by the limitations of the simulation and control interface for the robot, the journey was a rich and rewarding experience that we will carry with us into future projects.</p>"},{"location":"reports/2024/tom_and_jerry/","title":"Tom and Jerry","text":"<p>Github Repo</p>"},{"location":"reports/2024/tom_and_jerry/#tom-and-jerry-the-cheese-napping","title":"Tom and Jerry: The Cheese Napping","text":""},{"location":"reports/2024/tom_and_jerry/#tom-and-jerry-the-cheese-napping_1","title":"Tom and Jerry: The Cheese Napping","text":"<p>FINAL REPORT </p> <p>Vedanshi Shah - vedanshi@brandeis.edu Parthiv Ganguly - parthivganguly@brandeis.edu</p>"},{"location":"reports/2024/tom_and_jerry/#introduction","title":"Introduction","text":"<p>Problem statement, including original objectives </p> <p>Problem Statement: Develop a system where two robots, Tom and Jerry, engage in a game of tag, with Jerry navigating obstacles and retrieving a colored block while Tom chases and attempts to tag Jerry.  </p> <p>Our project centers on creating an interactive robotics game inspired by the classic dynamics of Tom and Jerry. In this game, the \"Jerry\" robot must navigate through an obstacle-filled environment to reach a preselected colored block at the goal. Meanwhile, the \"Tom\" robot is tasked with chasing and attempting to \"tag\" Jerry by either colliding or getting within close proximity. The key challenges include obstacle avoidance, multi-robot communication, and efficient target tracking.  </p> <p>The original objectives were:  </p> <ol> <li>Block detection  </li> <li>Block pickup  </li> <li>Navigating to the end wall while avoiding obstacles  </li> <li>Programming the Tom robot for the chasing functionality  </li> <li>Implementing obstacles  </li> <li>Robot communication  </li> </ol>"},{"location":"reports/2024/tom_and_jerry/#relevant-literature","title":"Relevant Literature","text":"<p>The following resources guided our design and implementation:  </p> <ul> <li>Model creation:   Techniques for setting up simulation environments and robot configurations  </li> <li>https://www.youtube.com/watch?v=YV8hlpBOhtw </li> <li> <p>https://www.youtube.com/watch?v=wUZO4wTvKCY </p> </li> <li> <p>Action lib:   Implementing action clients for autonomous behaviors  </p> </li> <li> <p>http://wiki.ros.org/actionlib </p> </li> <li> <p>HSV Color Detection:   Understanding hue, saturation, and value for color-based object detection  </p> </li> <li> <p>https://learn.leighcotnoir.com/artspeak/elements-color/hue-value-saturation/ </p> </li> <li> <p>Object and Color Detection:   Real-time color recognition and object localization  </p> </li> <li>https://www.geeksforgeeks.org/detect-an-object-with-opencv-python/ </li> <li> <p>https://www.geeksforgeeks.org/multiple-color-detection-in-real-time-using-python-opencv/ </p> </li> <li> <p>Follow-the-Gap Algorithm: </p> </li> <li> <p>https://www.sciencedirect.com/science/article/pii/S0921889012000838# </p> </li> <li> <p>Anti-Gravity Obstacle Avoidance: </p> </li> <li>https://www.cse.chalmers.se/~bergert/robowiki-mirror/RoboWiki/robowiki.net/wiki/Anti-Gravity_Movement.html </li> </ul>"},{"location":"reports/2024/tom_and_jerry/#what-was-created","title":"What Was Created","text":"<p>Technical descriptions, illustrations </p> <p> </p> <p>There are different goal states/milestones. The robots are started at different set distances, there is around a 1m gap between the two robots. There is an obstacle section with the red brick walls as the obstacles. Finally, there is an end stage with the goal of reaching the pre-chosen colored block.  </p> <p> </p> <p>We worked on getting the robots on the same <code>roscore</code> and then trying to figure out how to use the information for one to chase the other \u2192 what would be involved with that and decided to create the following functionality using the odoms of the robots.  </p>"},{"location":"reports/2024/tom_and_jerry/#discussion-of-interesting-algorithms-modules-techniques","title":"Discussion of Interesting Algorithms, Modules, Techniques","text":"<p>Follow-the-Gap Method Our first order of business for both the Tom and Jerry robots was writing code that allowed them to navigate to their goal coordinate while avoiding obstacles. One method we found online to do this was the Follow-the-Gap Method (FGM). This algorithm uses LIDAR data to find the widest gap between obstacles, and then follows that gap. This code worked well with point obstacles but did poorly with longer obstacles. So, we switched to a different algorithm which used gravity-based obstacle avoidance.  </p> <p>Gravity Obstacle Avoidance Algorithm Our next obstacle avoidance algorithm was based on the \u201cgravity/anti-gravity\u201d method. The gravity algorithm makes the target coordinate have an attractive gravitational force, and it makes obstacles detected by LIDAR have a repulsive gravitational force. The robot then adds the gravity force vectors to get the movement vector of the robot. We started using this algorithm, but it didn\u2019t do well in narrow corridors or when confronted with large surface obstacles. So, we decided to write our own algorithm for avoiding obstacles.  </p> <p>Custom Obstacle-Avoidance Algorithm Finally, we created our own version of an obstacle avoidance method by combining the previous two algorithms. The robot divides the region around it into 7 different zones, each with an associated cost based on how far it is (in terms of angular difference) from the front zone. It then stores all the LIDAR values in each zone that are below a threshold. It then finds the most \u201cobstacle-free\u201d zone (i.e., the zone with no LIDAR values below the threshold or the farthest value) with the lowest cost. This is the zone that the robot should point towards to avoid the obstacle. To do so, the robot slightly rotates backwards to point in this new direction, and then continues following its goal.  </p> <p>Chasing Algorithm For the chasing task, the class implements a chasing algorithm where the Tom robot continuously updates its position and yaw (orientation) and uses this information to calculate the distance and angle to the Jerry robot (the target). The <code>compute_angle_and_distance</code> method calculates these values, and Tom adjusts its velocity accordingly. If Tom is close enough to Jerry, it stops, simulating the capture of Jerry. This approach uses basic distance and angle calculations, ensuring that Tom efficiently chases Jerry while keeping its movement smooth by adjusting both linear and angular velocities.  </p> <p>Object Detection For object detection, the robot uses both camera and LIDAR data to detect and approach objects of a specific color (like blocks - in red, yellow, blue, and green). The <code>detect_block</code> method processes images using OpenCV to find color-specific objects by segmenting the image in the HSV color space. Once an object is detected, the robot uses its LIDAR data to avoid obstacles while moving towards the object. The <code>smooth_center</code> method smooths the position of the detected block to reduce erratic movements, and the robot adjusts its speed and direction based on the block\u2019s position relative to the camera\u2019s center. If obstacles are detected via LIDAR, the robot halts or adjusts its path to avoid collisions. This combination of object detection and obstacle avoidance creates a robust system for navigating towards a target while ensuring safety.  </p>"},{"location":"reports/2024/tom_and_jerry/#guide-on-how-to-use-the-code-written","title":"Guide on How to Use the Code Written","text":"<p>RUN THE CODE (ASSUMES SETUP IS COMPLETE) </p> <ol> <li><code>rosrun tom_and_jerry tom_odom.py</code> </li> <li><code>rosrun tom_and_jerry jerry_odom.py</code> </li> <li><code>rosrun tom_and_jerry tom_chases_jerry.py</code> </li> </ol> <p>SETUP - HOW TO ADD 2 ROBOTS ON THE SAME ROSCORE (IN REAL) </p> <ul> <li> <p>Access the VNC main window:   Open the VNC main window for the simulation environment (<code>sim:sim</code>).  </p> </li> <li> <p>Edit the <code>.bashrc</code> file: </p> </li> <li>In the terminal, type <code>nano .bashrc</code> to open the <code>.bashrc</code> file.  </li> <li>Add the master robot VPN (e.g., bran2) in the real section: <code># $(bru name bran2 -m 100.100.231.32)</code> </li> <li> <p>Save and exit the file (press <code>Ctrl+X</code>, then <code>Y</code>, and <code>Enter</code> to save).  </p> </li> <li> <p>SSH into the Master Robot (bran2): </p> </li> <li>In a new terminal, SSH into the master robot: <code>ssh ubuntu@100.100.231.32</code> </li> <li> <p>Launch the ROS nodes: <code>roslaunch sodacan bu.launch</code> </p> </li> <li> <p>SSH into the follower Robot (rafael): </p> </li> <li>In another terminal, SSH into the follower robot: <code>ssh ubuntu@rafael_vpn</code> </li> <li>Update the <code>.bashrc</code> file inside rafael to reflect the master robot as bran2 in 2 places: <pre><code>ROS_MASTER_URI=http://{vpn of bran2}:11311\nBru name with the bran2 vpn\n</code></pre></li> <li> <p>Launch the necessary ROS nodes: <code>roslaunch turtlebot3_bringup turtlebot3_multirobot.launch</code> </p> </li> <li> <p>Check ROS Topics: </p> </li> <li>In another terminal, run: <code>rostopic list</code> </li> <li>Check the list of active topics to ensure you see both:  <ul> <li><code>/rafael/cmd_vel</code> </li> <li><code>cmd_vel for bran2</code> </li> </ul> </li> </ul> <p>OTHER DEBUGGING TIPS </p> <ul> <li><code>printenv | grep ROS</code> </li> <li> <p>Outputs: <code>ROS_MASTER_URI=http://{vpn of robot}:11311</code> </p> </li> <li> <p>To make a Python script executable in ROS: <code>chmod +x &lt;script_name&gt;.py</code> </p> </li> </ul>"},{"location":"reports/2024/tom_and_jerry/#clear-description-and-tables-of-source-files-nodes-messages-actions-and-so-on","title":"Clear description and tables of source files, nodes, messages, actions and so on","text":"<p>1. Overview of Source Files | File Name                  | Description                                                                                         | |--------------------------------|---------------------------------------------------------------------------------------------------------| | <code>interface.py</code>                 | Flask-based web interface allowing users to interact with the system by selecting cheese colors and viewing game results. | | <code>jerry_odom.py</code>                | Handles Jerry\u2019s odometry, subscribes to <code>/odom</code>, and publishes processed odometry to <code>/jerry_odom</code>.     | | <code>jerry_robot.py</code>               | Implements Jerry's behavior: navigation, LIDAR-based obstacle avoidance, and block detection using image processing. | | <code>object_detection.py</code>          | Detects colored blocks using a camera feed, and provides movement commands to approach the detected block. | | <code>tom_odom.py</code>                  | Similar to <code>jerry_odom.py</code>, but for Tom. Subscribes to <code>/rafael/odom</code> and publishes data to <code>/tom_odom</code>. | | <code>tom_robot.py</code>                 | Implements Tom's behavior: chasing Jerry using positional data and basic pathfinding.                  | | <code>tom_robot_w_obstacle_avoidance.py</code> | Enhances Tom's chasing behavior with LIDAR-based obstacle avoidance.                                      |</p> <p>2. Nodes and Topics</p> Node Name File Published Topics Subscribed Topics Description <code>/interface</code> <code>interface.py</code> N/A N/A Flask server for web-based game interaction. <code>/jerry_odom</code> <code>jerry_odom.py</code> <code>/jerry_odom</code> <code>/odom</code> Processes and publishes Jerry's odometry data. <code>/jerry_robot</code> <code>jerry_robot.py</code> <code>/cmd_vel</code>, <code>/bounding_box</code> <code>/jerry_odom</code>, <code>/scan</code> Implements Jerry's navigation and block targeting behavior. <code>/object_detection</code> <code>object_detection.py</code> <code>/bounding_box</code>, <code>/cmd_vel</code> <code>/scan</code>, <code>/cv_camera/image_raw</code> Detects colored blocks and moves the robot toward them. <code>/tom_odom</code> <code>tom_odom.py</code> <code>/tom_odom</code> <code>/rafael/odom</code> Processes and publishes Tom's odometry data. <code>/tom_robot</code> <code>tom_robot.py</code> <code>/rafael/cmd_vel</code> <code>/tom_odom</code>, <code>/jerry_odom</code> Controls Tom's motion to chase Jerry. <code>/tom_chaser</code> <code>tom_robot_w_obstacle_avoidance.py</code> <code>/rafael/cmd_vel</code> <code>/rafael/scan</code> Adds obstacle avoidance to Tom\u2019s behavior. <p>3. Messages and Topics</p> Topic Type Publisher Subscriber Purpose <code>/cmd_vel</code> <code>geometry_msgs/Twist</code> <code>jerry_robot.py</code>, <code>object_detection.py</code> N/A Controls Jerry's movement. <code>/rafael/cmd_vel</code> <code>geometry_msgs/Twist</code> <code>tom_robot.py</code>, <code>tom_robot_w_obstacle_avoidance.py</code> N/A Controls Tom's movement. <code>/jerry_odom</code> <code>geometry_msgs/Point</code> <code>jerry_odom.py</code> <code>jerry_robot.py</code>, <code>tom_robot.py</code> Provides Jerry\u2019s processed odometry. <code>/tom_odom</code> <code>geometry_msgs/Point</code> <code>tom_odom.py</code> <code>tom_robot.py</code> Provides Tom\u2019s processed odometry. <code>/bounding_box</code> <code>geometry_msgs/Point</code> <code>object_detection.py</code>, <code>jerry_robot.py</code> N/A Publishes the bounding box center of detected blocks. <code>/scan</code> <code>sensor_msgs/LaserScan</code> ROS LIDAR hardware <code>jerry_robot.py</code>, <code>tom_robot_w_obstacle_avoidance.py</code> Provides LIDAR data for obstacle detection. <code>/cv_camera/image_raw</code> <code>sensor_msgs/Image</code> ROS Camera hardware <code>object_detection.py</code> Provides images for detecting colored blocks. <p>4. Actions Performed</p> Action Initiated By Details Navigate to Goal <code>jerry_robot.py</code> Jerry moves toward a specified goal while avoiding obstacles. Detect Colored Block <code>object_detection.py</code> Detects blocks of a specific color using HSV filtering and moves toward them. Chase Jerry <code>tom_robot.py</code>, <code>tom_robot_w_obstacle_avoidance.py</code> Tom chases Jerry based on positional data. Obstacle Avoidance <code>jerry_robot.py</code>, <code>tom_robot_w_obstacle_avoidance.py</code> Detects obstacles using LIDAR and avoids them by adjusting path. Web Interaction <code>interface.py</code> Users interact via a web interface to select cheese colors and play the game."},{"location":"reports/2024/tom_and_jerry/#story-of-the-project","title":"Story of the Project","text":"<p>How It Unfolded, How the Team Worked Together </p> <p>Our project involved two robots, and we decided to focus on one robot first. There were two major components to this robot's design: the first was navigating through a series of obstacles, and the second was using the camera to detect colored blocks and head towards the block with the chosen color. We split these tasks, with one of us working on navigating through the obstacles and reaching a goal coordinate, while the other worked on detecting the colored blocks and moving towards a specific colored block.</p> <p>Once both parts were functional, we focused on writing the chasing code for the second robot and combining the two components for the first robot. The next step was implementing the chasing functionality, which turned out to be quite challenging. As a result, both of us worked together on the chasing algorithm. After much trial and error, and with help from TAs, classmates, and the professor, we finally developed a working chasing algorithm that relied on the odometry of both robots.</p> <p>For the final run, we needed to integrate the obstacle avoidance code with the chasing robot and test the entire race. This also presented challenges, especially when deciding when the robot should switch states\u2014specifically, when to transition from \u201cobstacle avoidance\u201d mode to \u201cfind colored block\u201d mode.</p>"},{"location":"reports/2024/tom_and_jerry/#problems-that-were-solved-pivots-that-had-to-be-taken","title":"Problems That Were Solved, Pivots That Had to Be Taken","text":"<p>Inaccurate Odom Our target navigation code works by calculating the vector between the robot\u2019s current position, and the goal coordinate (in the robot\u2019s frame). This requires the odom to accurately reflect the robot\u2019s position from its origin, at every iteration of the main loop. Our initial plan was to use both branbots for Tom and Jerry. However, the odom on the branbots were quite inaccurate. When the robot\u2019s odom thought it had reached the goal coordinate, it was actually around 10 to 20 degrees off. This problem worsened the farther the branbots had to travel.  </p> <p>This problem worsened the farther the branbots had to travel.  </p> <p>We figured out why this was happening. The odom on branbots used the IMU as part of a sensor fusion algorithm, and the data returned by the IMU is imperfect (possibly due to an issue with the magnetometer) which causes the odom to be inaccurate. Turtlebots didn\u2019t have this issue, and had a much more accurate odom (still not perfect). However, we ultimately realized that it\u2019s not that much of a big issue if the branbot is off from the goal coordinate. As long as it stops before the row of colored blocks, the block detection code will take over and lead the robot to the correct block.  </p> <p>Multiple Robots on the Same ROSCORE Since the Tom robot needs to chase the Jerry robot, it means that both robots have to be on the same ROSCORE and share their odom/locations for the chasing code to work. Initially, we tried to follow some of the guides we found in the lab notebook. When those didn\u2019t work we collaborated with the TAs, professor, and classmates who were also using multiple robots. Ultimately, we got the Rafael turtlebot to act as an auxiliary robot which connects to a \u201ccore\u201d robot that runs the ROSCORE.  </p> <p>Chasing After getting multiple robots on the same ROSCORE, it was now time to have Tom chase Jerry. First, we decided to follow the \u201cDouble Turtle Follow\u201d homework and have both Tom and Jerry on the same TF tree where they shared a parent node. This would allow us to calculate a transform between their frames and Tom would follow that transform to chase Jerry.  </p> <p>However, this approach didn\u2019t work out. This is likely because the 2 different odoms are on 2 different TF trees and cannot be combined. So, we decided to do it manually. Instead of using the <code>tf2_ros</code> library, we get the 2 different odom coordinates, assume an initial offset between the 2 robots, and use that information to calculate the vector between Tom and Jerry. After some tweaking, this approach worked well enough for us.  </p> <p>Pivoting Away from BranArm Our initial plan was to have the BranArm robot (i.e., the robot with the claw) be the Jerry robot. The Jerry robot would use the claw to pick up a colored block and carry it to the end of the obstacle course. However, there were many complications with the BranArm bot and we didn\u2019t have enough time to solve all the problems. So, we had to use a branbot/turtlebot for Jerry.  </p>"},{"location":"reports/2024/tom_and_jerry/#your-own-assessment","title":"Your Own Assessment","text":"<p>Our project demonstrated the complexities and excitement of multi-robot interactions. While we achieved a functional system with Jerry navigating obstacles and Tom effectively chasing, there were several takeaways:  </p> <ul> <li>The importance of modular design: Breaking tasks into smaller, manageable components expedited debugging and integration.  </li> <li>Adapting to constraints: Switching hardware and rewriting algorithms were crucial pivots that kept the project on track.  </li> <li>Collaboration: By dividing responsibilities initially and converging efforts during challenges, we maximized productivity.  </li> <li>End-to-End Debugging: Given how many systems and modules (both hardware and software) could and did go wrong, we learned how to debug thoroughly, look for bugs, and narrow down seemingly vague problems.  </li> </ul> <p>This project not only enhanced our technical skills in ROS, LIDAR, and computer vision but also reinforced the value of perseverance and teamwork in robotics development.  </p>"}]}